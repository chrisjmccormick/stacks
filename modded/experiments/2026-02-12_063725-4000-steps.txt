import os
import sys

# Read the current file and the kernels file code ASAP, for logging
with open(sys.argv[0], 'r') as f:
    code = f.read()
with open(os.path.join(os.path.dirname(sys.argv[0]), 'triton_kernels.py'), 'r') as f:
    code += f"\n\n{'-'*40}\n# triton_kernels.py\n{'-'*40}\n\n"
    code += f.read()

import copy
import glob
import json
import math
import random
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate, pairwise
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch
import triton

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
from kernels import get_kernel
from torch import Tensor, nn

from triton_kernels import XXT, ba_plus_cAA, FusedLinearReLUSquareFunction, FusedSoftcappedCrossEntropy

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Distributed training setup
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
grad_scale = 1 / grad_accum_steps # consistent grad magnitudes between different num_devices
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng
# Transposed layout by @ChrisJMcCormick allows for faster gradient accumulation.

@torch.library.custom_op("nanogpt::mm_t", mutates_args=())
def mm_t_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    """Computes y = x @ w with F8 weights stored as (in_features, out_features)."""
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        assert x.shape[1] == w.shape[0]  # x: (batch, in), w: (in, out)

        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)

        # _scaled_mm requires column-major B. w_f8 is row-major (in, out).
        # .T.contiguous().T creates a column-major view without changing logical shape.
        w_f8_col_major = w_f8.T.contiguous().T

        out = torch._scaled_mm(
            x_f8,
            w_f8_col_major,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_t_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[0]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_t_backward", mutates_args=())
def mm_t_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()

        x_scale = grad.new_tensor(x_s, dtype=torch.float32)
        w_scale = grad.new_tensor(w_s, dtype=torch.float32)
        grad_scale = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)

        # grad_x = grad @ w.T
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=grad_scale,
            scale_b=w_scale,
            use_fast_accum=False,
        )

        # grad_w = x.T @ grad
        # Result is (in, out), naturally matching weight storage. No final .T needed.
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_scale,
            scale_b=grad_scale,
            use_fast_accum=False,
        )

        return grad_x, grad_w

    grad_x, grad_w = impl(g, x_f8, w_f8)

    return grad_x, grad_w

@mm_t_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward_t(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_t_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context_t(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_t_op.register_autograd(backward_t, setup_context=setup_context_t)

# -----------------------------------------------------------------------------
# Polar Express

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Combined NorMuon + Adam Optimizer

@dataclass
class ParamConfig:
    """Per-parameter configuration for NorMuonAndAdam optimizer."""
    label: str
    optim: str  # "adam" or "normuon"
    comms: str  # "none", "replicated", or "sharded"
    adam_betas: tuple[float, float] | None
    lr_mul: float
    wd_mul: float
    lr: float
    initial_lr: float
    weight_decay: float
    # Adam-specific
    eps: float | None = None
    # NorMuon-specific
    reshape: tuple | None = None
    chunk_size: int | None = None
    momentum: float | None = None
    beta2: float | None = None
    per_matrix_lr_mul: list[float] | None = None


class NorMuonAndAdam:
    """
    Combined optimizer that handles both NorMuon (for projection matrices) and
    Adam (for embeddings/scalars/gate weights).

    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, Muon uses a Newton-Schulz iteration (replaced
    here with Polar Express), which has the advantage that it can be stably run in bfloat16 on the GPU.

    Muon is applied only to the projection matrices in the attention and MLP layers, and is not recommended
    for embeddings, scalars, or individual weight vectors (e.g., bias terms or gate weights).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - Cautious weight decay, a gated version of decoupled weight decay
    - Mantissa tracking for precision

    Adam (for embeddings/scalars/gates):
    - Standard Adam with bias correction
    - Cautious weight decay

    Configuration:
    Unlike torch.optim.Optimizer, this class uses per-parameter configs from a `param_table` dict
    and does not include parameter "groups". All parameters require a .label attribute, and a
    corresponding entry in the param_table to specify their hyperparameters (lr_mul, wd_mul, adam_betas, etc.).

    Communication and ordering:
    Gradient communication is explicitly scheduled rather than hook-driven.
    Reductions are launched in `scatter_order`, while update math and final
    gathers are executed in `work_order`. These orders are independent and
    must each contain every parameter label exactly once.

    Two communication modes are supported per parameter:
    - 'replicated': Gradients are all-reduced and each rank computes the full update.
    - 'sharded': Gradients are reduce-scattered, each rank updates its shard,
      and results are all-gathered.

    Adam parameters may be freely sharded. NorMuon operates on full matrices; sharding is
    supported by grouping matrices into parameter banks. NorMuon parameters must have a
    `.reshape` attribute that reshapes the bank so that the leading dimension is divisible
    by world_size.

    # Contributors include @YouJiacheng, @KonstantinWilleke, @alexrgilbert, @adricarda,
    # @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
    """
    def __init__(self, named_params, param_table: dict, scatter_order: list, work_order: list,
                 adam_defaults: dict, normuon_defaults: dict):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1

        # Store defaults for each optimizer type
        self.adam_defaults = adam_defaults
        self.normuon_defaults = normuon_defaults
        self.param_table = param_table
        self.scatter_order = scatter_order
        self.work_order = work_order

        # Collect params by label and build config
        self.param_cfgs: dict[nn.Parameter, ParamConfig] = {}
        self.param_states: dict[nn.Parameter, dict] = {}
        self._param_by_label: dict[str, nn.Parameter] = {}
        for name, param in named_params:
            label = getattr(param, "label", None)
            assert label is not None and label in param_table  # all params must have valid label
            assert label not in self._param_by_label  # exactly one param per label
            self._param_by_label[label] = param
            self._build_param_cfg(param, label)

        # Assert scatter_order and work_order match present labels exactly
        present = set(self._param_by_label.keys())
        assert set(scatter_order) == present and set(work_order) == present

        # Handle world_size=1: overwrite comms to "none"
        if self.world_size == 1:
            for p_cfg in self.param_cfgs.values():
                p_cfg.comms = "none"

        # Initialize state for all params
        self._init_state()

        # 0-D CPU tensors to avoid recompilation
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_lr_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")

        # Track async operations
        self._reduce_futures: dict[nn.Parameter, tuple] = {}

        # Embed/lm_head tying state
        self.split_embed = False
        self._lm_head_param = self._param_by_label.get("lm_head")
        self._embed_param = self._param_by_label.get("embed")

    def _build_param_cfg(self, param: nn.Parameter, label: str):
        """Build config for a single parameter from param_table."""
        table_entry = self.param_table[label]
        optim = table_entry["optim"]
        comms = table_entry["comms"]
        adam_betas = table_entry.get("adam_betas")
        lr_mul = table_entry.get("lr_mul", 1.0)
        wd_mul = table_entry.get("wd_mul", 1.0)

        if optim == "adam":
            chunk_size = param.shape[0] // self.world_size if comms == "sharded" else None
            p_cfg = ParamConfig(
                label=label,
                optim=optim,
                comms=comms,
                adam_betas=tuple(adam_betas) if adam_betas else None,
                lr_mul=lr_mul,
                wd_mul=wd_mul,
                lr=self.adam_defaults["lr"],
                initial_lr=self.adam_defaults["lr"],
                weight_decay=self.adam_defaults["weight_decay"],
                eps=self.adam_defaults["eps"],
                chunk_size=chunk_size,
            )
        elif optim == "normuon":
            reshape = getattr(param, "reshape", None)
            if reshape is None:
                raise ValueError(f"NorMuon param {label} must have .reshape attribute")
            if reshape[0] % self.world_size != 0:
                raise ValueError(f"reshape[0]={reshape[0]} must be divisible by world_size")

            chunk_size = reshape[0] // self.world_size
            chunk_shape = (chunk_size, *reshape[1:])
            # Shape-based LR multiplier for NorMuon
            shape_mult = max(1.0, chunk_shape[-2] / chunk_shape[-1]) ** 0.5 if len(chunk_shape) >= 2 else 1.0
            lr_mul = shape_mult * lr_mul

            # Per-matrix LR multipliers for MLP c_proj (2x LR on odd indices)
            per_matrix_lr_mul = None
            if label == "mlp":
                rank = dist.get_rank() if dist.is_initialized() else 0
                start_idx = rank * chunk_size
                per_matrix_lr_mul = []
                for i in range(chunk_size):
                    global_idx = start_idx + i
                    is_c_proj = (global_idx % 2 == 1)
                    per_matrix_lr_mul.append(2.0 if is_c_proj else 1.0)

            p_cfg = ParamConfig(
                label=label,
                optim=optim,
                comms=comms,
                adam_betas=tuple(adam_betas) if adam_betas else None,
                lr_mul=lr_mul,
                wd_mul=wd_mul,
                lr=self.normuon_defaults["lr"],
                initial_lr=self.normuon_defaults["lr"],
                weight_decay=self.normuon_defaults["weight_decay"],
                reshape=reshape,
                chunk_size=chunk_size,
                momentum=self.normuon_defaults["momentum"],
                beta2=self.normuon_defaults["beta2"],
                per_matrix_lr_mul=per_matrix_lr_mul,
            )
        else:
            raise ValueError(f"Unknown optim type: {optim}")

        self.param_cfgs[param] = p_cfg

    def _init_state(self):
        """Initialize optimizer state for all parameters."""
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "adam":
                # Sharded params use chunk state, replicated use full state
                if p_cfg.comms == "sharded":
                    chunk = param[:p_cfg.chunk_size]
                else:
                    chunk = param
                exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=param.device)
                self.param_states[param] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

            elif p_cfg.optim == "normuon":
                chunk_shape = (p_cfg.chunk_size, *p_cfg.reshape[1:])

                # Momentum buffer (FP32 for precision)
                momentum_buffer = torch.zeros(
                    chunk_shape, dtype=torch.float32, device=param.device
                )

                # Second momentum buffer - reduced along one dimension
                if chunk_shape[-2] >= chunk_shape[-1]:
                    second_mom_shape = (*chunk_shape[:-1], 1)
                else:
                    second_mom_shape = (*chunk_shape[:-2], 1, chunk_shape[-1])
                second_momentum_buffer = torch.zeros(
                    second_mom_shape, dtype=torch.float32, device=param.device
                )

                # Mantissa buffer for precision tracking
                mantissa = torch.zeros(
                    chunk_shape, dtype=torch.uint16, device=param.device
                )

                self.param_states[param] = dict(
                    momentum_buffer=momentum_buffer,
                    second_momentum_buffer=second_momentum_buffer,
                    mantissa=mantissa,
                )

    # -----------------------------------
    # Reduce/Gather operations

    def _launch_reduce(self, param: nn.Parameter, grad: Tensor):
        """Launch async reduce for a parameter based on its comms policy."""
        p_cfg = self.param_cfgs[param]

        if p_cfg.comms == "none":
            if p_cfg.optim == "normuon":
                # NorMuon needs reshaped gradient even without communication
                grad = grad.view(p_cfg.reshape)
            self._reduce_futures[param] = (None, grad)
        elif p_cfg.comms == "replicated":
            future = dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future()
            self._reduce_futures[param] = (future, grad)
        elif p_cfg.comms == "sharded":
            if p_cfg.optim == "normuon":
                # NorMuon: reshape before reduce_scatter
                grad_reshaped = grad.view(p_cfg.reshape)
                grad_chunk = torch.empty(
                    (p_cfg.chunk_size, *grad_reshaped.shape[1:]),
                    dtype=grad.dtype,
                    device=grad.device
                )
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad_reshaped.contiguous(), op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)
            else:
                # Adam: simple reduce_scatter
                grad_chunk = torch.empty_like(grad[:p_cfg.chunk_size])
                future = dist.reduce_scatter_tensor(
                    grad_chunk, grad, op=dist.ReduceOp.AVG, async_op=True
                ).get_future()
                self._reduce_futures[param] = (future, grad_chunk)

    def _launch_gather(self, param: nn.Parameter, p_slice: Tensor) -> "torch.futures.Future":
        """Launch async all_gather for a sharded parameter."""
        p_cfg = self.param_cfgs[param]
        if p_cfg.optim == "normuon":
            full_param = param.data.view(p_cfg.reshape)
            assert full_param.is_contiguous()
            return dist.all_gather_into_tensor(
                full_param, p_slice.contiguous(), async_op=True
            ).get_future()
        else:
            return dist.all_gather_into_tensor(
                param, p_slice.contiguous(), async_op=True
            ).get_future()

    # -----------------------------------
    # State management

    def reset(self):
        """Reset NorMuon momentum buffers and split_embed state (called on training reset)."""
        self.split_embed = False
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "normuon":
                p_state = self.param_states[param]
                p_state["momentum_buffer"].zero_()
                p_state["mantissa"].zero_()
                p_state["second_momentum_buffer"].zero_()

    def copy_lm_state_to_embed(self):
        """
        Copy the optimizer state from the lm_head to the embed at the untie point.
        This requires an all-gather + reshard because of different sharding:
        - lm_head (768, 50304) is sharded to (96, 50304) per rank (along model_dim)
        - embed (50304, 768) is sharded to (6288, 768) per rank (along vocab_size)

        We all-gather the lm_head momentum, transpose it, then each rank takes their
        embed shard to get the correct momentum state.
        """
        lm_head = self._lm_head_param
        embed = self._embed_param
        lm_state = self.param_states[lm_head]
        embed_state = self.param_states[embed]
        lm_cfg = self.param_cfgs[lm_head]
        embed_cfg = self.param_cfgs[embed]

        embed_state['step'] = lm_state['step'] # Preserve step count for bias correction

        # Copy optimizer state with all-gather + transpose + reshard
        if self.world_size > 1:
            rank = dist.get_rank()
            lm_chunk_size = lm_cfg.chunk_size  # 96
            embed_chunk_size = embed_cfg.chunk_size  # 6288

            # All-gather lm_head momentum to get full (768, 50304) tensor
            for key in ["exp_avg", "exp_avg_sq"]:
                lm_chunk = lm_state[key]  # (96, 50304)
                full_lm = torch.empty(lm_head.shape[0], lm_head.shape[1], dtype=lm_chunk.dtype, device=lm_chunk.device)
                dist.all_gather_into_tensor(full_lm, lm_chunk.contiguous())
                embed_state[key].copy_(full_lm.T[rank * embed_chunk_size:(rank + 1) * embed_chunk_size])
        else:
            # Single GPU: simple transpose
            for key in ["exp_avg", "exp_avg_sq"]:
                embed_state[key].copy_(lm_state[key].T)

        # Mark as split
        self.split_embed = True

    def state_dict(self):
        """Return the optimizer state as a dict."""
        return {
            "param_states": {id(p): s for p, s in self.param_states.items()},
            "param_cfgs": {id(p): s for p, s in self.param_cfgs.items()},
        }

    def load_state_dict(self, state_dict):
        """Load optimizer state from a dict."""
        # Build id->param mapping
        id_to_param = {id(p): p for p in self.param_cfgs.keys()}

        # Load state, preserving dtypes
        for param_id, saved_p_state in state_dict["param_states"].items():
            if param_id in id_to_param:
                param = id_to_param[param_id]
                p_state = self.param_states[param]
                for k, v in saved_p_state.items():
                    if isinstance(v, torch.Tensor) and k in p_state:
                        target_dtype = p_state[k].dtype
                        p_state[k] = v.to(dtype=target_dtype, device=p_state[k].device)
                    else:
                        p_state[k] = v

    # -----------------------------------
    # Unified optimizer step with explicit ordering

    @torch.no_grad()
    def step(self, do_adam: bool = True):
        """
        Combined optimizer step with explicit ordering.

        Args:
            do_adam: If True, update Adam params. NorMuon params always updated.

        Flow:
        1. Scatter phase: Launch reduces in scatter_order
        2. Work phase: Process updates in work_order
           - Wait for reduce, compute update, launch gather
        3. Finalize phase: Wait for gathers

        While the embeddings are tied:
        - Comms and update math are only done on lm_head.
        - We add embed.grad.T into lm_head.grad before comms.
        - After lm_head gather, we copy lm_head.data.T --> embed.data
        """
        rank = dist.get_rank() if dist.is_initialized() else 0
        lm_param, embed_param = self._lm_head_param, self._embed_param

        # ===== Phase 1: Launch reduces in scatter_order =====
        for label in self.scatter_order:
            param = self._param_by_label[label]
            p_cfg = self.param_cfgs[param]

            if p_cfg.optim == "adam" and not do_adam:
                continue
            if param.grad is None:
                continue

            # lm_head when tied: aggregate embed.grad.T (transposed shapes)
            if label == "lm_head" and do_adam and not self.split_embed:
                if embed_param is not None and embed_param.grad is not None:
                    param.grad.add_(embed_param.grad.T)

            # Skip embed when tied (copied from lm_head after gather)
            if label == "embed" and not self.split_embed:
                continue

            self._launch_reduce(param, param.grad)

        # ===== Phase 2: Process updates in work_order =====
        gather_futures = []
        lm_head_gather_future = None

        for label in self.work_order:
            param = self._param_by_label[label]
            if param not in self._reduce_futures:
                continue

            p_cfg = self.param_cfgs[param]
            if p_cfg.optim == "adam" and not do_adam:
                continue
            # Wait for reduce
            future, grad_chunk = self._reduce_futures[param]
            if future is not None:
                future.wait()
            # Apply update based on optim type
            if p_cfg.optim == "adam":
                p_slice = self._adam_update(param, grad_chunk, p_cfg, rank)
            else:
                p_slice = self._normuon_update(param, grad_chunk, p_cfg, rank)
            # Launch gather for sharded params
            if p_cfg.comms == "sharded" and self.world_size > 1:
                gather_fut = self._launch_gather(param, p_slice)
                if label == "lm_head":
                    lm_head_gather_future = gather_fut
                else:
                    gather_futures.append(gather_fut)

        # ===== Phase 3: Wait for gathers, sync embed if tied =====
        # Wait for lm_head gather first so we can copy to embed while other gathers complete
        if lm_head_gather_future is not None:
            lm_head_gather_future.wait()

        # When tied: copy lm_head.T to embed
        if do_adam and not self.split_embed and embed_param is not None and lm_param is not None:
            embed_param.data.copy_(lm_param.data.T)

        # Wait for remaining gathers
        for fut in gather_futures:
            fut.wait()

        self._reduce_futures.clear()

        # Clear grads for updated params
        for param, p_cfg in self.param_cfgs.items():
            if p_cfg.optim == "adam" and not do_adam:
                continue  # Don't clear Adam grads on even steps
            param.grad = None

    # -----------------------------------
    # Adam update

    def _adam_update(self, param: nn.Parameter, grad_chunk: Tensor, p_cfg: ParamConfig, rank: int) -> Tensor:
        """Apply Adam update to a parameter. Returns the updated p_slice."""
        beta1, beta2 = p_cfg.adam_betas
        lr = p_cfg.lr * p_cfg.lr_mul

        # Get parameter slice
        if p_cfg.comms == "sharded":
            p_slice = param[rank * p_cfg.chunk_size:(rank + 1) * p_cfg.chunk_size]
        else:
            p_slice = param

        p_state = self.param_states[param]
        p_state["step"] += 1
        t = p_state["step"]

        bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
        self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
        self._eff_wd_t.fill_(lr * lr * p_cfg.weight_decay * p_cfg.wd_mul)

        NorMuonAndAdam._adam_update_step(
            p_slice, grad_chunk, p_state["exp_avg"], p_state["exp_avg_sq"],
            beta1, beta2, p_cfg.eps, self._step_size_t, self._eff_wd_t
        )

        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _adam_update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)
        # Cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)
        p_slice.add_(other=update, alpha=-1.0)

    # -----------------------------------
    # NorMuon update

    def _normuon_update(self, param: nn.Parameter, grad_chunk: Tensor, p_cfg: ParamConfig, rank: int) -> Tensor:
        """Apply NorMuon update to a parameter. Returns the updated p_slice."""
        chunk_shape = grad_chunk.shape

        p_state = self.param_states[param]
        grad_chunk = grad_chunk.float()  # FP32 for momentum

        # Momentum update
        momentum_buffer = p_state["momentum_buffer"]
        momentum_buffer.lerp_(grad_chunk, 1 - p_cfg.momentum)
        updated_grads = grad_chunk.lerp_(momentum_buffer, p_cfg.momentum)

        self._eff_lr_t.fill_(p_cfg.lr_mul * p_cfg.lr)
        self._eff_wd_t.fill_(p_cfg.wd_mul * p_cfg.weight_decay * p_cfg.lr)

        # Polar Express orthogonalization
        is_large_matrix = chunk_shape[-2] > 1024
        v_chunk = polar_express(updated_grads, split_baddbmm=is_large_matrix)

        # Variance reduction
        red_dim = -1 if chunk_shape[-2] >= chunk_shape[-1] else -2
        v_chunk = NorMuonAndAdam._apply_normuon_variance_reduction(
            v_chunk, p_state["second_momentum_buffer"], p_cfg.beta2, red_dim
        )

        # Update parameter, in place, with cautious weight decay
        param_view = param.data.view(p_cfg.reshape)
        p_slice = param_view[rank * p_cfg.chunk_size:(rank + 1) * p_cfg.chunk_size]

        # MLP has per-matrix LR multipliers (c_proj gets 2x LR)
        if p_cfg.per_matrix_lr_mul is not None:
            for mat_idx in range(p_cfg.chunk_size):
                self._eff_lr_t.fill_(p_cfg.lr_mul * p_cfg.per_matrix_lr_mul[mat_idx] * p_cfg.lr)
                self._eff_wd_t.fill_(p_cfg.wd_mul * p_cfg.weight_decay * p_cfg.lr)
                NorMuonAndAdam._cautious_wd_and_update_inplace(
                    p_slice[mat_idx].view(torch.uint16), p_state["mantissa"][mat_idx], v_chunk[mat_idx],
                    self._eff_wd_t, self._eff_lr_t
                )
        else:
            NorMuonAndAdam._cautious_wd_and_update_inplace(
                p_slice.view(torch.uint16), p_state["mantissa"], v_chunk,
                self._eff_wd_t, self._eff_lr_t
            )

        return p_slice

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
        """
        Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
        Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
        bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
        float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
        """
        assert p.dtype == mantissa.dtype == torch.uint16
        grad = grad.float()
        wd_factor = wd_tensor.to(torch.float32)
        lr_factor = lr_tensor.to(torch.float32)
        p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
        p_precise = p_precise_raw.view(torch.float32)
        mask = (grad * p_precise) >= 0
        p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
        p.copy_((p_precise_raw >> 16).to(torch.uint16))
        mantissa.copy_(p_precise_raw.to(torch.uint16))

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
        """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
        v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
        red_dim_size = v_chunk.size(red_dim)
        v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
        v_norm = v_norm_sq.sqrt_()
        second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
        step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
        scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
        v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
        final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
        return v_chunk.mul_(final_scale.type_as(v_chunk))

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinearT(nn.Module):
    """
    Linear layer with transposed weight storage (in_features, out_features) which
    addresses the slow kernel that was used for gradient accumulation. @chrisjmccormick
    """
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

        self.weight = nn.Parameter(torch.empty(in_features, out_features, dtype=torch.bfloat16))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        with torch.no_grad():
            nn.init.zeros_(self.weight) # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out = torch.ops.nanogpt.mm_t(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return x @ self.weight.type_as(x)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD, offset=0):
        """Apply rotary embeddings. offset is the starting position for KV cache inference."""
        T = x_BTHD.size(-3)
        assert self.factor1.size(0) >= offset + T, f"Rotary cache too small: {self.factor1.size(0)} < {offset + T}"
        factor1, factor2 = (
            self.factor1[None, offset:offset + T, None, :],
            self.factor2[None, offset:offset + T, None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)

        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )

        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)

        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class GPTConfig:
    """Config object expected by engine.py for inference."""
    n_head: int = 6
    n_kv_head: int = 6  # Same as n_head (no GQA in this model)
    n_embd: int = 768
    n_layer: int = 11
    sequence_len: int = 2048

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

# This works on ARM if you choose pytorch for 12.8 instead of 12.6
flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim
        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        max_len = args.train_max_seq_len

        q, k = norm(q), norm(k) # QK norm @Grad62304977

        q, k = yarn.rotary(q), yarn.rotary(k)

        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 2:] = k[:, :-1, :, self.head_dim // 2:]

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

    def forward_inference(self, x: Tensor, qkvo_w: Tensor, yarn: "Yarn", ve: Tensor, 
                          ve_gate_w: Tensor, attn_gate_w: Tensor, sa_lambdas: Tensor, 
                          window_size: int, kv_cache, layer_idx: int):
        """Inference forward pass using flash_attn_with_kvcache."""
        B, T = x.size(0), x.size(1)
        
        # Compute Q, K, V projections
        q, k, v = F.linear(x, sa_lambdas[0] * qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm
        
        # Apply rotary embeddings with position offset from KV cache
        rotary_offset = kv_cache.get_pos()
        q = yarn.rotary(q, offset=rotary_offset)
        k = yarn.rotary(k, offset=rotary_offset)
        
        # Value embeddings
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v)
        
        # Flash Attention with KV cache
        k_cache, v_cache = kv_cache.get_layer_cache(layer_idx)
        y = flash_attn_interface.flash_attn_with_kvcache(
            q, k_cache, v_cache,
            k=k, v=v,
            cache_seqlens=kv_cache.cache_seqlens,
            causal=True,
            softmax_scale=yarn.attn_scale,
            window_size=(window_size, 0),
        )
        
        # Advance cache position after last layer
        if layer_idx == kv_cache.n_layers - 1:
            kv_cache.advance(T)
        
        # Attention gate - sparse gated attention for context-based no-op
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        
        # Output projection
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        # Weights are stored in parameter banks and passed via forward()

    def forward(self, x: Tensor, c_fc: Tensor, c_proj: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        # Fused triton kernel for relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, c_fc, c_proj)

    def forward_inference(self, x: Tensor, c_fc: Tensor, c_proj: Tensor):
        """Inference forward without fused triton kernel (more portable)."""
        x = F.linear(x, c_fc)
        x = F.relu(x).square()
        x = F.linear(x, c_proj)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, has_attn: bool, has_mlp: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if has_attn else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP() if has_mlp else None

    def forward(self, x: Tensor, attn_args: AttnArgs, qkvo_w: Tensor = None, c_fc: Tensor = None, c_proj: Tensor = None):
        """Training forward pass."""
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args, qkvo_w)
        if self.mlp is not None:
            x = x + self.mlp(norm(x), c_fc, c_proj)
        return x

    def forward_inference(self, x: Tensor, qkvo_w: Tensor, c_fc: Tensor, c_proj: Tensor,
                          yarn: "Yarn", ve: Tensor, ve_gate_w: Tensor, attn_gate_w: Tensor,
                          sa_lambdas: Tensor, window_size: int, kv_cache, layer_idx: int):
        """Inference forward pass with KV cache."""
        if self.attn is not None:
            x = x + self.attn.forward_inference(
                norm(x), qkvo_w, yarn, ve, ve_gate_w, attn_gate_w, sa_lambdas, 
                window_size, kv_cache, layer_idx
            )
        if self.mlp is not None:
            x = x + self.mlp.forward_inference(norm(x), c_fc, c_proj)
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        self.vocab_size = vocab_size
        # Config object for engine.py compatibility
        # TODO - Review this
        self.config = GPTConfig(
            n_head=num_heads,
            n_kv_head=num_heads,  # No GQA
            n_embd=model_dim,
            n_layer=num_layers,
            sequence_len=max_seq_len,
        )

        self.smear_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.smear_gate.weight)
        self.smear_gate.weight.label = 'smear_gate'

        self.skip_gate = nn.Linear(12, 1, bias=False)
        nn.init.zeros_(self.skip_gate.weight)
        self.skip_gate.weight.label = 'skip_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.Parameter(torch.zeros(5 * self.vocab_size, model_dim, dtype=torch.bfloat16))
        self.value_embeds.label = 'value_embed'

        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 unique gates
        self.ve_gate_bank.label = 've_gate_bank'

        # -----------------------------------
        # Parameter banks for sharded optimization, by @chrisjmccormick

        # Identify which layers have attention/MLP
        # Attention is skipped in layer 6 by @YouJiacheng
        self.attn_layer_indices = [i for i in range(num_layers) if i != 6]
        # All layers have MLP (At 11 layers--dropped first layer @EmelyanenkoK)
        self.mlp_layer_indices = list(range(num_layers))

        hdim = num_heads * head_dim
        mlp_hdim = 4 * model_dim

        # Create index mappings: layer_idx -> bank_idx
        self.layer_to_attn_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.attn_layer_indices)}
        self.layer_to_mlp_idx = {layer_idx: bank_idx for bank_idx, layer_idx in enumerate(self.mlp_layer_indices)}

        # Attention bank: stores QKVO weights for all attention layers
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        # Shape: (num_attn_layers, 4*model_dim, hdim) = (10, 3072, 768)
        # Reshape for sharding: (40, 768, 768) for even distribution across 8 GPUs
        self.attn_bank = nn.Parameter(torch.empty(len(self.attn_layer_indices), 4 * model_dim, hdim))
        self.attn_bank.label = 'attn'
        self.attn_bank.reshape = (len(self.attn_layer_indices) * 4, hdim, hdim)  # (40, 768, 768)

        # MLP bank: stores c_fc and c_proj for all MLP layers
        # Shape: (num_mlp_layers + padding, 2, mlp_hdim, model_dim) = (12, 2, 3072, 768)
        # We add 1 padding layer (index 11) to get 12*2=24 matrices for even distribution across 8 GPUs
        # Reshape for sharding: (24, 3072, 768)
        num_mlp_with_padding = len(self.mlp_layer_indices) + 1  # 11 + 1 = 12
        self.mlp_bank = nn.Parameter(torch.empty(num_mlp_with_padding, 2, mlp_hdim, model_dim))
        self.mlp_bank.label = 'mlp'
        self.mlp_bank.reshape = (num_mlp_with_padding * 2, mlp_hdim, model_dim)  # (24, 3072, 768)

        # improved init scale by @YouJiacheng and @srashedll
        std = 0.5 * model_dim ** -0.5
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.attn_bank.uniform_(-bound, bound)
            self.mlp_bank[:, 0, :, :].uniform_(-bound, bound)  # c_fc
            self.mlp_bank[:, 1, :, :].zero_()  # c_proj - zero init suggested by @Grad62304977

        # Create blocks with has_attn/has_mlp flags
        self.blocks = nn.ModuleList([
            Block(model_dim, head_dim, num_heads,
                  has_attn=(i in self.layer_to_attn_idx),
                  has_mlp=(i in self.layer_to_mlp_idx))
            for i in range(num_layers)
        ])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        # Transposed weight storage for faster gradient accumulation
        self.lm_head = CastedLinearT(model_dim, self.vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=grad_scale * 0.75/448)

        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(self.vocab_size, model_dim)
        self.embed.weight.label = 'embed'
        with torch.no_grad():
            self.embed.weight.copy_(self.lm_head.weight.T)

        self.bigram_embed = nn.Embedding(args.bigram_vocab_size, model_dim)
        self.bigram_embed.weight.label = 'bigram_embed'
        nn.init.zeros_(self.bigram_embed.weight)

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    0.1 * torch.ones(num_layers), # bigram lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )
        self.scalars.label = 'scalars'

    def get_device(self):
        """Return the device of the model (for engine.py compatibility)."""
        # TODO - Review this, probably just assume cuda.
        return self.embed.weight.device
    def forward(self, input_seq: Tensor, target_seq: Tensor = None, seqlens: Tensor = None, bigram_input_seq: Tensor = None, schedule_cfg: ForwardScheduleConfig = None, loss_reduction: str = 'mean', *, kv_cache=None):
        # Inference mode: Engine calls forward(ids, kv_cache=cache)
        if kv_cache is not None:
            return self.forward_inference(input_seq, kv_cache)

        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        bigram_lambdas = self.scalars[3 * self.num_layers: 4 * self.num_layers]
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        bm_sizes = [ws_short, ws_short, ws_short, ws_long, ws_short, ws_short, None, ws_short, ws_short, ws_short, ws_long]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==ws_long for b in bm_sizes] # apply partial key offset to long windows

        # Embedding lookup - embed is synced from lm_head during tied phase by optimizer
        x = self.embed(input_seq)
        x0_bigram = self.bigram_embed(bigram_input_seq)[None]

        # Value embeddings - always computed (not precomputed)
        ve = self.value_embeds.view(5, self.vocab_size, -1)[:, input_seq]
        # 01 ... 234 structure on token value embeddings by @photomz
        ve = [ve[0], ve[1]] + [None] * (self.num_layers - 5) + [ve[2], ve[3], ve[4]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)]
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        # unbind weight banks to avoid select_backwards kernel
        attn_weights = self.attn_bank.unbind(0)  # tuple of [4*dim, hdim] tensors
        mlp_fcs = self.mlp_bank[:, 0, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors
        mlp_projs = self.mlp_bank[:, 1, :, :].unbind(0)  # tuple of [mlp_hdim, dim] tensors

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=self.yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x + bigram_lambdas[0] * x0_bigram
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0 + bigram_lambdas[i] * x0_bigram

            # Get weights for this layer from banks
            qkvo_w = attn_weights[self.layer_to_attn_idx[i]] if i in self.layer_to_attn_idx else None
            c_fc = mlp_fcs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            c_proj = mlp_projs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None

            x = self.blocks[i](x, attn_args, qkvo_w, c_fc, c_proj)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(x.view(-1, x.size(-1)), target_seq, mtp_weights, self.lm_head.weight, self.lm_head.x_s, self.lm_head.w_s, self.lm_head.grad_s)
            loss = losses.sum()
        else:
            logits = self.lm_head(x)
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            if target_seq is None:
                return logits_for_loss
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction=loss_reduction)
        return loss

    def forward_inference(self, idx: Tensor, kv_cache):
        """
        Inference forward pass for text generation (compatible with engine.py).
        
        Args:
            idx: Token indices of shape (B, T)
            kv_cache: KVCache object for incremental decoding (required)
            
        Returns:
            logits: Output logits of shape (B, T, vocab_size)
        """
        assert kv_cache is not None, "kv_cache is required for inference"
        B, T = idx.size()
        
        # Skip connection and backout config (matches training)
        skip_in = [3]   # save hidden state at layer 3
        skip_out = [6]  # apply skip connection at layer 6
        backout_layer = 7
        
        # Get scalars for residual connections and gates
        resid_lambdas = self.scalars[:self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[self.num_layers:3 * self.num_layers].view(-1, 2)
        bigram_lambdas = self.scalars[3 * self.num_layers:4 * self.num_layers]
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers + 1]
        skip_lambda = self.scalars[4 * self.num_layers + 2]
        
        # Compute bigram indices using XOR hash (must match get_bigram_hash exactly)
        rand_int_1 = 36313
        rand_int_2 = 27191
        mod = self.bigram_embed.num_embeddings - 1
        
        if T > 1:
            # Prefill: Compute for all adjacent pairs in the sequence
            curr = idx[:, 1:].to(torch.int32)
            prev = idx[:, :-1].to(torch.int32)
            
            # Initialize with the "reserved" token for the first position
            bigram_idx = torch.full_like(idx, mod, dtype=torch.int32)
            
            # Apply XOR hash to the rest
            bigram_idx[:, 1:] = torch.bitwise_xor(rand_int_1 * curr, rand_int_2 * prev) % mod
            
            # Cache the last token for subsequent decode steps
            kv_cache.prev_token = idx[:, -1:].clone()
        else:
            # Decode: Use cached previous token
            if kv_cache.prev_token is not None:
                curr = idx.to(torch.int32)
                prev = kv_cache.prev_token.to(torch.int32)
                
                # Apply XOR hash
                bigram_idx = torch.bitwise_xor(rand_int_1 * curr, rand_int_2 * prev) % mod
            else:
                # No previous token (start of generation if T=1 passed initially)
                bigram_idx = torch.full_like(idx, mod, dtype=torch.int32)
            
            # Update cache with current token
            kv_cache.prev_token = idx.clone()
        
        # Compute bigram embedding
        x0_bigram = self.bigram_embed(bigram_idx)  # (B, T, model_dim)
        
        # Embedding lookup
        x = self.embed(idx)
        
        # Smear gate: blend each token's embedding with previous token's embedding
        # During prefill (T > 1): apply smear gate like training
        # During decode (T == 1): use cached previous embedding from kv_cache
        if T > 1:
            # Prefill: apply smear gate across the sequence
            smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[:, 1:, :12]))
            x = torch.cat([x[:, :1], x[:, 1:] + smear_gate_out * x[:, :-1]], dim=1)
            # Cache the last embedding for subsequent decode steps
            kv_cache.prev_embed = x[:, -1:].clone()
        else:
            # Decode: use cached previous embedding
            if kv_cache.prev_embed is not None:
                smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[..., :12]))
                x = x + smear_gate_out * kv_cache.prev_embed
            # Update cache with current embedding
            kv_cache.prev_embed = x.clone()
        
        x = x0 = norm(x)
        
        # Value embeddings - single Parameter viewed as 5 embeddings, same pattern as training
        # Pattern: 01 ... 234 structure on token value embeddings by @photomz
        ve_all = self.value_embeds.view(5, self.vocab_size, -1)[:, idx]  # (5, B, T, D)
        ve = [ve_all[0], ve_all[1]] + [None] * (self.num_layers - 5) + [ve_all[2], ve_all[3], ve_all[4]]
        
        # veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)] (already bf16?)
        # VE gate bank - same pattern as training 
        veg = self.ve_gate_bank.unbind(0)
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        
        # ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] # already bf16?
        # Attention gate bank - same pattern as training (layer 6 has no attention)
        ag = self.attn_gate_bank.unbind(0)
        attn_gates = ag[:6] + [None] + ag[6:]
        
        # Use full context window for inference (no sliding window)
        window_size = self.config.sequence_len
        
        # Get weights from parameter banks
        attn_weights = self.attn_bank.unbind(0)
        mlp_fcs = self.mlp_bank[:, 0, :, :].unbind(0)
        mlp_projs = self.mlp_bank[:, 1, :, :].unbind(0)
        
        # Skip connection and backout buffers
        skip_buffer = None
        x_backout = None
        
        # Forward through transformer blocks
        for i in range(self.num_layers):
            # Apply skip connection at layer 6 (before residual scaling)
            if i in skip_out and skip_buffer is not None:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :12]))
                x = x + skip_gate_out * skip_buffer
            
            # Apply residual scaling with bigram embedding (matches training)
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x + bigram_lambdas[0] * x0_bigram
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0 + bigram_lambdas[i] * x0_bigram
            
            # Get weights for this layer
            qkvo_w = attn_weights[self.layer_to_attn_idx[i]] if i in self.layer_to_attn_idx else None
            c_fc = mlp_fcs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            c_proj = mlp_projs[self.layer_to_mlp_idx[i]] if i in self.layer_to_mlp_idx else None
            
            # Use inference forward path
            x = self.blocks[i].forward_inference(
                x, qkvo_w, c_fc, c_proj,
                self.yarn, ve[i], ve_gates[i], attn_gates[i], sa_lambdas[i],
                window_size, kv_cache, i
            )
            
            # Save skip connection at layer 3
            if i in skip_in:
                skip_buffer = x.clone()
            
            # Save backout at layer 7
            if i == backout_layer:
                x_backout = x.clone()
        
        # Backout: subtract scaled layer 7 output from final hidden state
        # This removes "context-building" contributions not needed for direct prediction
        x = x - backout_lambda * x_backout
        
        # Final norm and lm_head
        x = norm(x)
        logits = self.lm_head(x)
        
        # Apply softcap (inference version: 23 * sigmoid((logits + 5) / 7.5))
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        
        return logits

"""
Engine for efficient inference of our models.

Everything works around token sequences:
- The user can send token sequences to the engine
- The engine returns the next token

Notes:
- The engine knows nothing about tokenization, it's purely token id sequences.

The whole thing is made as efficient as possible.
"""

import signal
import warnings
from contextlib import contextmanager
from collections import deque

# -----------------------------------------------------------------------------
# Calculator tool helpers
@contextmanager
def timeout(duration, formula):
    def timeout_handler(signum, frame):
        raise Exception(f"'{formula}': timed out after {duration} seconds")

    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    yield
    signal.alarm(0)

def eval_with_timeout(formula, max_time=3):
    try:
        with timeout(max_time, formula):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", SyntaxWarning)
                return eval(formula, {"__builtins__": {}}, {})
    except Exception as e:
        signal.alarm(0)
        # print(f"Warning: Failed to eval {formula}, exception: {e}") # it's ok ignore wrong calculator usage
        return None

def use_calculator(expr):
    """
    Evaluate a Python expression safely.
    Supports both math expressions and string operations like .count()
    """
    # Remove commas from numbers
    expr = expr.replace(",", "")

    # Check if it's a pure math expression (old behavior)
    if all([x in "0123456789*+-/.() " for x in expr]):
        if "**" in expr:  # disallow power operator
            return None
        return eval_with_timeout(expr)

    # Check if it's a string operation we support
    # Allow: strings (single/double quotes), .count(), letters, numbers, spaces, parens
    allowed_chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\"()._ "
    if not all([x in allowed_chars for x in expr]):
        return None

    # Disallow dangerous patterns
    dangerous_patterns = ['__', 'import', 'exec', 'eval', 'compile', 'open', 'file',
                         'input', 'raw_input', 'globals', 'locals', 'vars', 'dir',
                         'getattr', 'setattr', 'delattr', 'hasattr']
    expr_lower = expr.lower()
    if any(pattern in expr_lower for pattern in dangerous_patterns):
        return None

    # Only allow .count() method for now (can expand later)
    if '.count(' not in expr:
        return None

    # Evaluate with timeout
    return eval_with_timeout(expr)

# -----------------------------------------------------------------------------
class KVCache:
    """
    KV Cache designed for Flash Attention 3's flash_attn_with_kvcache API.

    Key differences from FA2-style cache:
    - Tensors are (B, T, H, D) not (B, H, T, D)
    - FA3 updates the cache in-place during flash_attn_with_kvcache
    - Position tracked per batch element via cache_seqlens tensor
    """

    def __init__(self, batch_size, num_heads, seq_len, head_dim, num_layers, device, dtype):
        self.batch_size = batch_size
        self.max_seq_len = seq_len
        self.n_layers = num_layers
        self.n_heads = num_heads
        self.head_dim = head_dim
        # Pre-allocate cache tensors: (n_layers, B, T, H, D)
        self.k_cache = torch.zeros(num_layers, batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        self.v_cache = torch.zeros(num_layers, batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        # Current sequence length per batch element (FA3 needs int32)
        self.cache_seqlens = torch.zeros(batch_size, dtype=torch.int32, device=device)
        # Previous embedding for smear gate (modded_gpt only)
        self.prev_embed = None
        # Previous token for bigram embedding (modded_gpt only)
        self.prev_token = None

    def reset(self):
        """Reset cache to empty state."""
        self.cache_seqlens.zero_()
        self.prev_embed = None
        self.prev_token = None

    def get_pos(self):
        """Get current position (assumes all batch elements at same position)."""
        return self.cache_seqlens[0].item()

    def get_layer_cache(self, layer_idx):
        """Return (k_cache, v_cache) views for a specific layer."""
        return self.k_cache[layer_idx], self.v_cache[layer_idx]

    def advance(self, num_tokens):
        """Advance the cache position by num_tokens."""
        self.cache_seqlens += num_tokens

    def prefill(self, other):
        """
        Copy cached KV from another cache into this one.
        Used when we do batch=1 prefill and then want to generate multiple samples in parallel.
        """
        assert self.get_pos() == 0, "Cannot prefill a non-empty KV cache"
        assert self.n_layers == other.n_layers and self.n_heads == other.n_heads and self.head_dim == other.head_dim
        assert self.max_seq_len >= other.max_seq_len
        other_pos = other.get_pos()
        self.k_cache[:, :, :other_pos, :, :] = other.k_cache[:, :, :other_pos, :, :]
        self.v_cache[:, :, :other_pos, :, :] = other.v_cache[:, :, :other_pos, :, :]
        self.cache_seqlens.fill_(other_pos)
        # Copy prev_embed for smear gate (expand to batch size if needed)
        if other.prev_embed is not None:
            self.prev_embed = other.prev_embed.expand(self.batch_size, -1, -1).clone()
        else:
            self.prev_embed = None
        # Copy prev_token for bigram embedding (expand to batch size if needed)
        if other.prev_token is not None:
            self.prev_token = other.prev_token.expand(self.batch_size, -1).clone()
        else:
            self.prev_token = None

# -----------------------------------------------------------------------------
@torch.inference_mode()
def sample_next_token(logits, rng, temperature=1.0, top_k=None):
    """Sample a single next token from given logits of shape (B, vocab_size). Returns (B, 1)."""
    assert temperature >= 0.0, "temperature must be non-negative"
    if temperature == 0.0:
        return torch.argmax(logits, dim=-1, keepdim=True)
    if top_k is not None and top_k > 0:
        k = min(top_k, logits.size(-1))
        vals, idx = torch.topk(logits, k, dim=-1)
        vals = vals / temperature
        probs = F.softmax(vals, dim=-1)
        choice = torch.multinomial(probs, num_samples=1, generator=rng)
        return idx.gather(1, choice)
    else:
        logits = logits / temperature
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1, generator=rng)

# -----------------------------------------------------------------------------

class RowState:
    # Per-row state tracking during generation
    def __init__(self, current_tokens=None):
        self.current_tokens = current_tokens or [] # Current token sequence for this row
        self.forced_tokens = deque() # Queue of tokens to force inject
        self.in_python_block = False # Whether we are inside a python block
        self.python_expr_tokens = [] # Tokens of the current python expression
        self.completed = False # Whether this row has completed generation

class Engine:

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer # needed for tool use

    @torch.inference_mode()
    def generate(self, tokens, num_samples=1, max_tokens=None, temperature=1.0, top_k=None, seed=42):
        """Generate tokens autoregressively. Prefills batch=1, then decodes num_samples in parallel."""
        assert isinstance(tokens, list) and isinstance(tokens[0], int), "expecting list of ints"
        device = self.model.get_device()
        dtype = torch.bfloat16  # CUDA-only, always bfloat16
        rng = torch.Generator(device=device)
        rng.manual_seed(seed)

        # Get the special tokens we need to coordinate the tool use state machine
        get_special = lambda s: self.tokenizer.encode_special(s)
        python_start = get_special("<|python_start|>")
        python_end = get_special("<|python_end|>")
        output_start = get_special("<|output_start|>")
        output_end = get_special("<|output_end|>")
        assistant_end = get_special("<|assistant_end|>") # if sampled, ends row
        bos = self.tokenizer.get_bos_token_id() # if sampled, ends row

        # 1) Run a batch 1 prefill of the prompt tokens
        m = self.model.config
        kv_model_kwargs = {"num_heads": m.n_kv_head, "head_dim": m.n_embd // m.n_head, "num_layers": m.n_layer}
        kv_cache_prefill = KVCache(
            batch_size=1,
            seq_len=len(tokens),
            device=device,
            dtype=dtype,
            **kv_model_kwargs,
        )
        ids = torch.tensor([tokens], dtype=torch.long, device=device)
        logits = self.model.forward(ids, kv_cache=kv_cache_prefill)
        logits = logits[:, -1, :].expand(num_samples, -1)  # (num_samples, vocab_size)

        # 2) Replicate the KV cache for each sample/row
        kv_length_hint = (len(tokens) + max_tokens) if max_tokens is not None else self.model.config.sequence_len
        kv_cache_decode = KVCache(
            batch_size=num_samples,
            seq_len=kv_length_hint,
            device=device,
            dtype=dtype,
            **kv_model_kwargs,
        )
        kv_cache_decode.prefill(kv_cache_prefill)
        del kv_cache_prefill # no need to keep this memory around

        # 3) Initialize states for each sample
        row_states = [RowState(tokens.copy()) for _ in range(num_samples)]

        # 4) Main generation loop
        num_generated = 0
        while True:
            # Stop condition: we've reached max tokens
            if max_tokens is not None and num_generated >= max_tokens:
                break
            # Stop condition: all rows are completed
            if all(state.completed for state in row_states):
                break

            # Sample the next token for each row
            next_ids = sample_next_token(logits, rng, temperature, top_k)  # (B, 1)
            sampled_tokens = next_ids[:, 0].tolist()

            # Process each row: choose the next token, update state, optional tool use
            token_column = [] # contains the next token id along each row
            token_masks = [] # contains the mask (was it sampled (1) or forced (0)?) along each row
            for i, state in enumerate(row_states):
                # Select the next token in this row
                is_forced = len(state.forced_tokens) > 0 # are there tokens waiting to be forced in deque?
                token_masks.append(0 if is_forced else 1) # mask is 0 if forced, 1 if sampled
                next_token = state.forced_tokens.popleft() if is_forced else sampled_tokens[i]
                token_column.append(next_token)
                # Update the state of this row to include the next token
                state.current_tokens.append(next_token)
                # On <|assistant_end|> or <|bos|>, mark the row as completed
                if next_token == assistant_end or next_token == bos:
                    state.completed = True
                # Handle tool logic
                if next_token == python_start:
                    state.in_python_block = True
                    state.python_expr_tokens = []
                elif next_token == python_end and state.in_python_block:
                    state.in_python_block = False
                    if state.python_expr_tokens:
                        expr = self.tokenizer.decode(state.python_expr_tokens)
                        result = use_calculator(expr)
                        if result is not None:
                            result_tokens = self.tokenizer.encode(str(result))
                            state.forced_tokens.append(output_start)
                            state.forced_tokens.extend(result_tokens)
                            state.forced_tokens.append(output_end)
                    state.python_expr_tokens = []
                elif state.in_python_block:
                    state.python_expr_tokens.append(next_token)

            # Yield the token column
            yield token_column, token_masks
            num_generated += 1

            # Prepare logits for next iteration
            ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)
            logits = self.model.forward(ids, kv_cache=kv_cache_decode)[:, -1, :]  # (B, vocab_size)

    def generate_batch(self, tokens, num_samples=1, **kwargs):
        """
        Non-streaming batch generation that just returns the final token sequences.
        Returns a list of token sequences (list of lists of ints).
        Terminal tokens (assistant_end, bos) are not included in the results.
        """
        assistant_end = self.tokenizer.encode_special("<|assistant_end|>")
        bos = self.tokenizer.get_bos_token_id()
        results = [tokens.copy() for _ in range(num_samples)]
        masks = [[0] * len(tokens) for _ in range(num_samples)]
        completed = [False] * num_samples
        for token_column, token_masks in self.generate(tokens, num_samples, **kwargs):
            for i, (token, mask) in enumerate(zip(token_column, token_masks)):
                if not completed[i]:
                    if token == assistant_end or token == bos:
                        completed[i] = True
                    else:
                        results[i].append(token)
                        masks[i].append(mask)
            # Stop if all rows are completed
            if all(completed):
                break
        return results, masks



# -----------------------------------------------------------------------------
# Dataset download

# Downloads everything in the repo (pre-training, validation, tokenizer, CORE eval) 
NUM_TRAIN_SHARDS = 20 # Only download the first 20 training shards
DATASET_NAME = "fineweb_edu_32k_8_370"
HF_REPO_ID = f"ChrisMcCormick/{DATASET_NAME}"
_data_path = os.environ.get("DATA_PATH", ".")
DATASET_DIR = os.path.join(_data_path, f"data/{DATASET_NAME}")
_config_path = os.path.join(DATASET_DIR, "config.json")

if not os.path.exists(_config_path):
    from huggingface_hub import HfApi, hf_hub_download, login
    hf_token = os.environ.get("HF_TOKEN")
    if hf_token:
        login(token=hf_token)
    os.makedirs(DATASET_DIR, exist_ok=True)
    print(f"=== Downloading dataset from {HF_REPO_ID} ===")
    api = HfApi()
    train_prefix = "train_"
    for fname in api.list_repo_files(repo_id=HF_REPO_ID, repo_type="dataset"):
        # Only download the first NUM_TRAIN_SHARDS training shards
        if fname.startswith(train_prefix) and int(fname[len(train_prefix):].split(".")[0]) >= NUM_TRAIN_SHARDS:
            continue
        if not os.path.exists(os.path.join(DATASET_DIR, fname)):
            print(f"  {fname}")
            hf_hub_download(repo_id=HF_REPO_ID, filename=fname, repo_type="dataset", local_dir=DATASET_DIR)
    assert os.path.exists(_config_path), "config.json missing after download"
    print("  Done.")

# Load vocab config
with open(_config_path) as f:
    _vocab_config = json.load(f)
VOCAB_SIZE = _vocab_config["vocab_size"]
BOS_ID = _vocab_config["bos_id"]

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

class Shard:
    def __init__(self, tokens: Tensor, world_size: int = 1):
        self.tokens = tokens
        self.size = tokens.numel()
        self.world_size = world_size
        self.i = 0

        # Partial index now, full index async
        self.bos_idx = (tokens[:6_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self._full_idx = None
        self._loader_thread = None
        self._ready = threading.Event()
        self._loader_thread = threading.Thread(target=self._scan)
        self._loader_thread.start()

    def _scan(self):
        self._full_idx = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self._ready.set()

    def _maybe_switch(self):
        # Switch to full index as soon as async scan completes
        if self.bos_idx is not self._full_idx and self._ready.is_set():
            self._loader_thread.join()
            self.bos_idx = self._full_idx

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        self._maybe_switch()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        return starts, ends

    @staticmethod
    def load_async(file: Path, world_size: int = 1):
        """Returns getter function for async shard loading"""
        result = {}
        ready = threading.Event()
        def load():
            tokens = _load_data_shard(file)
            result['shard'] = Shard(tokens, world_size)
            ready.set()
        thread = threading.Thread(target=load)
        thread.start()
        def get():
            ready.wait()
            thread.join()
            return result['shard']
        return get

def get_bigram_hash(x):
    """
    Computes bigram hash for each position using [prev_token, curr_token].
    Multiply by arbitary large ints to get even spread over int32 range.
    Position 0 is mapped to the reserved index (vocab_size - 1).
    BOS_tokens within the batch will hash based on last token of prior doc. Masking this ran slower and showed no improvement.
    """
    rand_int_1 = 36313
    rand_int_2 = 27191
    mod = args.bigram_vocab_size-1
    x = x.to(torch.int32).clone()
    x[0] = mod
    x[1:] = torch.bitwise_xor(rand_int_1 * x[1:], rand_int_2 * x[:-1]) % mod
    return x

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        shard = Shard(tokens, world_size)
        next_shard_getter = Shard.load_async(next(file_iter), world_size)
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = shard.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                shard = next_shard_getter()
                tokens = shard.tokens
                try:
                    next_shard_getter = Shard.load_async(next(file_iter), world_size)
                except StopIteration:
                    next_shard_getter = None  # no more shards to preload
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)
        _bigram_inputs = get_bigram_hash(_inputs)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True),
            _bigram_inputs.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# CORE Evaluation
"""
CORE evaluation for modded-nanogpt using pre-tokenized benchmark data.

The CORE metric (from the DCLM paper, https://arxiv.org/abs/2406.11794) evaluates
a base model on in-context learning tasks using logit-based scoring (no generation).

Pre-tokenized .pt files are produced by data/core_dataset.py and loaded at eval time.
Sequences are packed into fixed-size 1D buffers with cu_seqlens marking boundaries,
enabling batched evaluation through the compiled varlen flash attention model.
"""

# -----------------------------------------------------------------------------
# Packed CORE evaluation: batch multiple examples into fixed-length 1D buffers

def pack_for_eval(sequences, buffer_size, bos_id):
    """
    Pack pre-tokenized sequences into fixed-size 1D buffers for batched evaluation.

    Args:
        sequences: list of (tokens, start_idx, end_idx, example_idx, seq_idx_within_example)
        buffer_size: fixed buffer size (must be multiple of 16)
        bos_id: BOS token id for padding

    Returns:
        list of dicts with keys: input_ids, cu_seqlens, bigram_hash, metadata
    """
    assert buffer_size % 16 == 0
    # CORE eval sequences can be short (~50-200 tokens), so allow many more per buffer
    # than training's //300 estimate. Use //8 for generous headroom (memory is negligible).
    max_num_seqs = next_multiple_of_n(buffer_size // 8, n=128)

    buffers = []
    cur_tokens = []
    cur_cu = [0]
    cur_meta = []
    cur_pos = 0

    for tokens, start_idx, end_idx, example_idx, seq_idx in sequences:
        seq_len = len(tokens)
        if seq_len > buffer_size:
            continue  # should not happen after truncation

        if cur_pos + seq_len > buffer_size:
            # Finalize current buffer
            _finalize_eval_buffer(buffers, cur_tokens, cur_cu, cur_meta,
                                  buffer_size, max_num_seqs, bos_id)
            cur_tokens, cur_cu, cur_meta, cur_pos = [], [0], [], 0

        # Track answer span in global buffer coordinates
        global_start = cur_pos + start_idx
        global_end = cur_pos + end_idx
        cur_meta.append((example_idx, seq_idx, global_start, global_end))
        cur_tokens.extend(tokens)
        cur_pos += seq_len
        cur_cu.append(cur_pos)

    if cur_tokens:
        _finalize_eval_buffer(buffers, cur_tokens, cur_cu, cur_meta,
                              buffer_size, max_num_seqs, bos_id)

    return buffers


def _finalize_eval_buffer(buffers, cur_tokens, cur_cu, cur_meta,
                          buffer_size, max_num_seqs, bos_id):
    """Pad and finalize a packed eval buffer."""
    total_packed = len(cur_tokens)
    pad_count = buffer_size - total_packed

    # Input tokens: packed sequences + BOS padding
    input_ids = torch.full((buffer_size,), bos_id, dtype=torch.int32)
    input_ids[:total_packed] = torch.tensor(cur_tokens, dtype=torch.int32)

    # cu_seqlens: [0, end1, end2, ..., total_packed, buffer_size, buffer_size, ...]
    if pad_count > 0:
        cur_cu.append(buffer_size)  # ghost sequence for padding region
    cu_seqlens = torch.full((max_num_seqs,), buffer_size, dtype=torch.int32)
    cu_seqlens[:len(cur_cu)] = torch.tensor(cur_cu, dtype=torch.int32)

    bigram_hash = get_bigram_hash(input_ids)

    buffers.append({
        'input_ids': input_ids,
        'cu_seqlens': cu_seqlens,
        'bigram_hash': bigram_hash,
        'metadata': cur_meta,
    })


@torch.no_grad()
def forward_eval_packed(model, input_ids, cu_seqlens, bigram_hash, schedule_cfg):
    """
    Forward a packed 1D eval buffer through the compiled model.
    Returns logits of shape (buffer_size, vocab_size).
    """
    logits_3d = model(input_ids, target_seq=None, seqlens=cu_seqlens,
                      bigram_input_seq=bigram_hash, schedule_cfg=schedule_cfg)
    return logits_3d[0]  # (buffer_size, vocab_size)


@torch.no_grad()
def evaluate_task_packed(model, task_data, device, schedule_cfg, bos_id,
                         buffer_size=16384):
    """Evaluate one task using pre-tokenized sequences and packed batched evaluation."""
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    task_type = task_data['task_type']
    num_examples = task_data['num_examples']
    all_sequences = task_data['sequences']
    num_seqs_per_example = task_data['num_seqs_per_example']
    gold_labels = task_data['gold_labels']

    # Step 1: Select this rank's share of pre-tokenized sequences
    rank_examples = set(range(rank, num_examples, world_size))
    sequences = [
        (s['tokens'], s['start_idx'], s['end_idx'], s['example_idx'], s['seq_idx'])
        for s in all_sequences if s['example_idx'] in rank_examples
    ]

    # Step 2: Pack into fixed-size buffers
    packed_buffers = pack_for_eval(sequences, buffer_size, bos_id)

    # Step 3: Forward pass each buffer and collect per-sequence results
    seq_results = {}

    for buf in packed_buffers:
        input_ids = buf['input_ids'].to(device)
        cu_seqlens = buf['cu_seqlens'].to(device)
        bigram_hash = buf['bigram_hash'].to(device)

        logits = forward_eval_packed(model, input_ids, cu_seqlens, bigram_hash, schedule_cfg)

        # Per-position losses: loss[j] = -log p(input_ids[j+1] | context up to j)
        target_ids = torch.roll(input_ids.long(), shifts=-1)
        all_losses = F.cross_entropy(logits.float(), target_ids, reduction='none')
        all_predictions = logits.argmax(dim=-1)

        for example_idx, seq_idx, gs, ge in buf['metadata']:
            # Answer span [gs, ge): logits at [gs-1, ge-1) predict tokens at [gs, ge)
            seq_results[(example_idx, seq_idx)] = {
                'losses': all_losses[gs - 1 : ge - 1],
                'predictions': all_predictions[gs - 1 : ge - 1],
                'input_ids': input_ids[gs : ge].long(),
            }

    # Step 4: Evaluate per-example correctness
    correct = torch.zeros(num_examples, dtype=torch.float32, device=device)

    for idx in range(rank, num_examples, world_size):
        if task_type == 'language_modeling':
            r = seq_results[(idx, 0)]
            is_correct = torch.all(r['predictions'] == r['input_ids']).item()
        elif task_type in ['multiple_choice', 'schema']:
            mean_losses = []
            for seq_j in range(num_seqs_per_example[idx]):
                r = seq_results[(idx, seq_j)]
                mean_losses.append(r['losses'].mean().item())
            pred_idx = mean_losses.index(min(mean_losses))
            is_correct = pred_idx == gold_labels[idx]
        else:
            raise ValueError(f"Unsupported task type: {task_type}")
        correct[idx] = float(is_correct)

    if world_size > 1:
        dist.barrier()
        dist.all_reduce(correct, op=dist.ReduceOp.SUM)
    return correct.mean().item()


def evaluate_core(model, device, schedule_cfg, bos_id):
    """
    Evaluate a base model on the CORE benchmark using pre-tokenized data.
    Returns dict with results, centered_results, and core_metric.
    """
    data_path = os.environ.get("DATA_PATH", ".")
    core_eval_dir = os.path.join(data_path, f"data/{DATASET_NAME}/core_eval")
    config_path = os.path.join(core_eval_dir, "config.json")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # Evaluate each task
    results = {}
    centered_results = {}
    for task_info in config['tasks']:
        torch.cuda.synchronize()
        start_time = time.time()
        label = task_info['label']

        task_data = torch.load(os.path.join(core_eval_dir, task_info['file']),
                               weights_only=False)
        print0(f"Evaluating: {label} ({task_data['task_type']}, "
               f"{task_data['num_examples']} examples)... ", console=True)

        accuracy = evaluate_task_packed(model, task_data, device, schedule_cfg, bos_id)
        torch.cuda.synchronize()
        results[label] = accuracy
        random_baseline = task_data['random_baseline']
        centered_result = (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)
        centered_results[label] = centered_result
        elapsed = time.time() - start_time
        print0(f"accuracy: {accuracy:.4f} | centered: {centered_result:.4f} | time: {elapsed:.2f}s", console=True)

    core_metric = sum(centered_results.values()) / len(centered_results)
    out = {
        "results": results,
        "centered_results": centered_results,
        "core_metric": core_metric
    }
    return out


# -----------------------------------------------------------------------------
# Training Management

import datetime

@dataclass
class Hyperparameters:
    # data
    data_path = os.environ.get("DATA_PATH", ".")
    train_files: str = os.path.join(data_path, f"data/{DATASET_NAME}/train_*.bin") # input .bin to train on
    val_files: str = os.path.join(data_path, f"data/{DATASET_NAME}/val_*.bin") # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # schedule
    num_scheduled_iterations: int = 3960  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    # evaluation and logging
    #run_id: str = f"2000-steps-{uuid.uuid4()}"
    run_id: str = f"{str(datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S'))}-4000-steps"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = True
    # bigram hash embedding
    bigram_vocab_size: int = VOCAB_SIZE * 5

args = Hyperparameters()

@dataclass
class TrainingStage:
    lr_mul: float
    batch_size: int
    window_sizes: tuple[int, int]  # (short, long) in block units
    mtp_weights_start: list[float]
    mtp_weights_end: list[float]
    duration: float = None

class TrainingSchedule:
    """
    Training schedule initialized via TRAINING_STAGES
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """

    def __init__(self, stages: list[TrainingStage], scheduled_iterations: int, extension_iterations: int,
                 cooldown_frac: float = 0.5, split_embed_stage: int = 2, ws_post_yarn_ext: int = 20):
        self.stages = stages
        self.scheduled_iterations = scheduled_iterations
        self.cooldown_frac = cooldown_frac
        # increase final validation ws, used for YaRN extension and short window size @classiclarryd
        self.ws_post_yarn_ext = ws_post_yarn_ext

        self.total_steps = self.scheduled_iterations + extension_iterations

        # Build stage boundaries (last is extension stage)
        ends = [0] + [round(c * scheduled_iterations) for c in accumulate(s.duration for s in stages[:-1])] + [self.total_steps]
        assert self.scheduled_iterations == ends[-2]
        self.boundaries = list(pairwise(ends))

        # Split embed at specified stage (ensure odd step for Adam)
        self.split_step = self.boundaries[split_embed_stage][0] | 1

        # Precompute MTP weights for all steps
        self.mtp_weights = []
        for step in range(self.total_steps + 1):
            stage, t = self.lookup(step)
            w = [a + (b - a) * t for a, b in zip(stage.mtp_weights_start, stage.mtp_weights_end)]
            self.mtp_weights.append(torch.tensor(w, device=device))

    def lookup(self, step: int) -> tuple[TrainingStage, float]:
        # Returns stage and % of the way through that stage
        for i, (start, end) in enumerate(self.boundaries):
            if step < end:
                t = (step - start) / (end - start)
                return self.stages[i], t
        return self.stages[-1], 1.0

    def get_lr(self, step: int) -> float:
        # learning rate schedule: tied to batch size schedule, with cooldown at the end
        stage, _ = self.lookup(step)
        lr = stage.lr_mul
        cd_start = int(self.scheduled_iterations * (1 - self.cooldown_frac))
        if step >= cd_start:
            t = min(1.0, (step - cd_start) / (self.scheduled_iterations - cd_start))
            lr = lr * (1 - t) + 0.1 * t
        return lr

# window_sizes are in units of `block_size` tokens (defined in TrainingManager)
TRAINING_STAGES = [
    TrainingStage(duration=1/3, batch_size=8 * 2048 * 8, window_sizes=(1, 3), lr_mul=1.0,
                  mtp_weights_start=[1.0, 0.5, 0.25], mtp_weights_end=[1.0, 0.5, 0.0]),
    TrainingStage(duration=1/3, batch_size=16 * 2048 * 8, window_sizes=(3, 7), lr_mul=1.52,  # (16/8)**0.6
                  mtp_weights_start=[1.0, 0.5], mtp_weights_end=[1.0, 0.0]),
    TrainingStage(duration=1/3, batch_size=24 * 2048 * 8, window_sizes=(5, 11), lr_mul=1.73,  # (24/8)**0.5
                  mtp_weights_start=[1.0], mtp_weights_end=[1.0]),
    # extension stage
    TrainingStage(batch_size=24 * 2048 * 8, window_sizes=(6, 13), lr_mul=1.0,  # lr_mul is not used
                  mtp_weights_start=[1.0], mtp_weights_end=[1.0]),
]

training_schedule = TrainingSchedule(TRAINING_STAGES, args.num_scheduled_iterations, args.num_extension_iterations, cooldown_frac=0.55)

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = training_schedule.total_steps - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages the NorMuonAndAdam for all parameters with explicit ordering.
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Adam optimizers are only stepped on odd steps @classiclarryd
        3. Explicit scatter_order and work_order for communication scheduling (no backward hooks)
        4. Muon has a linear momentum warmup and cooldown schedule
        5. Learning rates follow a linear decay schedule
        6. Embed is tied to lm_head until split step (2/3 of training), then untied @classiclarryd
    """
    def __init__(self, model):
        self.model = model
        self.block_size = 128

        # - Ordering dictates when to launch reduce/reduce_scatter operations
        # - "sharded" parameters use reduce_scatter/all_gather and "replicated" ones use all_reduce
        # - lr_mul and wd_mul are per-parameter learning rate and weight decay multipliers
        self.param_table = {
            "attn":           {"optim": "normuon", "comms": "sharded",    "adam_betas": None},
            "mlp":            {"optim": "normuon", "comms": "sharded",    "adam_betas": None},
            "scalars":        {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 5.0,  "wd_mul": 0.0},
            "value_embed":    {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.75, 0.95], "lr_mul": 75.,  "wd_mul": 5.0},
            "bigram_embed":   {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.75, 0.95], "lr_mul": 75.,  "wd_mul": 5.0},
            "smear_gate":     {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 0.01, "wd_mul": 0.0},
            "skip_gate":      {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99], "lr_mul": 0.05, "wd_mul": 0.0},
            "attn_gate_bank": {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99]},
            "ve_gate_bank":   {"optim": "adam",    "comms": "replicated", "adam_betas": [0.9,  0.99]},
            "x0_lambdas":     {"optim": "adam",    "comms": "replicated", "adam_betas": [0.65, 0.95], "lr_mul": 5.0,  "wd_mul": 0.0},
            "lm_head":        {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.5,  0.95], "wd_mul": 150.},
            "embed":          {"optim": "adam",    "comms": "sharded",    "adam_betas": [0.5,  0.95], "wd_mul": 150.},
        }

        # - Process smaller/faster params first while large reduces complete
        # - lm_head must complete before embed sync (when tied)
        self.work_order = [
            "scalars", "smear_gate", "skip_gate", "attn_gate_bank", "ve_gate_bank", "x0_lambdas",  # Small, fast
            "value_embed", "bigram_embed",  # Medium
            "lm_head", "embed",   # lm_head must complete before embed sync (when tied)
            "attn", "mlp",        # Large, polar express - process last to maximize overlap
        ]

        adam_defaults = dict(
            lr=0.008,
            eps=1e-10,
            weight_decay=0.005,
        )

        normuon_defaults = dict(
            lr=0.023,
            momentum=0.95,
            beta2=0.95,
            weight_decay=1.2,
        )

        self.optimizer = NorMuonAndAdam(
            model.named_parameters(),
            param_table=self.param_table,
            scatter_order=list(self.param_table.keys()),  # Dict order defines scatter priority
            work_order=self.work_order,
            adam_defaults=adam_defaults,
            normuon_defaults=normuon_defaults,
        )

        # Split embed from lm_head at 2/3 of training (on an odd step so Adam updates)
        self.split_step = training_schedule.split_step

        self.reset()

    def apply_final_ws_ext(self):
        self.ws_long = training_schedule.ws_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short * self.block_size,
            ws_long = self.ws_long * self.block_size
        )

    def _is_adam_step(self, step: int):
        """Adam params are only updated on odd steps."""
        return step % 2 == 1

    def get_transition_steps(self):
        return [start for start, _ in training_schedule.boundaries[1:]]

    def advance_schedule(self, step: int):
        stage, _ = training_schedule.lookup(step)
        self.ws_short, new_ws_long = stage.window_sizes
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long * self.block_size, new_ws_long * self.block_size)

        new_batch_size = stage.batch_size
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
            self.batch_size = new_batch_size
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = training_schedule.mtp_weights[step]

    def step_optimizers(self, step: int):
        step_lr = training_schedule.get_lr(step)
        muon_momentum = get_muon_momentum(step)
        do_adam = self._is_adam_step(step)

        # Update learning rates and momentum for all params
        for param, p_cfg in self.optimizer.param_cfgs.items():
            p_cfg.lr = p_cfg.initial_lr * step_lr
            if p_cfg.optim == "normuon":
                p_cfg.momentum = muon_momentum

        # Step optimizer with do_adam flag
        self.optimizer.step(do_adam=do_adam)

        # At split step: copy lm_head optimizer state to embed and mark as split
        if step == self.split_step:
            self.optimizer.copy_lm_state_to_embed()

    def reset(self, state=None):
        if state is not None:
            self.optimizer.load_state_dict(state)

        # Reset NorMuon momentum buffers and split_embed state
        self.optimizer.reset()

        stage, _ = training_schedule.lookup(0)
        self.ws_short, self.ws_long = stage.window_sizes
        self.batch_size = stage.batch_size
        self.model.yarn.reset()

    def get_state(self):
        return copy.deepcopy(self.optimizer.state_dict())

# -----------------------------------------------------------------------------
# int main

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=VOCAB_SIZE,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
model.attn_bank.data = model.attn_bank.data.bfloat16()
model.mlp_bank.data = model.mlp_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizer=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, TRAINING_STAGES[0].batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens, bigram_inputs = next(val_loader)
        model(inputs, targets, cum_seqlens, bigram_inputs, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens, bigram_inputs = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, bigram_inputs, training_manager.get_forward_args()) * grad_scale).backward()
    training_manager.step_optimizers(step)
# Warmup CORE eval compiled path (eval mode, target_seq=None, fixed buffer size)
_core_buf_size = 16384
_core_max_seqs = next_multiple_of_n(_core_buf_size // 8, n=128)
model.eval()
with torch.no_grad():
    _dummy_ids = torch.zeros(_core_buf_size, dtype=torch.int32, device=device)
    _dummy_cu = torch.full((_core_max_seqs,), _core_buf_size, dtype=torch.int32, device=device)
    _dummy_cu[0] = 0
    _dummy_cu[1] = _core_buf_size
    _dummy_bigram = get_bigram_hash(_dummy_ids)
    model(_dummy_ids, target_seq=None, seqlens=_dummy_cu,
          bigram_input_seq=_dummy_bigram, schedule_cfg=training_manager.get_forward_args())
model.train()
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizer"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################

with open(f"data/{DATASET_NAME}/tokenizer/token_bytes.pt", "rb") as f:
    token_bytes = torch.load(f, map_location=device)

train_loader = distributed_data_generator(args.train_files, TRAINING_STAGES[0].batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

# ============ TEMPORARY HACK: Load checkpoint and run CORE eval only ============
if False:  # Set to False to disable this hack
    _ckpt_path = "/home/ubuntu/stacks/modded/logs/2000-steps-aaad572d-2e7f-4e81-8ec1-7a32f62c62a7/state_step002000.pt"
    print0(f"HACK: Loading checkpoint from {_ckpt_path}", console=True)
    _ckpt = torch.load(_ckpt_path, map_location=device, weights_only=False)
    model.load_state_dict(_ckpt["model"])
    print0(f"HACK: Loaded model state from step {_ckpt.get('step', '?')}", console=True)
    del _ckpt

    # Replay schedule transitions to get correct YaRN RoPE state and window sizes
    for _ts in training_manager.get_transition_steps():
        training_manager.advance_schedule(_ts)
    training_manager.advance_schedule(training_schedule.total_steps)
    training_manager.apply_final_ws_ext()

    # Run CORE eval
    core_eval_t0 = time.time()    
    model.eval()
    schedule_cfg = training_manager.get_forward_args()
    print0(f"HACK: Running CORE eval (ws_short={training_manager.ws_short}, ws_long={training_manager.ws_long})", console=True)
    core_out = evaluate_core(model, device, schedule_cfg, bos_id=BOS_ID)
    print0(f"CORE metric: {core_out['core_metric']:.4f}", console=True)
    for label, acc in core_out['results'].items():
        centered = core_out['centered_results'][label]
        print0(f"  {label}: accuracy={acc:.4f} centered={centered:.4f}", console=True)

    core_eval_elapsed = time.time() - core_eval_t0
    print0(f"CORE metric: {core_out['core_metric']:.4f} | total CORE eval time: {core_eval_elapsed:.2f}s", console=True)

    # print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    #        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
    dist.destroy_process_group()
    sys.exit(0)
# ============ END TEMPORARY HACK ============

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = training_schedule.total_steps
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        total_nats = torch.tensor(0.0, dtype=torch.float32, device=device)
        total_bytes = torch.tensor(0, dtype=torch.int64, device=device)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens, bigram_inputs = next(val_loader)
                loss_flat = model(inputs, targets, cum_seqlens, bigram_inputs, training_manager.get_forward_args(), loss_reduction='none')
                num_bytes_flat = token_bytes[targets]
                total_nats += (loss_flat * (num_bytes_flat > 0).float()).sum()
                total_bytes += num_bytes_flat.sum()
        del val_loader
        if world_size > 1:
            dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)
            dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)
        val_bpb = total_nats.item() / (math.log(2) * total_bytes.item()) if total_bytes.item() > 0 else float('inf')
        print0(f"step:{step}/{train_steps} val_bpb:{val_bpb:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizer=training_manager.get_state())
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # --------------- CORE EVALUATION -----------------
        model.eval()
        schedule_cfg = training_manager.get_forward_args()
        core_eval_t0 = time.time()
        core_out = evaluate_core(model, device, schedule_cfg, bos_id=BOS_ID)
        core_eval_elapsed = time.time() - core_eval_t0
        print0(f"CORE metric: {core_out['core_metric']:.4f} | total CORE eval time: {core_eval_elapsed:.2f}s", console=True)
        for label, acc in core_out['results'].items():
            centered = core_out['centered_results'][label]
            print0(f"  {label}: accuracy={acc:.4f} centered={centered:.4f}", console=True)
        # the last step only has the validation loop + CORE eval, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        inputs, targets, cum_seqlens, bigram_inputs = train_loader.send(training_manager.train_loader_send_args)
        (model(inputs, targets, cum_seqlens, bigram_inputs, training_manager.get_forward_args()) * grad_scale).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

# ========================================================================================
#                                       SFT TRAINING                                     #
# ========================================================================================

"""
Supervised fine-tuning (SFT) the model.
Run as:

python -m scripts.chat_sft

Or torchrun for training:

torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- --device-batch-size=16
"""

import argparse
import os
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import time
import wandb
import torch
from contextlib import nullcontext
from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, get_base_dir, autodetect_device_type
from nanochat.tokenizer import get_token_bytes
from nanochat.checkpoint_manager import save_checkpoint
from nanochat.loss_eval import evaluate_bpb
from nanochat.checkpoint_manager import load_model
import torch.distributed as dist

from tasks.common import TaskMixture
from tasks.gsm8k import GSM8K
from tasks.mmlu import MMLU
from tasks.smoltalk import SmolTalk
from tasks.customjson import CustomJSON
from tasks.spellingbee import SimpleSpelling, SpellingBee

# -----------------------------------------------------------------------------
# CLI arguments
parser = argparse.ArgumentParser(description="Supervised fine-tuning (SFT) the model")
# Logging
parser.add_argument("--run", type=str, default="dummy", help="wandb run name ('dummy' disables wandb logging)")
# Runtime
parser.add_argument("--device-type", type=str, default="", help="cuda|cpu|mps (empty = autodetect)")
parser.add_argument("--dtype", type=str, default="bfloat16", help="float32|bfloat16")
# Model loading
parser.add_argument("--model-tag", type=str, default=None, help="model tag to load from")
parser.add_argument("--model-step", type=int, default=None, help="model step to load from")
# Training horizon
parser.add_argument("--num-iterations", type=int, default=-1, help="number of optimization steps (-1 = full epoch)")
# Batch sizes
parser.add_argument("--max-seq-len", type=int, default=2048, help="max context length")
parser.add_argument("--device-batch-size", type=int, default=32, help="per-device batch size")
parser.add_argument("--total-batch-size", type=int, default=524288, help="total batch size in tokens")
# Optimization
parser.add_argument("--embedding-lr", type=float, default=0.3, help="learning rate for embedding parameters (Adam)")
parser.add_argument("--unembedding-lr", type=float, default=0.004, help="learning rate for unembedding parameters (Adam)")
parser.add_argument("--matrix-lr", type=float, default=0.02, help="learning rate for matrix parameters (Muon)")
parser.add_argument("--weight-decay", type=float, default=0.0, help="weight decay for embedding/unembedding parameters (Adam)")
parser.add_argument("--init-lr-frac", type=float, default=1.0, help="initial LR as fraction of base LR")
# Evaluation
parser.add_argument("--eval-every", type=int, default=150, help="evaluate val bpb every N steps (-1 = disable)")
parser.add_argument("--eval-tokens", type=int, default=20*524288, help="number of tokens to evaluate val loss on")
# Output
parser.add_argument("--dry-run", action="store_true", help="log to wandb but skip checkpoints/report")
args = parser.parse_args()
user_config = vars(args).copy()
# -----------------------------------------------------------------------------

# Compute init
device_type = autodetect_device_type() if args.device_type == "" else args.device_type
ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
master_process = ddp_rank == 0
ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16
autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == "cuda" else nullcontext()
synchronize = torch.cuda.synchronize if device_type == "cuda" else lambda: None
get_max_memory = torch.cuda.max_memory_allocated if device_type == "cuda" else lambda: 0

# wandb logging init
use_dummy_wandb = args.run == "dummy" or not master_process
wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project="nanochat-sft", name=args.run, config=user_config)

# Load the model and tokenizer
model, tokenizer, meta = load_model("base", device, phase="train", model_tag=args.model_tag, step=args.model_step)
pretrain_batch_size = meta.get("device_batch_size", None)
if pretrain_batch_size is not None and args.device_batch_size > pretrain_batch_size:
    print0(f"FOOTGUN WARNING: base model training used device_batch_size {pretrain_batch_size}, did you pass in a good --device-batch-size to this script?")
orig_model = model
model = torch.compile(model, dynamic=False)
depth = model.config.n_layer
num_flops_per_token = model.estimate_flops()
tokens_per_fwdbwd = args.device_batch_size * args.max_seq_len # tokens per iteration for a single rank
world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks
assert args.total_batch_size % world_tokens_per_fwdbwd == 0
grad_accum_steps = args.total_batch_size // world_tokens_per_fwdbwd
print0(f"Tokens / micro-batch / rank: {args.device_batch_size} x {args.max_seq_len} = {tokens_per_fwdbwd:,}")
print0(f"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}")
print0(f"Total batch size {args.total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}")
token_bytes = get_token_bytes(device=device)

# Initialize the Optimizer (combined MuonAdamW: Muon for matrix params, AdamW for rest)
optimizer = model.setup_optimizer(unembedding_lr=args.unembedding_lr, embedding_lr=args.embedding_lr, matrix_lr=args.matrix_lr, weight_decay=args.weight_decay)
# Override the initial learning rate as a fraction of the base learning rate
for group in optimizer.param_groups:
    group["lr"] = group["lr"] * args.init_lr_frac
    group["initial_lr"] = group["lr"]

# SFT data mixture and DataLoader
base_dir = get_base_dir()
identity_conversations_filepath = os.path.join(base_dir, "identity_conversations.jsonl")
train_dataset = TaskMixture([
    SmolTalk(split="train"), # 460K rows of general conversations
    MMLU(subset="auxiliary_train", split="train"), # 100K rows of multiple choice problems drawn from ARC, MC_TEST, OBQA, RACE
    GSM8K(subset="main", split="train"), # 8K rows teaching simple math and (calculator) tool use
    GSM8K(subset="main", split="train"), # 2 epochs of GSM8K
    CustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations
    CustomJSON(filepath=identity_conversations_filepath), # let's do 2 epochs of these
    SimpleSpelling(size=200000, split="train"), # 200K rows of Simple Spelling (e.g. spell the word 'apple')
    SpellingBee(size=80000, split="train"), # 80K rows of Spelling Bee (e.g. how many 'r' are in 'strawberry'?)
]) # total: 460K + 100K + 16K + 200K + 80K = 856K rows
val_dataset = TaskMixture([
    SmolTalk(split="test"), # 24K rows in test set
    MMLU(subset="all", split="test", stop=5200), # 14K rows in test set, use only 5.2K to match the train ratios
    GSM8K(subset="main", split="test", stop=420), # 1.32K rows in test set, use only 420 to match the train ratios
]) # total: 24K + 14K + 1.32K ~= 39K rows
# DataLoader is defined here, it emits inputs, targets : 2D tensors of shape (device_batch_size, max_seq_len)
# A big problem is that we don't know the final num_iterations in advance. So we create
# these two global variables and update them from within the data generator.
last_step = False # we will toggle this to True when we reach the end of the training dataset
approx_progress = 0.0 # will go from 0 to 1 over the course of the epoch
current_epoch = 1 # track epoch for logging
def sft_data_generator_bos_bestfit(split, buffer_size=100):
    """
    BOS-aligned dataloader for SFT with bestfit-pad packing.

    Each row in the batch starts with BOS (beginning of a conversation).
    Conversations are packed using best-fit algorithm. When no conversation fits,
    the row is padded (instead of cropping) to ensure no tokens are ever discarded.
    Padding positions have targets masked with -1 (ignore_index for cross-entropy).
    """
    global last_step, approx_progress, current_epoch
    assert split in {"train", "val"}, "split must be 'train' or 'val'"
    dataset = train_dataset if split == "train" else val_dataset
    dataset_size = len(dataset)
    assert dataset_size > 0
    row_capacity = args.max_seq_len + 1  # +1 for target at last position
    bos_token = tokenizer.get_bos_token_id()

    # Conversation buffer: list of token lists
    conv_buffer = []
    cursor = ddp_rank  # Each rank processes different conversations (for fetching)
    consumed = ddp_rank  # Track actual consumption separately from buffering
    epoch = 1
    it = 0  # iteration counter

    def refill_buffer():
        nonlocal cursor, epoch
        while len(conv_buffer) < buffer_size:
            conversation = dataset[cursor]
            ids, _ = tokenizer.render_conversation(conversation)
            conv_buffer.append(ids)
            cursor += ddp_world_size
            if cursor >= dataset_size:
                cursor = cursor % dataset_size
                epoch += 1
                # Note: last_step is now triggered based on consumption, not fetching

    while True:
        rows = []
        row_lengths = []  # Track actual content length (excluding padding) for each row
        for _ in range(args.device_batch_size):
            row = []
            padded = False
            while len(row) < row_capacity:
                # Ensure buffer has conversations
                while len(conv_buffer) < buffer_size:
                    refill_buffer()

                remaining = row_capacity - len(row)

                # Find largest conversation that fits entirely
                best_idx = -1
                best_len = 0
                for i, conv in enumerate(conv_buffer):
                    conv_len = len(conv)
                    if conv_len <= remaining and conv_len > best_len:
                        best_idx = i
                        best_len = conv_len

                if best_idx >= 0:
                    # Found a conversation that fits - use it entirely
                    conv = conv_buffer.pop(best_idx)
                    row.extend(conv)
                    consumed += ddp_world_size  # Track actual consumption
                else:
                    # No conversation fits - pad the remainder instead of cropping
                    # This ensures we never discard any tokens
                    content_len = len(row)
                    row.extend([bos_token] * remaining)  # Pad with BOS tokens
                    padded = True
                    break  # Row is now full (with padding)

            # Track content length: full row if no padding, otherwise the length before padding
            if padded:
                row_lengths.append(content_len)
            else:
                row_lengths.append(row_capacity)
            rows.append(row[:row_capacity])

        # Stopping condition to respect num_iterations, if given
        it += 1
        if 0 < args.num_iterations <= it and split == "train":
            last_step = True

        # Update progress tracking (based on consumed, not cursor, to account for buffering)
        if split == "train":
            current_epoch = epoch
            if args.num_iterations > 0:
                approx_progress = it / args.num_iterations
            else:
                approx_progress = consumed / dataset_size
            # Trigger last_step when we've consumed enough (instead of when cursor wraps)
            if consumed >= dataset_size:
                last_step = True

        # Build tensors
        use_cuda = device_type == "cuda"
        batch_tensor = torch.tensor(rows, dtype=torch.long, pin_memory=use_cuda)
        inputs = batch_tensor[:, :-1].to(device=device, dtype=torch.int32, non_blocking=use_cuda)
        targets = batch_tensor[:, 1:].to(device=device, dtype=torch.int64, non_blocking=use_cuda)

        # Mask out padding positions in targets (set to -1 = ignore_index)
        # For each row, positions >= (content_length - 1) in targets should be masked
        for i, content_len in enumerate(row_lengths):
            if content_len < row_capacity:
                targets[i, content_len-1:] = -1

        yield inputs, targets

train_loader = sft_data_generator_bos_bestfit("train")
build_val_loader = lambda: sft_data_generator_bos_bestfit("val")
progress = 0 # will go from 0 to 1 over the course of the epoch

# Learning rate scheduler
def get_lr_multiplier(progress):
    # first 80% of training: no decay, then linearly ramp down to 0.
    return 1 if progress < 0.8 else 1 - (progress - 0.8) / 0.2

# Momentum scheduler for Muon optimizer
def get_muon_momentum(it):
    frac = min(it / 300, 1)
    momentum = (1 - frac) * 0.85 + frac * 0.95
    return momentum

# -----------------------------------------------------------------------------
# Training loop
x, y = next(train_loader) # prefetch the very first batch of data
min_val_bpb = float("inf")
smooth_train_loss = 0 # EMA of training loss
ema_beta = 0.9 # EMA decay factor
total_training_time = 0 # total wall-clock time of training
step = 0
while True:
    flops_so_far = num_flops_per_token * args.total_batch_size * step

    # Synchronize last_step across all ranks to avoid hangs in the distributed setting
    if ddp:
        last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)
        dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)
        last_step = bool(last_step_tensor.item())

    # once in a while: evaluate the val bpb (all ranks participate)
    if last_step or (args.eval_every > 0 and step % args.eval_every == 0):
        model.eval()
        val_loader = build_val_loader()
        eval_steps = args.eval_tokens // (args.device_batch_size * args.max_seq_len * ddp_world_size)
        with autocast_ctx:
            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)
        print0(f"Step {step:05d} | Validation bpb: {val_bpb:.4f}")
        if val_bpb < min_val_bpb:
            min_val_bpb = val_bpb
        wandb_run.log({
            "step": step,
            "total_training_flops": flops_so_far,
            "total_training_time": total_training_time,
            "val/bpb": val_bpb,
        })
        model.train()

    # save checkpoint at the end of the run (only on master process)
    if master_process and last_step and not args.dry_run:
        output_dirname = args.model_tag if args.model_tag else f"d{depth}" # e.g. d12
        checkpoint_dir = os.path.join(base_dir, "chatsft_checkpoints", output_dirname)
        save_checkpoint(
            checkpoint_dir,
            step,
            orig_model.state_dict(),
            optimizer.state_dict(),
            {
                "step": step,
                "val_bpb": val_bpb, # loss at last step
                "model_config": {
                    "sequence_len": args.max_seq_len,
                    "vocab_size": tokenizer.get_vocab_size(),
                    "n_layer": depth,
                    "n_head": model.config.n_head,
                    "n_kv_head": model.config.n_kv_head,
                    "n_embd": model.config.n_embd,
                    "window_pattern": model.config.window_pattern,
                },
                "user_config": user_config, # inputs to the training script
            }
        )

    if last_step:
        break

    # -------------------------------------------------------------------------
    # single training step
    # evaluate the gradient
    synchronize()
    t0 = time.time()
    for micro_step in range(grad_accum_steps):
        with autocast_ctx:
            loss = model(x, y)
        train_loss = loss.detach() # for logging
        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here
        loss.backward()
        x, y = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward
        progress = max(progress, approx_progress) # only increase progress monotonically
    # step the optimizer
    lrm = get_lr_multiplier(progress)
    muon_momentum = get_muon_momentum(step)
    for group in optimizer.param_groups:
        group["lr"] = group["initial_lr"] * lrm
        if group['kind'] == 'muon':
            group["momentum"] = muon_momentum
    optimizer.step()
    model.zero_grad(set_to_none=True)
    synchronize()
    t1 = time.time()
    dt = t1 - t0
    # -------------------------------------------------------------------------

    # State
    step += 1

    # logging
    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() # EMA the training loss
    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA
    pct_done = 100 * progress
    tok_per_sec = int(args.total_batch_size / dt)
    flops_per_sec = num_flops_per_token * args.total_batch_size / dt
    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # bfloat16 H100 SXM and without 2:4 sparsity
    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %
    if step > 10:
        total_training_time += dt # only count the time after the first 10 steps
    print0(f"step {step:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | epoch: {current_epoch} | total time: {total_training_time/60:.2f}m")
    if step % 10 == 0:
        wandb_run.log({
            "step": step,
            "total_training_flops": flops_so_far,
            "total_training_time": total_training_time,
            "train/loss": debiased_smooth_loss,
            "train/lrm": lrm,
            "train/dt": dt,
            "train/tok_per_sec": tok_per_sec,
            "train/mfu": mfu,
            "train/epoch": current_epoch,
        })

# print a few more stats
print0(f"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB")
print0(f"Total training time: {total_training_time/60:.2f}m")
print0(f"Minimum validation bpb: {min_val_bpb:.4f}")

# Log to report
if not args.dry_run:
    from nanochat.report import get_report
    get_report().log(section="SFT", data=[
        user_config, # CLI args
        { # stats about the training setup
            "Number of iterations": step,
            "DDP world size": ddp_world_size,
        },
        { # stats about training outcomes
            "Minimum validation bpb": min_val_bpb,
        }
    ])

# cleanup
wandb_run.finish() # wandb run finish
compute_cleanup()


----------------------------------------
# triton_kernels.py
----------------------------------------

import torch
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    # Hardcoded configs based on H100 autotuning
    if K == 768:
        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 128, 64
        num_stages, num_warps = 4, 4
    else:
        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 64, 128, 128
        num_stages, num_warps = 4, 4

    grid = (batch_size * triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(M, BLOCK_SIZE_N),)
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=8,
        LOWER_UPPER=1,
        num_stages=num_stages,
        num_warps=num_warps,
    )
    return out

@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    # Hardcoded config based on H100 autotuning (M=768)
    BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 128, 64
    num_stages, num_warps = 4, 4

    grid = (batch_size * triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(M, BLOCK_SIZE_N),)
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=8,
        LOWER_UPPER=1,
        num_stages=num_stages,
        num_warps=num_warps,
    )
    return out

# -----------------------------------------------------------------------------
# Triton kernel for MLP: relu(x @ W1.T)^2, by @andrewbriand, @jrauvola

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy


@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n

    max_val = -float('inf')
    sum_exp = 0.0

    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max

    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)

    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)

    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    grad_s,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n

    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)

    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)

    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)

        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)

        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        grad_x = grad_x / grad_s
        grad_x = grad_x.to(tl.float8e5)
        tl.store(grad_row_ptr + cols, grad_x, mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, targets, mtp_weights, lm_head_weight, x_s, w_s, grad_s, A=23.0, B=5.0, C=7.5):

        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = lm_head_weight.div(w_s).to(torch.float8_e4m3fn)

        w_f8_col_major = w_f8.T.contiguous().T

        logits = torch._scaled_mm(
            x_f8,
            w_f8_col_major,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )

        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)

        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=2
        )

        ctx.save_for_backward(logits, targets, mtp_weights, lse, x, lm_head_weight, x_f8, w_f8)
        ctx.params = (A, B, C, x_s, w_s, grad_s)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse, x, lm_head_weight, x_f8, w_f8 = ctx.saved_tensors
        A, B, C, x_s, w_s, grad_s = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]

        grad_input = torch.empty((n_rows, n_cols), dtype=torch.float8_e5m2, device=logits.device)
        grad_output = grad_output.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            grad_s,
            BLOCK_SIZE=1024,
            num_warps=2
        )

        x_scale = grad_input.new_tensor(x_s, dtype=torch.float32)
        w_scale = grad_input.new_tensor(w_s, dtype=torch.float32)
        grad_scale = grad_input.new_tensor(grad_s, dtype=torch.float32)

        grad_x = torch._scaled_mm(
            grad_input,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=grad_scale,
            scale_b=w_scale,
            use_fast_accum=False,
        )

        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_input.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_scale,
            scale_b=grad_scale,
            use_fast_accum=False,
        )

        return grad_x, None, None, grad_w, None, None, None

====================================================================================================
Running Python 3.12.12 | packaged by conda-forge | (main, Jan 26 2026, 23:42:12) [GCC 14.3.0]
Running PyTorch 2.10.0+cu128 compiled for CUDA 12.8
Running Triton version 3.6.0
Thu Feb 12 06:37:25 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 480GB             On  |   00000000:DD:00.0 Off |                    0 |
| N/A   36C    P0            129W /  700W |    1208MiB /  97871MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           26163      C   .../envs/speedrun/bin/python3.12       1198MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 1319, 1320, 1321, 2639, 2640, 2641, 3959, 3960, 3961] for warmup
Resetting Model
step:0/4000 val_bpb:3.2312 train_time:0ms step_avg:0.05ms
step:1/4000 train_time:218ms step_avg:217.92ms
step:2/4000 train_time:429ms step_avg:214.67ms
step:3/4000 train_time:644ms step_avg:214.53ms
step:4/4000 train_time:859ms step_avg:214.78ms
step:5/4000 train_time:1073ms step_avg:214.61ms
step:6/4000 train_time:1288ms step_avg:214.75ms
step:7/4000 train_time:1505ms step_avg:214.99ms
step:8/4000 train_time:1720ms step_avg:215.02ms
step:9/4000 train_time:1936ms step_avg:215.07ms
step:10/4000 train_time:2152ms step_avg:215.15ms
step:11/4000 train_time:2367ms step_avg:215.14ms
step:12/4000 train_time:2583ms step_avg:215.25ms
step:13/4000 train_time:2798ms step_avg:215.25ms
step:14/4000 train_time:3014ms step_avg:215.28ms
step:15/4000 train_time:3231ms step_avg:215.38ms
step:16/4000 train_time:3446ms step_avg:215.39ms
step:17/4000 train_time:3663ms step_avg:215.44ms
step:18/4000 train_time:3879ms step_avg:215.49ms
step:19/4000 train_time:4094ms step_avg:215.47ms
step:20/4000 train_time:4309ms step_avg:215.47ms
step:21/4000 train_time:4526ms step_avg:215.50ms
step:22/4000 train_time:4742ms step_avg:215.55ms
step:23/4000 train_time:4957ms step_avg:215.50ms
step:24/4000 train_time:5173ms step_avg:215.54ms
step:25/4000 train_time:5389ms step_avg:215.56ms
step:26/4000 train_time:5605ms step_avg:215.59ms
step:27/4000 train_time:5822ms step_avg:215.62ms
step:28/4000 train_time:6038ms step_avg:215.65ms
step:29/4000 train_time:6253ms step_avg:215.61ms
step:30/4000 train_time:6468ms step_avg:215.61ms
step:31/4000 train_time:6684ms step_avg:215.60ms
step:32/4000 train_time:6899ms step_avg:215.58ms
step:33/4000 train_time:7115ms step_avg:215.62ms
step:34/4000 train_time:7332ms step_avg:215.64ms
step:35/4000 train_time:7547ms step_avg:215.64ms
step:36/4000 train_time:7763ms step_avg:215.64ms
step:37/4000 train_time:7978ms step_avg:215.63ms
step:38/4000 train_time:8193ms step_avg:215.62ms
step:39/4000 train_time:8409ms step_avg:215.61ms
step:40/4000 train_time:8625ms step_avg:215.63ms
step:41/4000 train_time:8841ms step_avg:215.65ms
step:42/4000 train_time:9056ms step_avg:215.62ms
step:43/4000 train_time:9272ms step_avg:215.62ms
step:44/4000 train_time:9488ms step_avg:215.63ms
step:45/4000 train_time:9703ms step_avg:215.63ms
step:46/4000 train_time:9919ms step_avg:215.62ms
step:47/4000 train_time:10134ms step_avg:215.62ms
step:48/4000 train_time:10350ms step_avg:215.63ms
step:49/4000 train_time:10565ms step_avg:215.62ms
step:50/4000 train_time:10782ms step_avg:215.63ms
step:51/4000 train_time:10996ms step_avg:215.61ms
step:52/4000 train_time:11213ms step_avg:215.63ms
step:53/4000 train_time:11428ms step_avg:215.62ms
step:54/4000 train_time:11643ms step_avg:215.61ms
step:55/4000 train_time:11858ms step_avg:215.61ms
step:56/4000 train_time:12073ms step_avg:215.60ms
step:57/4000 train_time:12289ms step_avg:215.61ms
step:58/4000 train_time:12506ms step_avg:215.62ms
step:59/4000 train_time:12721ms step_avg:215.60ms
step:60/4000 train_time:12936ms step_avg:215.60ms
step:61/4000 train_time:13152ms step_avg:215.61ms
step:62/4000 train_time:13368ms step_avg:215.61ms
step:63/4000 train_time:13584ms step_avg:215.62ms
step:64/4000 train_time:13798ms step_avg:215.60ms
step:65/4000 train_time:14015ms step_avg:215.61ms
step:66/4000 train_time:14229ms step_avg:215.60ms
step:67/4000 train_time:14446ms step_avg:215.61ms
step:68/4000 train_time:14661ms step_avg:215.61ms
step:69/4000 train_time:14877ms step_avg:215.61ms
step:70/4000 train_time:15094ms step_avg:215.63ms
step:71/4000 train_time:15309ms step_avg:215.62ms
step:72/4000 train_time:15524ms step_avg:215.61ms
step:73/4000 train_time:15738ms step_avg:215.59ms
step:74/4000 train_time:15953ms step_avg:215.59ms
step:75/4000 train_time:16167ms step_avg:215.57ms
step:76/4000 train_time:16382ms step_avg:215.56ms
step:77/4000 train_time:16598ms step_avg:215.56ms
step:78/4000 train_time:16813ms step_avg:215.55ms
step:79/4000 train_time:17029ms step_avg:215.56ms
step:80/4000 train_time:17245ms step_avg:215.56ms
step:81/4000 train_time:17460ms step_avg:215.55ms
step:82/4000 train_time:17673ms step_avg:215.53ms
step:83/4000 train_time:17888ms step_avg:215.52ms
step:84/4000 train_time:18101ms step_avg:215.49ms
step:85/4000 train_time:18316ms step_avg:215.48ms
step:86/4000 train_time:18530ms step_avg:215.46ms
step:87/4000 train_time:18746ms step_avg:215.47ms
step:88/4000 train_time:18960ms step_avg:215.46ms
step:89/4000 train_time:19174ms step_avg:215.44ms
step:90/4000 train_time:19389ms step_avg:215.43ms
step:91/4000 train_time:19604ms step_avg:215.43ms
step:92/4000 train_time:19819ms step_avg:215.43ms
step:93/4000 train_time:20034ms step_avg:215.42ms
step:94/4000 train_time:20249ms step_avg:215.42ms
step:95/4000 train_time:20465ms step_avg:215.42ms
step:96/4000 train_time:20681ms step_avg:215.43ms
step:97/4000 train_time:20896ms step_avg:215.42ms
step:98/4000 train_time:21109ms step_avg:215.40ms
step:99/4000 train_time:21324ms step_avg:215.40ms
step:100/4000 train_time:21539ms step_avg:215.39ms
step:101/4000 train_time:21754ms step_avg:215.39ms
step:102/4000 train_time:21970ms step_avg:215.39ms
step:103/4000 train_time:22186ms step_avg:215.39ms
step:104/4000 train_time:22399ms step_avg:215.37ms
step:105/4000 train_time:22614ms step_avg:215.37ms
step:106/4000 train_time:22828ms step_avg:215.36ms
step:107/4000 train_time:23045ms step_avg:215.37ms
step:108/4000 train_time:23259ms step_avg:215.36ms
step:109/4000 train_time:23473ms step_avg:215.35ms
step:110/4000 train_time:23687ms step_avg:215.34ms
step:111/4000 train_time:23901ms step_avg:215.32ms
step:112/4000 train_time:24117ms step_avg:215.33ms
step:113/4000 train_time:24331ms step_avg:215.31ms
step:114/4000 train_time:24546ms step_avg:215.32ms
step:115/4000 train_time:24760ms step_avg:215.31ms
step:116/4000 train_time:24975ms step_avg:215.30ms
step:117/4000 train_time:25190ms step_avg:215.30ms
step:118/4000 train_time:25405ms step_avg:215.29ms
step:119/4000 train_time:25620ms step_avg:215.29ms
step:120/4000 train_time:25836ms step_avg:215.30ms
step:121/4000 train_time:26052ms step_avg:215.30ms
step:122/4000 train_time:26268ms step_avg:215.31ms
step:123/4000 train_time:26483ms step_avg:215.31ms
step:124/4000 train_time:26698ms step_avg:215.31ms
step:125/4000 train_time:26912ms step_avg:215.29ms
step:126/4000 train_time:27128ms step_avg:215.30ms
step:127/4000 train_time:27342ms step_avg:215.29ms
step:128/4000 train_time:27559ms step_avg:215.30ms
step:129/4000 train_time:27773ms step_avg:215.29ms
step:130/4000 train_time:27989ms step_avg:215.30ms
step:131/4000 train_time:28203ms step_avg:215.29ms
step:132/4000 train_time:28419ms step_avg:215.29ms
step:133/4000 train_time:28633ms step_avg:215.29ms
step:134/4000 train_time:28849ms step_avg:215.29ms
step:135/4000 train_time:29065ms step_avg:215.29ms
step:136/4000 train_time:29280ms step_avg:215.30ms
step:137/4000 train_time:29495ms step_avg:215.29ms
step:138/4000 train_time:29710ms step_avg:215.29ms
step:139/4000 train_time:29925ms step_avg:215.28ms
step:140/4000 train_time:30140ms step_avg:215.28ms
step:141/4000 train_time:30355ms step_avg:215.28ms
step:142/4000 train_time:30571ms step_avg:215.29ms
step:143/4000 train_time:30786ms step_avg:215.29ms
step:144/4000 train_time:31003ms step_avg:215.30ms
step:145/4000 train_time:31218ms step_avg:215.30ms
step:146/4000 train_time:31432ms step_avg:215.29ms
step:147/4000 train_time:31648ms step_avg:215.29ms
step:148/4000 train_time:31862ms step_avg:215.29ms
step:149/4000 train_time:32078ms step_avg:215.29ms
step:150/4000 train_time:32293ms step_avg:215.28ms
step:151/4000 train_time:32508ms step_avg:215.28ms
step:152/4000 train_time:32723ms step_avg:215.28ms
step:153/4000 train_time:32938ms step_avg:215.28ms
step:154/4000 train_time:33153ms step_avg:215.28ms
step:155/4000 train_time:33369ms step_avg:215.28ms
step:156/4000 train_time:33583ms step_avg:215.28ms
step:157/4000 train_time:33798ms step_avg:215.27ms
step:158/4000 train_time:34014ms step_avg:215.28ms
step:159/4000 train_time:34229ms step_avg:215.27ms
step:160/4000 train_time:34443ms step_avg:215.27ms
step:161/4000 train_time:34658ms step_avg:215.27ms
step:162/4000 train_time:34873ms step_avg:215.26ms
step:163/4000 train_time:35088ms step_avg:215.26ms
step:164/4000 train_time:35302ms step_avg:215.26ms
step:165/4000 train_time:35517ms step_avg:215.26ms
step:166/4000 train_time:35733ms step_avg:215.26ms
step:167/4000 train_time:35947ms step_avg:215.25ms
step:168/4000 train_time:36163ms step_avg:215.26ms
step:169/4000 train_time:36378ms step_avg:215.26ms
step:170/4000 train_time:36594ms step_avg:215.26ms
step:171/4000 train_time:36809ms step_avg:215.26ms
step:172/4000 train_time:37024ms step_avg:215.26ms
step:173/4000 train_time:37241ms step_avg:215.27ms
step:174/4000 train_time:37456ms step_avg:215.26ms
step:175/4000 train_time:37672ms step_avg:215.27ms
step:176/4000 train_time:37887ms step_avg:215.27ms
step:177/4000 train_time:38103ms step_avg:215.27ms
step:178/4000 train_time:38318ms step_avg:215.27ms
step:179/4000 train_time:38532ms step_avg:215.26ms
step:180/4000 train_time:38747ms step_avg:215.26ms
step:181/4000 train_time:38963ms step_avg:215.27ms
step:182/4000 train_time:39178ms step_avg:215.26ms
step:183/4000 train_time:39393ms step_avg:215.26ms
step:184/4000 train_time:39608ms step_avg:215.26ms
step:185/4000 train_time:39822ms step_avg:215.26ms
step:186/4000 train_time:40037ms step_avg:215.26ms
step:187/4000 train_time:40254ms step_avg:215.26ms
step:188/4000 train_time:40470ms step_avg:215.26ms
step:189/4000 train_time:40686ms step_avg:215.27ms
step:190/4000 train_time:40900ms step_avg:215.26ms
step:191/4000 train_time:41116ms step_avg:215.26ms
step:192/4000 train_time:41331ms step_avg:215.27ms
step:193/4000 train_time:41546ms step_avg:215.27ms
step:194/4000 train_time:41763ms step_avg:215.27ms
step:195/4000 train_time:41977ms step_avg:215.27ms
step:196/4000 train_time:42192ms step_avg:215.26ms
step:197/4000 train_time:42407ms step_avg:215.26ms
step:198/4000 train_time:42622ms step_avg:215.26ms
step:199/4000 train_time:42837ms step_avg:215.26ms
step:200/4000 train_time:43052ms step_avg:215.26ms
step:201/4000 train_time:43267ms step_avg:215.26ms
step:202/4000 train_time:43482ms step_avg:215.26ms
step:203/4000 train_time:43698ms step_avg:215.26ms
step:204/4000 train_time:43913ms step_avg:215.26ms
step:205/4000 train_time:44128ms step_avg:215.26ms
step:206/4000 train_time:44343ms step_avg:215.26ms
step:207/4000 train_time:44559ms step_avg:215.26ms
step:208/4000 train_time:44774ms step_avg:215.26ms
step:209/4000 train_time:44990ms step_avg:215.26ms
step:210/4000 train_time:45206ms step_avg:215.26ms
step:211/4000 train_time:45421ms step_avg:215.26ms
step:212/4000 train_time:45636ms step_avg:215.26ms
step:213/4000 train_time:45851ms step_avg:215.26ms
step:214/4000 train_time:46067ms step_avg:215.27ms
step:215/4000 train_time:46282ms step_avg:215.26ms
step:216/4000 train_time:46497ms step_avg:215.27ms
step:217/4000 train_time:46713ms step_avg:215.27ms
step:218/4000 train_time:46928ms step_avg:215.27ms
step:219/4000 train_time:47145ms step_avg:215.27ms
step:220/4000 train_time:47359ms step_avg:215.27ms
step:221/4000 train_time:47574ms step_avg:215.27ms
step:222/4000 train_time:47790ms step_avg:215.27ms
step:223/4000 train_time:48006ms step_avg:215.27ms
step:224/4000 train_time:48220ms step_avg:215.27ms
step:225/4000 train_time:48435ms step_avg:215.27ms
step:226/4000 train_time:48651ms step_avg:215.27ms
step:227/4000 train_time:48866ms step_avg:215.27ms
step:228/4000 train_time:49081ms step_avg:215.27ms
step:229/4000 train_time:49296ms step_avg:215.27ms
step:230/4000 train_time:49512ms step_avg:215.27ms
step:231/4000 train_time:49727ms step_avg:215.27ms
step:232/4000 train_time:49943ms step_avg:215.27ms
step:233/4000 train_time:50159ms step_avg:215.27ms
step:234/4000 train_time:50374ms step_avg:215.27ms
step:235/4000 train_time:50588ms step_avg:215.27ms
step:236/4000 train_time:50804ms step_avg:215.27ms
step:237/4000 train_time:51022ms step_avg:215.28ms
step:238/4000 train_time:51237ms step_avg:215.28ms
step:239/4000 train_time:51452ms step_avg:215.28ms
step:240/4000 train_time:51667ms step_avg:215.28ms
step:241/4000 train_time:51883ms step_avg:215.28ms
step:242/4000 train_time:52099ms step_avg:215.28ms
step:243/4000 train_time:52314ms step_avg:215.29ms
step:244/4000 train_time:52530ms step_avg:215.29ms
step:245/4000 train_time:52745ms step_avg:215.29ms
step:246/4000 train_time:52960ms step_avg:215.29ms
step:247/4000 train_time:53175ms step_avg:215.28ms
step:248/4000 train_time:53391ms step_avg:215.29ms
step:249/4000 train_time:53606ms step_avg:215.29ms
step:250/4000 train_time:53822ms step_avg:215.29ms
step:250/4000 val_bpb:1.3614 train_time:53834ms step_avg:215.34ms
step:251/4000 train_time:54040ms step_avg:215.30ms
step:252/4000 train_time:54256ms step_avg:215.30ms
step:253/4000 train_time:54471ms step_avg:215.30ms
step:254/4000 train_time:54686ms step_avg:215.30ms
step:255/4000 train_time:54902ms step_avg:215.30ms
step:256/4000 train_time:55118ms step_avg:215.30ms
step:257/4000 train_time:55332ms step_avg:215.30ms
step:258/4000 train_time:55548ms step_avg:215.30ms
step:259/4000 train_time:55764ms step_avg:215.31ms
step:260/4000 train_time:55979ms step_avg:215.31ms
step:261/4000 train_time:56194ms step_avg:215.30ms
step:262/4000 train_time:56410ms step_avg:215.30ms
step:263/4000 train_time:56626ms step_avg:215.31ms
step:264/4000 train_time:56840ms step_avg:215.30ms
step:265/4000 train_time:57055ms step_avg:215.30ms
step:266/4000 train_time:57270ms step_avg:215.30ms
step:267/4000 train_time:57485ms step_avg:215.30ms
step:268/4000 train_time:57701ms step_avg:215.30ms
step:269/4000 train_time:57917ms step_avg:215.30ms
step:270/4000 train_time:58132ms step_avg:215.30ms
step:271/4000 train_time:58347ms step_avg:215.30ms
step:272/4000 train_time:58563ms step_avg:215.30ms
step:273/4000 train_time:58779ms step_avg:215.31ms
step:274/4000 train_time:58994ms step_avg:215.31ms
step:275/4000 train_time:59210ms step_avg:215.31ms
step:276/4000 train_time:59425ms step_avg:215.31ms
step:277/4000 train_time:59641ms step_avg:215.31ms
step:278/4000 train_time:59857ms step_avg:215.31ms
step:279/4000 train_time:60073ms step_avg:215.32ms
step:280/4000 train_time:60289ms step_avg:215.32ms
step:281/4000 train_time:60505ms step_avg:215.32ms
step:282/4000 train_time:60719ms step_avg:215.32ms
step:283/4000 train_time:60935ms step_avg:215.32ms
step:284/4000 train_time:61150ms step_avg:215.32ms
step:285/4000 train_time:61366ms step_avg:215.32ms
step:286/4000 train_time:61581ms step_avg:215.32ms
step:287/4000 train_time:61796ms step_avg:215.32ms
step:288/4000 train_time:62011ms step_avg:215.32ms
step:289/4000 train_time:62227ms step_avg:215.32ms
step:290/4000 train_time:62443ms step_avg:215.32ms
step:291/4000 train_time:62658ms step_avg:215.32ms
step:292/4000 train_time:62873ms step_avg:215.32ms
step:293/4000 train_time:63089ms step_avg:215.32ms
step:294/4000 train_time:63305ms step_avg:215.32ms
step:295/4000 train_time:63520ms step_avg:215.32ms
step:296/4000 train_time:63736ms step_avg:215.32ms
step:297/4000 train_time:63951ms step_avg:215.32ms
step:298/4000 train_time:64167ms step_avg:215.32ms
step:299/4000 train_time:64382ms step_avg:215.32ms
step:300/4000 train_time:64598ms step_avg:215.33ms
step:301/4000 train_time:64814ms step_avg:215.33ms
step:302/4000 train_time:65029ms step_avg:215.33ms
step:303/4000 train_time:65245ms step_avg:215.33ms
step:304/4000 train_time:65461ms step_avg:215.33ms
step:305/4000 train_time:65677ms step_avg:215.33ms
step:306/4000 train_time:65892ms step_avg:215.33ms
step:307/4000 train_time:66107ms step_avg:215.33ms
step:308/4000 train_time:66322ms step_avg:215.33ms
step:309/4000 train_time:66539ms step_avg:215.34ms
step:310/4000 train_time:66754ms step_avg:215.34ms
step:311/4000 train_time:66970ms step_avg:215.34ms
step:312/4000 train_time:67186ms step_avg:215.34ms
step:313/4000 train_time:67401ms step_avg:215.34ms
step:314/4000 train_time:67618ms step_avg:215.34ms
step:315/4000 train_time:67833ms step_avg:215.34ms
step:316/4000 train_time:68050ms step_avg:215.35ms
step:317/4000 train_time:68264ms step_avg:215.34ms
step:318/4000 train_time:68480ms step_avg:215.35ms
step:319/4000 train_time:68697ms step_avg:215.35ms
step:320/4000 train_time:68912ms step_avg:215.35ms
step:321/4000 train_time:69127ms step_avg:215.35ms
step:322/4000 train_time:69343ms step_avg:215.35ms
step:323/4000 train_time:69558ms step_avg:215.35ms
step:324/4000 train_time:69774ms step_avg:215.35ms
step:325/4000 train_time:69990ms step_avg:215.35ms
step:326/4000 train_time:70206ms step_avg:215.35ms
step:327/4000 train_time:70422ms step_avg:215.36ms
step:328/4000 train_time:70637ms step_avg:215.36ms
step:329/4000 train_time:70852ms step_avg:215.36ms
step:330/4000 train_time:71069ms step_avg:215.36ms
step:331/4000 train_time:71284ms step_avg:215.36ms
step:332/4000 train_time:71500ms step_avg:215.36ms
step:333/4000 train_time:71716ms step_avg:215.36ms
step:334/4000 train_time:71933ms step_avg:215.37ms
step:335/4000 train_time:72148ms step_avg:215.37ms
step:336/4000 train_time:72363ms step_avg:215.37ms
step:337/4000 train_time:72579ms step_avg:215.37ms
step:338/4000 train_time:72796ms step_avg:215.37ms
step:339/4000 train_time:73011ms step_avg:215.37ms
step:340/4000 train_time:73228ms step_avg:215.38ms
step:341/4000 train_time:73443ms step_avg:215.38ms
step:342/4000 train_time:73658ms step_avg:215.38ms
step:343/4000 train_time:73875ms step_avg:215.38ms
step:344/4000 train_time:74090ms step_avg:215.38ms
step:345/4000 train_time:74306ms step_avg:215.38ms
step:346/4000 train_time:74522ms step_avg:215.38ms
step:347/4000 train_time:74738ms step_avg:215.38ms
step:348/4000 train_time:74954ms step_avg:215.39ms
step:349/4000 train_time:75169ms step_avg:215.39ms
step:350/4000 train_time:75385ms step_avg:215.39ms
step:351/4000 train_time:75601ms step_avg:215.39ms
step:352/4000 train_time:75816ms step_avg:215.39ms
step:353/4000 train_time:76032ms step_avg:215.39ms
step:354/4000 train_time:76248ms step_avg:215.39ms
step:355/4000 train_time:76465ms step_avg:215.39ms
step:356/4000 train_time:76681ms step_avg:215.40ms
step:357/4000 train_time:76896ms step_avg:215.40ms
step:358/4000 train_time:77113ms step_avg:215.40ms
step:359/4000 train_time:77330ms step_avg:215.40ms
step:360/4000 train_time:77545ms step_avg:215.40ms
step:361/4000 train_time:77761ms step_avg:215.40ms
step:362/4000 train_time:77977ms step_avg:215.41ms
step:363/4000 train_time:78192ms step_avg:215.40ms
step:364/4000 train_time:78408ms step_avg:215.41ms
step:365/4000 train_time:78624ms step_avg:215.41ms
step:366/4000 train_time:78841ms step_avg:215.41ms
step:367/4000 train_time:79057ms step_avg:215.42ms
step:368/4000 train_time:79274ms step_avg:215.42ms
step:369/4000 train_time:79490ms step_avg:215.42ms
step:370/4000 train_time:79706ms step_avg:215.42ms
step:371/4000 train_time:79921ms step_avg:215.42ms
step:372/4000 train_time:80138ms step_avg:215.42ms
step:373/4000 train_time:80354ms step_avg:215.43ms
step:374/4000 train_time:80571ms step_avg:215.43ms
step:375/4000 train_time:80786ms step_avg:215.43ms
step:376/4000 train_time:81003ms step_avg:215.43ms
step:377/4000 train_time:81218ms step_avg:215.43ms
step:378/4000 train_time:81434ms step_avg:215.43ms
step:379/4000 train_time:81649ms step_avg:215.43ms
step:380/4000 train_time:81864ms step_avg:215.43ms
step:381/4000 train_time:82080ms step_avg:215.43ms
step:382/4000 train_time:82296ms step_avg:215.43ms
step:383/4000 train_time:82511ms step_avg:215.43ms
step:384/4000 train_time:82729ms step_avg:215.44ms
step:385/4000 train_time:82946ms step_avg:215.44ms
step:386/4000 train_time:83162ms step_avg:215.44ms
step:387/4000 train_time:83378ms step_avg:215.45ms
step:388/4000 train_time:83594ms step_avg:215.45ms
step:389/4000 train_time:83810ms step_avg:215.45ms
step:390/4000 train_time:84026ms step_avg:215.45ms
step:391/4000 train_time:84243ms step_avg:215.45ms
step:392/4000 train_time:84460ms step_avg:215.46ms
step:393/4000 train_time:84676ms step_avg:215.46ms
step:394/4000 train_time:84892ms step_avg:215.46ms
step:395/4000 train_time:85109ms step_avg:215.47ms
step:396/4000 train_time:85326ms step_avg:215.47ms
step:397/4000 train_time:85541ms step_avg:215.47ms
step:398/4000 train_time:85757ms step_avg:215.47ms
step:399/4000 train_time:85973ms step_avg:215.47ms
step:400/4000 train_time:86191ms step_avg:215.48ms
step:401/4000 train_time:86408ms step_avg:215.48ms
step:402/4000 train_time:86624ms step_avg:215.48ms
step:403/4000 train_time:86839ms step_avg:215.48ms
step:404/4000 train_time:87056ms step_avg:215.49ms
step:405/4000 train_time:87272ms step_avg:215.49ms
step:406/4000 train_time:87487ms step_avg:215.48ms
step:407/4000 train_time:87704ms step_avg:215.49ms
step:408/4000 train_time:87920ms step_avg:215.49ms
step:409/4000 train_time:88136ms step_avg:215.49ms
step:410/4000 train_time:88353ms step_avg:215.50ms
step:411/4000 train_time:88568ms step_avg:215.49ms
step:412/4000 train_time:88785ms step_avg:215.50ms
step:413/4000 train_time:89001ms step_avg:215.50ms
step:414/4000 train_time:89217ms step_avg:215.50ms
step:415/4000 train_time:89434ms step_avg:215.50ms
step:416/4000 train_time:89651ms step_avg:215.51ms
step:417/4000 train_time:89868ms step_avg:215.51ms
step:418/4000 train_time:90083ms step_avg:215.51ms
step:419/4000 train_time:90298ms step_avg:215.51ms
step:420/4000 train_time:90514ms step_avg:215.51ms
step:421/4000 train_time:90730ms step_avg:215.51ms
step:422/4000 train_time:90946ms step_avg:215.51ms
step:423/4000 train_time:91160ms step_avg:215.51ms
step:424/4000 train_time:91377ms step_avg:215.51ms
step:425/4000 train_time:91592ms step_avg:215.51ms
step:426/4000 train_time:91808ms step_avg:215.51ms
step:427/4000 train_time:92024ms step_avg:215.51ms
step:428/4000 train_time:92240ms step_avg:215.51ms
step:429/4000 train_time:92456ms step_avg:215.52ms
step:430/4000 train_time:92672ms step_avg:215.52ms
step:431/4000 train_time:92888ms step_avg:215.52ms
step:432/4000 train_time:93104ms step_avg:215.52ms
step:433/4000 train_time:93319ms step_avg:215.52ms
step:434/4000 train_time:93535ms step_avg:215.52ms
step:435/4000 train_time:93749ms step_avg:215.52ms
step:436/4000 train_time:93966ms step_avg:215.52ms
step:437/4000 train_time:94182ms step_avg:215.52ms
step:438/4000 train_time:94398ms step_avg:215.52ms
step:439/4000 train_time:94614ms step_avg:215.52ms
step:440/4000 train_time:94830ms step_avg:215.52ms
step:441/4000 train_time:95046ms step_avg:215.52ms
step:442/4000 train_time:95263ms step_avg:215.53ms
step:443/4000 train_time:95478ms step_avg:215.53ms
step:444/4000 train_time:95695ms step_avg:215.53ms
step:445/4000 train_time:95911ms step_avg:215.53ms
step:446/4000 train_time:96127ms step_avg:215.53ms
step:447/4000 train_time:96342ms step_avg:215.53ms
step:448/4000 train_time:96558ms step_avg:215.53ms
step:449/4000 train_time:96774ms step_avg:215.53ms
step:450/4000 train_time:96990ms step_avg:215.53ms
step:451/4000 train_time:97207ms step_avg:215.54ms
step:452/4000 train_time:97424ms step_avg:215.54ms
step:453/4000 train_time:97640ms step_avg:215.54ms
step:454/4000 train_time:97856ms step_avg:215.54ms
step:455/4000 train_time:98072ms step_avg:215.54ms
step:456/4000 train_time:98289ms step_avg:215.55ms
step:457/4000 train_time:98506ms step_avg:215.55ms
step:458/4000 train_time:98721ms step_avg:215.55ms
step:459/4000 train_time:98938ms step_avg:215.55ms
step:460/4000 train_time:99154ms step_avg:215.55ms
step:461/4000 train_time:99370ms step_avg:215.55ms
step:462/4000 train_time:99586ms step_avg:215.55ms
step:463/4000 train_time:99801ms step_avg:215.55ms
step:464/4000 train_time:100017ms step_avg:215.55ms
step:465/4000 train_time:100234ms step_avg:215.56ms
step:466/4000 train_time:100449ms step_avg:215.56ms
step:467/4000 train_time:100665ms step_avg:215.56ms
step:468/4000 train_time:100881ms step_avg:215.56ms
step:469/4000 train_time:101096ms step_avg:215.56ms
step:470/4000 train_time:101313ms step_avg:215.56ms
step:471/4000 train_time:101529ms step_avg:215.56ms
step:472/4000 train_time:101748ms step_avg:215.57ms
step:473/4000 train_time:101963ms step_avg:215.57ms
step:474/4000 train_time:102179ms step_avg:215.57ms
step:475/4000 train_time:102395ms step_avg:215.57ms
step:476/4000 train_time:102613ms step_avg:215.57ms
step:477/4000 train_time:102829ms step_avg:215.57ms
step:478/4000 train_time:103046ms step_avg:215.58ms
step:479/4000 train_time:103261ms step_avg:215.58ms
step:480/4000 train_time:103479ms step_avg:215.58ms
step:481/4000 train_time:103695ms step_avg:215.58ms
step:482/4000 train_time:103912ms step_avg:215.59ms
step:483/4000 train_time:104129ms step_avg:215.59ms
step:484/4000 train_time:104344ms step_avg:215.59ms
step:485/4000 train_time:104561ms step_avg:215.59ms
step:486/4000 train_time:104776ms step_avg:215.59ms
step:487/4000 train_time:104993ms step_avg:215.59ms
step:488/4000 train_time:105209ms step_avg:215.59ms
step:489/4000 train_time:105425ms step_avg:215.59ms
step:490/4000 train_time:105640ms step_avg:215.59ms
step:491/4000 train_time:105856ms step_avg:215.59ms
step:492/4000 train_time:106072ms step_avg:215.59ms
step:493/4000 train_time:106287ms step_avg:215.59ms
step:494/4000 train_time:106503ms step_avg:215.59ms
step:495/4000 train_time:106719ms step_avg:215.59ms
step:496/4000 train_time:106936ms step_avg:215.60ms
step:497/4000 train_time:107152ms step_avg:215.60ms
step:498/4000 train_time:107367ms step_avg:215.60ms
step:499/4000 train_time:107584ms step_avg:215.60ms
step:500/4000 train_time:107800ms step_avg:215.60ms
step:500/4000 val_bpb:1.2758 train_time:107813ms step_avg:215.63ms
step:501/4000 train_time:108019ms step_avg:215.61ms
step:502/4000 train_time:108235ms step_avg:215.61ms
step:503/4000 train_time:108451ms step_avg:215.61ms
step:504/4000 train_time:108666ms step_avg:215.61ms
step:505/4000 train_time:108883ms step_avg:215.61ms
step:506/4000 train_time:109101ms step_avg:215.61ms
step:507/4000 train_time:109317ms step_avg:215.62ms
step:508/4000 train_time:109534ms step_avg:215.62ms
step:509/4000 train_time:109750ms step_avg:215.62ms
step:510/4000 train_time:109967ms step_avg:215.62ms
step:511/4000 train_time:110184ms step_avg:215.62ms
step:512/4000 train_time:110400ms step_avg:215.63ms
step:513/4000 train_time:110615ms step_avg:215.62ms
step:514/4000 train_time:110832ms step_avg:215.63ms
step:515/4000 train_time:111048ms step_avg:215.63ms
step:516/4000 train_time:111264ms step_avg:215.63ms
step:517/4000 train_time:111481ms step_avg:215.63ms
step:518/4000 train_time:111698ms step_avg:215.63ms
step:519/4000 train_time:111915ms step_avg:215.64ms
step:520/4000 train_time:112131ms step_avg:215.64ms
step:521/4000 train_time:112347ms step_avg:215.64ms
step:522/4000 train_time:112563ms step_avg:215.64ms
step:523/4000 train_time:112780ms step_avg:215.64ms
step:524/4000 train_time:112996ms step_avg:215.64ms
step:525/4000 train_time:113212ms step_avg:215.64ms
step:526/4000 train_time:113429ms step_avg:215.64ms
step:527/4000 train_time:113644ms step_avg:215.64ms
step:528/4000 train_time:113860ms step_avg:215.64ms
step:529/4000 train_time:114076ms step_avg:215.64ms
step:530/4000 train_time:114292ms step_avg:215.65ms
step:531/4000 train_time:114508ms step_avg:215.65ms
step:532/4000 train_time:114724ms step_avg:215.65ms
step:533/4000 train_time:114943ms step_avg:215.65ms
step:534/4000 train_time:115160ms step_avg:215.66ms
step:535/4000 train_time:115376ms step_avg:215.66ms
step:536/4000 train_time:115593ms step_avg:215.66ms
step:537/4000 train_time:115809ms step_avg:215.66ms
step:538/4000 train_time:116025ms step_avg:215.66ms
step:539/4000 train_time:116242ms step_avg:215.66ms
step:540/4000 train_time:116459ms step_avg:215.67ms
step:541/4000 train_time:116676ms step_avg:215.67ms
step:542/4000 train_time:116892ms step_avg:215.67ms
step:543/4000 train_time:117108ms step_avg:215.67ms
step:544/4000 train_time:117325ms step_avg:215.67ms
step:545/4000 train_time:117540ms step_avg:215.67ms
step:546/4000 train_time:117757ms step_avg:215.67ms
step:547/4000 train_time:117972ms step_avg:215.67ms
step:548/4000 train_time:118188ms step_avg:215.67ms
step:549/4000 train_time:118403ms step_avg:215.67ms
step:550/4000 train_time:118620ms step_avg:215.67ms
step:551/4000 train_time:118836ms step_avg:215.67ms
step:552/4000 train_time:119052ms step_avg:215.67ms
step:553/4000 train_time:119269ms step_avg:215.68ms
step:554/4000 train_time:119486ms step_avg:215.68ms
step:555/4000 train_time:119703ms step_avg:215.68ms
step:556/4000 train_time:119920ms step_avg:215.68ms
step:557/4000 train_time:120137ms step_avg:215.69ms
step:558/4000 train_time:120354ms step_avg:215.69ms
step:559/4000 train_time:120570ms step_avg:215.69ms
step:560/4000 train_time:120786ms step_avg:215.69ms
step:561/4000 train_time:121003ms step_avg:215.69ms
step:562/4000 train_time:121220ms step_avg:215.69ms
step:563/4000 train_time:121436ms step_avg:215.69ms
step:564/4000 train_time:121652ms step_avg:215.70ms
step:565/4000 train_time:121871ms step_avg:215.70ms
step:566/4000 train_time:122088ms step_avg:215.70ms
step:567/4000 train_time:122304ms step_avg:215.70ms
step:568/4000 train_time:122520ms step_avg:215.70ms
step:569/4000 train_time:122737ms step_avg:215.71ms
step:570/4000 train_time:122955ms step_avg:215.71ms
step:571/4000 train_time:123172ms step_avg:215.71ms
step:572/4000 train_time:123390ms step_avg:215.72ms
step:573/4000 train_time:123606ms step_avg:215.72ms
step:574/4000 train_time:123824ms step_avg:215.72ms
step:575/4000 train_time:124040ms step_avg:215.72ms
step:576/4000 train_time:124257ms step_avg:215.72ms
step:577/4000 train_time:124474ms step_avg:215.73ms
step:578/4000 train_time:124689ms step_avg:215.73ms
step:579/4000 train_time:124906ms step_avg:215.73ms
step:580/4000 train_time:125123ms step_avg:215.73ms
step:581/4000 train_time:125340ms step_avg:215.73ms
step:582/4000 train_time:125555ms step_avg:215.73ms
step:583/4000 train_time:125771ms step_avg:215.73ms
step:584/4000 train_time:125988ms step_avg:215.73ms
step:585/4000 train_time:126204ms step_avg:215.73ms
step:586/4000 train_time:126421ms step_avg:215.74ms
step:587/4000 train_time:126639ms step_avg:215.74ms
step:588/4000 train_time:126855ms step_avg:215.74ms
step:589/4000 train_time:127072ms step_avg:215.74ms
step:590/4000 train_time:127289ms step_avg:215.74ms
step:591/4000 train_time:127506ms step_avg:215.75ms
step:592/4000 train_time:127740ms step_avg:215.78ms
step:593/4000 train_time:127956ms step_avg:215.78ms
step:594/4000 train_time:128173ms step_avg:215.78ms
step:595/4000 train_time:128390ms step_avg:215.78ms
step:596/4000 train_time:128607ms step_avg:215.78ms
step:597/4000 train_time:128824ms step_avg:215.78ms
step:598/4000 train_time:129040ms step_avg:215.79ms
step:599/4000 train_time:129256ms step_avg:215.79ms
step:600/4000 train_time:129472ms step_avg:215.79ms
step:601/4000 train_time:129691ms step_avg:215.79ms
step:602/4000 train_time:129908ms step_avg:215.79ms
step:603/4000 train_time:130125ms step_avg:215.80ms
step:604/4000 train_time:130341ms step_avg:215.80ms
step:605/4000 train_time:130557ms step_avg:215.80ms
step:606/4000 train_time:130772ms step_avg:215.80ms
step:607/4000 train_time:130990ms step_avg:215.80ms
step:608/4000 train_time:131206ms step_avg:215.80ms
step:609/4000 train_time:131423ms step_avg:215.80ms
step:610/4000 train_time:131641ms step_avg:215.80ms
step:611/4000 train_time:131857ms step_avg:215.81ms
step:612/4000 train_time:132075ms step_avg:215.81ms
step:613/4000 train_time:132291ms step_avg:215.81ms
step:614/4000 train_time:132507ms step_avg:215.81ms
step:615/4000 train_time:132725ms step_avg:215.81ms
step:616/4000 train_time:132941ms step_avg:215.81ms
step:617/4000 train_time:133159ms step_avg:215.82ms
step:618/4000 train_time:133373ms step_avg:215.81ms
step:619/4000 train_time:133589ms step_avg:215.81ms
step:620/4000 train_time:133804ms step_avg:215.81ms
step:621/4000 train_time:134020ms step_avg:215.81ms
step:622/4000 train_time:134235ms step_avg:215.81ms
step:623/4000 train_time:134450ms step_avg:215.81ms
step:624/4000 train_time:134666ms step_avg:215.81ms
step:625/4000 train_time:134881ms step_avg:215.81ms
step:626/4000 train_time:135097ms step_avg:215.81ms
step:627/4000 train_time:135312ms step_avg:215.81ms
step:628/4000 train_time:135527ms step_avg:215.81ms
step:629/4000 train_time:135742ms step_avg:215.81ms
step:630/4000 train_time:135958ms step_avg:215.81ms
step:631/4000 train_time:136173ms step_avg:215.81ms
step:632/4000 train_time:136388ms step_avg:215.80ms
step:633/4000 train_time:136603ms step_avg:215.80ms
step:634/4000 train_time:136819ms step_avg:215.80ms
step:635/4000 train_time:137033ms step_avg:215.80ms
step:636/4000 train_time:137248ms step_avg:215.80ms
step:637/4000 train_time:137465ms step_avg:215.80ms
step:638/4000 train_time:137680ms step_avg:215.80ms
step:639/4000 train_time:137896ms step_avg:215.80ms
step:640/4000 train_time:138110ms step_avg:215.80ms
step:641/4000 train_time:138325ms step_avg:215.80ms
step:642/4000 train_time:138542ms step_avg:215.80ms
step:643/4000 train_time:138757ms step_avg:215.80ms
step:644/4000 train_time:138973ms step_avg:215.80ms
step:645/4000 train_time:139189ms step_avg:215.80ms
step:646/4000 train_time:139404ms step_avg:215.80ms
step:647/4000 train_time:139619ms step_avg:215.79ms
step:648/4000 train_time:139835ms step_avg:215.79ms
step:649/4000 train_time:140050ms step_avg:215.79ms
step:650/4000 train_time:140266ms step_avg:215.79ms
step:651/4000 train_time:140481ms step_avg:215.79ms
step:652/4000 train_time:140696ms step_avg:215.79ms
step:653/4000 train_time:140912ms step_avg:215.79ms
step:654/4000 train_time:141127ms step_avg:215.79ms
step:655/4000 train_time:141342ms step_avg:215.79ms
step:656/4000 train_time:141559ms step_avg:215.79ms
step:657/4000 train_time:141773ms step_avg:215.79ms
step:658/4000 train_time:141990ms step_avg:215.79ms
step:659/4000 train_time:142205ms step_avg:215.79ms
step:660/4000 train_time:142420ms step_avg:215.79ms
step:661/4000 train_time:142636ms step_avg:215.79ms
step:662/4000 train_time:142851ms step_avg:215.79ms
step:663/4000 train_time:143067ms step_avg:215.79ms
step:664/4000 train_time:143283ms step_avg:215.79ms
step:665/4000 train_time:143499ms step_avg:215.79ms
step:666/4000 train_time:143714ms step_avg:215.79ms
step:667/4000 train_time:143929ms step_avg:215.79ms
step:668/4000 train_time:144146ms step_avg:215.79ms
step:669/4000 train_time:144360ms step_avg:215.78ms
step:670/4000 train_time:144575ms step_avg:215.78ms
step:671/4000 train_time:144790ms step_avg:215.78ms
step:672/4000 train_time:145005ms step_avg:215.78ms
step:673/4000 train_time:145221ms step_avg:215.78ms
step:674/4000 train_time:145437ms step_avg:215.78ms
step:675/4000 train_time:145652ms step_avg:215.78ms
step:676/4000 train_time:145868ms step_avg:215.78ms
step:677/4000 train_time:146083ms step_avg:215.78ms
step:678/4000 train_time:146299ms step_avg:215.78ms
step:679/4000 train_time:146515ms step_avg:215.78ms
step:680/4000 train_time:146730ms step_avg:215.78ms
step:681/4000 train_time:146945ms step_avg:215.78ms
step:682/4000 train_time:147160ms step_avg:215.78ms
step:683/4000 train_time:147376ms step_avg:215.78ms
step:684/4000 train_time:147591ms step_avg:215.78ms
step:685/4000 train_time:147806ms step_avg:215.78ms
step:686/4000 train_time:148021ms step_avg:215.77ms
step:687/4000 train_time:148237ms step_avg:215.77ms
step:688/4000 train_time:148452ms step_avg:215.77ms
step:689/4000 train_time:148667ms step_avg:215.77ms
step:690/4000 train_time:148883ms step_avg:215.77ms
step:691/4000 train_time:149098ms step_avg:215.77ms
step:692/4000 train_time:149314ms step_avg:215.77ms
step:693/4000 train_time:149530ms step_avg:215.77ms
step:694/4000 train_time:149745ms step_avg:215.77ms
step:695/4000 train_time:149960ms step_avg:215.77ms
step:696/4000 train_time:150175ms step_avg:215.77ms
step:697/4000 train_time:150391ms step_avg:215.77ms
step:698/4000 train_time:150606ms step_avg:215.77ms
step:699/4000 train_time:150822ms step_avg:215.77ms
step:700/4000 train_time:151036ms step_avg:215.77ms
step:701/4000 train_time:151251ms step_avg:215.76ms
step:702/4000 train_time:151466ms step_avg:215.76ms
step:703/4000 train_time:151680ms step_avg:215.76ms
step:704/4000 train_time:151896ms step_avg:215.76ms
step:705/4000 train_time:152111ms step_avg:215.76ms
step:706/4000 train_time:152326ms step_avg:215.76ms
step:707/4000 train_time:152542ms step_avg:215.76ms
step:708/4000 train_time:152758ms step_avg:215.76ms
step:709/4000 train_time:152973ms step_avg:215.76ms
step:710/4000 train_time:153189ms step_avg:215.76ms
step:711/4000 train_time:153403ms step_avg:215.76ms
step:712/4000 train_time:153618ms step_avg:215.76ms
step:713/4000 train_time:153833ms step_avg:215.75ms
step:714/4000 train_time:154048ms step_avg:215.75ms
step:715/4000 train_time:154262ms step_avg:215.75ms
step:716/4000 train_time:154479ms step_avg:215.75ms
step:717/4000 train_time:154694ms step_avg:215.75ms
step:718/4000 train_time:154910ms step_avg:215.75ms
step:719/4000 train_time:155125ms step_avg:215.75ms
step:720/4000 train_time:155341ms step_avg:215.75ms
step:721/4000 train_time:155556ms step_avg:215.75ms
step:722/4000 train_time:155771ms step_avg:215.75ms
step:723/4000 train_time:155986ms step_avg:215.75ms
step:724/4000 train_time:156201ms step_avg:215.75ms
step:725/4000 train_time:156416ms step_avg:215.75ms
step:726/4000 train_time:156632ms step_avg:215.75ms
step:727/4000 train_time:156847ms step_avg:215.75ms
step:728/4000 train_time:157063ms step_avg:215.75ms
step:729/4000 train_time:157279ms step_avg:215.75ms
step:730/4000 train_time:157494ms step_avg:215.75ms
step:731/4000 train_time:157709ms step_avg:215.74ms
step:732/4000 train_time:157925ms step_avg:215.74ms
step:733/4000 train_time:158141ms step_avg:215.74ms
step:734/4000 train_time:158357ms step_avg:215.74ms
step:735/4000 train_time:158573ms step_avg:215.75ms
step:736/4000 train_time:158788ms step_avg:215.75ms
step:737/4000 train_time:159003ms step_avg:215.74ms
step:738/4000 train_time:159219ms step_avg:215.74ms
step:739/4000 train_time:159434ms step_avg:215.74ms
step:740/4000 train_time:159648ms step_avg:215.74ms
step:741/4000 train_time:159862ms step_avg:215.74ms
step:742/4000 train_time:160078ms step_avg:215.74ms
step:743/4000 train_time:160294ms step_avg:215.74ms
step:744/4000 train_time:160509ms step_avg:215.74ms
step:745/4000 train_time:160725ms step_avg:215.74ms
step:746/4000 train_time:160940ms step_avg:215.74ms
step:747/4000 train_time:161156ms step_avg:215.74ms
step:748/4000 train_time:161371ms step_avg:215.74ms
step:749/4000 train_time:161586ms step_avg:215.74ms
step:750/4000 train_time:161801ms step_avg:215.73ms
step:750/4000 val_bpb:1.2361 train_time:161813ms step_avg:215.75ms
step:751/4000 train_time:162020ms step_avg:215.74ms
step:752/4000 train_time:162234ms step_avg:215.74ms
step:753/4000 train_time:162449ms step_avg:215.74ms
step:754/4000 train_time:162664ms step_avg:215.73ms
step:755/4000 train_time:162880ms step_avg:215.73ms
step:756/4000 train_time:163095ms step_avg:215.73ms
step:757/4000 train_time:163311ms step_avg:215.73ms
step:758/4000 train_time:163527ms step_avg:215.74ms
step:759/4000 train_time:163742ms step_avg:215.73ms
step:760/4000 train_time:163958ms step_avg:215.73ms
step:761/4000 train_time:164174ms step_avg:215.73ms
step:762/4000 train_time:164389ms step_avg:215.73ms
step:763/4000 train_time:164604ms step_avg:215.73ms
step:764/4000 train_time:164820ms step_avg:215.73ms
step:765/4000 train_time:165035ms step_avg:215.73ms
step:766/4000 train_time:165251ms step_avg:215.73ms
step:767/4000 train_time:165467ms step_avg:215.73ms
step:768/4000 train_time:165683ms step_avg:215.73ms
step:769/4000 train_time:165897ms step_avg:215.73ms
step:770/4000 train_time:166112ms step_avg:215.73ms
step:771/4000 train_time:166328ms step_avg:215.73ms
step:772/4000 train_time:166543ms step_avg:215.73ms
step:773/4000 train_time:166758ms step_avg:215.73ms
step:774/4000 train_time:166973ms step_avg:215.73ms
step:775/4000 train_time:167189ms step_avg:215.73ms
step:776/4000 train_time:167404ms step_avg:215.73ms
step:777/4000 train_time:167620ms step_avg:215.73ms
step:778/4000 train_time:167835ms step_avg:215.73ms
step:779/4000 train_time:168051ms step_avg:215.73ms
step:780/4000 train_time:168266ms step_avg:215.73ms
step:781/4000 train_time:168481ms step_avg:215.72ms
step:782/4000 train_time:168696ms step_avg:215.72ms
step:783/4000 train_time:168911ms step_avg:215.72ms
step:784/4000 train_time:169127ms step_avg:215.72ms
step:785/4000 train_time:169343ms step_avg:215.72ms
step:786/4000 train_time:169559ms step_avg:215.72ms
step:787/4000 train_time:169774ms step_avg:215.72ms
step:788/4000 train_time:169989ms step_avg:215.72ms
step:789/4000 train_time:170204ms step_avg:215.72ms
step:790/4000 train_time:170420ms step_avg:215.72ms
step:791/4000 train_time:170634ms step_avg:215.72ms
step:792/4000 train_time:170850ms step_avg:215.72ms
step:793/4000 train_time:171065ms step_avg:215.72ms
step:794/4000 train_time:171281ms step_avg:215.72ms
step:795/4000 train_time:171496ms step_avg:215.72ms
step:796/4000 train_time:171712ms step_avg:215.72ms
step:797/4000 train_time:171926ms step_avg:215.72ms
step:798/4000 train_time:172141ms step_avg:215.72ms
step:799/4000 train_time:172357ms step_avg:215.72ms
step:800/4000 train_time:172571ms step_avg:215.71ms
step:801/4000 train_time:172786ms step_avg:215.71ms
step:802/4000 train_time:173002ms step_avg:215.71ms
step:803/4000 train_time:173218ms step_avg:215.71ms
step:804/4000 train_time:173433ms step_avg:215.71ms
step:805/4000 train_time:173648ms step_avg:215.71ms
step:806/4000 train_time:173865ms step_avg:215.71ms
step:807/4000 train_time:174080ms step_avg:215.71ms
step:808/4000 train_time:174296ms step_avg:215.71ms
step:809/4000 train_time:174512ms step_avg:215.71ms
step:810/4000 train_time:174727ms step_avg:215.71ms
step:811/4000 train_time:174943ms step_avg:215.71ms
step:812/4000 train_time:175158ms step_avg:215.71ms
step:813/4000 train_time:175374ms step_avg:215.71ms
step:814/4000 train_time:175590ms step_avg:215.71ms
step:815/4000 train_time:175805ms step_avg:215.71ms
step:816/4000 train_time:176021ms step_avg:215.71ms
step:817/4000 train_time:176235ms step_avg:215.71ms
step:818/4000 train_time:176451ms step_avg:215.71ms
step:819/4000 train_time:176667ms step_avg:215.71ms
step:820/4000 train_time:176883ms step_avg:215.71ms
step:821/4000 train_time:177097ms step_avg:215.71ms
step:822/4000 train_time:177313ms step_avg:215.71ms
step:823/4000 train_time:177529ms step_avg:215.71ms
step:824/4000 train_time:177744ms step_avg:215.71ms
step:825/4000 train_time:177960ms step_avg:215.71ms
step:826/4000 train_time:178176ms step_avg:215.71ms
step:827/4000 train_time:178391ms step_avg:215.71ms
step:828/4000 train_time:178607ms step_avg:215.71ms
step:829/4000 train_time:178823ms step_avg:215.71ms
step:830/4000 train_time:179038ms step_avg:215.71ms
step:831/4000 train_time:179254ms step_avg:215.71ms
step:832/4000 train_time:179469ms step_avg:215.71ms
step:833/4000 train_time:179684ms step_avg:215.71ms
step:834/4000 train_time:179899ms step_avg:215.71ms
step:835/4000 train_time:180115ms step_avg:215.71ms
step:836/4000 train_time:180331ms step_avg:215.71ms
step:837/4000 train_time:180546ms step_avg:215.71ms
step:838/4000 train_time:180762ms step_avg:215.71ms
step:839/4000 train_time:180976ms step_avg:215.70ms
step:840/4000 train_time:181193ms step_avg:215.71ms
step:841/4000 train_time:181407ms step_avg:215.70ms
step:842/4000 train_time:181623ms step_avg:215.70ms
step:843/4000 train_time:181838ms step_avg:215.70ms
step:844/4000 train_time:182054ms step_avg:215.70ms
step:845/4000 train_time:182270ms step_avg:215.70ms
step:846/4000 train_time:182485ms step_avg:215.70ms
step:847/4000 train_time:182701ms step_avg:215.70ms
step:848/4000 train_time:182915ms step_avg:215.70ms
step:849/4000 train_time:183130ms step_avg:215.70ms
step:850/4000 train_time:183346ms step_avg:215.70ms
step:851/4000 train_time:183561ms step_avg:215.70ms
step:852/4000 train_time:183776ms step_avg:215.70ms
step:853/4000 train_time:183991ms step_avg:215.70ms
step:854/4000 train_time:184207ms step_avg:215.70ms
step:855/4000 train_time:184422ms step_avg:215.70ms
step:856/4000 train_time:184637ms step_avg:215.70ms
step:857/4000 train_time:184853ms step_avg:215.70ms
step:858/4000 train_time:185069ms step_avg:215.70ms
step:859/4000 train_time:185285ms step_avg:215.70ms
step:860/4000 train_time:185500ms step_avg:215.70ms
step:861/4000 train_time:185716ms step_avg:215.70ms
step:862/4000 train_time:185932ms step_avg:215.70ms
step:863/4000 train_time:186147ms step_avg:215.70ms
step:864/4000 train_time:186363ms step_avg:215.70ms
step:865/4000 train_time:186578ms step_avg:215.70ms
step:866/4000 train_time:186794ms step_avg:215.70ms
step:867/4000 train_time:187009ms step_avg:215.70ms
step:868/4000 train_time:187225ms step_avg:215.70ms
step:869/4000 train_time:187440ms step_avg:215.70ms
step:870/4000 train_time:187655ms step_avg:215.70ms
step:871/4000 train_time:187870ms step_avg:215.69ms
step:872/4000 train_time:188086ms step_avg:215.70ms
step:873/4000 train_time:188302ms step_avg:215.70ms
step:874/4000 train_time:188518ms step_avg:215.70ms
step:875/4000 train_time:188733ms step_avg:215.69ms
step:876/4000 train_time:188950ms step_avg:215.70ms
step:877/4000 train_time:189166ms step_avg:215.70ms
step:878/4000 train_time:189381ms step_avg:215.70ms
step:879/4000 train_time:189595ms step_avg:215.69ms
step:880/4000 train_time:189811ms step_avg:215.69ms
step:881/4000 train_time:190026ms step_avg:215.69ms
step:882/4000 train_time:190242ms step_avg:215.69ms
step:883/4000 train_time:190457ms step_avg:215.69ms
step:884/4000 train_time:190672ms step_avg:215.69ms
step:885/4000 train_time:190888ms step_avg:215.69ms
step:886/4000 train_time:191104ms step_avg:215.69ms
step:887/4000 train_time:191320ms step_avg:215.69ms
step:888/4000 train_time:191534ms step_avg:215.69ms
step:889/4000 train_time:191749ms step_avg:215.69ms
step:890/4000 train_time:191964ms step_avg:215.69ms
step:891/4000 train_time:192180ms step_avg:215.69ms
step:892/4000 train_time:192397ms step_avg:215.69ms
step:893/4000 train_time:192613ms step_avg:215.69ms
step:894/4000 train_time:192829ms step_avg:215.69ms
step:895/4000 train_time:193045ms step_avg:215.69ms
step:896/4000 train_time:193260ms step_avg:215.69ms
step:897/4000 train_time:193475ms step_avg:215.69ms
step:898/4000 train_time:193691ms step_avg:215.69ms
step:899/4000 train_time:193906ms step_avg:215.69ms
step:900/4000 train_time:194121ms step_avg:215.69ms
step:901/4000 train_time:194338ms step_avg:215.69ms
step:902/4000 train_time:194555ms step_avg:215.69ms
step:903/4000 train_time:194770ms step_avg:215.69ms
step:904/4000 train_time:194985ms step_avg:215.69ms
step:905/4000 train_time:195200ms step_avg:215.69ms
step:906/4000 train_time:195417ms step_avg:215.69ms
step:907/4000 train_time:195631ms step_avg:215.69ms
step:908/4000 train_time:195847ms step_avg:215.69ms
step:909/4000 train_time:196063ms step_avg:215.69ms
step:910/4000 train_time:196277ms step_avg:215.69ms
step:911/4000 train_time:196493ms step_avg:215.69ms
step:912/4000 train_time:196708ms step_avg:215.69ms
step:913/4000 train_time:196923ms step_avg:215.69ms
step:914/4000 train_time:197139ms step_avg:215.69ms
step:915/4000 train_time:197355ms step_avg:215.69ms
step:916/4000 train_time:197571ms step_avg:215.69ms
step:917/4000 train_time:197787ms step_avg:215.69ms
step:918/4000 train_time:198001ms step_avg:215.69ms
step:919/4000 train_time:198217ms step_avg:215.69ms
step:920/4000 train_time:198433ms step_avg:215.69ms
step:921/4000 train_time:198649ms step_avg:215.69ms
step:922/4000 train_time:198865ms step_avg:215.69ms
step:923/4000 train_time:199080ms step_avg:215.69ms
step:924/4000 train_time:199295ms step_avg:215.69ms
step:925/4000 train_time:199511ms step_avg:215.69ms
step:926/4000 train_time:199726ms step_avg:215.69ms
step:927/4000 train_time:199942ms step_avg:215.69ms
step:928/4000 train_time:200157ms step_avg:215.69ms
step:929/4000 train_time:200372ms step_avg:215.69ms
step:930/4000 train_time:200587ms step_avg:215.68ms
step:931/4000 train_time:200803ms step_avg:215.69ms
step:932/4000 train_time:201018ms step_avg:215.68ms
step:933/4000 train_time:201233ms step_avg:215.68ms
step:934/4000 train_time:201448ms step_avg:215.68ms
step:935/4000 train_time:201663ms step_avg:215.68ms
step:936/4000 train_time:201879ms step_avg:215.68ms
step:937/4000 train_time:202094ms step_avg:215.68ms
step:938/4000 train_time:202310ms step_avg:215.68ms
step:939/4000 train_time:202524ms step_avg:215.68ms
step:940/4000 train_time:202741ms step_avg:215.68ms
step:941/4000 train_time:202956ms step_avg:215.68ms
step:942/4000 train_time:203172ms step_avg:215.68ms
step:943/4000 train_time:203387ms step_avg:215.68ms
step:944/4000 train_time:203603ms step_avg:215.68ms
step:945/4000 train_time:203819ms step_avg:215.68ms
step:946/4000 train_time:204036ms step_avg:215.68ms
step:947/4000 train_time:204251ms step_avg:215.68ms
step:948/4000 train_time:204467ms step_avg:215.68ms
step:949/4000 train_time:204681ms step_avg:215.68ms
step:950/4000 train_time:204897ms step_avg:215.68ms
step:951/4000 train_time:205113ms step_avg:215.68ms
step:952/4000 train_time:205329ms step_avg:215.68ms
step:953/4000 train_time:205545ms step_avg:215.68ms
step:954/4000 train_time:205760ms step_avg:215.68ms
step:955/4000 train_time:205976ms step_avg:215.68ms
step:956/4000 train_time:206191ms step_avg:215.68ms
step:957/4000 train_time:206406ms step_avg:215.68ms
step:958/4000 train_time:206621ms step_avg:215.68ms
step:959/4000 train_time:206837ms step_avg:215.68ms
step:960/4000 train_time:207053ms step_avg:215.68ms
step:961/4000 train_time:207268ms step_avg:215.68ms
step:962/4000 train_time:207484ms step_avg:215.68ms
step:963/4000 train_time:207700ms step_avg:215.68ms
step:964/4000 train_time:207916ms step_avg:215.68ms
step:965/4000 train_time:208131ms step_avg:215.68ms
step:966/4000 train_time:208347ms step_avg:215.68ms
step:967/4000 train_time:208562ms step_avg:215.68ms
step:968/4000 train_time:208778ms step_avg:215.68ms
step:969/4000 train_time:208993ms step_avg:215.68ms
step:970/4000 train_time:209208ms step_avg:215.68ms
step:971/4000 train_time:209425ms step_avg:215.68ms
step:972/4000 train_time:209640ms step_avg:215.68ms
step:973/4000 train_time:209856ms step_avg:215.68ms
step:974/4000 train_time:210072ms step_avg:215.68ms
step:975/4000 train_time:210288ms step_avg:215.68ms
step:976/4000 train_time:210503ms step_avg:215.68ms
step:977/4000 train_time:210718ms step_avg:215.68ms
step:978/4000 train_time:210934ms step_avg:215.68ms
step:979/4000 train_time:211149ms step_avg:215.68ms
step:980/4000 train_time:211365ms step_avg:215.68ms
step:981/4000 train_time:211580ms step_avg:215.68ms
step:982/4000 train_time:211796ms step_avg:215.68ms
step:983/4000 train_time:212011ms step_avg:215.68ms
step:984/4000 train_time:212227ms step_avg:215.68ms
step:985/4000 train_time:212444ms step_avg:215.68ms
step:986/4000 train_time:212660ms step_avg:215.68ms
step:987/4000 train_time:212875ms step_avg:215.68ms
step:988/4000 train_time:213090ms step_avg:215.68ms
step:989/4000 train_time:213306ms step_avg:215.68ms
step:990/4000 train_time:213522ms step_avg:215.68ms
step:991/4000 train_time:213737ms step_avg:215.68ms
step:992/4000 train_time:213953ms step_avg:215.68ms
step:993/4000 train_time:214168ms step_avg:215.68ms
step:994/4000 train_time:214384ms step_avg:215.68ms
step:995/4000 train_time:214599ms step_avg:215.68ms
step:996/4000 train_time:214814ms step_avg:215.68ms
step:997/4000 train_time:215030ms step_avg:215.68ms
step:998/4000 train_time:215245ms step_avg:215.68ms
step:999/4000 train_time:215461ms step_avg:215.68ms
step:1000/4000 train_time:215676ms step_avg:215.68ms
step:1000/4000 val_bpb:1.2053 train_time:215689ms step_avg:215.69ms
step:1001/4000 train_time:215895ms step_avg:215.68ms
step:1002/4000 train_time:216110ms step_avg:215.68ms
step:1003/4000 train_time:216326ms step_avg:215.68ms
step:1004/4000 train_time:216541ms step_avg:215.68ms
step:1005/4000 train_time:216757ms step_avg:215.68ms
step:1006/4000 train_time:216972ms step_avg:215.68ms
step:1007/4000 train_time:217188ms step_avg:215.68ms
step:1008/4000 train_time:217403ms step_avg:215.68ms
step:1009/4000 train_time:217618ms step_avg:215.68ms
step:1010/4000 train_time:217834ms step_avg:215.68ms
step:1011/4000 train_time:218049ms step_avg:215.68ms
step:1012/4000 train_time:218265ms step_avg:215.68ms
step:1013/4000 train_time:218480ms step_avg:215.68ms
step:1014/4000 train_time:218696ms step_avg:215.68ms
step:1015/4000 train_time:218911ms step_avg:215.68ms
step:1016/4000 train_time:219126ms step_avg:215.68ms
step:1017/4000 train_time:219343ms step_avg:215.68ms
step:1018/4000 train_time:219558ms step_avg:215.68ms
step:1019/4000 train_time:219773ms step_avg:215.68ms
step:1020/4000 train_time:219989ms step_avg:215.68ms
step:1021/4000 train_time:220204ms step_avg:215.67ms
step:1022/4000 train_time:220419ms step_avg:215.67ms
step:1023/4000 train_time:220635ms step_avg:215.67ms
step:1024/4000 train_time:220850ms step_avg:215.67ms
step:1025/4000 train_time:221066ms step_avg:215.67ms
step:1026/4000 train_time:221282ms step_avg:215.67ms
step:1027/4000 train_time:221497ms step_avg:215.67ms
step:1028/4000 train_time:221713ms step_avg:215.67ms
step:1029/4000 train_time:221927ms step_avg:215.67ms
step:1030/4000 train_time:222143ms step_avg:215.67ms
step:1031/4000 train_time:222359ms step_avg:215.67ms
step:1032/4000 train_time:222574ms step_avg:215.67ms
step:1033/4000 train_time:222790ms step_avg:215.67ms
step:1034/4000 train_time:223006ms step_avg:215.67ms
step:1035/4000 train_time:223222ms step_avg:215.67ms
step:1036/4000 train_time:223439ms step_avg:215.67ms
step:1037/4000 train_time:223654ms step_avg:215.67ms
step:1038/4000 train_time:223870ms step_avg:215.67ms
step:1039/4000 train_time:224085ms step_avg:215.67ms
step:1040/4000 train_time:224301ms step_avg:215.67ms
step:1041/4000 train_time:224517ms step_avg:215.67ms
step:1042/4000 train_time:224733ms step_avg:215.67ms
step:1043/4000 train_time:224948ms step_avg:215.67ms
step:1044/4000 train_time:225164ms step_avg:215.67ms
step:1045/4000 train_time:225379ms step_avg:215.67ms
step:1046/4000 train_time:225594ms step_avg:215.67ms
step:1047/4000 train_time:225809ms step_avg:215.67ms
step:1048/4000 train_time:226026ms step_avg:215.67ms
step:1049/4000 train_time:226240ms step_avg:215.67ms
step:1050/4000 train_time:226457ms step_avg:215.67ms
step:1051/4000 train_time:226673ms step_avg:215.67ms
step:1052/4000 train_time:226888ms step_avg:215.67ms
step:1053/4000 train_time:227104ms step_avg:215.67ms
step:1054/4000 train_time:227320ms step_avg:215.67ms
step:1055/4000 train_time:227535ms step_avg:215.67ms
step:1056/4000 train_time:227751ms step_avg:215.67ms
step:1057/4000 train_time:227966ms step_avg:215.67ms
step:1058/4000 train_time:228182ms step_avg:215.67ms
step:1059/4000 train_time:228398ms step_avg:215.67ms
step:1060/4000 train_time:228615ms step_avg:215.67ms
step:1061/4000 train_time:228828ms step_avg:215.67ms
step:1062/4000 train_time:229043ms step_avg:215.67ms
step:1063/4000 train_time:229259ms step_avg:215.67ms
step:1064/4000 train_time:229475ms step_avg:215.67ms
step:1065/4000 train_time:229691ms step_avg:215.67ms
step:1066/4000 train_time:229908ms step_avg:215.67ms
step:1067/4000 train_time:230122ms step_avg:215.67ms
step:1068/4000 train_time:230336ms step_avg:215.67ms
step:1069/4000 train_time:230553ms step_avg:215.67ms
step:1070/4000 train_time:230768ms step_avg:215.67ms
step:1071/4000 train_time:230984ms step_avg:215.67ms
step:1072/4000 train_time:231199ms step_avg:215.67ms
step:1073/4000 train_time:231414ms step_avg:215.67ms
step:1074/4000 train_time:231632ms step_avg:215.67ms
step:1075/4000 train_time:231847ms step_avg:215.67ms
step:1076/4000 train_time:232062ms step_avg:215.67ms
step:1077/4000 train_time:232278ms step_avg:215.67ms
step:1078/4000 train_time:232493ms step_avg:215.67ms
step:1079/4000 train_time:232709ms step_avg:215.67ms
step:1080/4000 train_time:232925ms step_avg:215.67ms
step:1081/4000 train_time:233140ms step_avg:215.67ms
step:1082/4000 train_time:233356ms step_avg:215.67ms
step:1083/4000 train_time:233571ms step_avg:215.67ms
step:1084/4000 train_time:233786ms step_avg:215.67ms
step:1085/4000 train_time:234002ms step_avg:215.67ms
step:1086/4000 train_time:234217ms step_avg:215.67ms
step:1087/4000 train_time:234433ms step_avg:215.67ms
step:1088/4000 train_time:234649ms step_avg:215.67ms
step:1089/4000 train_time:234865ms step_avg:215.67ms
step:1090/4000 train_time:235080ms step_avg:215.67ms
step:1091/4000 train_time:235295ms step_avg:215.67ms
step:1092/4000 train_time:235511ms step_avg:215.67ms
step:1093/4000 train_time:235727ms step_avg:215.67ms
step:1094/4000 train_time:235942ms step_avg:215.67ms
step:1095/4000 train_time:236158ms step_avg:215.67ms
step:1096/4000 train_time:236373ms step_avg:215.67ms
step:1097/4000 train_time:236589ms step_avg:215.67ms
step:1098/4000 train_time:236804ms step_avg:215.67ms
step:1099/4000 train_time:237020ms step_avg:215.67ms
step:1100/4000 train_time:237235ms step_avg:215.67ms
step:1101/4000 train_time:237451ms step_avg:215.67ms
step:1102/4000 train_time:237666ms step_avg:215.67ms
step:1103/4000 train_time:237882ms step_avg:215.67ms
step:1104/4000 train_time:238098ms step_avg:215.67ms
step:1105/4000 train_time:238312ms step_avg:215.67ms
step:1106/4000 train_time:238528ms step_avg:215.67ms
step:1107/4000 train_time:238743ms step_avg:215.67ms
step:1108/4000 train_time:238960ms step_avg:215.67ms
step:1109/4000 train_time:239175ms step_avg:215.67ms
step:1110/4000 train_time:239391ms step_avg:215.67ms
step:1111/4000 train_time:239607ms step_avg:215.67ms
step:1112/4000 train_time:239821ms step_avg:215.67ms
step:1113/4000 train_time:240037ms step_avg:215.67ms
step:1114/4000 train_time:240252ms step_avg:215.67ms
step:1115/4000 train_time:240466ms step_avg:215.66ms
step:1116/4000 train_time:240682ms step_avg:215.67ms
step:1117/4000 train_time:240898ms step_avg:215.67ms
step:1118/4000 train_time:241114ms step_avg:215.67ms
step:1119/4000 train_time:241328ms step_avg:215.66ms
step:1120/4000 train_time:241544ms step_avg:215.66ms
step:1121/4000 train_time:241759ms step_avg:215.66ms
step:1122/4000 train_time:241975ms step_avg:215.66ms
step:1123/4000 train_time:242192ms step_avg:215.66ms
step:1124/4000 train_time:242407ms step_avg:215.66ms
step:1125/4000 train_time:242623ms step_avg:215.66ms
step:1126/4000 train_time:242838ms step_avg:215.66ms
step:1127/4000 train_time:243054ms step_avg:215.66ms
step:1128/4000 train_time:243270ms step_avg:215.66ms
step:1129/4000 train_time:243484ms step_avg:215.66ms
step:1130/4000 train_time:243700ms step_avg:215.66ms
step:1131/4000 train_time:243916ms step_avg:215.66ms
step:1132/4000 train_time:244131ms step_avg:215.66ms
step:1133/4000 train_time:244347ms step_avg:215.66ms
step:1134/4000 train_time:244563ms step_avg:215.66ms
step:1135/4000 train_time:244780ms step_avg:215.66ms
step:1136/4000 train_time:244995ms step_avg:215.66ms
step:1137/4000 train_time:245211ms step_avg:215.67ms
step:1138/4000 train_time:245427ms step_avg:215.66ms
step:1139/4000 train_time:245642ms step_avg:215.66ms
step:1140/4000 train_time:245858ms step_avg:215.66ms
step:1141/4000 train_time:246074ms step_avg:215.67ms
step:1142/4000 train_time:246289ms step_avg:215.66ms
step:1143/4000 train_time:246505ms step_avg:215.66ms
step:1144/4000 train_time:246720ms step_avg:215.66ms
step:1145/4000 train_time:246936ms step_avg:215.66ms
step:1146/4000 train_time:247153ms step_avg:215.67ms
step:1147/4000 train_time:247369ms step_avg:215.67ms
step:1148/4000 train_time:247584ms step_avg:215.67ms
step:1149/4000 train_time:247800ms step_avg:215.67ms
step:1150/4000 train_time:248015ms step_avg:215.67ms
step:1151/4000 train_time:248231ms step_avg:215.67ms
step:1152/4000 train_time:248447ms step_avg:215.67ms
step:1153/4000 train_time:248662ms step_avg:215.67ms
step:1154/4000 train_time:248878ms step_avg:215.67ms
step:1155/4000 train_time:249094ms step_avg:215.67ms
step:1156/4000 train_time:249310ms step_avg:215.67ms
step:1157/4000 train_time:249526ms step_avg:215.67ms
step:1158/4000 train_time:249742ms step_avg:215.67ms
step:1159/4000 train_time:249957ms step_avg:215.67ms
step:1160/4000 train_time:250173ms step_avg:215.67ms
step:1161/4000 train_time:250389ms step_avg:215.67ms
step:1162/4000 train_time:250604ms step_avg:215.67ms
step:1163/4000 train_time:250819ms step_avg:215.67ms
step:1164/4000 train_time:251035ms step_avg:215.67ms
step:1165/4000 train_time:251250ms step_avg:215.67ms
step:1166/4000 train_time:251466ms step_avg:215.67ms
step:1167/4000 train_time:251682ms step_avg:215.67ms
step:1168/4000 train_time:251898ms step_avg:215.67ms
step:1169/4000 train_time:252114ms step_avg:215.67ms
step:1170/4000 train_time:252330ms step_avg:215.67ms
step:1171/4000 train_time:252545ms step_avg:215.67ms
step:1172/4000 train_time:252761ms step_avg:215.67ms
step:1173/4000 train_time:252977ms step_avg:215.67ms
step:1174/4000 train_time:253193ms step_avg:215.67ms
step:1175/4000 train_time:253408ms step_avg:215.67ms
step:1176/4000 train_time:253623ms step_avg:215.67ms
step:1177/4000 train_time:253839ms step_avg:215.67ms
step:1178/4000 train_time:254055ms step_avg:215.67ms
step:1179/4000 train_time:254271ms step_avg:215.67ms
step:1180/4000 train_time:254487ms step_avg:215.67ms
step:1181/4000 train_time:254703ms step_avg:215.67ms
step:1182/4000 train_time:254918ms step_avg:215.67ms
step:1183/4000 train_time:255134ms step_avg:215.67ms
step:1184/4000 train_time:255350ms step_avg:215.67ms
step:1185/4000 train_time:255565ms step_avg:215.67ms
step:1186/4000 train_time:255780ms step_avg:215.67ms
step:1187/4000 train_time:255996ms step_avg:215.67ms
step:1188/4000 train_time:256212ms step_avg:215.67ms
step:1189/4000 train_time:256427ms step_avg:215.67ms
step:1190/4000 train_time:256642ms step_avg:215.67ms
step:1191/4000 train_time:256857ms step_avg:215.67ms
step:1192/4000 train_time:257073ms step_avg:215.67ms
step:1193/4000 train_time:257289ms step_avg:215.67ms
step:1194/4000 train_time:257505ms step_avg:215.67ms
step:1195/4000 train_time:257721ms step_avg:215.67ms
step:1196/4000 train_time:257936ms step_avg:215.67ms
step:1197/4000 train_time:258151ms step_avg:215.67ms
step:1198/4000 train_time:258367ms step_avg:215.67ms
step:1199/4000 train_time:258583ms step_avg:215.67ms
step:1200/4000 train_time:258798ms step_avg:215.66ms
step:1201/4000 train_time:259014ms step_avg:215.67ms
step:1202/4000 train_time:259230ms step_avg:215.67ms
step:1203/4000 train_time:259445ms step_avg:215.67ms
step:1204/4000 train_time:259661ms step_avg:215.67ms
step:1205/4000 train_time:259876ms step_avg:215.66ms
step:1206/4000 train_time:260092ms step_avg:215.66ms
step:1207/4000 train_time:260307ms step_avg:215.66ms
step:1208/4000 train_time:260523ms step_avg:215.66ms
step:1209/4000 train_time:260738ms step_avg:215.66ms
step:1210/4000 train_time:260954ms step_avg:215.66ms
step:1211/4000 train_time:261170ms step_avg:215.66ms
step:1212/4000 train_time:261386ms step_avg:215.66ms
step:1213/4000 train_time:261602ms step_avg:215.66ms
step:1214/4000 train_time:261817ms step_avg:215.67ms
step:1215/4000 train_time:262033ms step_avg:215.67ms
step:1216/4000 train_time:262249ms step_avg:215.67ms
step:1217/4000 train_time:262464ms step_avg:215.66ms
step:1218/4000 train_time:262679ms step_avg:215.66ms
step:1219/4000 train_time:262894ms step_avg:215.66ms
step:1220/4000 train_time:263109ms step_avg:215.66ms
step:1221/4000 train_time:263324ms step_avg:215.66ms
step:1222/4000 train_time:263539ms step_avg:215.66ms
step:1223/4000 train_time:263756ms step_avg:215.66ms
step:1224/4000 train_time:263972ms step_avg:215.66ms
step:1225/4000 train_time:264187ms step_avg:215.66ms
step:1226/4000 train_time:264402ms step_avg:215.66ms
step:1227/4000 train_time:264617ms step_avg:215.66ms
step:1228/4000 train_time:264833ms step_avg:215.66ms
step:1229/4000 train_time:265049ms step_avg:215.66ms
step:1230/4000 train_time:265264ms step_avg:215.66ms
step:1231/4000 train_time:265479ms step_avg:215.66ms
step:1232/4000 train_time:265695ms step_avg:215.66ms
step:1233/4000 train_time:265911ms step_avg:215.66ms
step:1234/4000 train_time:266126ms step_avg:215.66ms
step:1235/4000 train_time:266342ms step_avg:215.66ms
step:1236/4000 train_time:266558ms step_avg:215.66ms
step:1237/4000 train_time:266773ms step_avg:215.66ms
step:1238/4000 train_time:266990ms step_avg:215.66ms
step:1239/4000 train_time:267205ms step_avg:215.66ms
step:1240/4000 train_time:267420ms step_avg:215.66ms
step:1241/4000 train_time:267635ms step_avg:215.66ms
step:1242/4000 train_time:267850ms step_avg:215.66ms
step:1243/4000 train_time:268067ms step_avg:215.66ms
step:1244/4000 train_time:268282ms step_avg:215.66ms
step:1245/4000 train_time:268497ms step_avg:215.66ms
step:1246/4000 train_time:268713ms step_avg:215.66ms
step:1247/4000 train_time:268928ms step_avg:215.66ms
step:1248/4000 train_time:269145ms step_avg:215.66ms
step:1249/4000 train_time:269360ms step_avg:215.66ms
step:1250/4000 train_time:269576ms step_avg:215.66ms
step:1250/4000 val_bpb:1.1804 train_time:269588ms step_avg:215.67ms
step:1251/4000 train_time:269795ms step_avg:215.66ms
step:1252/4000 train_time:270010ms step_avg:215.66ms
step:1253/4000 train_time:270226ms step_avg:215.66ms
step:1254/4000 train_time:270442ms step_avg:215.66ms
step:1255/4000 train_time:270657ms step_avg:215.66ms
step:1256/4000 train_time:270874ms step_avg:215.66ms
step:1257/4000 train_time:271089ms step_avg:215.66ms
step:1258/4000 train_time:271305ms step_avg:215.66ms
step:1259/4000 train_time:271521ms step_avg:215.66ms
step:1260/4000 train_time:271737ms step_avg:215.66ms
step:1261/4000 train_time:271952ms step_avg:215.66ms
step:1262/4000 train_time:272168ms step_avg:215.66ms
step:1263/4000 train_time:272383ms step_avg:215.66ms
step:1264/4000 train_time:272599ms step_avg:215.66ms
step:1265/4000 train_time:272815ms step_avg:215.66ms
step:1266/4000 train_time:273030ms step_avg:215.66ms
step:1267/4000 train_time:273245ms step_avg:215.66ms
step:1268/4000 train_time:273462ms step_avg:215.66ms
step:1269/4000 train_time:273677ms step_avg:215.66ms
step:1270/4000 train_time:273893ms step_avg:215.66ms
step:1271/4000 train_time:274108ms step_avg:215.66ms
step:1272/4000 train_time:274324ms step_avg:215.66ms
step:1273/4000 train_time:274539ms step_avg:215.66ms
step:1274/4000 train_time:274753ms step_avg:215.66ms
step:1275/4000 train_time:274969ms step_avg:215.66ms
step:1276/4000 train_time:275184ms step_avg:215.66ms
step:1277/4000 train_time:275401ms step_avg:215.66ms
step:1278/4000 train_time:275617ms step_avg:215.66ms
step:1279/4000 train_time:275832ms step_avg:215.66ms
step:1280/4000 train_time:276047ms step_avg:215.66ms
step:1281/4000 train_time:276262ms step_avg:215.66ms
step:1282/4000 train_time:276478ms step_avg:215.66ms
step:1283/4000 train_time:276693ms step_avg:215.66ms
step:1284/4000 train_time:276909ms step_avg:215.66ms
step:1285/4000 train_time:277125ms step_avg:215.66ms
step:1286/4000 train_time:277340ms step_avg:215.66ms
step:1287/4000 train_time:277555ms step_avg:215.66ms
step:1288/4000 train_time:277771ms step_avg:215.66ms
step:1289/4000 train_time:277986ms step_avg:215.66ms
step:1290/4000 train_time:278202ms step_avg:215.66ms
step:1291/4000 train_time:278416ms step_avg:215.66ms
step:1292/4000 train_time:278632ms step_avg:215.66ms
step:1293/4000 train_time:278847ms step_avg:215.66ms
step:1294/4000 train_time:279063ms step_avg:215.66ms
step:1295/4000 train_time:279278ms step_avg:215.66ms
step:1296/4000 train_time:279493ms step_avg:215.66ms
step:1297/4000 train_time:279708ms step_avg:215.66ms
step:1298/4000 train_time:279924ms step_avg:215.66ms
step:1299/4000 train_time:280139ms step_avg:215.66ms
step:1300/4000 train_time:280355ms step_avg:215.66ms
step:1301/4000 train_time:280571ms step_avg:215.66ms
step:1302/4000 train_time:280786ms step_avg:215.66ms
step:1303/4000 train_time:281002ms step_avg:215.66ms
step:1304/4000 train_time:281217ms step_avg:215.66ms
step:1305/4000 train_time:281432ms step_avg:215.66ms
step:1306/4000 train_time:281648ms step_avg:215.66ms
step:1307/4000 train_time:281864ms step_avg:215.66ms
step:1308/4000 train_time:282079ms step_avg:215.66ms
step:1309/4000 train_time:282295ms step_avg:215.66ms
step:1310/4000 train_time:282511ms step_avg:215.66ms
step:1311/4000 train_time:282726ms step_avg:215.66ms
step:1312/4000 train_time:282940ms step_avg:215.66ms
step:1313/4000 train_time:283156ms step_avg:215.66ms
step:1314/4000 train_time:283370ms step_avg:215.65ms
step:1315/4000 train_time:283586ms step_avg:215.66ms
step:1316/4000 train_time:283802ms step_avg:215.65ms
step:1317/4000 train_time:284017ms step_avg:215.65ms
step:1318/4000 train_time:284232ms step_avg:215.65ms
step:1319/4000 train_time:284448ms step_avg:215.65ms
step:1320/4000 train_time:284664ms step_avg:215.65ms
step:1321/4000 train_time:285041ms step_avg:215.78ms
step:1322/4000 train_time:285441ms step_avg:215.92ms
step:1323/4000 train_time:285840ms step_avg:216.05ms
step:1324/4000 train_time:286239ms step_avg:216.19ms
step:1325/4000 train_time:286636ms step_avg:216.33ms
step:1326/4000 train_time:287034ms step_avg:216.47ms
step:1327/4000 train_time:287437ms step_avg:216.61ms
step:1328/4000 train_time:287837ms step_avg:216.74ms
step:1329/4000 train_time:288236ms step_avg:216.88ms
step:1330/4000 train_time:288635ms step_avg:217.02ms
step:1331/4000 train_time:289034ms step_avg:217.16ms
step:1332/4000 train_time:289433ms step_avg:217.29ms
step:1333/4000 train_time:289834ms step_avg:217.43ms
step:1334/4000 train_time:290235ms step_avg:217.57ms
step:1335/4000 train_time:290635ms step_avg:217.70ms
step:1336/4000 train_time:291035ms step_avg:217.84ms
step:1337/4000 train_time:291435ms step_avg:217.98ms
step:1338/4000 train_time:291836ms step_avg:218.11ms
step:1339/4000 train_time:292236ms step_avg:218.25ms
step:1340/4000 train_time:292636ms step_avg:218.39ms
step:1341/4000 train_time:293037ms step_avg:218.52ms
step:1342/4000 train_time:293437ms step_avg:218.66ms
step:1343/4000 train_time:293837ms step_avg:218.79ms
step:1344/4000 train_time:294238ms step_avg:218.93ms
step:1345/4000 train_time:294637ms step_avg:219.06ms
step:1346/4000 train_time:295038ms step_avg:219.20ms
step:1347/4000 train_time:295438ms step_avg:219.33ms
step:1348/4000 train_time:295839ms step_avg:219.47ms
step:1349/4000 train_time:296238ms step_avg:219.60ms
step:1350/4000 train_time:296639ms step_avg:219.73ms
step:1351/4000 train_time:297039ms step_avg:219.87ms
step:1352/4000 train_time:297440ms step_avg:220.00ms
step:1353/4000 train_time:297840ms step_avg:220.13ms
step:1354/4000 train_time:298240ms step_avg:220.27ms
step:1355/4000 train_time:298640ms step_avg:220.40ms
step:1356/4000 train_time:299040ms step_avg:220.53ms
step:1357/4000 train_time:299437ms step_avg:220.66ms
step:1358/4000 train_time:299838ms step_avg:220.79ms
step:1359/4000 train_time:300238ms step_avg:220.93ms
step:1360/4000 train_time:300637ms step_avg:221.06ms
step:1361/4000 train_time:301036ms step_avg:221.19ms
step:1362/4000 train_time:301437ms step_avg:221.32ms
step:1363/4000 train_time:301836ms step_avg:221.45ms
step:1364/4000 train_time:302236ms step_avg:221.58ms
step:1365/4000 train_time:302636ms step_avg:221.71ms
step:1366/4000 train_time:303035ms step_avg:221.84ms
step:1367/4000 train_time:303436ms step_avg:221.97ms
step:1368/4000 train_time:303836ms step_avg:222.10ms
step:1369/4000 train_time:304237ms step_avg:222.23ms
step:1370/4000 train_time:304637ms step_avg:222.36ms
step:1371/4000 train_time:305035ms step_avg:222.49ms
step:1372/4000 train_time:305437ms step_avg:222.62ms
step:1373/4000 train_time:305838ms step_avg:222.75ms
step:1374/4000 train_time:306239ms step_avg:222.88ms
step:1375/4000 train_time:306637ms step_avg:223.01ms
step:1376/4000 train_time:307037ms step_avg:223.14ms
step:1377/4000 train_time:307438ms step_avg:223.27ms
step:1378/4000 train_time:307837ms step_avg:223.39ms
step:1379/4000 train_time:308238ms step_avg:223.52ms
step:1380/4000 train_time:308637ms step_avg:223.65ms
step:1381/4000 train_time:309036ms step_avg:223.78ms
step:1382/4000 train_time:309437ms step_avg:223.91ms
step:1383/4000 train_time:309836ms step_avg:224.03ms
step:1384/4000 train_time:310235ms step_avg:224.16ms
step:1385/4000 train_time:310636ms step_avg:224.29ms
step:1386/4000 train_time:311036ms step_avg:224.41ms
step:1387/4000 train_time:311433ms step_avg:224.54ms
step:1388/4000 train_time:311833ms step_avg:224.66ms
step:1389/4000 train_time:312232ms step_avg:224.79ms
step:1390/4000 train_time:312632ms step_avg:224.91ms
step:1391/4000 train_time:313028ms step_avg:225.04ms
step:1392/4000 train_time:313425ms step_avg:225.16ms
step:1393/4000 train_time:313824ms step_avg:225.29ms
step:1394/4000 train_time:314223ms step_avg:225.41ms
step:1395/4000 train_time:314621ms step_avg:225.53ms
step:1396/4000 train_time:315019ms step_avg:225.66ms
step:1397/4000 train_time:315417ms step_avg:225.78ms
step:1398/4000 train_time:315816ms step_avg:225.91ms
step:1399/4000 train_time:316215ms step_avg:226.03ms
step:1400/4000 train_time:316614ms step_avg:226.15ms
step:1401/4000 train_time:317013ms step_avg:226.28ms
step:1402/4000 train_time:317411ms step_avg:226.40ms
step:1403/4000 train_time:317809ms step_avg:226.52ms
step:1404/4000 train_time:318207ms step_avg:226.64ms
step:1405/4000 train_time:318606ms step_avg:226.77ms
step:1406/4000 train_time:319006ms step_avg:226.89ms
step:1407/4000 train_time:319404ms step_avg:227.01ms
step:1408/4000 train_time:319803ms step_avg:227.13ms
step:1409/4000 train_time:320201ms step_avg:227.25ms
step:1410/4000 train_time:320601ms step_avg:227.38ms
step:1411/4000 train_time:320999ms step_avg:227.50ms
step:1412/4000 train_time:321397ms step_avg:227.62ms
step:1413/4000 train_time:321796ms step_avg:227.74ms
step:1414/4000 train_time:322195ms step_avg:227.86ms
step:1415/4000 train_time:322595ms step_avg:227.98ms
step:1416/4000 train_time:322995ms step_avg:228.10ms
step:1417/4000 train_time:323395ms step_avg:228.23ms
step:1418/4000 train_time:323793ms step_avg:228.34ms
step:1419/4000 train_time:324193ms step_avg:228.47ms
step:1420/4000 train_time:324590ms step_avg:228.58ms
step:1421/4000 train_time:324988ms step_avg:228.70ms
step:1422/4000 train_time:325388ms step_avg:228.82ms
step:1423/4000 train_time:325786ms step_avg:228.94ms
step:1424/4000 train_time:326184ms step_avg:229.06ms
step:1425/4000 train_time:326583ms step_avg:229.18ms
step:1426/4000 train_time:326984ms step_avg:229.30ms
step:1427/4000 train_time:327382ms step_avg:229.42ms
step:1428/4000 train_time:327783ms step_avg:229.54ms
step:1429/4000 train_time:328181ms step_avg:229.66ms
step:1430/4000 train_time:328580ms step_avg:229.78ms
step:1431/4000 train_time:328976ms step_avg:229.89ms
step:1432/4000 train_time:329375ms step_avg:230.01ms
step:1433/4000 train_time:329775ms step_avg:230.13ms
step:1434/4000 train_time:330171ms step_avg:230.24ms
step:1435/4000 train_time:330569ms step_avg:230.36ms
step:1436/4000 train_time:330967ms step_avg:230.48ms
step:1437/4000 train_time:331364ms step_avg:230.59ms
step:1438/4000 train_time:331761ms step_avg:230.71ms
step:1439/4000 train_time:332158ms step_avg:230.83ms
step:1440/4000 train_time:332557ms step_avg:230.94ms
step:1441/4000 train_time:332954ms step_avg:231.06ms
step:1442/4000 train_time:333352ms step_avg:231.17ms
step:1443/4000 train_time:333748ms step_avg:231.29ms
step:1444/4000 train_time:334147ms step_avg:231.40ms
step:1445/4000 train_time:334546ms step_avg:231.52ms
step:1446/4000 train_time:334945ms step_avg:231.64ms
step:1447/4000 train_time:335345ms step_avg:231.75ms
step:1448/4000 train_time:335745ms step_avg:231.87ms
step:1449/4000 train_time:336144ms step_avg:231.98ms
step:1450/4000 train_time:336543ms step_avg:232.10ms
step:1451/4000 train_time:336945ms step_avg:232.22ms
step:1452/4000 train_time:337345ms step_avg:232.33ms
step:1453/4000 train_time:337742ms step_avg:232.44ms
step:1454/4000 train_time:338139ms step_avg:232.56ms
step:1455/4000 train_time:338537ms step_avg:232.67ms
step:1456/4000 train_time:338937ms step_avg:232.79ms
step:1457/4000 train_time:339336ms step_avg:232.90ms
step:1458/4000 train_time:339735ms step_avg:233.01ms
step:1459/4000 train_time:340132ms step_avg:233.13ms
step:1460/4000 train_time:340530ms step_avg:233.24ms
step:1461/4000 train_time:340927ms step_avg:233.35ms
step:1462/4000 train_time:341325ms step_avg:233.46ms
step:1463/4000 train_time:341724ms step_avg:233.58ms
step:1464/4000 train_time:342121ms step_avg:233.69ms
step:1465/4000 train_time:342518ms step_avg:233.80ms
step:1466/4000 train_time:342914ms step_avg:233.91ms
step:1467/4000 train_time:343314ms step_avg:234.02ms
step:1468/4000 train_time:343711ms step_avg:234.14ms
step:1469/4000 train_time:344107ms step_avg:234.25ms
step:1470/4000 train_time:344505ms step_avg:234.36ms
step:1471/4000 train_time:344902ms step_avg:234.47ms
step:1472/4000 train_time:345301ms step_avg:234.58ms
step:1473/4000 train_time:345699ms step_avg:234.69ms
step:1474/4000 train_time:346096ms step_avg:234.80ms
step:1475/4000 train_time:346494ms step_avg:234.91ms
step:1476/4000 train_time:346894ms step_avg:235.02ms
step:1477/4000 train_time:347291ms step_avg:235.13ms
step:1478/4000 train_time:347688ms step_avg:235.24ms
step:1479/4000 train_time:348086ms step_avg:235.35ms
step:1480/4000 train_time:348486ms step_avg:235.46ms
step:1481/4000 train_time:348884ms step_avg:235.57ms
step:1482/4000 train_time:349280ms step_avg:235.68ms
step:1483/4000 train_time:349676ms step_avg:235.79ms
step:1484/4000 train_time:350072ms step_avg:235.90ms
step:1485/4000 train_time:350469ms step_avg:236.01ms
step:1486/4000 train_time:350867ms step_avg:236.12ms
step:1487/4000 train_time:351265ms step_avg:236.22ms
step:1488/4000 train_time:351663ms step_avg:236.33ms
step:1489/4000 train_time:352060ms step_avg:236.44ms
step:1490/4000 train_time:352458ms step_avg:236.55ms
step:1491/4000 train_time:352856ms step_avg:236.66ms
step:1492/4000 train_time:353256ms step_avg:236.77ms
step:1493/4000 train_time:353652ms step_avg:236.87ms
step:1494/4000 train_time:354051ms step_avg:236.98ms
step:1495/4000 train_time:354447ms step_avg:237.09ms
step:1496/4000 train_time:354845ms step_avg:237.20ms
step:1497/4000 train_time:355243ms step_avg:237.30ms
step:1498/4000 train_time:355641ms step_avg:237.41ms
step:1499/4000 train_time:356039ms step_avg:237.52ms
step:1500/4000 train_time:356434ms step_avg:237.62ms
step:1500/4000 val_bpb:1.1493 train_time:356469ms step_avg:237.65ms
step:1501/4000 train_time:356832ms step_avg:237.73ms
step:1502/4000 train_time:357226ms step_avg:237.83ms
step:1503/4000 train_time:357619ms step_avg:237.94ms
step:1504/4000 train_time:358013ms step_avg:238.04ms
step:1505/4000 train_time:358408ms step_avg:238.15ms
step:1506/4000 train_time:358802ms step_avg:238.25ms
step:1507/4000 train_time:359196ms step_avg:238.35ms
step:1508/4000 train_time:359591ms step_avg:238.46ms
step:1509/4000 train_time:359986ms step_avg:238.56ms
step:1510/4000 train_time:360382ms step_avg:238.66ms
step:1511/4000 train_time:360774ms step_avg:238.76ms
step:1512/4000 train_time:361168ms step_avg:238.87ms
step:1513/4000 train_time:361561ms step_avg:238.97ms
step:1514/4000 train_time:361955ms step_avg:239.07ms
step:1515/4000 train_time:362348ms step_avg:239.17ms
step:1516/4000 train_time:362742ms step_avg:239.28ms
step:1517/4000 train_time:363138ms step_avg:239.38ms
step:1518/4000 train_time:363533ms step_avg:239.48ms
step:1519/4000 train_time:363928ms step_avg:239.58ms
step:1520/4000 train_time:364323ms step_avg:239.69ms
step:1521/4000 train_time:364717ms step_avg:239.79ms
step:1522/4000 train_time:365110ms step_avg:239.89ms
step:1523/4000 train_time:365505ms step_avg:239.99ms
step:1524/4000 train_time:365898ms step_avg:240.09ms
step:1525/4000 train_time:366292ms step_avg:240.19ms
step:1526/4000 train_time:366686ms step_avg:240.29ms
step:1527/4000 train_time:367080ms step_avg:240.39ms
step:1528/4000 train_time:367475ms step_avg:240.49ms
step:1529/4000 train_time:367869ms step_avg:240.59ms
step:1530/4000 train_time:368263ms step_avg:240.69ms
step:1531/4000 train_time:368657ms step_avg:240.79ms
step:1532/4000 train_time:369051ms step_avg:240.90ms
step:1533/4000 train_time:369445ms step_avg:240.99ms
step:1534/4000 train_time:369840ms step_avg:241.10ms
step:1535/4000 train_time:370234ms step_avg:241.20ms
step:1536/4000 train_time:370629ms step_avg:241.29ms
step:1537/4000 train_time:371022ms step_avg:241.39ms
step:1538/4000 train_time:371416ms step_avg:241.49ms
step:1539/4000 train_time:371811ms step_avg:241.59ms
step:1540/4000 train_time:372206ms step_avg:241.69ms
step:1541/4000 train_time:372600ms step_avg:241.79ms
step:1542/4000 train_time:372996ms step_avg:241.89ms
step:1543/4000 train_time:373391ms step_avg:241.99ms
step:1544/4000 train_time:373786ms step_avg:242.09ms
step:1545/4000 train_time:374181ms step_avg:242.19ms
step:1546/4000 train_time:374575ms step_avg:242.29ms
step:1547/4000 train_time:374970ms step_avg:242.39ms
step:1548/4000 train_time:375366ms step_avg:242.48ms
step:1549/4000 train_time:375760ms step_avg:242.58ms
step:1550/4000 train_time:376156ms step_avg:242.68ms
step:1551/4000 train_time:376549ms step_avg:242.78ms
step:1552/4000 train_time:376946ms step_avg:242.88ms
step:1553/4000 train_time:377341ms step_avg:242.98ms
step:1554/4000 train_time:377735ms step_avg:243.07ms
step:1555/4000 train_time:378129ms step_avg:243.17ms
step:1556/4000 train_time:378524ms step_avg:243.27ms
step:1557/4000 train_time:378920ms step_avg:243.37ms
step:1558/4000 train_time:379314ms step_avg:243.46ms
step:1559/4000 train_time:379708ms step_avg:243.56ms
step:1560/4000 train_time:380103ms step_avg:243.66ms
step:1561/4000 train_time:380498ms step_avg:243.75ms
step:1562/4000 train_time:380892ms step_avg:243.85ms
step:1563/4000 train_time:381289ms step_avg:243.95ms
step:1564/4000 train_time:381684ms step_avg:244.04ms
step:1565/4000 train_time:382079ms step_avg:244.14ms
step:1566/4000 train_time:382475ms step_avg:244.24ms
step:1567/4000 train_time:382868ms step_avg:244.33ms
step:1568/4000 train_time:383263ms step_avg:244.43ms
step:1569/4000 train_time:383658ms step_avg:244.52ms
step:1570/4000 train_time:384052ms step_avg:244.62ms
step:1571/4000 train_time:384448ms step_avg:244.72ms
step:1572/4000 train_time:384842ms step_avg:244.81ms
step:1573/4000 train_time:385239ms step_avg:244.91ms
step:1574/4000 train_time:385634ms step_avg:245.00ms
step:1575/4000 train_time:386028ms step_avg:245.10ms
step:1576/4000 train_time:386422ms step_avg:245.19ms
step:1577/4000 train_time:386817ms step_avg:245.29ms
step:1578/4000 train_time:387213ms step_avg:245.38ms
step:1579/4000 train_time:387607ms step_avg:245.48ms
step:1580/4000 train_time:388002ms step_avg:245.57ms
step:1581/4000 train_time:388398ms step_avg:245.67ms
step:1582/4000 train_time:388792ms step_avg:245.76ms
step:1583/4000 train_time:389188ms step_avg:245.85ms
step:1584/4000 train_time:389583ms step_avg:245.95ms
step:1585/4000 train_time:389979ms step_avg:246.04ms
step:1586/4000 train_time:390374ms step_avg:246.14ms
step:1587/4000 train_time:390770ms step_avg:246.23ms
step:1588/4000 train_time:391165ms step_avg:246.33ms
step:1589/4000 train_time:391560ms step_avg:246.42ms
step:1590/4000 train_time:391955ms step_avg:246.51ms
step:1591/4000 train_time:392350ms step_avg:246.61ms
step:1592/4000 train_time:392744ms step_avg:246.70ms
step:1593/4000 train_time:393141ms step_avg:246.79ms
step:1594/4000 train_time:393536ms step_avg:246.89ms
step:1595/4000 train_time:393931ms step_avg:246.98ms
step:1596/4000 train_time:394326ms step_avg:247.07ms
step:1597/4000 train_time:394721ms step_avg:247.16ms
step:1598/4000 train_time:395116ms step_avg:247.26ms
step:1599/4000 train_time:395510ms step_avg:247.35ms
step:1600/4000 train_time:395904ms step_avg:247.44ms
step:1601/4000 train_time:396299ms step_avg:247.53ms
step:1602/4000 train_time:396695ms step_avg:247.62ms
step:1603/4000 train_time:397090ms step_avg:247.72ms
step:1604/4000 train_time:397485ms step_avg:247.81ms
step:1605/4000 train_time:397880ms step_avg:247.90ms
step:1606/4000 train_time:398277ms step_avg:247.99ms
step:1607/4000 train_time:398672ms step_avg:248.08ms
step:1608/4000 train_time:399067ms step_avg:248.18ms
step:1609/4000 train_time:399463ms step_avg:248.27ms
step:1610/4000 train_time:399857ms step_avg:248.36ms
step:1611/4000 train_time:400251ms step_avg:248.45ms
step:1612/4000 train_time:400645ms step_avg:248.54ms
step:1613/4000 train_time:401042ms step_avg:248.63ms
step:1614/4000 train_time:401436ms step_avg:248.72ms
step:1615/4000 train_time:401830ms step_avg:248.81ms
step:1616/4000 train_time:402226ms step_avg:248.90ms
step:1617/4000 train_time:402621ms step_avg:248.99ms
step:1618/4000 train_time:403017ms step_avg:249.08ms
step:1619/4000 train_time:403411ms step_avg:249.17ms
step:1620/4000 train_time:403807ms step_avg:249.26ms
step:1621/4000 train_time:404201ms step_avg:249.35ms
step:1622/4000 train_time:404596ms step_avg:249.44ms
step:1623/4000 train_time:404992ms step_avg:249.53ms
step:1624/4000 train_time:405388ms step_avg:249.62ms
step:1625/4000 train_time:405783ms step_avg:249.71ms
step:1626/4000 train_time:406178ms step_avg:249.80ms
step:1627/4000 train_time:406572ms step_avg:249.89ms
step:1628/4000 train_time:406969ms step_avg:249.98ms
step:1629/4000 train_time:407364ms step_avg:250.07ms
step:1630/4000 train_time:407758ms step_avg:250.16ms
step:1631/4000 train_time:408153ms step_avg:250.25ms
step:1632/4000 train_time:408547ms step_avg:250.34ms
step:1633/4000 train_time:408943ms step_avg:250.42ms
step:1634/4000 train_time:409340ms step_avg:250.51ms
step:1635/4000 train_time:409734ms step_avg:250.60ms
step:1636/4000 train_time:410129ms step_avg:250.69ms
step:1637/4000 train_time:410523ms step_avg:250.78ms
step:1638/4000 train_time:410918ms step_avg:250.87ms
step:1639/4000 train_time:411313ms step_avg:250.95ms
step:1640/4000 train_time:411709ms step_avg:251.04ms
step:1641/4000 train_time:412103ms step_avg:251.13ms
step:1642/4000 train_time:412499ms step_avg:251.22ms
step:1643/4000 train_time:412895ms step_avg:251.31ms
step:1644/4000 train_time:413291ms step_avg:251.39ms
step:1645/4000 train_time:413687ms step_avg:251.48ms
step:1646/4000 train_time:414082ms step_avg:251.57ms
step:1647/4000 train_time:414476ms step_avg:251.66ms
step:1648/4000 train_time:414872ms step_avg:251.74ms
step:1649/4000 train_time:415267ms step_avg:251.83ms
step:1650/4000 train_time:415663ms step_avg:251.92ms
step:1651/4000 train_time:416058ms step_avg:252.00ms
step:1652/4000 train_time:416453ms step_avg:252.09ms
step:1653/4000 train_time:416848ms step_avg:252.18ms
step:1654/4000 train_time:417243ms step_avg:252.26ms
step:1655/4000 train_time:417639ms step_avg:252.35ms
step:1656/4000 train_time:418034ms step_avg:252.44ms
step:1657/4000 train_time:418429ms step_avg:252.52ms
step:1658/4000 train_time:418824ms step_avg:252.61ms
step:1659/4000 train_time:419220ms step_avg:252.69ms
step:1660/4000 train_time:419615ms step_avg:252.78ms
step:1661/4000 train_time:420010ms step_avg:252.87ms
step:1662/4000 train_time:420403ms step_avg:252.95ms
step:1663/4000 train_time:420800ms step_avg:253.04ms
step:1664/4000 train_time:421197ms step_avg:253.12ms
step:1665/4000 train_time:421591ms step_avg:253.21ms
step:1666/4000 train_time:421987ms step_avg:253.29ms
step:1667/4000 train_time:422381ms step_avg:253.38ms
step:1668/4000 train_time:422776ms step_avg:253.46ms
step:1669/4000 train_time:423171ms step_avg:253.55ms
step:1670/4000 train_time:423566ms step_avg:253.63ms
step:1671/4000 train_time:423961ms step_avg:253.72ms
step:1672/4000 train_time:424355ms step_avg:253.80ms
step:1673/4000 train_time:424751ms step_avg:253.89ms
step:1674/4000 train_time:425147ms step_avg:253.97ms
step:1675/4000 train_time:425543ms step_avg:254.06ms
step:1676/4000 train_time:425937ms step_avg:254.14ms
step:1677/4000 train_time:426332ms step_avg:254.22ms
step:1678/4000 train_time:426727ms step_avg:254.31ms
step:1679/4000 train_time:427120ms step_avg:254.39ms
step:1680/4000 train_time:427516ms step_avg:254.47ms
step:1681/4000 train_time:427911ms step_avg:254.56ms
step:1682/4000 train_time:428306ms step_avg:254.64ms
step:1683/4000 train_time:428702ms step_avg:254.72ms
step:1684/4000 train_time:429096ms step_avg:254.81ms
step:1685/4000 train_time:429492ms step_avg:254.89ms
step:1686/4000 train_time:429887ms step_avg:254.97ms
step:1687/4000 train_time:430283ms step_avg:255.06ms
step:1688/4000 train_time:430678ms step_avg:255.14ms
step:1689/4000 train_time:431072ms step_avg:255.22ms
step:1690/4000 train_time:431468ms step_avg:255.31ms
step:1691/4000 train_time:431862ms step_avg:255.39ms
step:1692/4000 train_time:432258ms step_avg:255.47ms
step:1693/4000 train_time:432652ms step_avg:255.55ms
step:1694/4000 train_time:433046ms step_avg:255.64ms
step:1695/4000 train_time:433442ms step_avg:255.72ms
step:1696/4000 train_time:433838ms step_avg:255.80ms
step:1697/4000 train_time:434231ms step_avg:255.88ms
step:1698/4000 train_time:434626ms step_avg:255.96ms
step:1699/4000 train_time:435021ms step_avg:256.05ms
step:1700/4000 train_time:435416ms step_avg:256.13ms
step:1701/4000 train_time:435811ms step_avg:256.21ms
step:1702/4000 train_time:436205ms step_avg:256.29ms
step:1703/4000 train_time:436601ms step_avg:256.37ms
step:1704/4000 train_time:436996ms step_avg:256.45ms
step:1705/4000 train_time:437391ms step_avg:256.53ms
step:1706/4000 train_time:437787ms step_avg:256.62ms
step:1707/4000 train_time:438183ms step_avg:256.70ms
step:1708/4000 train_time:438578ms step_avg:256.78ms
step:1709/4000 train_time:438974ms step_avg:256.86ms
step:1710/4000 train_time:439369ms step_avg:256.94ms
step:1711/4000 train_time:439766ms step_avg:257.02ms
step:1712/4000 train_time:440161ms step_avg:257.10ms
step:1713/4000 train_time:440555ms step_avg:257.18ms
step:1714/4000 train_time:440952ms step_avg:257.26ms
step:1715/4000 train_time:441347ms step_avg:257.35ms
step:1716/4000 train_time:441742ms step_avg:257.43ms
step:1717/4000 train_time:442137ms step_avg:257.51ms
step:1718/4000 train_time:442534ms step_avg:257.59ms
step:1719/4000 train_time:442930ms step_avg:257.67ms
step:1720/4000 train_time:443325ms step_avg:257.75ms
step:1721/4000 train_time:443721ms step_avg:257.83ms
step:1722/4000 train_time:444116ms step_avg:257.91ms
step:1723/4000 train_time:444511ms step_avg:257.99ms
step:1724/4000 train_time:444906ms step_avg:258.07ms
step:1725/4000 train_time:445301ms step_avg:258.15ms
step:1726/4000 train_time:445695ms step_avg:258.22ms
step:1727/4000 train_time:446092ms step_avg:258.30ms
step:1728/4000 train_time:446487ms step_avg:258.38ms
step:1729/4000 train_time:446883ms step_avg:258.46ms
step:1730/4000 train_time:447277ms step_avg:258.54ms
step:1731/4000 train_time:447672ms step_avg:258.62ms
step:1732/4000 train_time:448068ms step_avg:258.70ms
step:1733/4000 train_time:448463ms step_avg:258.78ms
step:1734/4000 train_time:448858ms step_avg:258.86ms
step:1735/4000 train_time:449252ms step_avg:258.93ms
step:1736/4000 train_time:449648ms step_avg:259.01ms
step:1737/4000 train_time:450043ms step_avg:259.09ms
step:1738/4000 train_time:450440ms step_avg:259.17ms
step:1739/4000 train_time:450834ms step_avg:259.25ms
step:1740/4000 train_time:451230ms step_avg:259.33ms
step:1741/4000 train_time:451624ms step_avg:259.40ms
step:1742/4000 train_time:452019ms step_avg:259.48ms
step:1743/4000 train_time:452412ms step_avg:259.56ms
step:1744/4000 train_time:452807ms step_avg:259.64ms
step:1745/4000 train_time:453202ms step_avg:259.71ms
step:1746/4000 train_time:453596ms step_avg:259.79ms
step:1747/4000 train_time:453993ms step_avg:259.87ms
step:1748/4000 train_time:454389ms step_avg:259.95ms
step:1749/4000 train_time:454784ms step_avg:260.02ms
step:1750/4000 train_time:455179ms step_avg:260.10ms
step:1750/4000 val_bpb:1.1209 train_time:455214ms step_avg:260.12ms
step:1751/4000 train_time:455578ms step_avg:260.18ms
step:1752/4000 train_time:455974ms step_avg:260.26ms
step:1753/4000 train_time:456369ms step_avg:260.34ms
step:1754/4000 train_time:456763ms step_avg:260.41ms
step:1755/4000 train_time:457158ms step_avg:260.49ms
step:1756/4000 train_time:457552ms step_avg:260.57ms
step:1757/4000 train_time:457947ms step_avg:260.64ms
step:1758/4000 train_time:458341ms step_avg:260.72ms
step:1759/4000 train_time:458735ms step_avg:260.79ms
step:1760/4000 train_time:459130ms step_avg:260.87ms
step:1761/4000 train_time:459524ms step_avg:260.95ms
step:1762/4000 train_time:459921ms step_avg:261.02ms
step:1763/4000 train_time:460315ms step_avg:261.10ms
step:1764/4000 train_time:460709ms step_avg:261.17ms
step:1765/4000 train_time:461104ms step_avg:261.25ms
step:1766/4000 train_time:461499ms step_avg:261.32ms
step:1767/4000 train_time:461892ms step_avg:261.40ms
step:1768/4000 train_time:462290ms step_avg:261.48ms
step:1769/4000 train_time:462685ms step_avg:261.55ms
step:1770/4000 train_time:463079ms step_avg:261.63ms
step:1771/4000 train_time:463475ms step_avg:261.70ms
step:1772/4000 train_time:463871ms step_avg:261.78ms
step:1773/4000 train_time:464264ms step_avg:261.85ms
step:1774/4000 train_time:464660ms step_avg:261.93ms
step:1775/4000 train_time:465053ms step_avg:262.00ms
step:1776/4000 train_time:465449ms step_avg:262.08ms
step:1777/4000 train_time:465843ms step_avg:262.15ms
step:1778/4000 train_time:466238ms step_avg:262.23ms
step:1779/4000 train_time:466633ms step_avg:262.30ms
step:1780/4000 train_time:467028ms step_avg:262.38ms
step:1781/4000 train_time:467423ms step_avg:262.45ms
step:1782/4000 train_time:467818ms step_avg:262.52ms
step:1783/4000 train_time:468213ms step_avg:262.60ms
step:1784/4000 train_time:468608ms step_avg:262.67ms
step:1785/4000 train_time:469002ms step_avg:262.75ms
step:1786/4000 train_time:469398ms step_avg:262.82ms
step:1787/4000 train_time:469794ms step_avg:262.90ms
step:1788/4000 train_time:470189ms step_avg:262.97ms
step:1789/4000 train_time:470584ms step_avg:263.04ms
step:1790/4000 train_time:470980ms step_avg:263.12ms
step:1791/4000 train_time:471375ms step_avg:263.19ms
step:1792/4000 train_time:471771ms step_avg:263.27ms
step:1793/4000 train_time:472165ms step_avg:263.34ms
step:1794/4000 train_time:472560ms step_avg:263.41ms
step:1795/4000 train_time:472956ms step_avg:263.49ms
step:1796/4000 train_time:473351ms step_avg:263.56ms
step:1797/4000 train_time:473746ms step_avg:263.63ms
step:1798/4000 train_time:474141ms step_avg:263.70ms
step:1799/4000 train_time:474535ms step_avg:263.78ms
step:1800/4000 train_time:474931ms step_avg:263.85ms
step:1801/4000 train_time:475326ms step_avg:263.92ms
step:1802/4000 train_time:475722ms step_avg:264.00ms
step:1803/4000 train_time:476117ms step_avg:264.07ms
step:1804/4000 train_time:476511ms step_avg:264.14ms
step:1805/4000 train_time:476905ms step_avg:264.21ms
step:1806/4000 train_time:477300ms step_avg:264.29ms
step:1807/4000 train_time:477695ms step_avg:264.36ms
step:1808/4000 train_time:478090ms step_avg:264.43ms
step:1809/4000 train_time:478484ms step_avg:264.50ms
step:1810/4000 train_time:478880ms step_avg:264.57ms
step:1811/4000 train_time:479274ms step_avg:264.65ms
step:1812/4000 train_time:479670ms step_avg:264.72ms
step:1813/4000 train_time:480064ms step_avg:264.79ms
step:1814/4000 train_time:480458ms step_avg:264.86ms
step:1815/4000 train_time:480854ms step_avg:264.93ms
step:1816/4000 train_time:481248ms step_avg:265.00ms
step:1817/4000 train_time:481643ms step_avg:265.08ms
step:1818/4000 train_time:482038ms step_avg:265.15ms
step:1819/4000 train_time:482432ms step_avg:265.22ms
step:1820/4000 train_time:482826ms step_avg:265.29ms
step:1821/4000 train_time:483222ms step_avg:265.36ms
step:1822/4000 train_time:483617ms step_avg:265.43ms
step:1823/4000 train_time:484012ms step_avg:265.50ms
step:1824/4000 train_time:484406ms step_avg:265.57ms
step:1825/4000 train_time:484800ms step_avg:265.64ms
step:1826/4000 train_time:485194ms step_avg:265.71ms
step:1827/4000 train_time:485589ms step_avg:265.78ms
step:1828/4000 train_time:485984ms step_avg:265.86ms
step:1829/4000 train_time:486380ms step_avg:265.93ms
step:1830/4000 train_time:486775ms step_avg:266.00ms
step:1831/4000 train_time:487171ms step_avg:266.07ms
step:1832/4000 train_time:487566ms step_avg:266.14ms
step:1833/4000 train_time:487961ms step_avg:266.21ms
step:1834/4000 train_time:488356ms step_avg:266.28ms
step:1835/4000 train_time:488751ms step_avg:266.35ms
step:1836/4000 train_time:489145ms step_avg:266.42ms
step:1837/4000 train_time:489539ms step_avg:266.49ms
step:1838/4000 train_time:489933ms step_avg:266.56ms
step:1839/4000 train_time:490328ms step_avg:266.63ms
step:1840/4000 train_time:490723ms step_avg:266.70ms
step:1841/4000 train_time:491119ms step_avg:266.77ms
step:1842/4000 train_time:491514ms step_avg:266.84ms
step:1843/4000 train_time:491909ms step_avg:266.91ms
step:1844/4000 train_time:492304ms step_avg:266.98ms
step:1845/4000 train_time:492697ms step_avg:267.04ms
step:1846/4000 train_time:493092ms step_avg:267.11ms
step:1847/4000 train_time:493485ms step_avg:267.18ms
step:1848/4000 train_time:493880ms step_avg:267.25ms
step:1849/4000 train_time:494274ms step_avg:267.32ms
step:1850/4000 train_time:494671ms step_avg:267.39ms
step:1851/4000 train_time:495065ms step_avg:267.46ms
step:1852/4000 train_time:495459ms step_avg:267.53ms
step:1853/4000 train_time:495854ms step_avg:267.60ms
step:1854/4000 train_time:496248ms step_avg:267.66ms
step:1855/4000 train_time:496642ms step_avg:267.73ms
step:1856/4000 train_time:497036ms step_avg:267.80ms
step:1857/4000 train_time:497432ms step_avg:267.87ms
step:1858/4000 train_time:497828ms step_avg:267.94ms
step:1859/4000 train_time:498223ms step_avg:268.01ms
step:1860/4000 train_time:498620ms step_avg:268.08ms
step:1861/4000 train_time:499012ms step_avg:268.14ms
step:1862/4000 train_time:499408ms step_avg:268.21ms
step:1863/4000 train_time:499803ms step_avg:268.28ms
step:1864/4000 train_time:500198ms step_avg:268.35ms
step:1865/4000 train_time:500593ms step_avg:268.41ms
step:1866/4000 train_time:500988ms step_avg:268.48ms
step:1867/4000 train_time:501383ms step_avg:268.55ms
step:1868/4000 train_time:501778ms step_avg:268.62ms
step:1869/4000 train_time:502172ms step_avg:268.69ms
step:1870/4000 train_time:502567ms step_avg:268.75ms
step:1871/4000 train_time:502961ms step_avg:268.82ms
step:1872/4000 train_time:503356ms step_avg:268.89ms
step:1873/4000 train_time:503751ms step_avg:268.95ms
step:1874/4000 train_time:504145ms step_avg:269.02ms
step:1875/4000 train_time:504539ms step_avg:269.09ms
step:1876/4000 train_time:504936ms step_avg:269.16ms
step:1877/4000 train_time:505330ms step_avg:269.22ms
step:1878/4000 train_time:505725ms step_avg:269.29ms
step:1879/4000 train_time:506120ms step_avg:269.36ms
step:1880/4000 train_time:506515ms step_avg:269.42ms
step:1881/4000 train_time:506911ms step_avg:269.49ms
step:1882/4000 train_time:507305ms step_avg:269.56ms
step:1883/4000 train_time:507698ms step_avg:269.62ms
step:1884/4000 train_time:508092ms step_avg:269.69ms
step:1885/4000 train_time:508488ms step_avg:269.75ms
step:1886/4000 train_time:508884ms step_avg:269.82ms
step:1887/4000 train_time:509278ms step_avg:269.89ms
step:1888/4000 train_time:509674ms step_avg:269.95ms
step:1889/4000 train_time:510067ms step_avg:270.02ms
step:1890/4000 train_time:510461ms step_avg:270.09ms
step:1891/4000 train_time:510856ms step_avg:270.15ms
step:1892/4000 train_time:511251ms step_avg:270.22ms
step:1893/4000 train_time:511644ms step_avg:270.28ms
step:1894/4000 train_time:512040ms step_avg:270.35ms
step:1895/4000 train_time:512434ms step_avg:270.41ms
step:1896/4000 train_time:512830ms step_avg:270.48ms
step:1897/4000 train_time:513225ms step_avg:270.55ms
step:1898/4000 train_time:513622ms step_avg:270.61ms
step:1899/4000 train_time:514016ms step_avg:270.68ms
step:1900/4000 train_time:514410ms step_avg:270.74ms
step:1901/4000 train_time:514803ms step_avg:270.81ms
step:1902/4000 train_time:515198ms step_avg:270.87ms
step:1903/4000 train_time:515593ms step_avg:270.94ms
step:1904/4000 train_time:515987ms step_avg:271.00ms
step:1905/4000 train_time:516382ms step_avg:271.07ms
step:1906/4000 train_time:516777ms step_avg:271.13ms
step:1907/4000 train_time:517173ms step_avg:271.20ms
step:1908/4000 train_time:517568ms step_avg:271.26ms
step:1909/4000 train_time:517962ms step_avg:271.33ms
step:1910/4000 train_time:518357ms step_avg:271.39ms
step:1911/4000 train_time:518751ms step_avg:271.46ms
step:1912/4000 train_time:519146ms step_avg:271.52ms
step:1913/4000 train_time:519540ms step_avg:271.58ms
step:1914/4000 train_time:519933ms step_avg:271.65ms
step:1915/4000 train_time:520327ms step_avg:271.71ms
step:1916/4000 train_time:520724ms step_avg:271.78ms
step:1917/4000 train_time:521119ms step_avg:271.84ms
step:1918/4000 train_time:521512ms step_avg:271.90ms
step:1919/4000 train_time:521906ms step_avg:271.97ms
step:1920/4000 train_time:522302ms step_avg:272.03ms
step:1921/4000 train_time:522696ms step_avg:272.10ms
step:1922/4000 train_time:523090ms step_avg:272.16ms
step:1923/4000 train_time:523485ms step_avg:272.22ms
step:1924/4000 train_time:523882ms step_avg:272.29ms
step:1925/4000 train_time:524278ms step_avg:272.35ms
step:1926/4000 train_time:524673ms step_avg:272.42ms
step:1927/4000 train_time:525068ms step_avg:272.48ms
step:1928/4000 train_time:525462ms step_avg:272.54ms
step:1929/4000 train_time:525856ms step_avg:272.61ms
step:1930/4000 train_time:526251ms step_avg:272.67ms
step:1931/4000 train_time:526643ms step_avg:272.73ms
step:1932/4000 train_time:527038ms step_avg:272.79ms
step:1933/4000 train_time:527433ms step_avg:272.86ms
step:1934/4000 train_time:527829ms step_avg:272.92ms
step:1935/4000 train_time:528224ms step_avg:272.98ms
step:1936/4000 train_time:528618ms step_avg:273.05ms
step:1937/4000 train_time:529013ms step_avg:273.11ms
step:1938/4000 train_time:529408ms step_avg:273.17ms
step:1939/4000 train_time:529802ms step_avg:273.23ms
step:1940/4000 train_time:530197ms step_avg:273.30ms
step:1941/4000 train_time:530590ms step_avg:273.36ms
step:1942/4000 train_time:530984ms step_avg:273.42ms
step:1943/4000 train_time:531382ms step_avg:273.49ms
step:1944/4000 train_time:531777ms step_avg:273.55ms
step:1945/4000 train_time:532173ms step_avg:273.61ms
step:1946/4000 train_time:532568ms step_avg:273.67ms
step:1947/4000 train_time:532962ms step_avg:273.74ms
step:1948/4000 train_time:533357ms step_avg:273.80ms
step:1949/4000 train_time:533751ms step_avg:273.86ms
step:1950/4000 train_time:534145ms step_avg:273.92ms
step:1951/4000 train_time:534541ms step_avg:273.98ms
step:1952/4000 train_time:534936ms step_avg:274.05ms
step:1953/4000 train_time:535333ms step_avg:274.11ms
step:1954/4000 train_time:535728ms step_avg:274.17ms
step:1955/4000 train_time:536123ms step_avg:274.23ms
step:1956/4000 train_time:536516ms step_avg:274.29ms
step:1957/4000 train_time:536911ms step_avg:274.35ms
step:1958/4000 train_time:537307ms step_avg:274.42ms
step:1959/4000 train_time:537702ms step_avg:274.48ms
step:1960/4000 train_time:538098ms step_avg:274.54ms
step:1961/4000 train_time:538493ms step_avg:274.60ms
step:1962/4000 train_time:538887ms step_avg:274.66ms
step:1963/4000 train_time:539282ms step_avg:274.72ms
step:1964/4000 train_time:539679ms step_avg:274.79ms
step:1965/4000 train_time:540074ms step_avg:274.85ms
step:1966/4000 train_time:540469ms step_avg:274.91ms
step:1967/4000 train_time:540863ms step_avg:274.97ms
step:1968/4000 train_time:541257ms step_avg:275.03ms
step:1969/4000 train_time:541652ms step_avg:275.09ms
step:1970/4000 train_time:542045ms step_avg:275.15ms
step:1971/4000 train_time:542440ms step_avg:275.21ms
step:1972/4000 train_time:542836ms step_avg:275.27ms
step:1973/4000 train_time:543231ms step_avg:275.33ms
step:1974/4000 train_time:543626ms step_avg:275.39ms
step:1975/4000 train_time:544019ms step_avg:275.45ms
step:1976/4000 train_time:544414ms step_avg:275.51ms
step:1977/4000 train_time:544810ms step_avg:275.57ms
step:1978/4000 train_time:545205ms step_avg:275.63ms
step:1979/4000 train_time:545600ms step_avg:275.69ms
step:1980/4000 train_time:545995ms step_avg:275.75ms
step:1981/4000 train_time:546391ms step_avg:275.82ms
step:1982/4000 train_time:546785ms step_avg:275.88ms
step:1983/4000 train_time:547179ms step_avg:275.94ms
step:1984/4000 train_time:547575ms step_avg:276.00ms
step:1985/4000 train_time:547972ms step_avg:276.06ms
step:1986/4000 train_time:548368ms step_avg:276.12ms
step:1987/4000 train_time:548762ms step_avg:276.18ms
step:1988/4000 train_time:549158ms step_avg:276.24ms
step:1989/4000 train_time:549552ms step_avg:276.30ms
step:1990/4000 train_time:549946ms step_avg:276.35ms
step:1991/4000 train_time:550342ms step_avg:276.41ms
step:1992/4000 train_time:550737ms step_avg:276.47ms
step:1993/4000 train_time:551131ms step_avg:276.53ms
step:1994/4000 train_time:551526ms step_avg:276.59ms
step:1995/4000 train_time:551922ms step_avg:276.65ms
step:1996/4000 train_time:552317ms step_avg:276.71ms
step:1997/4000 train_time:552712ms step_avg:276.77ms
step:1998/4000 train_time:553106ms step_avg:276.83ms
step:1999/4000 train_time:553501ms step_avg:276.89ms
step:2000/4000 train_time:553895ms step_avg:276.95ms
step:2000/4000 val_bpb:1.0901 train_time:553929ms step_avg:276.96ms
step:2001/4000 train_time:554292ms step_avg:277.01ms
step:2002/4000 train_time:554686ms step_avg:277.07ms
step:2003/4000 train_time:555080ms step_avg:277.12ms
step:2004/4000 train_time:555474ms step_avg:277.18ms
step:2005/4000 train_time:555869ms step_avg:277.24ms
step:2006/4000 train_time:556265ms step_avg:277.30ms
step:2007/4000 train_time:556661ms step_avg:277.36ms
step:2008/4000 train_time:557057ms step_avg:277.42ms
step:2009/4000 train_time:557452ms step_avg:277.48ms
step:2010/4000 train_time:557847ms step_avg:277.54ms
step:2011/4000 train_time:558241ms step_avg:277.59ms
step:2012/4000 train_time:558636ms step_avg:277.65ms
step:2013/4000 train_time:559032ms step_avg:277.71ms
step:2014/4000 train_time:559428ms step_avg:277.77ms
step:2015/4000 train_time:559823ms step_avg:277.83ms
step:2016/4000 train_time:560218ms step_avg:277.89ms
step:2017/4000 train_time:560614ms step_avg:277.94ms
step:2018/4000 train_time:561009ms step_avg:278.00ms
step:2019/4000 train_time:561404ms step_avg:278.06ms
step:2020/4000 train_time:561799ms step_avg:278.12ms
step:2021/4000 train_time:562194ms step_avg:278.18ms
step:2022/4000 train_time:562589ms step_avg:278.23ms
step:2023/4000 train_time:562984ms step_avg:278.29ms
step:2024/4000 train_time:563379ms step_avg:278.35ms
step:2025/4000 train_time:563775ms step_avg:278.41ms
step:2026/4000 train_time:564169ms step_avg:278.46ms
step:2027/4000 train_time:564566ms step_avg:278.52ms
step:2028/4000 train_time:564963ms step_avg:278.58ms
step:2029/4000 train_time:565357ms step_avg:278.64ms
step:2030/4000 train_time:565754ms step_avg:278.70ms
step:2031/4000 train_time:566149ms step_avg:278.75ms
step:2032/4000 train_time:566543ms step_avg:278.81ms
step:2033/4000 train_time:566938ms step_avg:278.87ms
step:2034/4000 train_time:567332ms step_avg:278.92ms
step:2035/4000 train_time:567727ms step_avg:278.98ms
step:2036/4000 train_time:568125ms step_avg:279.04ms
step:2037/4000 train_time:568519ms step_avg:279.10ms
step:2038/4000 train_time:568913ms step_avg:279.15ms
step:2039/4000 train_time:569308ms step_avg:279.21ms
step:2040/4000 train_time:569702ms step_avg:279.27ms
step:2041/4000 train_time:570097ms step_avg:279.32ms
step:2042/4000 train_time:570493ms step_avg:279.38ms
step:2043/4000 train_time:570887ms step_avg:279.44ms
step:2044/4000 train_time:571282ms step_avg:279.49ms
step:2045/4000 train_time:571677ms step_avg:279.55ms
step:2046/4000 train_time:572072ms step_avg:279.61ms
step:2047/4000 train_time:572468ms step_avg:279.66ms
step:2048/4000 train_time:572864ms step_avg:279.72ms
step:2049/4000 train_time:573260ms step_avg:279.78ms
step:2050/4000 train_time:573656ms step_avg:279.83ms
step:2051/4000 train_time:574051ms step_avg:279.89ms
step:2052/4000 train_time:574446ms step_avg:279.94ms
step:2053/4000 train_time:574842ms step_avg:280.00ms
step:2054/4000 train_time:575237ms step_avg:280.06ms
step:2055/4000 train_time:575633ms step_avg:280.11ms
step:2056/4000 train_time:576027ms step_avg:280.17ms
step:2057/4000 train_time:576422ms step_avg:280.22ms
step:2058/4000 train_time:576818ms step_avg:280.28ms
step:2059/4000 train_time:577215ms step_avg:280.34ms
step:2060/4000 train_time:577610ms step_avg:280.39ms
step:2061/4000 train_time:578006ms step_avg:280.45ms
step:2062/4000 train_time:578400ms step_avg:280.50ms
step:2063/4000 train_time:578794ms step_avg:280.56ms
step:2064/4000 train_time:579189ms step_avg:280.61ms
step:2065/4000 train_time:579583ms step_avg:280.67ms
step:2066/4000 train_time:579977ms step_avg:280.72ms
step:2067/4000 train_time:580374ms step_avg:280.78ms
step:2068/4000 train_time:580769ms step_avg:280.84ms
step:2069/4000 train_time:581165ms step_avg:280.89ms
step:2070/4000 train_time:581561ms step_avg:280.95ms
step:2071/4000 train_time:581956ms step_avg:281.00ms
step:2072/4000 train_time:582352ms step_avg:281.06ms
step:2073/4000 train_time:582746ms step_avg:281.11ms
step:2074/4000 train_time:583141ms step_avg:281.17ms
step:2075/4000 train_time:583536ms step_avg:281.22ms
step:2076/4000 train_time:583932ms step_avg:281.28ms
step:2077/4000 train_time:584327ms step_avg:281.33ms
step:2078/4000 train_time:584722ms step_avg:281.39ms
step:2079/4000 train_time:585117ms step_avg:281.44ms
step:2080/4000 train_time:585514ms step_avg:281.50ms
step:2081/4000 train_time:585910ms step_avg:281.55ms
step:2082/4000 train_time:586306ms step_avg:281.61ms
step:2083/4000 train_time:586701ms step_avg:281.66ms
step:2084/4000 train_time:587097ms step_avg:281.72ms
step:2085/4000 train_time:587492ms step_avg:281.77ms
step:2086/4000 train_time:587887ms step_avg:281.83ms
step:2087/4000 train_time:588283ms step_avg:281.88ms
step:2088/4000 train_time:588677ms step_avg:281.93ms
step:2089/4000 train_time:589072ms step_avg:281.99ms
step:2090/4000 train_time:589467ms step_avg:282.04ms
step:2091/4000 train_time:589862ms step_avg:282.10ms
step:2092/4000 train_time:590258ms step_avg:282.15ms
step:2093/4000 train_time:590652ms step_avg:282.20ms
step:2094/4000 train_time:591048ms step_avg:282.26ms
step:2095/4000 train_time:591444ms step_avg:282.31ms
step:2096/4000 train_time:591839ms step_avg:282.37ms
step:2097/4000 train_time:592236ms step_avg:282.42ms
step:2098/4000 train_time:592633ms step_avg:282.48ms
step:2099/4000 train_time:593027ms step_avg:282.53ms
step:2100/4000 train_time:593424ms step_avg:282.58ms
step:2101/4000 train_time:593820ms step_avg:282.64ms
step:2102/4000 train_time:594217ms step_avg:282.69ms
step:2103/4000 train_time:594612ms step_avg:282.74ms
step:2104/4000 train_time:595008ms step_avg:282.80ms
step:2105/4000 train_time:595404ms step_avg:282.85ms
step:2106/4000 train_time:595801ms step_avg:282.91ms
step:2107/4000 train_time:596194ms step_avg:282.96ms
step:2108/4000 train_time:596590ms step_avg:283.01ms
step:2109/4000 train_time:596988ms step_avg:283.07ms
step:2110/4000 train_time:597382ms step_avg:283.12ms
step:2111/4000 train_time:597778ms step_avg:283.17ms
step:2112/4000 train_time:598174ms step_avg:283.23ms
step:2113/4000 train_time:598569ms step_avg:283.28ms
step:2114/4000 train_time:598965ms step_avg:283.33ms
step:2115/4000 train_time:599359ms step_avg:283.38ms
step:2116/4000 train_time:599758ms step_avg:283.44ms
step:2117/4000 train_time:600154ms step_avg:283.49ms
step:2118/4000 train_time:600549ms step_avg:283.55ms
step:2119/4000 train_time:600944ms step_avg:283.60ms
step:2120/4000 train_time:601338ms step_avg:283.65ms
step:2121/4000 train_time:601734ms step_avg:283.70ms
step:2122/4000 train_time:602129ms step_avg:283.76ms
step:2123/4000 train_time:602524ms step_avg:283.81ms
step:2124/4000 train_time:602920ms step_avg:283.86ms
step:2125/4000 train_time:603317ms step_avg:283.91ms
step:2126/4000 train_time:603712ms step_avg:283.97ms
step:2127/4000 train_time:604108ms step_avg:284.02ms
step:2128/4000 train_time:604504ms step_avg:284.07ms
step:2129/4000 train_time:604899ms step_avg:284.12ms
step:2130/4000 train_time:605295ms step_avg:284.18ms
step:2131/4000 train_time:605691ms step_avg:284.23ms
step:2132/4000 train_time:606087ms step_avg:284.28ms
step:2133/4000 train_time:606482ms step_avg:284.33ms
step:2134/4000 train_time:606878ms step_avg:284.39ms
step:2135/4000 train_time:607276ms step_avg:284.44ms
step:2136/4000 train_time:607674ms step_avg:284.49ms
step:2137/4000 train_time:608069ms step_avg:284.54ms
step:2138/4000 train_time:608463ms step_avg:284.59ms
step:2139/4000 train_time:608859ms step_avg:284.65ms
step:2140/4000 train_time:609257ms step_avg:284.70ms
step:2141/4000 train_time:609652ms step_avg:284.75ms
step:2142/4000 train_time:610047ms step_avg:284.80ms
step:2143/4000 train_time:610443ms step_avg:284.85ms
step:2144/4000 train_time:610839ms step_avg:284.91ms
step:2145/4000 train_time:611236ms step_avg:284.96ms
step:2146/4000 train_time:611633ms step_avg:285.01ms
step:2147/4000 train_time:612030ms step_avg:285.06ms
step:2148/4000 train_time:612426ms step_avg:285.11ms
step:2149/4000 train_time:612821ms step_avg:285.17ms
step:2150/4000 train_time:613219ms step_avg:285.22ms
step:2151/4000 train_time:613616ms step_avg:285.27ms
step:2152/4000 train_time:614012ms step_avg:285.32ms
step:2153/4000 train_time:614408ms step_avg:285.37ms
step:2154/4000 train_time:614805ms step_avg:285.42ms
step:2155/4000 train_time:615200ms step_avg:285.48ms
step:2156/4000 train_time:615596ms step_avg:285.53ms
step:2157/4000 train_time:615992ms step_avg:285.58ms
step:2158/4000 train_time:616387ms step_avg:285.63ms
step:2159/4000 train_time:616784ms step_avg:285.68ms
step:2160/4000 train_time:617180ms step_avg:285.73ms
step:2161/4000 train_time:617577ms step_avg:285.78ms
step:2162/4000 train_time:617973ms step_avg:285.83ms
step:2163/4000 train_time:618367ms step_avg:285.88ms
step:2164/4000 train_time:618766ms step_avg:285.94ms
step:2165/4000 train_time:619163ms step_avg:285.99ms
step:2166/4000 train_time:619559ms step_avg:286.04ms
step:2167/4000 train_time:619957ms step_avg:286.09ms
step:2168/4000 train_time:620353ms step_avg:286.14ms
step:2169/4000 train_time:620749ms step_avg:286.19ms
step:2170/4000 train_time:621145ms step_avg:286.24ms
step:2171/4000 train_time:621538ms step_avg:286.29ms
step:2172/4000 train_time:621934ms step_avg:286.34ms
step:2173/4000 train_time:622330ms step_avg:286.39ms
step:2174/4000 train_time:622727ms step_avg:286.44ms
step:2175/4000 train_time:623123ms step_avg:286.49ms
step:2176/4000 train_time:623520ms step_avg:286.54ms
step:2177/4000 train_time:623918ms step_avg:286.60ms
step:2178/4000 train_time:624315ms step_avg:286.65ms
step:2179/4000 train_time:624709ms step_avg:286.70ms
step:2180/4000 train_time:625105ms step_avg:286.75ms
step:2181/4000 train_time:625501ms step_avg:286.80ms
step:2182/4000 train_time:625898ms step_avg:286.85ms
step:2183/4000 train_time:626294ms step_avg:286.90ms
step:2184/4000 train_time:626690ms step_avg:286.95ms
step:2185/4000 train_time:627088ms step_avg:287.00ms
step:2186/4000 train_time:627483ms step_avg:287.05ms
step:2187/4000 train_time:627879ms step_avg:287.10ms
step:2188/4000 train_time:628275ms step_avg:287.15ms
step:2189/4000 train_time:628669ms step_avg:287.19ms
step:2190/4000 train_time:629066ms step_avg:287.24ms
step:2191/4000 train_time:629461ms step_avg:287.29ms
step:2192/4000 train_time:629857ms step_avg:287.34ms
step:2193/4000 train_time:630255ms step_avg:287.39ms
step:2194/4000 train_time:630650ms step_avg:287.44ms
step:2195/4000 train_time:631046ms step_avg:287.49ms
step:2196/4000 train_time:631442ms step_avg:287.54ms
step:2197/4000 train_time:631838ms step_avg:287.59ms
step:2198/4000 train_time:632234ms step_avg:287.64ms
step:2199/4000 train_time:632629ms step_avg:287.69ms
step:2200/4000 train_time:633027ms step_avg:287.74ms
step:2201/4000 train_time:633424ms step_avg:287.79ms
step:2202/4000 train_time:633820ms step_avg:287.84ms
step:2203/4000 train_time:634218ms step_avg:287.89ms
step:2204/4000 train_time:634614ms step_avg:287.94ms
step:2205/4000 train_time:635009ms step_avg:287.99ms
step:2206/4000 train_time:635407ms step_avg:288.04ms
step:2207/4000 train_time:635804ms step_avg:288.09ms
step:2208/4000 train_time:636201ms step_avg:288.13ms
step:2209/4000 train_time:636597ms step_avg:288.18ms
step:2210/4000 train_time:636994ms step_avg:288.23ms
step:2211/4000 train_time:637390ms step_avg:288.28ms
step:2212/4000 train_time:637786ms step_avg:288.33ms
step:2213/4000 train_time:638182ms step_avg:288.38ms
step:2214/4000 train_time:638577ms step_avg:288.43ms
step:2215/4000 train_time:638974ms step_avg:288.48ms
step:2216/4000 train_time:639369ms step_avg:288.52ms
step:2217/4000 train_time:639766ms step_avg:288.57ms
step:2218/4000 train_time:640165ms step_avg:288.62ms
step:2219/4000 train_time:640562ms step_avg:288.67ms
step:2220/4000 train_time:640961ms step_avg:288.72ms
step:2221/4000 train_time:641358ms step_avg:288.77ms
step:2222/4000 train_time:641755ms step_avg:288.82ms
step:2223/4000 train_time:642152ms step_avg:288.87ms
step:2224/4000 train_time:642548ms step_avg:288.92ms
step:2225/4000 train_time:642945ms step_avg:288.96ms
step:2226/4000 train_time:643341ms step_avg:289.01ms
step:2227/4000 train_time:643737ms step_avg:289.06ms
step:2228/4000 train_time:644132ms step_avg:289.11ms
step:2229/4000 train_time:644528ms step_avg:289.16ms
step:2230/4000 train_time:644925ms step_avg:289.20ms
step:2231/4000 train_time:645322ms step_avg:289.25ms
step:2232/4000 train_time:645718ms step_avg:289.30ms
step:2233/4000 train_time:646117ms step_avg:289.35ms
step:2234/4000 train_time:646514ms step_avg:289.40ms
step:2235/4000 train_time:646911ms step_avg:289.45ms
step:2236/4000 train_time:647309ms step_avg:289.49ms
step:2237/4000 train_time:647707ms step_avg:289.54ms
step:2238/4000 train_time:648103ms step_avg:289.59ms
step:2239/4000 train_time:648499ms step_avg:289.64ms
step:2240/4000 train_time:648895ms step_avg:289.69ms
step:2241/4000 train_time:649290ms step_avg:289.73ms
step:2242/4000 train_time:649688ms step_avg:289.78ms
step:2243/4000 train_time:650086ms step_avg:289.83ms
step:2244/4000 train_time:650480ms step_avg:289.88ms
step:2245/4000 train_time:650878ms step_avg:289.92ms
step:2246/4000 train_time:651275ms step_avg:289.97ms
step:2247/4000 train_time:651672ms step_avg:290.02ms
step:2248/4000 train_time:652070ms step_avg:290.07ms
step:2249/4000 train_time:652468ms step_avg:290.11ms
step:2250/4000 train_time:652865ms step_avg:290.16ms
step:2250/4000 val_bpb:1.0472 train_time:652900ms step_avg:290.18ms
step:2251/4000 train_time:653263ms step_avg:290.21ms
step:2252/4000 train_time:653659ms step_avg:290.26ms
step:2253/4000 train_time:654055ms step_avg:290.30ms
step:2254/4000 train_time:654451ms step_avg:290.35ms
step:2255/4000 train_time:654846ms step_avg:290.40ms
step:2256/4000 train_time:655240ms step_avg:290.44ms
step:2257/4000 train_time:655636ms step_avg:290.49ms
step:2258/4000 train_time:656031ms step_avg:290.54ms
step:2259/4000 train_time:656428ms step_avg:290.58ms
step:2260/4000 train_time:656825ms step_avg:290.63ms
step:2261/4000 train_time:657220ms step_avg:290.68ms
step:2262/4000 train_time:657616ms step_avg:290.72ms
step:2263/4000 train_time:658012ms step_avg:290.77ms
step:2264/4000 train_time:658408ms step_avg:290.82ms
step:2265/4000 train_time:658803ms step_avg:290.86ms
step:2266/4000 train_time:659199ms step_avg:290.91ms
step:2267/4000 train_time:659592ms step_avg:290.95ms
step:2268/4000 train_time:659989ms step_avg:291.00ms
step:2269/4000 train_time:660383ms step_avg:291.05ms
step:2270/4000 train_time:660779ms step_avg:291.09ms
step:2271/4000 train_time:661175ms step_avg:291.14ms
step:2272/4000 train_time:661571ms step_avg:291.18ms
step:2273/4000 train_time:661968ms step_avg:291.23ms
step:2274/4000 train_time:662362ms step_avg:291.28ms
step:2275/4000 train_time:662759ms step_avg:291.32ms
step:2276/4000 train_time:663153ms step_avg:291.37ms
step:2277/4000 train_time:663550ms step_avg:291.41ms
step:2278/4000 train_time:663948ms step_avg:291.46ms
step:2279/4000 train_time:664342ms step_avg:291.51ms
step:2280/4000 train_time:664736ms step_avg:291.55ms
step:2281/4000 train_time:665133ms step_avg:291.60ms
step:2282/4000 train_time:665530ms step_avg:291.64ms
step:2283/4000 train_time:665926ms step_avg:291.69ms
step:2284/4000 train_time:666323ms step_avg:291.74ms
step:2285/4000 train_time:666719ms step_avg:291.78ms
step:2286/4000 train_time:667115ms step_avg:291.83ms
step:2287/4000 train_time:667510ms step_avg:291.87ms
step:2288/4000 train_time:667906ms step_avg:291.92ms
step:2289/4000 train_time:668301ms step_avg:291.96ms
step:2290/4000 train_time:668697ms step_avg:292.01ms
step:2291/4000 train_time:669093ms step_avg:292.05ms
step:2292/4000 train_time:669489ms step_avg:292.10ms
step:2293/4000 train_time:669884ms step_avg:292.14ms
step:2294/4000 train_time:670283ms step_avg:292.19ms
step:2295/4000 train_time:670681ms step_avg:292.24ms
step:2296/4000 train_time:671078ms step_avg:292.28ms
step:2297/4000 train_time:671473ms step_avg:292.33ms
step:2298/4000 train_time:671872ms step_avg:292.37ms
step:2299/4000 train_time:672269ms step_avg:292.42ms
step:2300/4000 train_time:672666ms step_avg:292.46ms
step:2301/4000 train_time:673062ms step_avg:292.51ms
step:2302/4000 train_time:673458ms step_avg:292.55ms
step:2303/4000 train_time:673854ms step_avg:292.60ms
step:2304/4000 train_time:674252ms step_avg:292.64ms
step:2305/4000 train_time:674648ms step_avg:292.69ms
step:2306/4000 train_time:675044ms step_avg:292.73ms
step:2307/4000 train_time:675440ms step_avg:292.78ms
step:2308/4000 train_time:675838ms step_avg:292.82ms
step:2309/4000 train_time:676234ms step_avg:292.87ms
step:2310/4000 train_time:676631ms step_avg:292.91ms
step:2311/4000 train_time:677025ms step_avg:292.96ms
step:2312/4000 train_time:677422ms step_avg:293.00ms
step:2313/4000 train_time:677820ms step_avg:293.05ms
step:2314/4000 train_time:678215ms step_avg:293.09ms
step:2315/4000 train_time:678612ms step_avg:293.14ms
step:2316/4000 train_time:679008ms step_avg:293.18ms
step:2317/4000 train_time:679405ms step_avg:293.23ms
step:2318/4000 train_time:679802ms step_avg:293.27ms
step:2319/4000 train_time:680198ms step_avg:293.32ms
step:2320/4000 train_time:680594ms step_avg:293.36ms
step:2321/4000 train_time:680989ms step_avg:293.40ms
step:2322/4000 train_time:681385ms step_avg:293.45ms
step:2323/4000 train_time:681781ms step_avg:293.49ms
step:2324/4000 train_time:682178ms step_avg:293.54ms
step:2325/4000 train_time:682572ms step_avg:293.58ms
step:2326/4000 train_time:682969ms step_avg:293.62ms
step:2327/4000 train_time:683365ms step_avg:293.67ms
step:2328/4000 train_time:683759ms step_avg:293.71ms
step:2329/4000 train_time:684156ms step_avg:293.76ms
step:2330/4000 train_time:684552ms step_avg:293.80ms
step:2331/4000 train_time:684949ms step_avg:293.84ms
step:2332/4000 train_time:685347ms step_avg:293.89ms
step:2333/4000 train_time:685741ms step_avg:293.93ms
step:2334/4000 train_time:686138ms step_avg:293.98ms
step:2335/4000 train_time:686533ms step_avg:294.02ms
step:2336/4000 train_time:686931ms step_avg:294.06ms
step:2337/4000 train_time:687328ms step_avg:294.11ms
step:2338/4000 train_time:687725ms step_avg:294.15ms
step:2339/4000 train_time:688121ms step_avg:294.19ms
step:2340/4000 train_time:688519ms step_avg:294.24ms
step:2341/4000 train_time:688916ms step_avg:294.28ms
step:2342/4000 train_time:689312ms step_avg:294.33ms
step:2343/4000 train_time:689710ms step_avg:294.37ms
step:2344/4000 train_time:690106ms step_avg:294.41ms
step:2345/4000 train_time:690503ms step_avg:294.46ms
step:2346/4000 train_time:690900ms step_avg:294.50ms
step:2347/4000 train_time:691294ms step_avg:294.54ms
step:2348/4000 train_time:691690ms step_avg:294.59ms
step:2349/4000 train_time:692088ms step_avg:294.63ms
step:2350/4000 train_time:692485ms step_avg:294.67ms
step:2351/4000 train_time:692881ms step_avg:294.72ms
step:2352/4000 train_time:693277ms step_avg:294.76ms
step:2353/4000 train_time:693675ms step_avg:294.80ms
step:2354/4000 train_time:694073ms step_avg:294.85ms
step:2355/4000 train_time:694469ms step_avg:294.89ms
step:2356/4000 train_time:694866ms step_avg:294.93ms
step:2357/4000 train_time:695264ms step_avg:294.98ms
step:2358/4000 train_time:695663ms step_avg:295.02ms
step:2359/4000 train_time:696060ms step_avg:295.07ms
step:2360/4000 train_time:696455ms step_avg:295.11ms
step:2361/4000 train_time:696852ms step_avg:295.15ms
step:2362/4000 train_time:697249ms step_avg:295.19ms
step:2363/4000 train_time:697646ms step_avg:295.24ms
step:2364/4000 train_time:698043ms step_avg:295.28ms
step:2365/4000 train_time:698440ms step_avg:295.32ms
step:2366/4000 train_time:698836ms step_avg:295.37ms
step:2367/4000 train_time:699234ms step_avg:295.41ms
step:2368/4000 train_time:699630ms step_avg:295.45ms
step:2369/4000 train_time:700028ms step_avg:295.50ms
step:2370/4000 train_time:700424ms step_avg:295.54ms
step:2371/4000 train_time:700821ms step_avg:295.58ms
step:2372/4000 train_time:701219ms step_avg:295.62ms
step:2373/4000 train_time:701618ms step_avg:295.67ms
step:2374/4000 train_time:702015ms step_avg:295.71ms
step:2375/4000 train_time:702411ms step_avg:295.75ms
step:2376/4000 train_time:702811ms step_avg:295.80ms
step:2377/4000 train_time:703208ms step_avg:295.84ms
step:2378/4000 train_time:703605ms step_avg:295.88ms
step:2379/4000 train_time:704002ms step_avg:295.92ms
step:2380/4000 train_time:704401ms step_avg:295.97ms
step:2381/4000 train_time:704798ms step_avg:296.01ms
step:2382/4000 train_time:705195ms step_avg:296.05ms
step:2383/4000 train_time:705592ms step_avg:296.09ms
step:2384/4000 train_time:705990ms step_avg:296.14ms
step:2385/4000 train_time:706387ms step_avg:296.18ms
step:2386/4000 train_time:706784ms step_avg:296.22ms
step:2387/4000 train_time:707183ms step_avg:296.26ms
step:2388/4000 train_time:707584ms step_avg:296.31ms
step:2389/4000 train_time:707981ms step_avg:296.35ms
step:2390/4000 train_time:708377ms step_avg:296.39ms
step:2391/4000 train_time:708773ms step_avg:296.43ms
step:2392/4000 train_time:709172ms step_avg:296.48ms
step:2393/4000 train_time:709569ms step_avg:296.52ms
step:2394/4000 train_time:709967ms step_avg:296.56ms
step:2395/4000 train_time:710363ms step_avg:296.60ms
step:2396/4000 train_time:710762ms step_avg:296.65ms
step:2397/4000 train_time:711160ms step_avg:296.69ms
step:2398/4000 train_time:711557ms step_avg:296.73ms
step:2399/4000 train_time:711952ms step_avg:296.77ms
step:2400/4000 train_time:712350ms step_avg:296.81ms
step:2401/4000 train_time:712747ms step_avg:296.85ms
step:2402/4000 train_time:713143ms step_avg:296.90ms
step:2403/4000 train_time:713541ms step_avg:296.94ms
step:2404/4000 train_time:713939ms step_avg:296.98ms
step:2405/4000 train_time:714336ms step_avg:297.02ms
step:2406/4000 train_time:714734ms step_avg:297.06ms
step:2407/4000 train_time:715133ms step_avg:297.11ms
step:2408/4000 train_time:715531ms step_avg:297.15ms
step:2409/4000 train_time:715931ms step_avg:297.19ms
step:2410/4000 train_time:716328ms step_avg:297.23ms
step:2411/4000 train_time:716725ms step_avg:297.27ms
step:2412/4000 train_time:717123ms step_avg:297.31ms
step:2413/4000 train_time:717522ms step_avg:297.36ms
step:2414/4000 train_time:717921ms step_avg:297.40ms
step:2415/4000 train_time:718319ms step_avg:297.44ms
step:2416/4000 train_time:718716ms step_avg:297.48ms
step:2417/4000 train_time:719112ms step_avg:297.52ms
step:2418/4000 train_time:719510ms step_avg:297.56ms
step:2419/4000 train_time:719906ms step_avg:297.60ms
step:2420/4000 train_time:720304ms step_avg:297.65ms
step:2421/4000 train_time:720703ms step_avg:297.69ms
step:2422/4000 train_time:721101ms step_avg:297.73ms
step:2423/4000 train_time:721498ms step_avg:297.77ms
step:2424/4000 train_time:721894ms step_avg:297.81ms
step:2425/4000 train_time:722292ms step_avg:297.85ms
step:2426/4000 train_time:722688ms step_avg:297.89ms
step:2427/4000 train_time:723084ms step_avg:297.93ms
step:2428/4000 train_time:723483ms step_avg:297.97ms
step:2429/4000 train_time:723881ms step_avg:298.02ms
step:2430/4000 train_time:724281ms step_avg:298.06ms
step:2431/4000 train_time:724678ms step_avg:298.10ms
step:2432/4000 train_time:725076ms step_avg:298.14ms
step:2433/4000 train_time:725474ms step_avg:298.18ms
step:2434/4000 train_time:725872ms step_avg:298.22ms
step:2435/4000 train_time:726271ms step_avg:298.26ms
step:2436/4000 train_time:726669ms step_avg:298.30ms
step:2437/4000 train_time:727065ms step_avg:298.34ms
step:2438/4000 train_time:727462ms step_avg:298.38ms
step:2439/4000 train_time:727859ms step_avg:298.43ms
step:2440/4000 train_time:728258ms step_avg:298.47ms
step:2441/4000 train_time:728656ms step_avg:298.51ms
step:2442/4000 train_time:729054ms step_avg:298.55ms
step:2443/4000 train_time:729452ms step_avg:298.59ms
step:2444/4000 train_time:729850ms step_avg:298.63ms
step:2445/4000 train_time:730247ms step_avg:298.67ms
step:2446/4000 train_time:730645ms step_avg:298.71ms
step:2447/4000 train_time:731042ms step_avg:298.75ms
step:2448/4000 train_time:731441ms step_avg:298.79ms
step:2449/4000 train_time:731839ms step_avg:298.83ms
step:2450/4000 train_time:732238ms step_avg:298.87ms
step:2451/4000 train_time:732636ms step_avg:298.91ms
step:2452/4000 train_time:733034ms step_avg:298.95ms
step:2453/4000 train_time:733433ms step_avg:298.99ms
step:2454/4000 train_time:733829ms step_avg:299.03ms
step:2455/4000 train_time:734229ms step_avg:299.07ms
step:2456/4000 train_time:734629ms step_avg:299.12ms
step:2457/4000 train_time:735025ms step_avg:299.16ms
step:2458/4000 train_time:735424ms step_avg:299.20ms
step:2459/4000 train_time:735820ms step_avg:299.24ms
step:2460/4000 train_time:736218ms step_avg:299.28ms
step:2461/4000 train_time:736615ms step_avg:299.32ms
step:2462/4000 train_time:737012ms step_avg:299.35ms
step:2463/4000 train_time:737409ms step_avg:299.39ms
step:2464/4000 train_time:737805ms step_avg:299.43ms
step:2465/4000 train_time:738203ms step_avg:299.47ms
step:2466/4000 train_time:738602ms step_avg:299.51ms
step:2467/4000 train_time:738999ms step_avg:299.55ms
step:2468/4000 train_time:739396ms step_avg:299.59ms
step:2469/4000 train_time:739793ms step_avg:299.63ms
step:2470/4000 train_time:740193ms step_avg:299.67ms
step:2471/4000 train_time:740590ms step_avg:299.71ms
step:2472/4000 train_time:740987ms step_avg:299.75ms
step:2473/4000 train_time:741385ms step_avg:299.79ms
step:2474/4000 train_time:741784ms step_avg:299.83ms
step:2475/4000 train_time:742183ms step_avg:299.87ms
step:2476/4000 train_time:742584ms step_avg:299.91ms
step:2477/4000 train_time:742983ms step_avg:299.95ms
step:2478/4000 train_time:743382ms step_avg:299.99ms
step:2479/4000 train_time:743781ms step_avg:300.03ms
step:2480/4000 train_time:744179ms step_avg:300.07ms
step:2481/4000 train_time:744576ms step_avg:300.11ms
step:2482/4000 train_time:744971ms step_avg:300.15ms
step:2483/4000 train_time:745371ms step_avg:300.19ms
step:2484/4000 train_time:745770ms step_avg:300.23ms
step:2485/4000 train_time:746166ms step_avg:300.27ms
step:2486/4000 train_time:746563ms step_avg:300.31ms
step:2487/4000 train_time:746961ms step_avg:300.35ms
step:2488/4000 train_time:747360ms step_avg:300.39ms
step:2489/4000 train_time:747755ms step_avg:300.42ms
step:2490/4000 train_time:748154ms step_avg:300.46ms
step:2491/4000 train_time:748550ms step_avg:300.50ms
step:2492/4000 train_time:748947ms step_avg:300.54ms
step:2493/4000 train_time:749344ms step_avg:300.58ms
step:2494/4000 train_time:749740ms step_avg:300.62ms
step:2495/4000 train_time:750139ms step_avg:300.66ms
step:2496/4000 train_time:750538ms step_avg:300.70ms
step:2497/4000 train_time:750936ms step_avg:300.74ms
step:2498/4000 train_time:751335ms step_avg:300.77ms
step:2499/4000 train_time:751733ms step_avg:300.81ms
step:2500/4000 train_time:752133ms step_avg:300.85ms
step:2500/4000 val_bpb:1.0140 train_time:752168ms step_avg:300.87ms
step:2501/4000 train_time:752533ms step_avg:300.89ms
step:2502/4000 train_time:752933ms step_avg:300.93ms
step:2503/4000 train_time:753331ms step_avg:300.97ms
step:2504/4000 train_time:753730ms step_avg:301.01ms
step:2505/4000 train_time:754128ms step_avg:301.05ms
step:2506/4000 train_time:754527ms step_avg:301.09ms
step:2507/4000 train_time:754926ms step_avg:301.13ms
step:2508/4000 train_time:755324ms step_avg:301.17ms
step:2509/4000 train_time:755722ms step_avg:301.20ms
step:2510/4000 train_time:756118ms step_avg:301.24ms
step:2511/4000 train_time:756514ms step_avg:301.28ms
step:2512/4000 train_time:756911ms step_avg:301.32ms
step:2513/4000 train_time:757308ms step_avg:301.36ms
step:2514/4000 train_time:757707ms step_avg:301.40ms
step:2515/4000 train_time:758104ms step_avg:301.43ms
step:2516/4000 train_time:758502ms step_avg:301.47ms
step:2517/4000 train_time:758899ms step_avg:301.51ms
step:2518/4000 train_time:759295ms step_avg:301.55ms
step:2519/4000 train_time:759693ms step_avg:301.58ms
step:2520/4000 train_time:760093ms step_avg:301.62ms
step:2521/4000 train_time:760489ms step_avg:301.66ms
step:2522/4000 train_time:760888ms step_avg:301.70ms
step:2523/4000 train_time:761288ms step_avg:301.74ms
step:2524/4000 train_time:761687ms step_avg:301.78ms
step:2525/4000 train_time:762087ms step_avg:301.82ms
step:2526/4000 train_time:762487ms step_avg:301.86ms
step:2527/4000 train_time:762886ms step_avg:301.89ms
step:2528/4000 train_time:763284ms step_avg:301.93ms
step:2529/4000 train_time:763682ms step_avg:301.97ms
step:2530/4000 train_time:764081ms step_avg:302.01ms
step:2531/4000 train_time:764479ms step_avg:302.05ms
step:2532/4000 train_time:764879ms step_avg:302.08ms
step:2533/4000 train_time:765277ms step_avg:302.12ms
step:2534/4000 train_time:765675ms step_avg:302.16ms
step:2535/4000 train_time:766073ms step_avg:302.20ms
step:2536/4000 train_time:766471ms step_avg:302.24ms
step:2537/4000 train_time:766868ms step_avg:302.27ms
step:2538/4000 train_time:767267ms step_avg:302.31ms
step:2539/4000 train_time:767664ms step_avg:302.35ms
step:2540/4000 train_time:768062ms step_avg:302.39ms
step:2541/4000 train_time:768460ms step_avg:302.42ms
step:2542/4000 train_time:768858ms step_avg:302.46ms
step:2543/4000 train_time:769256ms step_avg:302.50ms
step:2544/4000 train_time:769654ms step_avg:302.54ms
step:2545/4000 train_time:770051ms step_avg:302.57ms
step:2546/4000 train_time:770448ms step_avg:302.61ms
step:2547/4000 train_time:770848ms step_avg:302.65ms
step:2548/4000 train_time:771248ms step_avg:302.69ms
step:2549/4000 train_time:771645ms step_avg:302.72ms
step:2550/4000 train_time:772043ms step_avg:302.76ms
step:2551/4000 train_time:772441ms step_avg:302.80ms
step:2552/4000 train_time:772841ms step_avg:302.84ms
step:2553/4000 train_time:773239ms step_avg:302.87ms
step:2554/4000 train_time:773638ms step_avg:302.91ms
step:2555/4000 train_time:774038ms step_avg:302.95ms
step:2556/4000 train_time:774439ms step_avg:302.99ms
step:2557/4000 train_time:774836ms step_avg:303.03ms
step:2558/4000 train_time:775233ms step_avg:303.06ms
step:2559/4000 train_time:775630ms step_avg:303.10ms
step:2560/4000 train_time:776029ms step_avg:303.14ms
step:2561/4000 train_time:776427ms step_avg:303.17ms
step:2562/4000 train_time:776827ms step_avg:303.21ms
step:2563/4000 train_time:777225ms step_avg:303.25ms
step:2564/4000 train_time:777625ms step_avg:303.29ms
step:2565/4000 train_time:778024ms step_avg:303.32ms
step:2566/4000 train_time:778421ms step_avg:303.36ms
step:2567/4000 train_time:778818ms step_avg:303.40ms
step:2568/4000 train_time:779217ms step_avg:303.43ms
step:2569/4000 train_time:779612ms step_avg:303.47ms
step:2570/4000 train_time:780009ms step_avg:303.51ms
step:2571/4000 train_time:780408ms step_avg:303.54ms
step:2572/4000 train_time:780808ms step_avg:303.58ms
step:2573/4000 train_time:781206ms step_avg:303.62ms
step:2574/4000 train_time:781605ms step_avg:303.65ms
step:2575/4000 train_time:782002ms step_avg:303.69ms
step:2576/4000 train_time:782401ms step_avg:303.73ms
step:2577/4000 train_time:782799ms step_avg:303.76ms
step:2578/4000 train_time:783197ms step_avg:303.80ms
step:2579/4000 train_time:783594ms step_avg:303.84ms
step:2580/4000 train_time:783992ms step_avg:303.87ms
step:2581/4000 train_time:784389ms step_avg:303.91ms
step:2582/4000 train_time:784789ms step_avg:303.95ms
step:2583/4000 train_time:785189ms step_avg:303.98ms
step:2584/4000 train_time:785590ms step_avg:304.02ms
step:2585/4000 train_time:785987ms step_avg:304.06ms
step:2586/4000 train_time:786385ms step_avg:304.09ms
step:2587/4000 train_time:786782ms step_avg:304.13ms
step:2588/4000 train_time:787180ms step_avg:304.17ms
step:2589/4000 train_time:787578ms step_avg:304.20ms
step:2590/4000 train_time:787978ms step_avg:304.24ms
step:2591/4000 train_time:788375ms step_avg:304.27ms
step:2592/4000 train_time:788772ms step_avg:304.31ms
step:2593/4000 train_time:789170ms step_avg:304.35ms
step:2594/4000 train_time:789567ms step_avg:304.38ms
step:2595/4000 train_time:789964ms step_avg:304.42ms
step:2596/4000 train_time:790360ms step_avg:304.45ms
step:2597/4000 train_time:790758ms step_avg:304.49ms
step:2598/4000 train_time:791157ms step_avg:304.53ms
step:2599/4000 train_time:791555ms step_avg:304.56ms
step:2600/4000 train_time:791951ms step_avg:304.60ms
step:2601/4000 train_time:792348ms step_avg:304.63ms
step:2602/4000 train_time:792746ms step_avg:304.67ms
step:2603/4000 train_time:793145ms step_avg:304.70ms
step:2604/4000 train_time:793543ms step_avg:304.74ms
step:2605/4000 train_time:793941ms step_avg:304.78ms
step:2606/4000 train_time:794337ms step_avg:304.81ms
step:2607/4000 train_time:794736ms step_avg:304.85ms
step:2608/4000 train_time:795134ms step_avg:304.88ms
step:2609/4000 train_time:795532ms step_avg:304.92ms
step:2610/4000 train_time:795930ms step_avg:304.95ms
step:2611/4000 train_time:796329ms step_avg:304.99ms
step:2612/4000 train_time:796727ms step_avg:305.03ms
step:2613/4000 train_time:797125ms step_avg:305.06ms
step:2614/4000 train_time:797523ms step_avg:305.10ms
step:2615/4000 train_time:797922ms step_avg:305.13ms
step:2616/4000 train_time:798320ms step_avg:305.17ms
step:2617/4000 train_time:798717ms step_avg:305.20ms
step:2618/4000 train_time:799115ms step_avg:305.24ms
step:2619/4000 train_time:799513ms step_avg:305.27ms
step:2620/4000 train_time:799910ms step_avg:305.31ms
step:2621/4000 train_time:800308ms step_avg:305.34ms
step:2622/4000 train_time:800707ms step_avg:305.38ms
step:2623/4000 train_time:801102ms step_avg:305.41ms
step:2624/4000 train_time:801501ms step_avg:305.45ms
step:2625/4000 train_time:801900ms step_avg:305.49ms
step:2626/4000 train_time:802297ms step_avg:305.52ms
step:2627/4000 train_time:802694ms step_avg:305.56ms
step:2628/4000 train_time:803092ms step_avg:305.59ms
step:2629/4000 train_time:803489ms step_avg:305.63ms
step:2630/4000 train_time:803889ms step_avg:305.66ms
step:2631/4000 train_time:804287ms step_avg:305.70ms
step:2632/4000 train_time:804687ms step_avg:305.73ms
step:2633/4000 train_time:805085ms step_avg:305.77ms
step:2634/4000 train_time:805481ms step_avg:305.80ms
step:2635/4000 train_time:805879ms step_avg:305.84ms
step:2636/4000 train_time:806276ms step_avg:305.87ms
step:2637/4000 train_time:806674ms step_avg:305.91ms
step:2638/4000 train_time:807072ms step_avg:305.94ms
step:2639/4000 train_time:807469ms step_avg:305.98ms
step:2640/4000 train_time:807866ms step_avg:306.01ms
step:2641/4000 train_time:808431ms step_avg:306.11ms
step:2642/4000 train_time:809017ms step_avg:306.21ms
step:2643/4000 train_time:809606ms step_avg:306.32ms
step:2644/4000 train_time:810192ms step_avg:306.43ms
step:2645/4000 train_time:810780ms step_avg:306.53ms
step:2646/4000 train_time:811370ms step_avg:306.64ms
step:2647/4000 train_time:811954ms step_avg:306.75ms
step:2648/4000 train_time:812542ms step_avg:306.85ms
step:2649/4000 train_time:813131ms step_avg:306.96ms
step:2650/4000 train_time:813719ms step_avg:307.06ms
step:2651/4000 train_time:814307ms step_avg:307.17ms
step:2652/4000 train_time:814894ms step_avg:307.28ms
step:2653/4000 train_time:815480ms step_avg:307.38ms
step:2654/4000 train_time:816068ms step_avg:307.49ms
step:2655/4000 train_time:816656ms step_avg:307.59ms
step:2656/4000 train_time:817245ms step_avg:307.70ms
step:2657/4000 train_time:817832ms step_avg:307.80ms
step:2658/4000 train_time:818420ms step_avg:307.91ms
step:2659/4000 train_time:819006ms step_avg:308.01ms
step:2660/4000 train_time:819593ms step_avg:308.12ms
step:2661/4000 train_time:820181ms step_avg:308.22ms
step:2662/4000 train_time:820768ms step_avg:308.33ms
step:2663/4000 train_time:821354ms step_avg:308.43ms
step:2664/4000 train_time:821943ms step_avg:308.54ms
step:2665/4000 train_time:822533ms step_avg:308.64ms
step:2666/4000 train_time:823120ms step_avg:308.75ms
step:2667/4000 train_time:823707ms step_avg:308.85ms
step:2668/4000 train_time:824291ms step_avg:308.95ms
step:2669/4000 train_time:824879ms step_avg:309.06ms
step:2670/4000 train_time:825468ms step_avg:309.16ms
step:2671/4000 train_time:826055ms step_avg:309.27ms
step:2672/4000 train_time:826641ms step_avg:309.37ms
step:2673/4000 train_time:827225ms step_avg:309.47ms
step:2674/4000 train_time:827812ms step_avg:309.58ms
step:2675/4000 train_time:828401ms step_avg:309.68ms
step:2676/4000 train_time:828990ms step_avg:309.79ms
step:2677/4000 train_time:829577ms step_avg:309.89ms
step:2678/4000 train_time:830165ms step_avg:309.99ms
step:2679/4000 train_time:830753ms step_avg:310.10ms
step:2680/4000 train_time:831342ms step_avg:310.20ms
step:2681/4000 train_time:831929ms step_avg:310.31ms
step:2682/4000 train_time:832514ms step_avg:310.41ms
step:2683/4000 train_time:833102ms step_avg:310.51ms
step:2684/4000 train_time:833689ms step_avg:310.61ms
step:2685/4000 train_time:834277ms step_avg:310.72ms
step:2686/4000 train_time:834864ms step_avg:310.82ms
step:2687/4000 train_time:835453ms step_avg:310.92ms
step:2688/4000 train_time:836044ms step_avg:311.03ms
step:2689/4000 train_time:836631ms step_avg:311.13ms
step:2690/4000 train_time:837218ms step_avg:311.23ms
step:2691/4000 train_time:837806ms step_avg:311.34ms
step:2692/4000 train_time:838392ms step_avg:311.44ms
step:2693/4000 train_time:838980ms step_avg:311.54ms
step:2694/4000 train_time:839566ms step_avg:311.64ms
step:2695/4000 train_time:840153ms step_avg:311.75ms
step:2696/4000 train_time:840742ms step_avg:311.85ms
step:2697/4000 train_time:841330ms step_avg:311.95ms
step:2698/4000 train_time:841914ms step_avg:312.05ms
step:2699/4000 train_time:842502ms step_avg:312.15ms
step:2700/4000 train_time:843089ms step_avg:312.26ms
step:2701/4000 train_time:843676ms step_avg:312.36ms
step:2702/4000 train_time:844265ms step_avg:312.46ms
step:2703/4000 train_time:844851ms step_avg:312.56ms
step:2704/4000 train_time:845438ms step_avg:312.66ms
step:2705/4000 train_time:846024ms step_avg:312.76ms
step:2706/4000 train_time:846612ms step_avg:312.86ms
step:2707/4000 train_time:847200ms step_avg:312.97ms
step:2708/4000 train_time:847786ms step_avg:313.07ms
step:2709/4000 train_time:848372ms step_avg:313.17ms
step:2710/4000 train_time:848959ms step_avg:313.27ms
step:2711/4000 train_time:849546ms step_avg:313.37ms
step:2712/4000 train_time:850134ms step_avg:313.47ms
step:2713/4000 train_time:850722ms step_avg:313.57ms
step:2714/4000 train_time:851309ms step_avg:313.67ms
step:2715/4000 train_time:851895ms step_avg:313.77ms
step:2716/4000 train_time:852480ms step_avg:313.87ms
step:2717/4000 train_time:853066ms step_avg:313.97ms
step:2718/4000 train_time:853654ms step_avg:314.07ms
step:2719/4000 train_time:854244ms step_avg:314.18ms
step:2720/4000 train_time:854832ms step_avg:314.28ms
step:2721/4000 train_time:855422ms step_avg:314.38ms
step:2722/4000 train_time:856008ms step_avg:314.48ms
step:2723/4000 train_time:856595ms step_avg:314.58ms
step:2724/4000 train_time:857181ms step_avg:314.68ms
step:2725/4000 train_time:857769ms step_avg:314.78ms
step:2726/4000 train_time:858357ms step_avg:314.88ms
step:2727/4000 train_time:858942ms step_avg:314.98ms
step:2728/4000 train_time:859531ms step_avg:315.08ms
step:2729/4000 train_time:860120ms step_avg:315.18ms
step:2730/4000 train_time:860706ms step_avg:315.28ms
step:2731/4000 train_time:861292ms step_avg:315.38ms
step:2732/4000 train_time:861880ms step_avg:315.48ms
step:2733/4000 train_time:862467ms step_avg:315.58ms
step:2734/4000 train_time:863056ms step_avg:315.68ms
step:2735/4000 train_time:863644ms step_avg:315.77ms
step:2736/4000 train_time:864232ms step_avg:315.87ms
step:2737/4000 train_time:864819ms step_avg:315.97ms
step:2738/4000 train_time:865408ms step_avg:316.07ms
step:2739/4000 train_time:865996ms step_avg:316.17ms
step:2740/4000 train_time:866583ms step_avg:316.27ms
step:2741/4000 train_time:867169ms step_avg:316.37ms
step:2742/4000 train_time:867759ms step_avg:316.47ms
step:2743/4000 train_time:868349ms step_avg:316.57ms
step:2744/4000 train_time:868936ms step_avg:316.67ms
step:2745/4000 train_time:869525ms step_avg:316.77ms
step:2746/4000 train_time:870113ms step_avg:316.87ms
step:2747/4000 train_time:870700ms step_avg:316.96ms
step:2748/4000 train_time:871289ms step_avg:317.06ms
step:2749/4000 train_time:871874ms step_avg:317.16ms
step:2750/4000 train_time:872462ms step_avg:317.26ms
step:2750/4000 val_bpb:0.9832 train_time:872520ms step_avg:317.28ms
step:2751/4000 train_time:873046ms step_avg:317.36ms
step:2752/4000 train_time:873630ms step_avg:317.45ms
step:2753/4000 train_time:874208ms step_avg:317.55ms
step:2754/4000 train_time:874790ms step_avg:317.64ms
step:2755/4000 train_time:875371ms step_avg:317.74ms
step:2756/4000 train_time:875953ms step_avg:317.83ms
step:2757/4000 train_time:876535ms step_avg:317.93ms
step:2758/4000 train_time:877119ms step_avg:318.03ms
step:2759/4000 train_time:877700ms step_avg:318.12ms
step:2760/4000 train_time:878283ms step_avg:318.22ms
step:2761/4000 train_time:878867ms step_avg:318.31ms
step:2762/4000 train_time:879448ms step_avg:318.41ms
step:2763/4000 train_time:880030ms step_avg:318.51ms
step:2764/4000 train_time:880614ms step_avg:318.60ms
step:2765/4000 train_time:881197ms step_avg:318.70ms
step:2766/4000 train_time:881782ms step_avg:318.79ms
step:2767/4000 train_time:882365ms step_avg:318.89ms
step:2768/4000 train_time:882948ms step_avg:318.98ms
step:2769/4000 train_time:883530ms step_avg:319.08ms
step:2770/4000 train_time:884112ms step_avg:319.17ms
step:2771/4000 train_time:884695ms step_avg:319.27ms
step:2772/4000 train_time:885277ms step_avg:319.36ms
step:2773/4000 train_time:885862ms step_avg:319.46ms
step:2774/4000 train_time:886447ms step_avg:319.56ms
step:2775/4000 train_time:887029ms step_avg:319.65ms
step:2776/4000 train_time:887613ms step_avg:319.75ms
step:2777/4000 train_time:888196ms step_avg:319.84ms
step:2778/4000 train_time:888781ms step_avg:319.94ms
step:2779/4000 train_time:889365ms step_avg:320.03ms
step:2780/4000 train_time:889951ms step_avg:320.13ms
step:2781/4000 train_time:890535ms step_avg:320.22ms
step:2782/4000 train_time:891117ms step_avg:320.32ms
step:2783/4000 train_time:891703ms step_avg:320.41ms
step:2784/4000 train_time:892288ms step_avg:320.51ms
step:2785/4000 train_time:892871ms step_avg:320.60ms
step:2786/4000 train_time:893455ms step_avg:320.69ms
step:2787/4000 train_time:894037ms step_avg:320.79ms
step:2788/4000 train_time:894621ms step_avg:320.88ms
step:2789/4000 train_time:895203ms step_avg:320.98ms
step:2790/4000 train_time:895788ms step_avg:321.07ms
step:2791/4000 train_time:896371ms step_avg:321.16ms
step:2792/4000 train_time:896954ms step_avg:321.26ms
step:2793/4000 train_time:897537ms step_avg:321.35ms
step:2794/4000 train_time:898123ms step_avg:321.45ms
step:2795/4000 train_time:898705ms step_avg:321.54ms
step:2796/4000 train_time:899290ms step_avg:321.63ms
step:2797/4000 train_time:899875ms step_avg:321.73ms
step:2798/4000 train_time:900458ms step_avg:321.82ms
step:2799/4000 train_time:901041ms step_avg:321.92ms
step:2800/4000 train_time:901627ms step_avg:322.01ms
step:2801/4000 train_time:902210ms step_avg:322.10ms
step:2802/4000 train_time:902797ms step_avg:322.20ms
step:2803/4000 train_time:903381ms step_avg:322.29ms
step:2804/4000 train_time:903965ms step_avg:322.38ms
step:2805/4000 train_time:904549ms step_avg:322.48ms
step:2806/4000 train_time:905132ms step_avg:322.57ms
step:2807/4000 train_time:905717ms step_avg:322.66ms
step:2808/4000 train_time:906300ms step_avg:322.76ms
step:2809/4000 train_time:906885ms step_avg:322.85ms
step:2810/4000 train_time:907469ms step_avg:322.94ms
step:2811/4000 train_time:908052ms step_avg:323.04ms
step:2812/4000 train_time:908638ms step_avg:323.13ms
step:2813/4000 train_time:909221ms step_avg:323.22ms
step:2814/4000 train_time:909809ms step_avg:323.32ms
step:2815/4000 train_time:910392ms step_avg:323.41ms
step:2816/4000 train_time:910977ms step_avg:323.50ms
step:2817/4000 train_time:911560ms step_avg:323.59ms
step:2818/4000 train_time:912144ms step_avg:323.68ms
step:2819/4000 train_time:912727ms step_avg:323.78ms
step:2820/4000 train_time:913314ms step_avg:323.87ms
step:2821/4000 train_time:913899ms step_avg:323.96ms
step:2822/4000 train_time:914486ms step_avg:324.06ms
step:2823/4000 train_time:915070ms step_avg:324.15ms
step:2824/4000 train_time:915655ms step_avg:324.24ms
step:2825/4000 train_time:916238ms step_avg:324.33ms
step:2826/4000 train_time:916822ms step_avg:324.42ms
step:2827/4000 train_time:917405ms step_avg:324.52ms
step:2828/4000 train_time:917989ms step_avg:324.61ms
step:2829/4000 train_time:918574ms step_avg:324.70ms
step:2830/4000 train_time:919160ms step_avg:324.79ms
step:2831/4000 train_time:919743ms step_avg:324.88ms
step:2832/4000 train_time:920325ms step_avg:324.97ms
step:2833/4000 train_time:920908ms step_avg:325.06ms
step:2834/4000 train_time:921493ms step_avg:325.16ms
step:2835/4000 train_time:922077ms step_avg:325.25ms
step:2836/4000 train_time:922663ms step_avg:325.34ms
step:2837/4000 train_time:923249ms step_avg:325.43ms
step:2838/4000 train_time:923832ms step_avg:325.52ms
step:2839/4000 train_time:924415ms step_avg:325.61ms
step:2840/4000 train_time:924999ms step_avg:325.70ms
step:2841/4000 train_time:925584ms step_avg:325.80ms
step:2842/4000 train_time:926169ms step_avg:325.89ms
step:2843/4000 train_time:926754ms step_avg:325.98ms
step:2844/4000 train_time:927337ms step_avg:326.07ms
step:2845/4000 train_time:927923ms step_avg:326.16ms
step:2846/4000 train_time:928507ms step_avg:326.25ms
step:2847/4000 train_time:929091ms step_avg:326.34ms
step:2848/4000 train_time:929677ms step_avg:326.43ms
step:2849/4000 train_time:930259ms step_avg:326.52ms
step:2850/4000 train_time:930844ms step_avg:326.61ms
step:2851/4000 train_time:931429ms step_avg:326.70ms
step:2852/4000 train_time:932013ms step_avg:326.79ms
step:2853/4000 train_time:932598ms step_avg:326.88ms
step:2854/4000 train_time:933181ms step_avg:326.97ms
step:2855/4000 train_time:933766ms step_avg:327.06ms
step:2856/4000 train_time:934347ms step_avg:327.15ms
step:2857/4000 train_time:934931ms step_avg:327.24ms
step:2858/4000 train_time:935516ms step_avg:327.33ms
step:2859/4000 train_time:936099ms step_avg:327.42ms
step:2860/4000 train_time:936682ms step_avg:327.51ms
step:2861/4000 train_time:937268ms step_avg:327.60ms
step:2862/4000 train_time:937851ms step_avg:327.69ms
step:2863/4000 train_time:938435ms step_avg:327.78ms
step:2864/4000 train_time:939021ms step_avg:327.87ms
step:2865/4000 train_time:939605ms step_avg:327.96ms
step:2866/4000 train_time:940188ms step_avg:328.05ms
step:2867/4000 train_time:940771ms step_avg:328.14ms
step:2868/4000 train_time:941355ms step_avg:328.23ms
step:2869/4000 train_time:941942ms step_avg:328.32ms
step:2870/4000 train_time:942527ms step_avg:328.41ms
step:2871/4000 train_time:943110ms step_avg:328.50ms
step:2872/4000 train_time:943695ms step_avg:328.58ms
step:2873/4000 train_time:944277ms step_avg:328.67ms
step:2874/4000 train_time:944862ms step_avg:328.76ms
step:2875/4000 train_time:945447ms step_avg:328.85ms
step:2876/4000 train_time:946031ms step_avg:328.94ms
step:2877/4000 train_time:946616ms step_avg:329.03ms
step:2878/4000 train_time:947199ms step_avg:329.12ms
step:2879/4000 train_time:947783ms step_avg:329.21ms
step:2880/4000 train_time:948369ms step_avg:329.29ms
step:2881/4000 train_time:948952ms step_avg:329.38ms
step:2882/4000 train_time:949536ms step_avg:329.47ms
step:2883/4000 train_time:950117ms step_avg:329.56ms
step:2884/4000 train_time:950702ms step_avg:329.65ms
step:2885/4000 train_time:951286ms step_avg:329.74ms
step:2886/4000 train_time:951872ms step_avg:329.82ms
step:2887/4000 train_time:952456ms step_avg:329.91ms
step:2888/4000 train_time:953041ms step_avg:330.00ms
step:2889/4000 train_time:953623ms step_avg:330.09ms
step:2890/4000 train_time:954209ms step_avg:330.18ms
step:2891/4000 train_time:954793ms step_avg:330.26ms
step:2892/4000 train_time:955376ms step_avg:330.35ms
step:2893/4000 train_time:955961ms step_avg:330.44ms
step:2894/4000 train_time:956545ms step_avg:330.53ms
step:2895/4000 train_time:957130ms step_avg:330.61ms
step:2896/4000 train_time:957712ms step_avg:330.70ms
step:2897/4000 train_time:958295ms step_avg:330.79ms
step:2898/4000 train_time:958880ms step_avg:330.88ms
step:2899/4000 train_time:959464ms step_avg:330.96ms
step:2900/4000 train_time:960051ms step_avg:331.05ms
step:2901/4000 train_time:960636ms step_avg:331.14ms
step:2902/4000 train_time:961220ms step_avg:331.23ms
step:2903/4000 train_time:961804ms step_avg:331.31ms
step:2904/4000 train_time:962390ms step_avg:331.40ms
step:2905/4000 train_time:962973ms step_avg:331.49ms
step:2906/4000 train_time:963558ms step_avg:331.58ms
step:2907/4000 train_time:964140ms step_avg:331.66ms
step:2908/4000 train_time:964726ms step_avg:331.75ms
step:2909/4000 train_time:965309ms step_avg:331.84ms
step:2910/4000 train_time:965893ms step_avg:331.92ms
step:2911/4000 train_time:966476ms step_avg:332.01ms
step:2912/4000 train_time:967060ms step_avg:332.09ms
step:2913/4000 train_time:967645ms step_avg:332.18ms
step:2914/4000 train_time:968231ms step_avg:332.27ms
step:2915/4000 train_time:968813ms step_avg:332.35ms
step:2916/4000 train_time:969400ms step_avg:332.44ms
step:2917/4000 train_time:969984ms step_avg:332.53ms
step:2918/4000 train_time:970568ms step_avg:332.61ms
step:2919/4000 train_time:971152ms step_avg:332.70ms
step:2920/4000 train_time:971736ms step_avg:332.79ms
step:2921/4000 train_time:972319ms step_avg:332.87ms
step:2922/4000 train_time:972901ms step_avg:332.96ms
step:2923/4000 train_time:973485ms step_avg:333.04ms
step:2924/4000 train_time:974071ms step_avg:333.13ms
step:2925/4000 train_time:974654ms step_avg:333.22ms
step:2926/4000 train_time:975236ms step_avg:333.30ms
step:2927/4000 train_time:975821ms step_avg:333.39ms
step:2928/4000 train_time:976407ms step_avg:333.47ms
step:2929/4000 train_time:976990ms step_avg:333.56ms
step:2930/4000 train_time:977577ms step_avg:333.64ms
step:2931/4000 train_time:978160ms step_avg:333.73ms
step:2932/4000 train_time:978744ms step_avg:333.81ms
step:2933/4000 train_time:979327ms step_avg:333.90ms
step:2934/4000 train_time:979912ms step_avg:333.98ms
step:2935/4000 train_time:980493ms step_avg:334.07ms
step:2936/4000 train_time:981075ms step_avg:334.15ms
step:2937/4000 train_time:981658ms step_avg:334.24ms
step:2938/4000 train_time:982241ms step_avg:334.32ms
step:2939/4000 train_time:982825ms step_avg:334.41ms
step:2940/4000 train_time:983409ms step_avg:334.49ms
step:2941/4000 train_time:983993ms step_avg:334.58ms
step:2942/4000 train_time:984576ms step_avg:334.66ms
step:2943/4000 train_time:985159ms step_avg:334.75ms
step:2944/4000 train_time:985740ms step_avg:334.83ms
step:2945/4000 train_time:986324ms step_avg:334.91ms
step:2946/4000 train_time:986908ms step_avg:335.00ms
step:2947/4000 train_time:987492ms step_avg:335.08ms
step:2948/4000 train_time:988075ms step_avg:335.17ms
step:2949/4000 train_time:988659ms step_avg:335.25ms
step:2950/4000 train_time:989243ms step_avg:335.34ms
step:2951/4000 train_time:989827ms step_avg:335.42ms
step:2952/4000 train_time:990413ms step_avg:335.51ms
step:2953/4000 train_time:990996ms step_avg:335.59ms
step:2954/4000 train_time:991580ms step_avg:335.67ms
step:2955/4000 train_time:992160ms step_avg:335.76ms
step:2956/4000 train_time:992744ms step_avg:335.84ms
step:2957/4000 train_time:993327ms step_avg:335.92ms
step:2958/4000 train_time:993909ms step_avg:336.01ms
step:2959/4000 train_time:994491ms step_avg:336.09ms
step:2960/4000 train_time:995074ms step_avg:336.17ms
step:2961/4000 train_time:995658ms step_avg:336.26ms
step:2962/4000 train_time:996241ms step_avg:336.34ms
step:2963/4000 train_time:996827ms step_avg:336.42ms
step:2964/4000 train_time:997409ms step_avg:336.51ms
step:2965/4000 train_time:997991ms step_avg:336.59ms
step:2966/4000 train_time:998576ms step_avg:336.67ms
step:2967/4000 train_time:999158ms step_avg:336.76ms
step:2968/4000 train_time:999741ms step_avg:336.84ms
step:2969/4000 train_time:1000324ms step_avg:336.92ms
step:2970/4000 train_time:1000907ms step_avg:337.01ms
step:2971/4000 train_time:1001489ms step_avg:337.09ms
step:2972/4000 train_time:1002073ms step_avg:337.17ms
step:2973/4000 train_time:1002657ms step_avg:337.25ms
step:2974/4000 train_time:1003239ms step_avg:337.34ms
step:2975/4000 train_time:1003821ms step_avg:337.42ms
step:2976/4000 train_time:1004405ms step_avg:337.50ms
step:2977/4000 train_time:1004990ms step_avg:337.58ms
step:2978/4000 train_time:1005573ms step_avg:337.67ms
step:2979/4000 train_time:1006155ms step_avg:337.75ms
step:2980/4000 train_time:1006738ms step_avg:337.83ms
step:2981/4000 train_time:1007321ms step_avg:337.91ms
step:2982/4000 train_time:1007904ms step_avg:338.00ms
step:2983/4000 train_time:1008486ms step_avg:338.08ms
step:2984/4000 train_time:1009070ms step_avg:338.16ms
step:2985/4000 train_time:1009651ms step_avg:338.24ms
step:2986/4000 train_time:1010232ms step_avg:338.32ms
step:2987/4000 train_time:1010818ms step_avg:338.41ms
step:2988/4000 train_time:1011399ms step_avg:338.49ms
step:2989/4000 train_time:1011980ms step_avg:338.57ms
step:2990/4000 train_time:1012566ms step_avg:338.65ms
step:2991/4000 train_time:1013149ms step_avg:338.73ms
step:2992/4000 train_time:1013732ms step_avg:338.81ms
step:2993/4000 train_time:1014313ms step_avg:338.90ms
step:2994/4000 train_time:1014897ms step_avg:338.98ms
step:2995/4000 train_time:1015479ms step_avg:339.06ms
step:2996/4000 train_time:1016060ms step_avg:339.14ms
step:2997/4000 train_time:1016644ms step_avg:339.22ms
step:2998/4000 train_time:1017228ms step_avg:339.30ms
step:2999/4000 train_time:1017809ms step_avg:339.38ms
step:3000/4000 train_time:1018391ms step_avg:339.46ms
step:3000/4000 val_bpb:0.9658 train_time:1018449ms step_avg:339.48ms
step:3001/4000 train_time:1018975ms step_avg:339.55ms
step:3002/4000 train_time:1019557ms step_avg:339.63ms
step:3003/4000 train_time:1020142ms step_avg:339.71ms
step:3004/4000 train_time:1020725ms step_avg:339.79ms
step:3005/4000 train_time:1021308ms step_avg:339.87ms
step:3006/4000 train_time:1021893ms step_avg:339.95ms
step:3007/4000 train_time:1022475ms step_avg:340.03ms
step:3008/4000 train_time:1023058ms step_avg:340.11ms
step:3009/4000 train_time:1023640ms step_avg:340.19ms
step:3010/4000 train_time:1024225ms step_avg:340.27ms
step:3011/4000 train_time:1024807ms step_avg:340.35ms
step:3012/4000 train_time:1025390ms step_avg:340.43ms
step:3013/4000 train_time:1025974ms step_avg:340.52ms
step:3014/4000 train_time:1026558ms step_avg:340.60ms
step:3015/4000 train_time:1027142ms step_avg:340.68ms
step:3016/4000 train_time:1027725ms step_avg:340.76ms
step:3017/4000 train_time:1028306ms step_avg:340.84ms
step:3018/4000 train_time:1028890ms step_avg:340.92ms
step:3019/4000 train_time:1029473ms step_avg:341.00ms
step:3020/4000 train_time:1030056ms step_avg:341.08ms
step:3021/4000 train_time:1030639ms step_avg:341.16ms
step:3022/4000 train_time:1031225ms step_avg:341.24ms
step:3023/4000 train_time:1031807ms step_avg:341.32ms
step:3024/4000 train_time:1032389ms step_avg:341.40ms
step:3025/4000 train_time:1032969ms step_avg:341.48ms
step:3026/4000 train_time:1033554ms step_avg:341.56ms
step:3027/4000 train_time:1034137ms step_avg:341.64ms
step:3028/4000 train_time:1034719ms step_avg:341.72ms
step:3029/4000 train_time:1035301ms step_avg:341.80ms
step:3030/4000 train_time:1035886ms step_avg:341.88ms
step:3031/4000 train_time:1036468ms step_avg:341.96ms
step:3032/4000 train_time:1037051ms step_avg:342.04ms
step:3033/4000 train_time:1037634ms step_avg:342.11ms
step:3034/4000 train_time:1038218ms step_avg:342.19ms
step:3035/4000 train_time:1038800ms step_avg:342.27ms
step:3036/4000 train_time:1039384ms step_avg:342.35ms
step:3037/4000 train_time:1039967ms step_avg:342.43ms
step:3038/4000 train_time:1040549ms step_avg:342.51ms
step:3039/4000 train_time:1041133ms step_avg:342.59ms
step:3040/4000 train_time:1041715ms step_avg:342.67ms
step:3041/4000 train_time:1042297ms step_avg:342.75ms
step:3042/4000 train_time:1042878ms step_avg:342.83ms
step:3043/4000 train_time:1043461ms step_avg:342.91ms
step:3044/4000 train_time:1044044ms step_avg:342.98ms
step:3045/4000 train_time:1044627ms step_avg:343.06ms
step:3046/4000 train_time:1045209ms step_avg:343.14ms
step:3047/4000 train_time:1045791ms step_avg:343.22ms
step:3048/4000 train_time:1046375ms step_avg:343.30ms
step:3049/4000 train_time:1046956ms step_avg:343.38ms
step:3050/4000 train_time:1047540ms step_avg:343.46ms
step:3051/4000 train_time:1048123ms step_avg:343.53ms
step:3052/4000 train_time:1048707ms step_avg:343.61ms
step:3053/4000 train_time:1049287ms step_avg:343.69ms
step:3054/4000 train_time:1049871ms step_avg:343.77ms
step:3055/4000 train_time:1050454ms step_avg:343.85ms
step:3056/4000 train_time:1051037ms step_avg:343.93ms
step:3057/4000 train_time:1051617ms step_avg:344.00ms
step:3058/4000 train_time:1052198ms step_avg:344.08ms
step:3059/4000 train_time:1052779ms step_avg:344.16ms
step:3060/4000 train_time:1053361ms step_avg:344.24ms
step:3061/4000 train_time:1053945ms step_avg:344.31ms
step:3062/4000 train_time:1054525ms step_avg:344.39ms
step:3063/4000 train_time:1055108ms step_avg:344.47ms
step:3064/4000 train_time:1055690ms step_avg:344.55ms
step:3065/4000 train_time:1056274ms step_avg:344.62ms
step:3066/4000 train_time:1056857ms step_avg:344.70ms
step:3067/4000 train_time:1057443ms step_avg:344.78ms
step:3068/4000 train_time:1058027ms step_avg:344.86ms
step:3069/4000 train_time:1058607ms step_avg:344.94ms
step:3070/4000 train_time:1059189ms step_avg:345.01ms
step:3071/4000 train_time:1059771ms step_avg:345.09ms
step:3072/4000 train_time:1060354ms step_avg:345.17ms
step:3073/4000 train_time:1060935ms step_avg:345.24ms
step:3074/4000 train_time:1061517ms step_avg:345.32ms
step:3075/4000 train_time:1062098ms step_avg:345.40ms
step:3076/4000 train_time:1062678ms step_avg:345.47ms
step:3077/4000 train_time:1063261ms step_avg:345.55ms
step:3078/4000 train_time:1063844ms step_avg:345.63ms
step:3079/4000 train_time:1064426ms step_avg:345.71ms
step:3080/4000 train_time:1065007ms step_avg:345.78ms
step:3081/4000 train_time:1065590ms step_avg:345.86ms
step:3082/4000 train_time:1066171ms step_avg:345.93ms
step:3083/4000 train_time:1066754ms step_avg:346.01ms
step:3084/4000 train_time:1067335ms step_avg:346.09ms
step:3085/4000 train_time:1067916ms step_avg:346.16ms
step:3086/4000 train_time:1068498ms step_avg:346.24ms
step:3087/4000 train_time:1069079ms step_avg:346.32ms
step:3088/4000 train_time:1069662ms step_avg:346.39ms
step:3089/4000 train_time:1070245ms step_avg:346.47ms
step:3090/4000 train_time:1070827ms step_avg:346.55ms
step:3091/4000 train_time:1071409ms step_avg:346.62ms
step:3092/4000 train_time:1071991ms step_avg:346.70ms
step:3093/4000 train_time:1072574ms step_avg:346.77ms
step:3094/4000 train_time:1073157ms step_avg:346.85ms
step:3095/4000 train_time:1073741ms step_avg:346.93ms
step:3096/4000 train_time:1074323ms step_avg:347.00ms
step:3097/4000 train_time:1074907ms step_avg:347.08ms
step:3098/4000 train_time:1075489ms step_avg:347.16ms
step:3099/4000 train_time:1076069ms step_avg:347.23ms
step:3100/4000 train_time:1076652ms step_avg:347.31ms
step:3101/4000 train_time:1077236ms step_avg:347.38ms
step:3102/4000 train_time:1077819ms step_avg:347.46ms
step:3103/4000 train_time:1078401ms step_avg:347.54ms
step:3104/4000 train_time:1078982ms step_avg:347.61ms
step:3105/4000 train_time:1079564ms step_avg:347.69ms
step:3106/4000 train_time:1080147ms step_avg:347.76ms
step:3107/4000 train_time:1080730ms step_avg:347.84ms
step:3108/4000 train_time:1081311ms step_avg:347.91ms
step:3109/4000 train_time:1081893ms step_avg:347.99ms
step:3110/4000 train_time:1082475ms step_avg:348.06ms
step:3111/4000 train_time:1083059ms step_avg:348.14ms
step:3112/4000 train_time:1083644ms step_avg:348.21ms
step:3113/4000 train_time:1084226ms step_avg:348.29ms
step:3114/4000 train_time:1084808ms step_avg:348.36ms
step:3115/4000 train_time:1085390ms step_avg:348.44ms
step:3116/4000 train_time:1085972ms step_avg:348.51ms
step:3117/4000 train_time:1086557ms step_avg:348.59ms
step:3118/4000 train_time:1087140ms step_avg:348.67ms
step:3119/4000 train_time:1087722ms step_avg:348.74ms
step:3120/4000 train_time:1088304ms step_avg:348.82ms
step:3121/4000 train_time:1088887ms step_avg:348.89ms
step:3122/4000 train_time:1089469ms step_avg:348.97ms
step:3123/4000 train_time:1090051ms step_avg:349.04ms
step:3124/4000 train_time:1090633ms step_avg:349.11ms
step:3125/4000 train_time:1091214ms step_avg:349.19ms
step:3126/4000 train_time:1091796ms step_avg:349.26ms
step:3127/4000 train_time:1092378ms step_avg:349.34ms
step:3128/4000 train_time:1092961ms step_avg:349.41ms
step:3129/4000 train_time:1093544ms step_avg:349.49ms
step:3130/4000 train_time:1094127ms step_avg:349.56ms
step:3131/4000 train_time:1094708ms step_avg:349.64ms
step:3132/4000 train_time:1095289ms step_avg:349.71ms
step:3133/4000 train_time:1095871ms step_avg:349.78ms
step:3134/4000 train_time:1096453ms step_avg:349.86ms
step:3135/4000 train_time:1097036ms step_avg:349.93ms
step:3136/4000 train_time:1097618ms step_avg:350.01ms
step:3137/4000 train_time:1098200ms step_avg:350.08ms
step:3138/4000 train_time:1098781ms step_avg:350.15ms
step:3139/4000 train_time:1099365ms step_avg:350.23ms
step:3140/4000 train_time:1099947ms step_avg:350.30ms
step:3141/4000 train_time:1100528ms step_avg:350.38ms
step:3142/4000 train_time:1101111ms step_avg:350.45ms
step:3143/4000 train_time:1101695ms step_avg:350.52ms
step:3144/4000 train_time:1102275ms step_avg:350.60ms
step:3145/4000 train_time:1102859ms step_avg:350.67ms
step:3146/4000 train_time:1103442ms step_avg:350.74ms
step:3147/4000 train_time:1104024ms step_avg:350.82ms
step:3148/4000 train_time:1104607ms step_avg:350.89ms
step:3149/4000 train_time:1105190ms step_avg:350.97ms
step:3150/4000 train_time:1105772ms step_avg:351.04ms
step:3151/4000 train_time:1106353ms step_avg:351.11ms
step:3152/4000 train_time:1106938ms step_avg:351.19ms
step:3153/4000 train_time:1107519ms step_avg:351.26ms
step:3154/4000 train_time:1108102ms step_avg:351.33ms
step:3155/4000 train_time:1108684ms step_avg:351.41ms
step:3156/4000 train_time:1109265ms step_avg:351.48ms
step:3157/4000 train_time:1109849ms step_avg:351.55ms
step:3158/4000 train_time:1110430ms step_avg:351.62ms
step:3159/4000 train_time:1111012ms step_avg:351.70ms
step:3160/4000 train_time:1111595ms step_avg:351.77ms
step:3161/4000 train_time:1112176ms step_avg:351.84ms
step:3162/4000 train_time:1112755ms step_avg:351.92ms
step:3163/4000 train_time:1113337ms step_avg:351.99ms
step:3164/4000 train_time:1113917ms step_avg:352.06ms
step:3165/4000 train_time:1114496ms step_avg:352.13ms
step:3166/4000 train_time:1115078ms step_avg:352.20ms
step:3167/4000 train_time:1115659ms step_avg:352.28ms
step:3168/4000 train_time:1116241ms step_avg:352.35ms
step:3169/4000 train_time:1116824ms step_avg:352.42ms
step:3170/4000 train_time:1117408ms step_avg:352.49ms
step:3171/4000 train_time:1117989ms step_avg:352.57ms
step:3172/4000 train_time:1118570ms step_avg:352.64ms
step:3173/4000 train_time:1119152ms step_avg:352.71ms
step:3174/4000 train_time:1119736ms step_avg:352.78ms
step:3175/4000 train_time:1120314ms step_avg:352.85ms
step:3176/4000 train_time:1120897ms step_avg:352.93ms
step:3177/4000 train_time:1121478ms step_avg:353.00ms
step:3178/4000 train_time:1122058ms step_avg:353.07ms
step:3179/4000 train_time:1122637ms step_avg:353.14ms
step:3180/4000 train_time:1123218ms step_avg:353.21ms
step:3181/4000 train_time:1123800ms step_avg:353.29ms
step:3182/4000 train_time:1124385ms step_avg:353.36ms
step:3183/4000 train_time:1124966ms step_avg:353.43ms
step:3184/4000 train_time:1125548ms step_avg:353.50ms
step:3185/4000 train_time:1126128ms step_avg:353.57ms
step:3186/4000 train_time:1126709ms step_avg:353.64ms
step:3187/4000 train_time:1127290ms step_avg:353.72ms
step:3188/4000 train_time:1127872ms step_avg:353.79ms
step:3189/4000 train_time:1128451ms step_avg:353.86ms
step:3190/4000 train_time:1129032ms step_avg:353.93ms
step:3191/4000 train_time:1129616ms step_avg:354.00ms
step:3192/4000 train_time:1130200ms step_avg:354.07ms
step:3193/4000 train_time:1130782ms step_avg:354.14ms
step:3194/4000 train_time:1131366ms step_avg:354.22ms
step:3195/4000 train_time:1131946ms step_avg:354.29ms
step:3196/4000 train_time:1132528ms step_avg:354.36ms
step:3197/4000 train_time:1133111ms step_avg:354.43ms
step:3198/4000 train_time:1133693ms step_avg:354.50ms
step:3199/4000 train_time:1134273ms step_avg:354.57ms
step:3200/4000 train_time:1134855ms step_avg:354.64ms
step:3201/4000 train_time:1135439ms step_avg:354.71ms
step:3202/4000 train_time:1136021ms step_avg:354.78ms
step:3203/4000 train_time:1136605ms step_avg:354.86ms
step:3204/4000 train_time:1137187ms step_avg:354.93ms
step:3205/4000 train_time:1137768ms step_avg:355.00ms
step:3206/4000 train_time:1138350ms step_avg:355.07ms
step:3207/4000 train_time:1138932ms step_avg:355.14ms
step:3208/4000 train_time:1139515ms step_avg:355.21ms
step:3209/4000 train_time:1140099ms step_avg:355.28ms
step:3210/4000 train_time:1140680ms step_avg:355.35ms
step:3211/4000 train_time:1141262ms step_avg:355.42ms
step:3212/4000 train_time:1141844ms step_avg:355.49ms
step:3213/4000 train_time:1142426ms step_avg:355.56ms
step:3214/4000 train_time:1143008ms step_avg:355.63ms
step:3215/4000 train_time:1143588ms step_avg:355.70ms
step:3216/4000 train_time:1144170ms step_avg:355.77ms
step:3217/4000 train_time:1144751ms step_avg:355.84ms
step:3218/4000 train_time:1145335ms step_avg:355.92ms
step:3219/4000 train_time:1145917ms step_avg:355.99ms
step:3220/4000 train_time:1146498ms step_avg:356.06ms
step:3221/4000 train_time:1147082ms step_avg:356.13ms
step:3222/4000 train_time:1147665ms step_avg:356.20ms
step:3223/4000 train_time:1148247ms step_avg:356.27ms
step:3224/4000 train_time:1148828ms step_avg:356.34ms
step:3225/4000 train_time:1149409ms step_avg:356.41ms
step:3226/4000 train_time:1149992ms step_avg:356.48ms
step:3227/4000 train_time:1150576ms step_avg:356.55ms
step:3228/4000 train_time:1151155ms step_avg:356.62ms
step:3229/4000 train_time:1151737ms step_avg:356.69ms
step:3230/4000 train_time:1152319ms step_avg:356.76ms
step:3231/4000 train_time:1152902ms step_avg:356.83ms
step:3232/4000 train_time:1153483ms step_avg:356.89ms
step:3233/4000 train_time:1154065ms step_avg:356.96ms
step:3234/4000 train_time:1154646ms step_avg:357.03ms
step:3235/4000 train_time:1155227ms step_avg:357.10ms
step:3236/4000 train_time:1155809ms step_avg:357.17ms
step:3237/4000 train_time:1156390ms step_avg:357.24ms
step:3238/4000 train_time:1156973ms step_avg:357.31ms
step:3239/4000 train_time:1157554ms step_avg:357.38ms
step:3240/4000 train_time:1158137ms step_avg:357.45ms
step:3241/4000 train_time:1158718ms step_avg:357.52ms
step:3242/4000 train_time:1159299ms step_avg:357.59ms
step:3243/4000 train_time:1159880ms step_avg:357.66ms
step:3244/4000 train_time:1160460ms step_avg:357.73ms
step:3245/4000 train_time:1161046ms step_avg:357.80ms
step:3246/4000 train_time:1161628ms step_avg:357.86ms
step:3247/4000 train_time:1162211ms step_avg:357.93ms
step:3248/4000 train_time:1162793ms step_avg:358.00ms
step:3249/4000 train_time:1163376ms step_avg:358.07ms
step:3250/4000 train_time:1163958ms step_avg:358.14ms
step:3250/4000 val_bpb:0.9507 train_time:1164016ms step_avg:358.16ms
step:3251/4000 train_time:1164543ms step_avg:358.21ms
step:3252/4000 train_time:1165124ms step_avg:358.28ms
step:3253/4000 train_time:1165704ms step_avg:358.35ms
step:3254/4000 train_time:1166286ms step_avg:358.42ms
step:3255/4000 train_time:1166869ms step_avg:358.49ms
step:3256/4000 train_time:1167451ms step_avg:358.55ms
step:3257/4000 train_time:1168034ms step_avg:358.62ms
step:3258/4000 train_time:1168616ms step_avg:358.69ms
step:3259/4000 train_time:1169196ms step_avg:358.76ms
step:3260/4000 train_time:1169779ms step_avg:358.83ms
step:3261/4000 train_time:1170362ms step_avg:358.90ms
step:3262/4000 train_time:1170942ms step_avg:358.96ms
step:3263/4000 train_time:1171523ms step_avg:359.03ms
step:3264/4000 train_time:1172107ms step_avg:359.10ms
step:3265/4000 train_time:1172688ms step_avg:359.17ms
step:3266/4000 train_time:1173272ms step_avg:359.24ms
step:3267/4000 train_time:1173852ms step_avg:359.31ms
step:3268/4000 train_time:1174434ms step_avg:359.37ms
step:3269/4000 train_time:1175015ms step_avg:359.44ms
step:3270/4000 train_time:1175597ms step_avg:359.51ms
step:3271/4000 train_time:1176177ms step_avg:359.58ms
step:3272/4000 train_time:1176758ms step_avg:359.64ms
step:3273/4000 train_time:1177337ms step_avg:359.71ms
step:3274/4000 train_time:1177920ms step_avg:359.78ms
step:3275/4000 train_time:1178501ms step_avg:359.85ms
step:3276/4000 train_time:1179084ms step_avg:359.92ms
step:3277/4000 train_time:1179666ms step_avg:359.98ms
step:3278/4000 train_time:1180247ms step_avg:360.05ms
step:3279/4000 train_time:1180829ms step_avg:360.12ms
step:3280/4000 train_time:1181409ms step_avg:360.19ms
step:3281/4000 train_time:1181989ms step_avg:360.25ms
step:3282/4000 train_time:1182572ms step_avg:360.32ms
step:3283/4000 train_time:1183153ms step_avg:360.39ms
step:3284/4000 train_time:1183737ms step_avg:360.46ms
step:3285/4000 train_time:1184318ms step_avg:360.52ms
step:3286/4000 train_time:1184901ms step_avg:360.59ms
step:3287/4000 train_time:1185483ms step_avg:360.66ms
step:3288/4000 train_time:1186065ms step_avg:360.73ms
step:3289/4000 train_time:1186647ms step_avg:360.79ms
step:3290/4000 train_time:1187229ms step_avg:360.86ms
step:3291/4000 train_time:1187809ms step_avg:360.93ms
step:3292/4000 train_time:1188390ms step_avg:360.99ms
step:3293/4000 train_time:1188970ms step_avg:361.06ms
step:3294/4000 train_time:1189554ms step_avg:361.13ms
step:3295/4000 train_time:1190136ms step_avg:361.19ms
step:3296/4000 train_time:1190718ms step_avg:361.26ms
step:3297/4000 train_time:1191300ms step_avg:361.33ms
step:3298/4000 train_time:1191880ms step_avg:361.39ms
step:3299/4000 train_time:1192464ms step_avg:361.46ms
step:3300/4000 train_time:1193047ms step_avg:361.53ms
step:3301/4000 train_time:1193628ms step_avg:361.60ms
step:3302/4000 train_time:1194209ms step_avg:361.66ms
step:3303/4000 train_time:1194790ms step_avg:361.73ms
step:3304/4000 train_time:1195371ms step_avg:361.80ms
step:3305/4000 train_time:1195952ms step_avg:361.86ms
step:3306/4000 train_time:1196532ms step_avg:361.93ms
step:3307/4000 train_time:1197113ms step_avg:361.99ms
step:3308/4000 train_time:1197694ms step_avg:362.06ms
step:3309/4000 train_time:1198275ms step_avg:362.13ms
step:3310/4000 train_time:1198857ms step_avg:362.19ms
step:3311/4000 train_time:1199440ms step_avg:362.26ms
step:3312/4000 train_time:1200023ms step_avg:362.33ms
step:3313/4000 train_time:1200605ms step_avg:362.39ms
step:3314/4000 train_time:1201190ms step_avg:362.46ms
step:3315/4000 train_time:1201772ms step_avg:362.53ms
step:3316/4000 train_time:1202355ms step_avg:362.59ms
step:3317/4000 train_time:1202938ms step_avg:362.66ms
step:3318/4000 train_time:1203520ms step_avg:362.72ms
step:3319/4000 train_time:1204102ms step_avg:362.79ms
step:3320/4000 train_time:1204684ms step_avg:362.86ms
step:3321/4000 train_time:1205267ms step_avg:362.92ms
step:3322/4000 train_time:1205848ms step_avg:362.99ms
step:3323/4000 train_time:1206431ms step_avg:363.05ms
step:3324/4000 train_time:1207011ms step_avg:363.12ms
step:3325/4000 train_time:1207591ms step_avg:363.19ms
step:3326/4000 train_time:1208176ms step_avg:363.25ms
step:3327/4000 train_time:1208756ms step_avg:363.32ms
step:3328/4000 train_time:1209338ms step_avg:363.38ms
step:3329/4000 train_time:1209919ms step_avg:363.45ms
step:3330/4000 train_time:1210501ms step_avg:363.51ms
step:3331/4000 train_time:1211083ms step_avg:363.58ms
step:3332/4000 train_time:1211666ms step_avg:363.65ms
step:3333/4000 train_time:1212248ms step_avg:363.71ms
step:3334/4000 train_time:1212830ms step_avg:363.78ms
step:3335/4000 train_time:1213413ms step_avg:363.84ms
step:3336/4000 train_time:1213995ms step_avg:363.91ms
step:3337/4000 train_time:1214576ms step_avg:363.97ms
step:3338/4000 train_time:1215158ms step_avg:364.04ms
step:3339/4000 train_time:1215738ms step_avg:364.10ms
step:3340/4000 train_time:1216321ms step_avg:364.17ms
step:3341/4000 train_time:1216902ms step_avg:364.23ms
step:3342/4000 train_time:1217485ms step_avg:364.30ms
step:3343/4000 train_time:1218068ms step_avg:364.36ms
step:3344/4000 train_time:1218652ms step_avg:364.43ms
step:3345/4000 train_time:1219234ms step_avg:364.49ms
step:3346/4000 train_time:1219815ms step_avg:364.56ms
step:3347/4000 train_time:1220398ms step_avg:364.62ms
step:3348/4000 train_time:1220980ms step_avg:364.69ms
step:3349/4000 train_time:1221559ms step_avg:364.75ms
step:3350/4000 train_time:1222142ms step_avg:364.82ms
step:3351/4000 train_time:1222725ms step_avg:364.88ms
step:3352/4000 train_time:1223305ms step_avg:364.95ms
step:3353/4000 train_time:1223888ms step_avg:365.01ms
step:3354/4000 train_time:1224467ms step_avg:365.08ms
step:3355/4000 train_time:1225049ms step_avg:365.14ms
step:3356/4000 train_time:1225631ms step_avg:365.21ms
step:3357/4000 train_time:1226211ms step_avg:365.27ms
step:3358/4000 train_time:1226793ms step_avg:365.33ms
step:3359/4000 train_time:1227375ms step_avg:365.40ms
step:3360/4000 train_time:1227957ms step_avg:365.46ms
step:3361/4000 train_time:1228539ms step_avg:365.53ms
step:3362/4000 train_time:1229120ms step_avg:365.59ms
step:3363/4000 train_time:1229702ms step_avg:365.66ms
step:3364/4000 train_time:1230285ms step_avg:365.72ms
step:3365/4000 train_time:1230867ms step_avg:365.79ms
step:3366/4000 train_time:1231448ms step_avg:365.85ms
step:3367/4000 train_time:1232030ms step_avg:365.91ms
step:3368/4000 train_time:1232610ms step_avg:365.98ms
step:3369/4000 train_time:1233191ms step_avg:366.04ms
step:3370/4000 train_time:1233773ms step_avg:366.10ms
step:3371/4000 train_time:1234356ms step_avg:366.17ms
step:3372/4000 train_time:1234938ms step_avg:366.23ms
step:3373/4000 train_time:1235518ms step_avg:366.30ms
step:3374/4000 train_time:1236102ms step_avg:366.36ms
step:3375/4000 train_time:1236683ms step_avg:366.42ms
step:3376/4000 train_time:1237265ms step_avg:366.49ms
step:3377/4000 train_time:1237846ms step_avg:366.55ms
step:3378/4000 train_time:1238428ms step_avg:366.62ms
step:3379/4000 train_time:1239010ms step_avg:366.68ms
step:3380/4000 train_time:1239593ms step_avg:366.74ms
step:3381/4000 train_time:1240174ms step_avg:366.81ms
step:3382/4000 train_time:1240754ms step_avg:366.87ms
step:3383/4000 train_time:1241335ms step_avg:366.93ms
step:3384/4000 train_time:1241916ms step_avg:367.00ms
step:3385/4000 train_time:1242498ms step_avg:367.06ms
step:3386/4000 train_time:1243082ms step_avg:367.12ms
step:3387/4000 train_time:1243663ms step_avg:367.19ms
step:3388/4000 train_time:1244244ms step_avg:367.25ms
step:3389/4000 train_time:1244825ms step_avg:367.31ms
step:3390/4000 train_time:1245408ms step_avg:367.38ms
step:3391/4000 train_time:1245989ms step_avg:367.44ms
step:3392/4000 train_time:1246570ms step_avg:367.50ms
step:3393/4000 train_time:1247154ms step_avg:367.57ms
step:3394/4000 train_time:1247737ms step_avg:367.63ms
step:3395/4000 train_time:1248319ms step_avg:367.69ms
step:3396/4000 train_time:1248901ms step_avg:367.76ms
step:3397/4000 train_time:1249482ms step_avg:367.82ms
step:3398/4000 train_time:1250064ms step_avg:367.88ms
step:3399/4000 train_time:1250647ms step_avg:367.95ms
step:3400/4000 train_time:1251230ms step_avg:368.01ms
step:3401/4000 train_time:1251810ms step_avg:368.07ms
step:3402/4000 train_time:1252393ms step_avg:368.13ms
step:3403/4000 train_time:1252975ms step_avg:368.20ms
step:3404/4000 train_time:1253559ms step_avg:368.26ms
step:3405/4000 train_time:1254140ms step_avg:368.32ms
step:3406/4000 train_time:1254723ms step_avg:368.39ms
step:3407/4000 train_time:1255303ms step_avg:368.45ms
step:3408/4000 train_time:1255886ms step_avg:368.51ms
step:3409/4000 train_time:1256468ms step_avg:368.57ms
step:3410/4000 train_time:1257049ms step_avg:368.64ms
step:3411/4000 train_time:1257629ms step_avg:368.70ms
step:3412/4000 train_time:1258212ms step_avg:368.76ms
step:3413/4000 train_time:1258792ms step_avg:368.82ms
step:3414/4000 train_time:1259373ms step_avg:368.88ms
step:3415/4000 train_time:1259954ms step_avg:368.95ms
step:3416/4000 train_time:1260536ms step_avg:369.01ms
step:3417/4000 train_time:1261119ms step_avg:369.07ms
step:3418/4000 train_time:1261700ms step_avg:369.13ms
step:3419/4000 train_time:1262282ms step_avg:369.20ms
step:3420/4000 train_time:1262865ms step_avg:369.26ms
step:3421/4000 train_time:1263449ms step_avg:369.32ms
step:3422/4000 train_time:1264030ms step_avg:369.38ms
step:3423/4000 train_time:1264612ms step_avg:369.45ms
step:3424/4000 train_time:1265197ms step_avg:369.51ms
step:3425/4000 train_time:1265780ms step_avg:369.57ms
step:3426/4000 train_time:1266361ms step_avg:369.63ms
step:3427/4000 train_time:1266942ms step_avg:369.69ms
step:3428/4000 train_time:1267523ms step_avg:369.76ms
step:3429/4000 train_time:1268106ms step_avg:369.82ms
step:3430/4000 train_time:1268689ms step_avg:369.88ms
step:3431/4000 train_time:1269270ms step_avg:369.94ms
step:3432/4000 train_time:1269851ms step_avg:370.00ms
step:3433/4000 train_time:1270434ms step_avg:370.07ms
step:3434/4000 train_time:1271016ms step_avg:370.13ms
step:3435/4000 train_time:1271597ms step_avg:370.19ms
step:3436/4000 train_time:1272180ms step_avg:370.25ms
step:3437/4000 train_time:1272760ms step_avg:370.31ms
step:3438/4000 train_time:1273343ms step_avg:370.37ms
step:3439/4000 train_time:1273926ms step_avg:370.44ms
step:3440/4000 train_time:1274509ms step_avg:370.50ms
step:3441/4000 train_time:1275092ms step_avg:370.56ms
step:3442/4000 train_time:1275676ms step_avg:370.62ms
step:3443/4000 train_time:1276258ms step_avg:370.68ms
step:3444/4000 train_time:1276839ms step_avg:370.74ms
step:3445/4000 train_time:1277421ms step_avg:370.80ms
step:3446/4000 train_time:1278005ms step_avg:370.87ms
step:3447/4000 train_time:1278585ms step_avg:370.93ms
step:3448/4000 train_time:1279168ms step_avg:370.99ms
step:3449/4000 train_time:1279751ms step_avg:371.05ms
step:3450/4000 train_time:1280333ms step_avg:371.11ms
step:3451/4000 train_time:1280915ms step_avg:371.17ms
step:3452/4000 train_time:1281499ms step_avg:371.23ms
step:3453/4000 train_time:1282081ms step_avg:371.29ms
step:3454/4000 train_time:1282663ms step_avg:371.36ms
step:3455/4000 train_time:1283248ms step_avg:371.42ms
step:3456/4000 train_time:1283831ms step_avg:371.48ms
step:3457/4000 train_time:1284415ms step_avg:371.54ms
step:3458/4000 train_time:1284999ms step_avg:371.60ms
step:3459/4000 train_time:1285579ms step_avg:371.66ms
step:3460/4000 train_time:1286163ms step_avg:371.72ms
step:3461/4000 train_time:1286745ms step_avg:371.78ms
step:3462/4000 train_time:1287330ms step_avg:371.85ms
step:3463/4000 train_time:1287911ms step_avg:371.91ms
step:3464/4000 train_time:1288496ms step_avg:371.97ms
step:3465/4000 train_time:1289078ms step_avg:372.03ms
step:3466/4000 train_time:1289661ms step_avg:372.09ms
step:3467/4000 train_time:1290243ms step_avg:372.15ms
step:3468/4000 train_time:1290827ms step_avg:372.21ms
step:3469/4000 train_time:1291410ms step_avg:372.27ms
step:3470/4000 train_time:1291993ms step_avg:372.33ms
step:3471/4000 train_time:1292575ms step_avg:372.39ms
step:3472/4000 train_time:1293161ms step_avg:372.45ms
step:3473/4000 train_time:1293743ms step_avg:372.51ms
step:3474/4000 train_time:1294326ms step_avg:372.58ms
step:3475/4000 train_time:1294909ms step_avg:372.64ms
step:3476/4000 train_time:1295492ms step_avg:372.70ms
step:3477/4000 train_time:1296072ms step_avg:372.76ms
step:3478/4000 train_time:1296655ms step_avg:372.82ms
step:3479/4000 train_time:1297238ms step_avg:372.88ms
step:3480/4000 train_time:1297821ms step_avg:372.94ms
step:3481/4000 train_time:1298404ms step_avg:373.00ms
step:3482/4000 train_time:1298987ms step_avg:373.06ms
step:3483/4000 train_time:1299570ms step_avg:373.12ms
step:3484/4000 train_time:1300153ms step_avg:373.18ms
step:3485/4000 train_time:1300735ms step_avg:373.24ms
step:3486/4000 train_time:1301320ms step_avg:373.30ms
step:3487/4000 train_time:1301904ms step_avg:373.36ms
step:3488/4000 train_time:1302487ms step_avg:373.42ms
step:3489/4000 train_time:1303068ms step_avg:373.48ms
step:3490/4000 train_time:1303650ms step_avg:373.54ms
step:3491/4000 train_time:1304233ms step_avg:373.60ms
step:3492/4000 train_time:1304815ms step_avg:373.66ms
step:3493/4000 train_time:1305401ms step_avg:373.72ms
step:3494/4000 train_time:1305985ms step_avg:373.78ms
step:3495/4000 train_time:1306570ms step_avg:373.84ms
step:3496/4000 train_time:1307154ms step_avg:373.90ms
step:3497/4000 train_time:1307737ms step_avg:373.96ms
step:3498/4000 train_time:1308320ms step_avg:374.02ms
step:3499/4000 train_time:1308905ms step_avg:374.08ms
step:3500/4000 train_time:1309488ms step_avg:374.14ms
step:3500/4000 val_bpb:0.9373 train_time:1309546ms step_avg:374.16ms
step:3501/4000 train_time:1310075ms step_avg:374.20ms
step:3502/4000 train_time:1310660ms step_avg:374.26ms
step:3503/4000 train_time:1311244ms step_avg:374.32ms
step:3504/4000 train_time:1311829ms step_avg:374.38ms
step:3505/4000 train_time:1312411ms step_avg:374.44ms
step:3506/4000 train_time:1312996ms step_avg:374.50ms
step:3507/4000 train_time:1313579ms step_avg:374.56ms
step:3508/4000 train_time:1314165ms step_avg:374.62ms
step:3509/4000 train_time:1314748ms step_avg:374.68ms
step:3510/4000 train_time:1315333ms step_avg:374.74ms
step:3511/4000 train_time:1315918ms step_avg:374.80ms
step:3512/4000 train_time:1316503ms step_avg:374.86ms
step:3513/4000 train_time:1317086ms step_avg:374.92ms
step:3514/4000 train_time:1317671ms step_avg:374.98ms
step:3515/4000 train_time:1318253ms step_avg:375.04ms
step:3516/4000 train_time:1318836ms step_avg:375.10ms
step:3517/4000 train_time:1319419ms step_avg:375.15ms
step:3518/4000 train_time:1320004ms step_avg:375.21ms
step:3519/4000 train_time:1320585ms step_avg:375.27ms
step:3520/4000 train_time:1321170ms step_avg:375.33ms
step:3521/4000 train_time:1321755ms step_avg:375.39ms
step:3522/4000 train_time:1322341ms step_avg:375.45ms
step:3523/4000 train_time:1322925ms step_avg:375.51ms
step:3524/4000 train_time:1323508ms step_avg:375.57ms
step:3525/4000 train_time:1324095ms step_avg:375.63ms
step:3526/4000 train_time:1324680ms step_avg:375.69ms
step:3527/4000 train_time:1325265ms step_avg:375.75ms
step:3528/4000 train_time:1325848ms step_avg:375.81ms
step:3529/4000 train_time:1326433ms step_avg:375.87ms
step:3530/4000 train_time:1327015ms step_avg:375.92ms
step:3531/4000 train_time:1327600ms step_avg:375.98ms
step:3532/4000 train_time:1328185ms step_avg:376.04ms
step:3533/4000 train_time:1328771ms step_avg:376.10ms
step:3534/4000 train_time:1329354ms step_avg:376.16ms
step:3535/4000 train_time:1329938ms step_avg:376.22ms
step:3536/4000 train_time:1330525ms step_avg:376.28ms
step:3537/4000 train_time:1331107ms step_avg:376.34ms
step:3538/4000 train_time:1331694ms step_avg:376.40ms
step:3539/4000 train_time:1332276ms step_avg:376.46ms
step:3540/4000 train_time:1332861ms step_avg:376.51ms
step:3541/4000 train_time:1333446ms step_avg:376.57ms
step:3542/4000 train_time:1334030ms step_avg:376.63ms
step:3543/4000 train_time:1334616ms step_avg:376.69ms
step:3544/4000 train_time:1335200ms step_avg:376.75ms
step:3545/4000 train_time:1335785ms step_avg:376.81ms
step:3546/4000 train_time:1336369ms step_avg:376.87ms
step:3547/4000 train_time:1336953ms step_avg:376.93ms
step:3548/4000 train_time:1337540ms step_avg:376.98ms
step:3549/4000 train_time:1338123ms step_avg:377.04ms
step:3550/4000 train_time:1338707ms step_avg:377.10ms
step:3551/4000 train_time:1339290ms step_avg:377.16ms
step:3552/4000 train_time:1339876ms step_avg:377.22ms
step:3553/4000 train_time:1340459ms step_avg:377.28ms
step:3554/4000 train_time:1341046ms step_avg:377.33ms
step:3555/4000 train_time:1341630ms step_avg:377.39ms
step:3556/4000 train_time:1342213ms step_avg:377.45ms
step:3557/4000 train_time:1342799ms step_avg:377.51ms
step:3558/4000 train_time:1343386ms step_avg:377.57ms
step:3559/4000 train_time:1343971ms step_avg:377.63ms
step:3560/4000 train_time:1344556ms step_avg:377.68ms
step:3561/4000 train_time:1345141ms step_avg:377.74ms
step:3562/4000 train_time:1345727ms step_avg:377.80ms
step:3563/4000 train_time:1346310ms step_avg:377.86ms
step:3564/4000 train_time:1346896ms step_avg:377.92ms
step:3565/4000 train_time:1347479ms step_avg:377.97ms
step:3566/4000 train_time:1348063ms step_avg:378.03ms
step:3567/4000 train_time:1348647ms step_avg:378.09ms
step:3568/4000 train_time:1349231ms step_avg:378.15ms
step:3569/4000 train_time:1349815ms step_avg:378.21ms
step:3570/4000 train_time:1350398ms step_avg:378.26ms
step:3571/4000 train_time:1350985ms step_avg:378.32ms
step:3572/4000 train_time:1351570ms step_avg:378.38ms
step:3573/4000 train_time:1352156ms step_avg:378.44ms
step:3574/4000 train_time:1352741ms step_avg:378.50ms
step:3575/4000 train_time:1353326ms step_avg:378.55ms
step:3576/4000 train_time:1353912ms step_avg:378.61ms
step:3577/4000 train_time:1354499ms step_avg:378.67ms
step:3578/4000 train_time:1355086ms step_avg:378.73ms
step:3579/4000 train_time:1355669ms step_avg:378.78ms
step:3580/4000 train_time:1356255ms step_avg:378.84ms
step:3581/4000 train_time:1356840ms step_avg:378.90ms
step:3582/4000 train_time:1357427ms step_avg:378.96ms
step:3583/4000 train_time:1358011ms step_avg:379.02ms
step:3584/4000 train_time:1358595ms step_avg:379.07ms
step:3585/4000 train_time:1359181ms step_avg:379.13ms
step:3586/4000 train_time:1359766ms step_avg:379.19ms
step:3587/4000 train_time:1360351ms step_avg:379.24ms
step:3588/4000 train_time:1360934ms step_avg:379.30ms
step:3589/4000 train_time:1361520ms step_avg:379.36ms
step:3590/4000 train_time:1362105ms step_avg:379.42ms
step:3591/4000 train_time:1362690ms step_avg:379.47ms
step:3592/4000 train_time:1363275ms step_avg:379.53ms
step:3593/4000 train_time:1363858ms step_avg:379.59ms
step:3594/4000 train_time:1364445ms step_avg:379.65ms
step:3595/4000 train_time:1365030ms step_avg:379.70ms
step:3596/4000 train_time:1365613ms step_avg:379.76ms
step:3597/4000 train_time:1366199ms step_avg:379.82ms
step:3598/4000 train_time:1366786ms step_avg:379.87ms
step:3599/4000 train_time:1367372ms step_avg:379.93ms
step:3600/4000 train_time:1367956ms step_avg:379.99ms
step:3601/4000 train_time:1368542ms step_avg:380.05ms
step:3602/4000 train_time:1369128ms step_avg:380.10ms
step:3603/4000 train_time:1369712ms step_avg:380.16ms
step:3604/4000 train_time:1370299ms step_avg:380.22ms
step:3605/4000 train_time:1370885ms step_avg:380.27ms
step:3606/4000 train_time:1371468ms step_avg:380.33ms
step:3607/4000 train_time:1372054ms step_avg:380.39ms
step:3608/4000 train_time:1372641ms step_avg:380.44ms
step:3609/4000 train_time:1373225ms step_avg:380.50ms
step:3610/4000 train_time:1373810ms step_avg:380.56ms
step:3611/4000 train_time:1374397ms step_avg:380.61ms
step:3612/4000 train_time:1374982ms step_avg:380.67ms
step:3613/4000 train_time:1375568ms step_avg:380.73ms
step:3614/4000 train_time:1376154ms step_avg:380.78ms
step:3615/4000 train_time:1376739ms step_avg:380.84ms
step:3616/4000 train_time:1377328ms step_avg:380.90ms
step:3617/4000 train_time:1377913ms step_avg:380.95ms
step:3618/4000 train_time:1378501ms step_avg:381.01ms
step:3619/4000 train_time:1379086ms step_avg:381.07ms
step:3620/4000 train_time:1379676ms step_avg:381.13ms
step:3621/4000 train_time:1380261ms step_avg:381.18ms
step:3622/4000 train_time:1380849ms step_avg:381.24ms
step:3623/4000 train_time:1381435ms step_avg:381.30ms
step:3624/4000 train_time:1382022ms step_avg:381.35ms
step:3625/4000 train_time:1382607ms step_avg:381.41ms
step:3626/4000 train_time:1383195ms step_avg:381.47ms
step:3627/4000 train_time:1383781ms step_avg:381.52ms
step:3628/4000 train_time:1384368ms step_avg:381.58ms
step:3629/4000 train_time:1384954ms step_avg:381.64ms
step:3630/4000 train_time:1385540ms step_avg:381.69ms
step:3631/4000 train_time:1386127ms step_avg:381.75ms
step:3632/4000 train_time:1386715ms step_avg:381.80ms
step:3633/4000 train_time:1387298ms step_avg:381.86ms
step:3634/4000 train_time:1387887ms step_avg:381.92ms
step:3635/4000 train_time:1388475ms step_avg:381.97ms
step:3636/4000 train_time:1389058ms step_avg:382.03ms
step:3637/4000 train_time:1389647ms step_avg:382.09ms
step:3638/4000 train_time:1390231ms step_avg:382.14ms
step:3639/4000 train_time:1390819ms step_avg:382.20ms
step:3640/4000 train_time:1391409ms step_avg:382.26ms
step:3641/4000 train_time:1391995ms step_avg:382.31ms
step:3642/4000 train_time:1392578ms step_avg:382.37ms
step:3643/4000 train_time:1393162ms step_avg:382.42ms
step:3644/4000 train_time:1393748ms step_avg:382.48ms
step:3645/4000 train_time:1394331ms step_avg:382.53ms
step:3646/4000 train_time:1394916ms step_avg:382.59ms
step:3647/4000 train_time:1395500ms step_avg:382.64ms
step:3648/4000 train_time:1396085ms step_avg:382.70ms
step:3649/4000 train_time:1396667ms step_avg:382.75ms
step:3650/4000 train_time:1397250ms step_avg:382.81ms
step:3651/4000 train_time:1397836ms step_avg:382.86ms
step:3652/4000 train_time:1398420ms step_avg:382.92ms
step:3653/4000 train_time:1399004ms step_avg:382.97ms
step:3654/4000 train_time:1399586ms step_avg:383.03ms
step:3655/4000 train_time:1400171ms step_avg:383.08ms
step:3656/4000 train_time:1400756ms step_avg:383.14ms
step:3657/4000 train_time:1401338ms step_avg:383.19ms
step:3658/4000 train_time:1401923ms step_avg:383.25ms
step:3659/4000 train_time:1402507ms step_avg:383.30ms
step:3660/4000 train_time:1403092ms step_avg:383.36ms
step:3661/4000 train_time:1403676ms step_avg:383.41ms
step:3662/4000 train_time:1404260ms step_avg:383.47ms
step:3663/4000 train_time:1404844ms step_avg:383.52ms
step:3664/4000 train_time:1405431ms step_avg:383.58ms
step:3665/4000 train_time:1406015ms step_avg:383.63ms
step:3666/4000 train_time:1406601ms step_avg:383.69ms
step:3667/4000 train_time:1407187ms step_avg:383.74ms
step:3668/4000 train_time:1407773ms step_avg:383.80ms
step:3669/4000 train_time:1408356ms step_avg:383.85ms
step:3670/4000 train_time:1408938ms step_avg:383.91ms
step:3671/4000 train_time:1409523ms step_avg:383.96ms
step:3672/4000 train_time:1410107ms step_avg:384.02ms
step:3673/4000 train_time:1410690ms step_avg:384.07ms
step:3674/4000 train_time:1411274ms step_avg:384.12ms
step:3675/4000 train_time:1411858ms step_avg:384.18ms
step:3676/4000 train_time:1412442ms step_avg:384.23ms
step:3677/4000 train_time:1413026ms step_avg:384.29ms
step:3678/4000 train_time:1413608ms step_avg:384.34ms
step:3679/4000 train_time:1414192ms step_avg:384.40ms
step:3680/4000 train_time:1414777ms step_avg:384.45ms
step:3681/4000 train_time:1415362ms step_avg:384.50ms
step:3682/4000 train_time:1415946ms step_avg:384.56ms
step:3683/4000 train_time:1416530ms step_avg:384.61ms
step:3684/4000 train_time:1417115ms step_avg:384.67ms
step:3685/4000 train_time:1417701ms step_avg:384.72ms
step:3686/4000 train_time:1418285ms step_avg:384.78ms
step:3687/4000 train_time:1418868ms step_avg:384.83ms
step:3688/4000 train_time:1419454ms step_avg:384.88ms
step:3689/4000 train_time:1420039ms step_avg:384.94ms
step:3690/4000 train_time:1420624ms step_avg:384.99ms
step:3691/4000 train_time:1421207ms step_avg:385.05ms
step:3692/4000 train_time:1421792ms step_avg:385.10ms
step:3693/4000 train_time:1422376ms step_avg:385.15ms
step:3694/4000 train_time:1422960ms step_avg:385.21ms
step:3695/4000 train_time:1423545ms step_avg:385.26ms
step:3696/4000 train_time:1424128ms step_avg:385.32ms
step:3697/4000 train_time:1424712ms step_avg:385.37ms
step:3698/4000 train_time:1425297ms step_avg:385.42ms
step:3699/4000 train_time:1425880ms step_avg:385.48ms
step:3700/4000 train_time:1426466ms step_avg:385.53ms
step:3701/4000 train_time:1427049ms step_avg:385.58ms
step:3702/4000 train_time:1427634ms step_avg:385.64ms
step:3703/4000 train_time:1428219ms step_avg:385.69ms
step:3704/4000 train_time:1428802ms step_avg:385.75ms
step:3705/4000 train_time:1429387ms step_avg:385.80ms
step:3706/4000 train_time:1429972ms step_avg:385.85ms
step:3707/4000 train_time:1430556ms step_avg:385.91ms
step:3708/4000 train_time:1431140ms step_avg:385.96ms
step:3709/4000 train_time:1431726ms step_avg:386.01ms
step:3710/4000 train_time:1432309ms step_avg:386.07ms
step:3711/4000 train_time:1432892ms step_avg:386.12ms
step:3712/4000 train_time:1433475ms step_avg:386.17ms
step:3713/4000 train_time:1434056ms step_avg:386.23ms
step:3714/4000 train_time:1434640ms step_avg:386.28ms
step:3715/4000 train_time:1435224ms step_avg:386.33ms
step:3716/4000 train_time:1435808ms step_avg:386.39ms
step:3717/4000 train_time:1436393ms step_avg:386.44ms
step:3718/4000 train_time:1436977ms step_avg:386.49ms
step:3719/4000 train_time:1437561ms step_avg:386.55ms
step:3720/4000 train_time:1438146ms step_avg:386.60ms
step:3721/4000 train_time:1438728ms step_avg:386.65ms
step:3722/4000 train_time:1439314ms step_avg:386.70ms
step:3723/4000 train_time:1439896ms step_avg:386.76ms
step:3724/4000 train_time:1440482ms step_avg:386.81ms
step:3725/4000 train_time:1441068ms step_avg:386.86ms
step:3726/4000 train_time:1441653ms step_avg:386.92ms
step:3727/4000 train_time:1442235ms step_avg:386.97ms
step:3728/4000 train_time:1442817ms step_avg:387.02ms
step:3729/4000 train_time:1443403ms step_avg:387.08ms
step:3730/4000 train_time:1443986ms step_avg:387.13ms
step:3731/4000 train_time:1444570ms step_avg:387.18ms
step:3732/4000 train_time:1445152ms step_avg:387.23ms
step:3733/4000 train_time:1445734ms step_avg:387.28ms
step:3734/4000 train_time:1446321ms step_avg:387.34ms
step:3735/4000 train_time:1446904ms step_avg:387.39ms
step:3736/4000 train_time:1447490ms step_avg:387.44ms
step:3737/4000 train_time:1448074ms step_avg:387.50ms
step:3738/4000 train_time:1448659ms step_avg:387.55ms
step:3739/4000 train_time:1449244ms step_avg:387.60ms
step:3740/4000 train_time:1449827ms step_avg:387.65ms
step:3741/4000 train_time:1450413ms step_avg:387.71ms
step:3742/4000 train_time:1450998ms step_avg:387.76ms
step:3743/4000 train_time:1451584ms step_avg:387.81ms
step:3744/4000 train_time:1452169ms step_avg:387.87ms
step:3745/4000 train_time:1452754ms step_avg:387.92ms
step:3746/4000 train_time:1453340ms step_avg:387.97ms
step:3747/4000 train_time:1453922ms step_avg:388.02ms
step:3748/4000 train_time:1454508ms step_avg:388.08ms
step:3749/4000 train_time:1455094ms step_avg:388.13ms
step:3750/4000 train_time:1455682ms step_avg:388.18ms
step:3750/4000 val_bpb:0.9258 train_time:1455740ms step_avg:388.20ms
step:3751/4000 train_time:1456268ms step_avg:388.23ms
step:3752/4000 train_time:1456853ms step_avg:388.29ms
step:3753/4000 train_time:1457439ms step_avg:388.34ms
step:3754/4000 train_time:1458026ms step_avg:388.39ms
step:3755/4000 train_time:1458611ms step_avg:388.44ms
step:3756/4000 train_time:1459193ms step_avg:388.50ms
step:3757/4000 train_time:1459779ms step_avg:388.55ms
step:3758/4000 train_time:1460364ms step_avg:388.60ms
step:3759/4000 train_time:1460949ms step_avg:388.65ms
step:3760/4000 train_time:1461534ms step_avg:388.71ms
step:3761/4000 train_time:1462117ms step_avg:388.76ms
step:3762/4000 train_time:1462704ms step_avg:388.81ms
step:3763/4000 train_time:1463286ms step_avg:388.86ms
step:3764/4000 train_time:1463873ms step_avg:388.91ms
step:3765/4000 train_time:1464459ms step_avg:388.97ms
step:3766/4000 train_time:1465044ms step_avg:389.02ms
step:3767/4000 train_time:1465630ms step_avg:389.07ms
step:3768/4000 train_time:1466214ms step_avg:389.12ms
step:3769/4000 train_time:1466799ms step_avg:389.17ms
step:3770/4000 train_time:1467384ms step_avg:389.23ms
step:3771/4000 train_time:1467966ms step_avg:389.28ms
step:3772/4000 train_time:1468552ms step_avg:389.33ms
step:3773/4000 train_time:1469138ms step_avg:389.38ms
step:3774/4000 train_time:1469722ms step_avg:389.43ms
step:3775/4000 train_time:1470308ms step_avg:389.49ms
step:3776/4000 train_time:1470894ms step_avg:389.54ms
step:3777/4000 train_time:1471478ms step_avg:389.59ms
step:3778/4000 train_time:1472064ms step_avg:389.64ms
step:3779/4000 train_time:1472650ms step_avg:389.69ms
step:3780/4000 train_time:1473238ms step_avg:389.75ms
step:3781/4000 train_time:1473825ms step_avg:389.80ms
step:3782/4000 train_time:1474411ms step_avg:389.85ms
step:3783/4000 train_time:1474996ms step_avg:389.90ms
step:3784/4000 train_time:1475582ms step_avg:389.95ms
step:3785/4000 train_time:1476166ms step_avg:390.00ms
step:3786/4000 train_time:1476754ms step_avg:390.06ms
step:3787/4000 train_time:1477341ms step_avg:390.11ms
step:3788/4000 train_time:1477926ms step_avg:390.16ms
step:3789/4000 train_time:1478512ms step_avg:390.21ms
step:3790/4000 train_time:1479096ms step_avg:390.26ms
step:3791/4000 train_time:1479682ms step_avg:390.31ms
step:3792/4000 train_time:1480269ms step_avg:390.37ms
step:3793/4000 train_time:1480856ms step_avg:390.42ms
step:3794/4000 train_time:1481442ms step_avg:390.47ms
step:3795/4000 train_time:1482027ms step_avg:390.52ms
step:3796/4000 train_time:1482614ms step_avg:390.57ms
step:3797/4000 train_time:1483199ms step_avg:390.62ms
step:3798/4000 train_time:1483783ms step_avg:390.67ms
step:3799/4000 train_time:1484368ms step_avg:390.73ms
step:3800/4000 train_time:1484954ms step_avg:390.78ms
step:3801/4000 train_time:1485539ms step_avg:390.83ms
step:3802/4000 train_time:1486125ms step_avg:390.88ms
step:3803/4000 train_time:1486709ms step_avg:390.93ms
step:3804/4000 train_time:1487292ms step_avg:390.98ms
step:3805/4000 train_time:1487876ms step_avg:391.03ms
step:3806/4000 train_time:1488463ms step_avg:391.08ms
step:3807/4000 train_time:1489044ms step_avg:391.13ms
step:3808/4000 train_time:1489629ms step_avg:391.18ms
step:3809/4000 train_time:1490214ms step_avg:391.23ms
step:3810/4000 train_time:1490797ms step_avg:391.29ms
step:3811/4000 train_time:1491381ms step_avg:391.34ms
step:3812/4000 train_time:1491964ms step_avg:391.39ms
step:3813/4000 train_time:1492548ms step_avg:391.44ms
step:3814/4000 train_time:1493132ms step_avg:391.49ms
step:3815/4000 train_time:1493715ms step_avg:391.54ms
step:3816/4000 train_time:1494299ms step_avg:391.59ms
step:3817/4000 train_time:1494883ms step_avg:391.64ms
step:3818/4000 train_time:1495467ms step_avg:391.69ms
step:3819/4000 train_time:1496054ms step_avg:391.74ms
step:3820/4000 train_time:1496641ms step_avg:391.79ms
step:3821/4000 train_time:1497227ms step_avg:391.84ms
step:3822/4000 train_time:1497811ms step_avg:391.89ms
step:3823/4000 train_time:1498396ms step_avg:391.94ms
step:3824/4000 train_time:1498981ms step_avg:391.99ms
step:3825/4000 train_time:1499563ms step_avg:392.04ms
step:3826/4000 train_time:1500148ms step_avg:392.09ms
step:3827/4000 train_time:1500733ms step_avg:392.14ms
step:3828/4000 train_time:1501318ms step_avg:392.19ms
step:3829/4000 train_time:1501903ms step_avg:392.24ms
step:3830/4000 train_time:1502487ms step_avg:392.29ms
step:3831/4000 train_time:1503074ms step_avg:392.34ms
step:3832/4000 train_time:1503660ms step_avg:392.40ms
step:3833/4000 train_time:1504244ms step_avg:392.45ms
step:3834/4000 train_time:1504831ms step_avg:392.50ms
step:3835/4000 train_time:1505415ms step_avg:392.55ms
step:3836/4000 train_time:1506004ms step_avg:392.60ms
step:3837/4000 train_time:1506590ms step_avg:392.65ms
step:3838/4000 train_time:1507174ms step_avg:392.70ms
step:3839/4000 train_time:1507760ms step_avg:392.75ms
step:3840/4000 train_time:1508344ms step_avg:392.80ms
step:3841/4000 train_time:1508931ms step_avg:392.85ms
step:3842/4000 train_time:1509517ms step_avg:392.90ms
step:3843/4000 train_time:1510103ms step_avg:392.95ms
step:3844/4000 train_time:1510687ms step_avg:393.00ms
step:3845/4000 train_time:1511272ms step_avg:393.05ms
step:3846/4000 train_time:1511859ms step_avg:393.10ms
step:3847/4000 train_time:1512445ms step_avg:393.15ms
step:3848/4000 train_time:1513031ms step_avg:393.20ms
step:3849/4000 train_time:1513616ms step_avg:393.25ms
step:3850/4000 train_time:1514202ms step_avg:393.30ms
step:3851/4000 train_time:1514786ms step_avg:393.35ms
step:3852/4000 train_time:1515373ms step_avg:393.40ms
step:3853/4000 train_time:1515959ms step_avg:393.45ms
step:3854/4000 train_time:1516545ms step_avg:393.50ms
step:3855/4000 train_time:1517129ms step_avg:393.55ms
step:3856/4000 train_time:1517717ms step_avg:393.60ms
step:3857/4000 train_time:1518302ms step_avg:393.65ms
step:3858/4000 train_time:1518886ms step_avg:393.70ms
step:3859/4000 train_time:1519470ms step_avg:393.75ms
step:3860/4000 train_time:1520056ms step_avg:393.80ms
step:3861/4000 train_time:1520643ms step_avg:393.85ms
step:3862/4000 train_time:1521228ms step_avg:393.90ms
step:3863/4000 train_time:1521814ms step_avg:393.95ms
step:3864/4000 train_time:1522399ms step_avg:394.00ms
step:3865/4000 train_time:1522985ms step_avg:394.05ms
step:3866/4000 train_time:1523571ms step_avg:394.09ms
step:3867/4000 train_time:1524155ms step_avg:394.14ms
step:3868/4000 train_time:1524743ms step_avg:394.19ms
step:3869/4000 train_time:1525326ms step_avg:394.24ms
step:3870/4000 train_time:1525913ms step_avg:394.29ms
step:3871/4000 train_time:1526498ms step_avg:394.34ms
step:3872/4000 train_time:1527084ms step_avg:394.39ms
step:3873/4000 train_time:1527669ms step_avg:394.44ms
step:3874/4000 train_time:1528255ms step_avg:394.49ms
step:3875/4000 train_time:1528842ms step_avg:394.54ms
step:3876/4000 train_time:1529426ms step_avg:394.59ms
step:3877/4000 train_time:1530012ms step_avg:394.64ms
step:3878/4000 train_time:1530598ms step_avg:394.69ms
step:3879/4000 train_time:1531183ms step_avg:394.74ms
step:3880/4000 train_time:1531769ms step_avg:394.79ms
step:3881/4000 train_time:1532356ms step_avg:394.84ms
step:3882/4000 train_time:1532942ms step_avg:394.88ms
step:3883/4000 train_time:1533527ms step_avg:394.93ms
step:3884/4000 train_time:1534113ms step_avg:394.98ms
step:3885/4000 train_time:1534701ms step_avg:395.03ms
step:3886/4000 train_time:1535285ms step_avg:395.08ms
step:3887/4000 train_time:1535872ms step_avg:395.13ms
step:3888/4000 train_time:1536454ms step_avg:395.18ms
step:3889/4000 train_time:1537039ms step_avg:395.23ms
step:3890/4000 train_time:1537625ms step_avg:395.28ms
step:3891/4000 train_time:1538211ms step_avg:395.33ms
step:3892/4000 train_time:1538797ms step_avg:395.37ms
step:3893/4000 train_time:1539381ms step_avg:395.42ms
step:3894/4000 train_time:1539965ms step_avg:395.47ms
step:3895/4000 train_time:1540550ms step_avg:395.52ms
step:3896/4000 train_time:1541134ms step_avg:395.57ms
step:3897/4000 train_time:1541719ms step_avg:395.62ms
step:3898/4000 train_time:1542304ms step_avg:395.67ms
step:3899/4000 train_time:1542889ms step_avg:395.71ms
step:3900/4000 train_time:1543476ms step_avg:395.76ms
step:3901/4000 train_time:1544063ms step_avg:395.81ms
step:3902/4000 train_time:1544647ms step_avg:395.86ms
step:3903/4000 train_time:1545231ms step_avg:395.91ms
step:3904/4000 train_time:1545817ms step_avg:395.96ms
step:3905/4000 train_time:1546403ms step_avg:396.01ms
step:3906/4000 train_time:1546987ms step_avg:396.05ms
step:3907/4000 train_time:1547573ms step_avg:396.10ms
step:3908/4000 train_time:1548158ms step_avg:396.15ms
step:3909/4000 train_time:1548739ms step_avg:396.20ms
step:3910/4000 train_time:1549325ms step_avg:396.25ms
step:3911/4000 train_time:1549910ms step_avg:396.30ms
step:3912/4000 train_time:1550495ms step_avg:396.34ms
step:3913/4000 train_time:1551081ms step_avg:396.39ms
step:3914/4000 train_time:1551666ms step_avg:396.44ms
step:3915/4000 train_time:1552248ms step_avg:396.49ms
step:3916/4000 train_time:1552833ms step_avg:396.54ms
step:3917/4000 train_time:1553418ms step_avg:396.58ms
step:3918/4000 train_time:1554002ms step_avg:396.63ms
step:3919/4000 train_time:1554586ms step_avg:396.68ms
step:3920/4000 train_time:1555173ms step_avg:396.73ms
step:3921/4000 train_time:1555760ms step_avg:396.78ms
step:3922/4000 train_time:1556346ms step_avg:396.82ms
step:3923/4000 train_time:1556931ms step_avg:396.87ms
step:3924/4000 train_time:1557515ms step_avg:396.92ms
step:3925/4000 train_time:1558097ms step_avg:396.97ms
step:3926/4000 train_time:1558684ms step_avg:397.02ms
step:3927/4000 train_time:1559266ms step_avg:397.06ms
step:3928/4000 train_time:1559854ms step_avg:397.11ms
step:3929/4000 train_time:1560439ms step_avg:397.16ms
step:3930/4000 train_time:1561025ms step_avg:397.21ms
step:3931/4000 train_time:1561610ms step_avg:397.26ms
step:3932/4000 train_time:1562195ms step_avg:397.30ms
step:3933/4000 train_time:1562779ms step_avg:397.35ms
step:3934/4000 train_time:1563363ms step_avg:397.40ms
step:3935/4000 train_time:1563948ms step_avg:397.45ms
step:3936/4000 train_time:1564533ms step_avg:397.49ms
step:3937/4000 train_time:1565116ms step_avg:397.54ms
step:3938/4000 train_time:1565702ms step_avg:397.59ms
step:3939/4000 train_time:1566286ms step_avg:397.64ms
step:3940/4000 train_time:1566871ms step_avg:397.68ms
step:3941/4000 train_time:1567457ms step_avg:397.73ms
step:3942/4000 train_time:1568042ms step_avg:397.78ms
step:3943/4000 train_time:1568626ms step_avg:397.83ms
step:3944/4000 train_time:1569211ms step_avg:397.87ms
step:3945/4000 train_time:1569798ms step_avg:397.92ms
step:3946/4000 train_time:1570385ms step_avg:397.97ms
step:3947/4000 train_time:1570969ms step_avg:398.02ms
step:3948/4000 train_time:1571553ms step_avg:398.06ms
step:3949/4000 train_time:1572139ms step_avg:398.11ms
step:3950/4000 train_time:1572724ms step_avg:398.16ms
step:3951/4000 train_time:1573306ms step_avg:398.20ms
step:3952/4000 train_time:1573890ms step_avg:398.25ms
step:3953/4000 train_time:1574473ms step_avg:398.30ms
step:3954/4000 train_time:1575059ms step_avg:398.35ms
step:3955/4000 train_time:1575643ms step_avg:398.39ms
step:3956/4000 train_time:1576225ms step_avg:398.44ms
step:3957/4000 train_time:1576810ms step_avg:398.49ms
step:3958/4000 train_time:1577395ms step_avg:398.53ms
step:3959/4000 train_time:1577979ms step_avg:398.58ms
step:3960/4000 train_time:1578564ms step_avg:398.63ms
step:3961/4000 train_time:1579151ms step_avg:398.67ms
step:3962/4000 train_time:1579739ms step_avg:398.72ms
step:3963/4000 train_time:1580326ms step_avg:398.77ms
step:3964/4000 train_time:1580915ms step_avg:398.82ms
step:3965/4000 train_time:1581504ms step_avg:398.87ms
step:3966/4000 train_time:1582090ms step_avg:398.91ms
step:3967/4000 train_time:1582678ms step_avg:398.96ms
step:3968/4000 train_time:1583267ms step_avg:399.01ms
step:3969/4000 train_time:1583856ms step_avg:399.06ms
step:3970/4000 train_time:1584443ms step_avg:399.10ms
step:3971/4000 train_time:1585030ms step_avg:399.15ms
step:3972/4000 train_time:1585615ms step_avg:399.20ms
step:3973/4000 train_time:1586203ms step_avg:399.25ms
step:3974/4000 train_time:1586791ms step_avg:399.29ms
step:3975/4000 train_time:1587376ms step_avg:399.34ms
step:3976/4000 train_time:1587963ms step_avg:399.39ms
step:3977/4000 train_time:1588548ms step_avg:399.43ms
step:3978/4000 train_time:1589134ms step_avg:399.48ms
step:3979/4000 train_time:1589722ms step_avg:399.53ms
step:3980/4000 train_time:1590308ms step_avg:399.57ms
step:3981/4000 train_time:1590896ms step_avg:399.62ms
step:3982/4000 train_time:1591486ms step_avg:399.67ms
step:3983/4000 train_time:1592073ms step_avg:399.72ms
step:3984/4000 train_time:1592661ms step_avg:399.76ms
step:3985/4000 train_time:1593248ms step_avg:399.81ms
step:3986/4000 train_time:1593835ms step_avg:399.86ms
step:3987/4000 train_time:1594424ms step_avg:399.91ms
step:3988/4000 train_time:1595011ms step_avg:399.95ms
step:3989/4000 train_time:1595596ms step_avg:400.00ms
step:3990/4000 train_time:1596185ms step_avg:400.05ms
step:3991/4000 train_time:1596773ms step_avg:400.09ms
step:3992/4000 train_time:1597359ms step_avg:400.14ms
step:3993/4000 train_time:1597946ms step_avg:400.19ms
step:3994/4000 train_time:1598533ms step_avg:400.23ms
step:3995/4000 train_time:1599118ms step_avg:400.28ms
step:3996/4000 train_time:1599706ms step_avg:400.33ms
step:3997/4000 train_time:1600292ms step_avg:400.37ms
step:3998/4000 train_time:1600880ms step_avg:400.42ms
step:3999/4000 train_time:1601468ms step_avg:400.47ms
step:4000/4000 train_time:1602054ms step_avg:400.51ms
step:4000/4000 val_bpb:0.9192 train_time:1602113ms step_avg:400.53ms
Evaluating: hellaswag_zeroshot (multiple_choice, 10042 examples)... 
accuracy: 0.3375 | centered: 0.1166 | time: 19.81s
Evaluating: jeopardy (language_modeling, 2117 examples)... 
accuracy: 0.0014 | centered: 0.0014 | time: 0.87s
Evaluating: bigbench_qa_wikidata (language_modeling, 20321 examples)... 
accuracy: 0.3570 | centered: 0.3570 | time: 3.68s
Evaluating: arc_easy (multiple_choice, 2376 examples)... 
accuracy: 0.5206 | centered: 0.3608 | time: 3.43s
Evaluating: arc_challenge (multiple_choice, 1172 examples)... 
accuracy: 0.2551 | centered: 0.0068 | time: 1.93s
Evaluating: copa (multiple_choice, 100 examples)... 
accuracy: 0.6000 | centered: 0.2000 | time: 0.04s
Evaluating: commonsense_qa (multiple_choice, 1221 examples)... 
accuracy: 0.2891 | centered: 0.1114 | time: 2.30s
Evaluating: piqa (multiple_choice, 1838 examples)... 
accuracy: 0.6328 | centered: 0.2655 | time: 1.58s
Evaluating: openbook_qa (multiple_choice, 500 examples)... 
accuracy: 0.3020 | centered: 0.0693 | time: 0.14s
Evaluating: lambada_openai (language_modeling, 5153 examples)... 
accuracy: 0.2874 | centered: 0.2874 | time: 0.71s
Evaluating: hellaswag (multiple_choice, 10042 examples)... 
accuracy: 0.3333 | centered: 0.1111 | time: 36.12s
Evaluating: winograd (schema, 273 examples)... 
accuracy: 0.5788 | centered: 0.1575 | time: 0.27s
Evaluating: winogrande (schema, 1267 examples)... 
accuracy: 0.5272 | centered: 0.0545 | time: 0.19s
Evaluating: bigbench_dyck_languages (language_modeling, 1000 examples)... 
accuracy: 0.1490 | centered: 0.1490 | time: 0.67s
Evaluating: agi_eval_lsat_ar (multiple_choice, 230 examples)... 
accuracy: 0.2565 | centered: 0.0707 | time: 0.78s
Evaluating: bigbench_cs_algorithms (language_modeling, 1320 examples)... 
accuracy: 0.4280 | centered: 0.4280 | time: 0.74s
Evaluating: bigbench_operators (language_modeling, 210 examples)... 
accuracy: 0.1238 | centered: 0.1238 | time: 0.10s
Evaluating: bigbench_repeat_copy_logic (language_modeling, 32 examples)... 
accuracy: 0.0000 | centered: 0.0000 | time: 0.02s
Evaluating: squad (language_modeling, 10570 examples)... 
accuracy: 0.0712 | centered: 0.0712 | time: 19.83s
Evaluating: coqa (language_modeling, 7983 examples)... 
accuracy: 0.1214 | centered: 0.1214 | time: 4.45s
Evaluating: boolq (multiple_choice, 3270 examples)... 
accuracy: 0.5930 | centered: -0.0711 | time: 10.11s
Evaluating: bigbench_language_identification (multiple_choice, 10000 examples)... 
accuracy: 0.2511 | centered: 0.1761 | time: 63.69s
CORE metric: 0.1440 | total CORE eval time: 171.79s
  hellaswag_zeroshot: accuracy=0.3375 centered=0.1166
  jeopardy: accuracy=0.0014 centered=0.0014
  bigbench_qa_wikidata: accuracy=0.3570 centered=0.3570
  arc_easy: accuracy=0.5206 centered=0.3608
  arc_challenge: accuracy=0.2551 centered=0.0068
  copa: accuracy=0.6000 centered=0.2000
  commonsense_qa: accuracy=0.2891 centered=0.1114
  piqa: accuracy=0.6328 centered=0.2655
  openbook_qa: accuracy=0.3020 centered=0.0693
  lambada_openai: accuracy=0.2874 centered=0.2874
  hellaswag: accuracy=0.3333 centered=0.1111
  winograd: accuracy=0.5788 centered=0.1575
  winogrande: accuracy=0.5272 centered=0.0545
  bigbench_dyck_languages: accuracy=0.1490 centered=0.1490
  agi_eval_lsat_ar: accuracy=0.2565 centered=0.0707
  bigbench_cs_algorithms: accuracy=0.4280 centered=0.4280
  bigbench_operators: accuracy=0.1238 centered=0.1238
  bigbench_repeat_copy_logic: accuracy=0.0000 centered=0.0000
  squad: accuracy=0.0712 centered=0.0712
  coqa: accuracy=0.1214 centered=0.1214
  boolq: accuracy=0.5930 centered=-0.0711
  bigbench_language_identification: accuracy=0.2511 centered=0.1761
peak memory allocated: 28530 MiB reserved: 40856 MiB
