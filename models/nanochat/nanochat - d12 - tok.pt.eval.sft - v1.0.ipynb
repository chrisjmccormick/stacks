{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chrisjmccormick/stacks/blob/main/models/nanochat/nanochat%20-%20d12%20-%20tok.pt.eval.sft%20-%20v1.0.ipynb) [![View on GitHub](https://img.shields.io/badge/View%20on-GitHub-181717?logo=github)](https://github.com/chrisjmccormick/stacks/blob/main/models/nanochat/nanochat%20-%20d12%20-%20tok.pt.eval.sft%20-%20v1.0.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is `nanochat` unrolled into a self-contained Colab Notebook, configured to train and evaluate a \"d12\" model (12 layers, model dim 768) on a Colab 40GB A100.\n",
    "\n",
    "The goal here is to provide:\n",
    "1. A nice format for reading / stepping through the full nanochat pipeline.\n",
    "2. A way for anyone to get nanochat running in seconds--no GPU rental or environment setup headaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's Included**\n",
    "\n",
    "```\n",
    "1. Custom vocabulary / tokenizer training --- < 2 minutes\n",
    "2. Pre-training on FineWeb-Edu -------------- ~77 minutes\n",
    "3. CORE Metric Eval ------------------------- ~32 minutes\n",
    "4. SFT training ----------------------------- ~36 minutes\n",
    "      and evaluation ------------------------ 1h+, at least 30min for GSM8K.\n",
    "```\n",
    "\n",
    "The price I'm currently seeing in Colab for a 40GB A100 is ~\\$0.54/hr, so this Notebook makes it through pre-training and the CORE evaluation for about \\$1.\n",
    "\n",
    "> Note: Make sure to uncheck the \"High-RAM\" option, or you may get a more-expensive 80GB A100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's Missing**\n",
    "\n",
    "* (TODO) I haven't included the chat interface yet--I'll look at adding that.\n",
    "* I've left out RL for now. It takes hours, so it's not really a good fit for Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration**\n",
    "\n",
    "It's all set up to run for a 12-layer, 768-dim GPT model.\n",
    "\n",
    "Headings with icons:\n",
    "* âš™ï¸ Gear icons indicate places where key configurations are defined.\n",
    "* â–¶ Play button icons indicate the long-running sections, where the actual training and evaluations happen.\n",
    "* ðŸ”‘ Steps that involve a login (wandb and huggingface)--they'll attempt to pull your API key from Colab secrets if you've set it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it was made**\n",
    "\n",
    "I have a utility for back-and-forth notebook-to-script conversions that makes it relatively easy to assemble something like this.\n",
    "\n",
    "For the initial conversion, it breaks up the classes and functions into separate cells and adds headings for each.\n",
    "\n",
    "I tried to change the code as little as possible. What I did change:\n",
    "\n",
    "1. Removed imports,\n",
    "2. Converted command line parsers / arguments to `Namespace` objects.\n",
    "3. Added FA2 support for A100s\n",
    "4. Added HuggingFace model upload to the end.\n",
    "\n",
    "The Notebook includes the following nanochat source files, in this order:\n",
    "\n",
    "```\n",
    "nanochat/flash_attention.py\n",
    "nanochat/dataset.py\n",
    "nanochat/tokenizer.py\n",
    "scripts/tok_train.py\n",
    "nanochat/dataloader.py\n",
    "nanochat/optim.py\n",
    "nanochat/gpt.py\n",
    "nanochat/checkpoint_manager.py\n",
    "nanochat/engine.py\n",
    "nanochat/loss_eval.py\n",
    "nanochat/base_eval.py\n",
    "scripts/base_train.py\n",
    "scripts/chat_sft.py\n",
    "scripts/chat_eval.py\n",
    "```\n",
    "\n",
    "(You'll notice that many of the notebook sections include the name of the file that they came from, for reference).\n",
    "\n",
    "There are a few required source files I opted not to fold into the Notebook, and so the Notebook downloads (using their permalinks) and imports them. Specifically:\n",
    "* The helper functions in `nanochat/common.py`\n",
    "* Reporting from `nanochat/report.py`\n",
    "* The CORE eval harness in `nanochat/core_eval.py`\n",
    "    * And the implementations of each of the eight benchmarks in `nanochat/tasks`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Ways to Run**\n",
    "\n",
    "Something I like about Lambda Labs (GPU provider) is that their instances launch with Jupyter Lab running in your browser. You can download this notebook, drag it into Jupyter Lab, and hit \"run all\".\n",
    "\n",
    "That's about as close to the simplicity of running in Colab as you can get, and is much better suited to long runs (Colab is \"intended for interactive work\" and will disconnect you if left idle too long).\n",
    "\n",
    "I like the GH200 (it's an ARM-based system with a single H100 GPU) because it's $2/hr. and often available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging**\n",
    "\n",
    "Use the Colab 'Secrets' panel (the key icon in the sidebar) to provide your wandb api key as WANDB_API_KEY and the code will log your training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation**\n",
    "\n",
    "I recommend using the AI at https://deepwiki.com/karpathy/nanochat for questions about the codebase--it's quite impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version**\n",
    "\n",
    "`v1.0`\n",
    "* From 2026-02-05, is based on `nanochat` commit [5fdd5cd](https://github.com/karpathy/nanochat/commit/5fdd5cdb246d2e82a1fcc05fd4c0468df824d875).\n",
    "* `stacks/models/nanochat/nanochat - d12 - tok.pt.eval.sft - v1.0.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speedrun.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference. The Notebook implements this overall process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/bin/bash\n",
    "\n",
    "# This script is configured to train your own GPT-2 grade LLM (pretraining + finetuning)\n",
    "# It is designed to run on a blank 8XH100 GPU node and takes approximately 3 hours to complete.\n",
    "\n",
    "# 1) Example launch (simplest):\n",
    "# bash runs/speedrun.sh\n",
    "# 2) Example launch in a screen session (because the run takes ~3 hours):\n",
    "# screen -L -Logfile runs/speedrun.log -S speedrun bash runs/speedrun.sh\n",
    "# 3) Example launch with wandb logging, but see below for setting up wandb first:\n",
    "# WANDB_RUN=speedrun screen -L -Logfile runs/speedrun.log -S speedrun bash runs/speedrun.sh\n",
    "\n",
    "# Default intermediate artifacts directory is in ~/.cache/nanochat\n",
    "export OMP_NUM_THREADS=1\n",
    "export NANOCHAT_BASE_DIR=\"$HOME/.cache/nanochat\"\n",
    "mkdir -p $NANOCHAT_BASE_DIR\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Python venv setup with uv\n",
    "\n",
    "# install uv (if not already installed)\n",
    "command -v uv &> /dev/null || curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "# create a .venv local virtual environment (if it doesn't exist)\n",
    "[ -d \".venv\" ] || uv venv\n",
    "# install the repo dependencies\n",
    "uv sync --extra gpu\n",
    "# activate venv so that `python` uses the project's venv instead of system python\n",
    "source .venv/bin/activate\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# wandb setup\n",
    "# If you wish to use wandb for logging (it's nice!, recommended).\n",
    "# 1) Make sure to first log in to wandb, e.g. run:\n",
    "#    `wandb login`\n",
    "# 2) Set the WANDB_RUN environment variable when running this script, e.g.:\n",
    "#    `WANDB_RUN=d26 bash speedrun.sh`\n",
    "if [ -z \"$WANDB_RUN\" ]; then\n",
    "    # by default use \"dummy\" : it's handled as a special case, skips logging to wandb\n",
    "    WANDB_RUN=dummy\n",
    "fi\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# During the course of the run, we will be writing markdown reports to the report/\n",
    "# directory in the base dir. This command clears it out and writes a header section\n",
    "# with a bunch of system info and a timestamp that marks the start of the run.\n",
    "python -m nanochat.report reset\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Tokenizer\n",
    "\n",
    "# Download the first ~2B characters of pretraining dataset\n",
    "# each data shard is ~250M chars\n",
    "# so we download 2e9 / 250e6 = 8 data shards at this point\n",
    "# each shard is ~100MB of text (compressed), so this is about ~800MB of data on disk\n",
    "# look at dev/repackage_data_reference.py for details on how this data was prepared\n",
    "python -m nanochat.dataset -n 8\n",
    "# Immediately also kick off downloading more shards in the background while tokenizer trains\n",
    "# Approximately 350 shards are needed for 10B tokens of data for pretraining.\n",
    "# The maximum total number of shards available in the entire dataset is 1822.\n",
    "python -m nanochat.dataset -n 370 &\n",
    "DATASET_DOWNLOAD_PID=$!\n",
    "# train the tokenizer with vocab size 2**15 = 32768 on ~2B characters of data\n",
    "python -m scripts.tok_train\n",
    "# evaluate the tokenizer (report compression ratio etc.)\n",
    "python -m scripts.tok_eval\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Base model (pretraining)\n",
    "echo \"Waiting for dataset download to complete...\"\n",
    "wait $DATASET_DOWNLOAD_PID\n",
    "\n",
    "# d24 model (slightly overtrained is enough to beat GPT-2 => increase data:params ratio from compute optimal 10.5 (default) to 12)\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --target-param-data-ratio=8.5 --device-batch-size=16 --fp8 --run=$WANDB_RUN\n",
    "# evaluate the model: CORE metric, BPB on train/val, and draw samples\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.base_eval -- --device-batch-size=16\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SFT (teach the model conversation special tokens, tool use, multiple choice)\n",
    "\n",
    "# download 2.3MB of synthetic identity conversations to impart a personality to nanochat\n",
    "# see dev/gen_synthetic_data.py for details on how this data was prepared and to get a sense of how you can easily tune it\n",
    "curl -L -o $NANOCHAT_BASE_DIR/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl\n",
    "\n",
    "# run SFT and eval the model\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- --device-batch-size=16 --run=$WANDB_RUN\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_eval -- -i sft\n",
    "\n",
    "# chat with the model over CLI! Leave out the -p to chat interactively\n",
    "# python -m scripts.chat_cli -p \"Why is the sky blue?\"\n",
    "\n",
    "# even better, chat with your model over a pretty WebUI ChatGPT style\n",
    "# python -m scripts.chat_web\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate the full report by putting together all the sections\n",
    "# report.md is the output and will be copied to current directory for convenience\n",
    "python -m nanochat.report generate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-02-06 22:48:49'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrismccormick\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from google.colab import userdata\n",
    "\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_RUN = \"nanochat - d12 - v1.0\"\n",
    "# WANDB_RUN = \"dummy\"  # Set to dummy to disable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only two required libraries missing from Colab are `rustbpe` and huggingface `kernels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rustbpe>=0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kernels in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
      "Requirement already satisfied: huggingface_hub<2.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from kernels) (1.3.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from kernels) (26.0)\n",
      "Requirement already satisfied: pyyaml>=6 in /usr/local/lib/python3.12/dist-packages (from kernels) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (4.67.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<2.0,>=0.26.0->kernels) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub<2.0,>=0.26.0->kernels) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub<2.0,>=0.26.0->kernels) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub<2.0,>=0.26.0->kernels) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub<2.0,>=0.26.0->kernels) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub<2.0,>=0.26.0->kernels) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub<2.0,>=0.26.0->kernels) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded nanochat/common.py\n",
      "Downloaded nanochat/core_eval.py\n",
      "Downloaded nanochat/report.py\n",
      "Downloaded nanochat/execution.py\n",
      "Downloaded tasks/common.py\n",
      "Downloaded tasks/gsm8k.py\n",
      "Downloaded tasks/mmlu.py\n",
      "Downloaded tasks/smoltalk.py\n",
      "Downloaded tasks/customjson.py\n",
      "Downloaded tasks/spellingbee.py\n",
      "Downloaded tasks/humaneval.py\n",
      "Downloaded tasks/arc.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "os.makedirs(\"nanochat\", exist_ok=True)\n",
    "os.makedirs(\"tasks\", exist_ok=True)\n",
    "\n",
    "# Using requests to download the file directly to the nanochat directory\n",
    "base_url = \"https://raw.githubusercontent.com/karpathy/nanochat/5fdd5cdb246d2e82a1fcc05fd4c0468df824d875/\"\n",
    "\n",
    "files = [\n",
    "    \"nanochat/common.py\",\n",
    "    \"nanochat/core_eval.py\",\n",
    "    \"nanochat/report.py\",\n",
    "    \"nanochat/execution.py\", # For SFT evaluation\n",
    "    \"tasks/common.py\",\n",
    "    \"tasks/gsm8k.py\",\n",
    "    \"tasks/mmlu.py\",\n",
    "    \"tasks/smoltalk.py\",\n",
    "    \"tasks/customjson.py\",\n",
    "    \"tasks/spellingbee.py\",\n",
    "    \"tasks/humaneval.py\", # For SFT evaluation\n",
    "    \"tasks/arc.py\", # For SFT evaluation\n",
    "]\n",
    "\n",
    "for filename in files:\n",
    "    # Retrieve the file.\n",
    "    response = requests.get(base_url + filename)\n",
    "    response.raise_for_status() # Raise an exception for HTTP errors\n",
    "\n",
    "    # Write to disk.\n",
    "    with open(os.path.join(\".\", filename), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example on A100 w/o FA2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "step 00748/01785 (41.90%) | loss: 3.318540 | lrm: 1.00 | dt: 3901.35ms | tok/sec: 134,386 | mfu: 34.55 | epoch: 1 | total time: 47.93m | eta: 67.3m\n",
    "step 00749/01785 (41.96%) | loss: 3.317277 | lrm: 1.00 | dt: 3904.52ms | tok/sec: 134,277 | mfu: 34.52 | epoch: 1 | total time: 47.99m | eta: 67.3m\n",
    "Step 00750 | Validation bpb: 1.014328\n",
    "step 00750/01785 (42.02%) | loss: 3.322498 | lrm: 1.00 | dt: 3886.83ms | tok/sec: 134,888 | mfu: 34.68 | epoch: 1 | total time: 48.06m | eta: 67.2m\n",
    "step 00751/01785 (42.07%) | loss: 3.326269 | lrm: 1.00 | dt: 3898.44ms | tok/sec: 134,486 | mfu: 34.58 | epoch: 1 | total time: 48.12m | eta: 67.2m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With FA2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "step 00748/01785 (41.90%) | loss: 3.314679 | lrm: 1.00 | dt: 2582.12ms | tok/sec: 203,045 | mfu: 52.20 | epoch: 1 | total time: 31.76m | eta: 44.6m\n",
    "step 00749/01785 (41.96%) | loss: 3.313010 | lrm: 1.00 | dt: 2585.93ms | tok/sec: 202,746 | mfu: 52.13 | epoch: 1 | total time: 31.80m | eta: 44.6m\n",
    "Step 00750 | Validation bpb: 1.013885\n",
    "step 00750/01785 (42.02%) | loss: 3.318731 | lrm: 1.00 | dt: 2566.58ms | tok/sec: 204,275 | mfu: 52.52 | epoch: 1 | total time: 31.84m | eta: 44.5m\n",
    "step 00751/01785 (42.07%) | loss: 3.321754 | lrm: 1.00 | dt: 2568.81ms | tok/sec: 204,097 | mfu: 52.47 | epoch: 1 | total time: 31.88m | eta: 44.5m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/flash_attention.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified Flash Attention interface with automatic FA3/SDPA switching.\n",
    "\n",
    "Exports `flash_attn` module that matches the FA3 API exactly, but falls back\n",
    "to PyTorch SDPA on non-Hopper GPUs (including Blackwell), MPS, and CPU.\n",
    "\n",
    "Usage (drop-in replacement for FA3):\n",
    "    from nanochat.flash_attention import flash_attn\n",
    "\n",
    "    # Training (no KV cache)\n",
    "    y = flash_attn.flash_attn_func(q, k, v, causal=True, window_size=window_size)\n",
    "\n",
    "    # Inference (with KV cache)\n",
    "    y = flash_attn.flash_attn_with_kvcache(q, k_cache, v_cache, k=k, v=v, ...)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_load_flash_attention_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Detection: Try to load FA3 on Hopper+ GPUs\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabf49cad598400f93569e90d1a198c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fee8625319148e0989a50c3e5af2a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _load_flash_attention_3():\n",
    "    \"\"\"Try to load Flash Attention 3 (requires Hopper GPU, sm90).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    try:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major < 9:  # Hopper is sm90\n",
    "            from kernels import get_kernel\n",
    "            return get_kernel(\"kernels-community/flash-attn2\").flash_attn_interface\n",
    "        # FA3 kernels are compiled for Hopper (sm90) only\n",
    "        # Ada (sm89), Blackwell (sm100) need SDPA fallback until FA3 is recompiled\n",
    "        else:\n",
    "            import os\n",
    "            os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "            from kernels import get_kernel\n",
    "            return get_kernel('varunneal/flash-attention-3').flash_attn_interface\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_fa3 = _load_flash_attention_3()\n",
    "HAS_FA3 = _fa3 is not None\n",
    "\n",
    "# Override for testing: set to 'fa3', 'sdpa', or None (auto)\n",
    "_override_impl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAS_FA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_use_fa3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _use_fa3():\n",
    "    \"\"\"Determine whether to use FA3 based on availability and override.\"\"\"\n",
    "    if _override_impl == 'fa3':\n",
    "        assert HAS_FA3, \"Cannot override to FA3: not available on this hardware\"\n",
    "        return True\n",
    "    if _override_impl == 'sdpa':\n",
    "        return False\n",
    "    return HAS_FA3  # auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_sdpa_attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SDPA helpers\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sdpa_attention(q, k, v, window_size, enable_gqa):\n",
    "    \"\"\"\n",
    "    SDPA attention with sliding window support.\n",
    "    q, k, v are (B, H, T, D) format.\n",
    "    \"\"\"\n",
    "    Tq = q.size(2)\n",
    "    Tk = k.size(2)\n",
    "    window = window_size[0]\n",
    "\n",
    "    # Full context, same length\n",
    "    if (window < 0 or window >= Tq) and Tq == Tk:\n",
    "        return F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "\n",
    "    # Single token generation\n",
    "    if Tq == 1:\n",
    "        if window >= 0 and window < Tk:\n",
    "            # window is \"left\" tokens we need to include (window + 1) keys total\n",
    "            start = max(0, Tk - (window + 1))\n",
    "            k = k[:, :, start:, :]\n",
    "            v = v[:, :, start:, :]\n",
    "        return F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n",
    "\n",
    "    # Need explicit mask for sliding window/chunk inference\n",
    "    device = q.device\n",
    "    # For chunk inference (Tq != Tk), is_causal is not aligned to cache position => build an explicit bool mask\n",
    "    row_idx = (Tk - Tq) + torch.arange(Tq, device=device).unsqueeze(1)\n",
    "    col_idx = torch.arange(Tk, device=device).unsqueeze(0)\n",
    "    mask = col_idx <= row_idx\n",
    "\n",
    "    # sliding window (left)\n",
    "    if window >= 0 and window < Tk:\n",
    "        mask = mask & ((row_idx - col_idx) <= window)\n",
    "\n",
    "    return F.scaled_dot_product_attention(q, k, v, attn_mask=mask, enable_gqa=enable_gqa)\n",
    "\n",
    "# =============================================================================\n",
    "# Public API: Same interface as FA3\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flash_attn_func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_func(q, k, v, causal=False, window_size=(-1, -1)):\n",
    "    \"\"\"\n",
    "    Flash Attention for training (no KV cache).\n",
    "\n",
    "    Args:\n",
    "        q, k, v: Tensors of shape (B, T, H, D)\n",
    "        causal: Whether to use causal masking\n",
    "        window_size: (left, right) sliding window. -1 means unlimited.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor of shape (B, T, H, D)\n",
    "    \"\"\"\n",
    "    if _use_fa3():\n",
    "        return _fa3.flash_attn_func(q, k, v, causal=causal, window_size=window_size)\n",
    "\n",
    "    # SDPA fallback: transpose (B, T, H, D) -> (B, H, T, D)\n",
    "    q = q.transpose(1, 2)\n",
    "    k = k.transpose(1, 2)\n",
    "    v = v.transpose(1, 2)\n",
    "    enable_gqa = q.size(1) != k.size(1)\n",
    "    y = _sdpa_attention(q, k, v, window_size, enable_gqa)\n",
    "    return y.transpose(1, 2)  # back to (B, T, H, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flash_attn_with_kvcache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_with_kvcache(q, k_cache, v_cache, k=None, v=None, cache_seqlens=None,\n",
    "                            causal=False, window_size=(-1, -1)):\n",
    "    \"\"\"\n",
    "    Flash Attention with KV cache for inference.\n",
    "\n",
    "    FA3 updates k_cache/v_cache in-place. Our SDPA fallback does the same.\n",
    "\n",
    "    Args:\n",
    "        q: Queries, shape (B, T_new, H, D)\n",
    "        k_cache, v_cache: Pre-allocated cache tensors, shape (B, T_max, H_kv, D)\n",
    "        k, v: New keys/values to insert, shape (B, T_new, H_kv, D)\n",
    "        cache_seqlens: Current position in cache, shape (B,) int32\n",
    "        causal: Whether to use causal masking\n",
    "        window_size: (left, right) sliding window. -1 means unlimited.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor of shape (B, T_new, H, D)\n",
    "    \"\"\"\n",
    "    if _use_fa3():\n",
    "        return _fa3.flash_attn_with_kvcache(\n",
    "            q, k_cache, v_cache, k=k, v=v, cache_seqlens=cache_seqlens,\n",
    "            causal=causal, window_size=window_size\n",
    "        )\n",
    "\n",
    "    # SDPA fallback: manually manage KV cache\n",
    "    B, T_new, H, D = q.shape\n",
    "    pos = cache_seqlens[0].item()  # assume uniform position across batch\n",
    "\n",
    "    # Insert new k, v into cache (in-place, matching FA3 behavior)\n",
    "    if k is not None and v is not None:\n",
    "        k_cache[:, pos:pos+T_new, :, :] = k\n",
    "        v_cache[:, pos:pos+T_new, :, :] = v\n",
    "\n",
    "    # Get full cache up to current position + new tokens\n",
    "    end_pos = pos + T_new\n",
    "    k_full = k_cache[:, :end_pos, :, :]\n",
    "    v_full = v_cache[:, :end_pos, :, :]\n",
    "\n",
    "    # Transpose to SDPA layout: (B, T, H, D) -> (B, H, T, D)\n",
    "    q_sdpa = q.transpose(1, 2)\n",
    "    k_sdpa = k_full.transpose(1, 2)\n",
    "    v_sdpa = v_full.transpose(1, 2)\n",
    "\n",
    "    enable_gqa = q_sdpa.size(1) != k_sdpa.size(1)\n",
    "    y_sdpa = _sdpa_attention(q_sdpa, k_sdpa, v_sdpa, window_size, enable_gqa)\n",
    "\n",
    "    return y_sdpa.transpose(1, 2)  # back to (B, T, H, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Export: flash_attn module interface (drop-in replacement for FA3)\n",
    "# =============================================================================\n",
    "from types import SimpleNamespace\n",
    "flash_attn = SimpleNamespace(\n",
    "    flash_attn_func=flash_attn_func,\n",
    "    flash_attn_with_kvcache=flash_attn_with_kvcache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-stage reports are generated and saved in\n",
    "`/root/.cache/nanochat/report/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset report and wrote header to /root/.cache/nanochat/report/header.md\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# During the course of the run, we will be writing markdown reports to the report/\n",
    "# directory in the base dir. This command clears it out and writes a header section\n",
    "# with a bunch of system info and a timestamp that marks the start of the run.\n",
    "!python -m nanochat.report reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The base/pretraining dataset is a set of parquet files.\n",
    "This file contains utilities for:\n",
    "- iterating over the parquet files and yielding documents from it\n",
    "- download the files on demand if they are not on disk\n",
    "\n",
    "For details of how the dataset was prepared, see `repackage_data_reference.py`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from nanochat.common import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# The specifics of the current pretraining dataset\n",
    "\n",
    "# The URL on the internet where the data is hosted and downloaded from on demand\n",
    "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"\n",
    "MAX_SHARD = 1822 # the last datashard is shard_01822.parquet\n",
    "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\" # format of the filenames\n",
    "base_dir = get_base_dir()\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# These functions are useful utilities to other modules, can/should be imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`list_parquet_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquet_files(data_dir=None):\n",
    "    \"\"\" Looks into a data dir and returns full paths to all parquet files. \"\"\"\n",
    "    data_dir = DATA_DIR if data_dir is None else data_dir\n",
    "    parquet_files = sorted([\n",
    "        f for f in os.listdir(data_dir)\n",
    "        if f.endswith('.parquet') and not f.endswith('.tmp')\n",
    "    ])\n",
    "    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]\n",
    "    return parquet_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parquets_iter_batched`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquets_iter_batched(split, start=0, step=1):\n",
    "    \"\"\"\n",
    "    Iterate through the dataset, in batches of underlying row_groups for efficiency.\n",
    "    - split can be \"train\" or \"val\". the last parquet file will be val.\n",
    "    - start/step are useful for skipping rows in DDP. e.g. start=rank, step=world_size\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "    parquet_paths = list_parquet_files()\n",
    "    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n",
    "    for filepath in parquet_paths:\n",
    "        pf = pq.ParquetFile(filepath)\n",
    "        for rg_idx in range(start, pf.num_row_groups, step):\n",
    "            rg = pf.read_row_group(rg_idx)\n",
    "            texts = rg.column('text').to_pylist()\n",
    "            yield texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`download_single_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_file(index):\n",
    "    \"\"\" Downloads a single file index, with some backoff \"\"\"\n",
    "\n",
    "    # Construct the local filepath for this file and skip if it already exists\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Skipping {filepath} (already exists)\")\n",
    "        return True\n",
    "\n",
    "    # Construct the remote URL for this file\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    print(f\"Downloading {filename}...\")\n",
    "\n",
    "    # Download with retries\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            # Write to temporary file first\n",
    "            temp_path = filepath + f\".tmp\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            # Move temp file to final location\n",
    "            os.rename(temp_path, filepath)\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "            return True\n",
    "\n",
    "        except (requests.RequestException, IOError) as e:\n",
    "            print(f\"Attempt {attempt}/{max_attempts} failed for {filename}: {e}\")\n",
    "            # Clean up any partial files\n",
    "            for path in [filepath + f\".tmp\", filepath]:\n",
    "                if os.path.exists(path):\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "            # Try a few times with exponential backoff: 2^attempt seconds\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to download {filename} after {max_attempts} attempts\")\n",
    "                return False\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoked with:\n",
    "\n",
    "```python\n",
    "# Download the first ~2B characters of pretraining dataset\n",
    "# each data shard is ~250M chars\n",
    "# so we download 2e9 / 250e6 = 8 data shards at this point\n",
    "# each shard is ~100MB of text (compressed), so this is about ~800MB of data on disk\n",
    "# look at dev/repackage_data_reference.py for details on how this data was prepared\n",
    "python -m nanochat.dataset -n 8\n",
    "# Immediately also kick off downloading more shards in the background while tokenizer trains\n",
    "# Approximately 350 shards are needed for 10B tokens of data for pretraining.\n",
    "# The maximum total number of shards available in the entire dataset is 1822.\n",
    "python -m nanochat.dataset -n 370 &\n",
    "DATASET_DOWNLOAD_PID=$!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description=\"Download FineWeb-Edu 100BT dataset shards\")\n",
    "    # parser.add_argument(\"-n\", \"--num-files\", type=int, default=-1, help=\"Number of shards to download (default: -1), -1 = disable\")\n",
    "    # parser.add_argument(\"-w\", \"--num-workers\", type=int, default=4, help=\"Number of parallel download workers (default: 4)\")\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    num_files=20,  # 20 shards for d12\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = MAX_SHARD + 1 if args.num_files == -1 else min(args.num_files, MAX_SHARD + 1)\n",
    "ids_to_download = list(range(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the shards, takes about TODO minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 20 shards using 4 workers...\n",
      "Target directory: /root/.cache/nanochat/base_data\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00000.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00004.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00002.parquet (already exists)\n",
      "\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00001.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00005.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00006.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00008.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00010.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00003.parquet (already exists)\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00009.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00012.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00011.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00014.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00013.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00007.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00015.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00016.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00018.parquet (already exists)\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00019.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00017.parquet (already exists)\n",
      "\n",
      "Done! Downloaded: 20/20 shards to /root/.cache/nanochat/base_data\n",
      "Downloading took 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "print(f\"Downloading {len(ids_to_download)} shards using {args.num_workers} workers...\")\n",
    "print(f\"Target directory: {DATA_DIR}\")\n",
    "print()\n",
    "\n",
    "with Pool(processes=args.num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\n",
    "# Report results\n",
    "successful = sum(1 for success in results if success)\n",
    "print(f\"Done! Downloaded: {successful}/{len(ids_to_download)} shards to {DATA_DIR}\")\n",
    "\n",
    "print(f\"Downloading took {time.time() - t0:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/tokenizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BPE Tokenizer in the style of GPT-4.\n",
    "\n",
    "Two implementations are available:\n",
    "1) HuggingFace Tokenizer that can do both training and inference but is really confusing\n",
    "2) Our own RustBPE Tokenizer for training and tiktoken for efficient inference\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from functools import lru_cache\n",
    "\n",
    "SPECIAL_TOKENS = [\n",
    "    # every document begins with the Beginning of Sequence (BOS) token that delimits documents\n",
    "    \"<|bos|>\",\n",
    "    # tokens below are only used during finetuning to render Conversations into token ids\n",
    "    \"<|user_start|>\", # user messages\n",
    "    \"<|user_end|>\",\n",
    "    \"<|assistant_start|>\", # assistant messages\n",
    "    \"<|assistant_end|>\",\n",
    "    \"<|python_start|>\", # assistant invokes python REPL tool\n",
    "    \"<|python_end|>\",\n",
    "    \"<|output_start|>\", # python REPL outputs back to assistant\n",
    "    \"<|output_end|>\",\n",
    "]\n",
    "\n",
    "# NOTE: this split pattern deviates from GPT-4 in that we use \\p{N}{1,2} instead of \\p{N}{1,3}\n",
    "# I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes.\n",
    "# I verified that 2 is the sweet spot for vocab size of 32K. 1 is a bit worse, 3 was worse still.\n",
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Generic GPT-4-style tokenizer based on HuggingFace Tokenizer\n",
    "from tokenizers import Tokenizer as HFTokenizer\n",
    "from tokenizers import pre_tokenizers, decoders, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HuggingFaceTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceTokenizer:\n",
    "    \"\"\"Light wrapper around HuggingFace Tokenizer for some utilities\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, hf_path):\n",
    "        # init from a HuggingFace pretrained tokenizer (e.g. \"gpt2\")\n",
    "        tokenizer = HFTokenizer.from_pretrained(hf_path)\n",
    "        return cls(tokenizer)\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, tokenizer_dir):\n",
    "        # init from a local directory on disk (e.g. \"out/tokenizer\")\n",
    "        tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "        tokenizer = HFTokenizer.from_file(tokenizer_path)\n",
    "        return cls(tokenizer)\n",
    "\n",
    "    @classmethod\n",
    "    def train_from_iterator(cls, text_iterator, vocab_size):\n",
    "        # train from an iterator of text\n",
    "        # Configure the HuggingFace Tokenizer\n",
    "        tokenizer = HFTokenizer(BPE(\n",
    "            byte_fallback=True, # needed!\n",
    "            unk_token=None,\n",
    "            fuse_unk=False,\n",
    "        ))\n",
    "        # Normalizer: None\n",
    "        tokenizer.normalizer = None\n",
    "        # Pre-tokenizer: GPT-4 style\n",
    "        # the regex pattern used by GPT-4 to split text into groups before BPE\n",
    "        # NOTE: The pattern was changed from \\p{N}{1,3} to \\p{N}{1,2} because I suspect it is harmful to\n",
    "        # very small models and smaller vocab sizes, because it is a little bit wasteful in the token space.\n",
    "        # (but I haven't validated this! TODO)\n",
    "        gpt4_split_regex = Regex(SPLIT_PATTERN) # huggingface demands that you wrap it in Regex!!\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior=\"isolated\", invert=False),\n",
    "            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "        ])\n",
    "        # Decoder: ByteLevel (it pairs together with the ByteLevel pre-tokenizer)\n",
    "        tokenizer.decoder = decoders.ByteLevel()\n",
    "        # Post-processor: None\n",
    "        tokenizer.post_processor = None\n",
    "        # Trainer: BPE\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            show_progress=True,\n",
    "            min_frequency=0, # no minimum frequency\n",
    "            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "            special_tokens=SPECIAL_TOKENS,\n",
    "        )\n",
    "        # Kick off the training\n",
    "        tokenizer.train_from_iterator(text_iterator, trainer)\n",
    "        return cls(tokenizer)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        special_tokens_map = self.tokenizer.get_added_tokens_decoder()\n",
    "        special_tokens = [w.content for w in special_tokens_map.values()]\n",
    "        return special_tokens\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.id_to_token(id)\n",
    "\n",
    "    def _encode_one(self, text, prepend=None, append=None, num_threads=None):\n",
    "        # encode a single string\n",
    "        # prepend/append can be either a string of a special token or a token id directly.\n",
    "        # num_threads is ignored (only used by the nanochat Tokenizer for parallel encoding)\n",
    "        assert isinstance(text, str)\n",
    "        ids = []\n",
    "        if prepend is not None:\n",
    "            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n",
    "            ids.append(prepend_id)\n",
    "        ids.extend(self.tokenizer.encode(text, add_special_tokens=False).ids)\n",
    "        if append is not None:\n",
    "            append_id = append if isinstance(append, int) else self.encode_special(append)\n",
    "            ids.append(append_id)\n",
    "        return ids\n",
    "\n",
    "    def encode_special(self, text):\n",
    "        # encode a single special token via exact match\n",
    "        return self.tokenizer.token_to_id(text)\n",
    "\n",
    "    def get_bos_token_id(self):\n",
    "        # Different HuggingFace models use different BOS tokens and there is little consistency\n",
    "        # 1) attempt to find a <|bos|> token\n",
    "        bos = self.encode_special(\"<|bos|>\")\n",
    "        # 2) if that fails, attempt to find a <|endoftext|> token (e.g. GPT-2 models)\n",
    "        if bos is None:\n",
    "            bos = self.encode_special(\"<|endoftext|>\")\n",
    "        # 3) if these fail, it's better to crash than to silently return None\n",
    "        assert bos is not None, \"Failed to find BOS token in tokenizer\"\n",
    "        return bos\n",
    "\n",
    "    def encode(self, text, *args, **kwargs):\n",
    "        if isinstance(text, str):\n",
    "            return self._encode_one(text, *args, **kwargs)\n",
    "        elif isinstance(text, list):\n",
    "            return [self._encode_one(t, *args, **kwargs) for t in text]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "    def save(self, tokenizer_dir):\n",
    "        # save the tokenizer to disk\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "        self.tokenizer.save(tokenizer_path)\n",
    "        print(f\"Saved tokenizer to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RustBPETokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Tokenizer based on rustbpe + tiktoken combo\n",
    "import pickle\n",
    "import rustbpe\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RustBPETokenizer:\n",
    "    \"\"\"Light wrapper around tiktoken (for efficient inference) but train with rustbpe\"\"\"\n",
    "\n",
    "    def __init__(self, enc, bos_token):\n",
    "        self.enc = enc\n",
    "        self.bos_token_id = self.encode_special(bos_token)\n",
    "\n",
    "    @classmethod\n",
    "    def train_from_iterator(cls, text_iterator, vocab_size):\n",
    "        # 1) train using rustbpe\n",
    "        tokenizer = rustbpe.Tokenizer()\n",
    "        # the special tokens are inserted later in __init__, we don't train them here\n",
    "        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n",
    "        assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n",
    "        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n",
    "        # 2) construct the associated tiktoken encoding for inference\n",
    "        pattern = tokenizer.get_pattern()\n",
    "        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n",
    "        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n",
    "        tokens_offset = len(mergeable_ranks)\n",
    "        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n",
    "        enc = tiktoken.Encoding(\n",
    "            name=\"rustbpe\",\n",
    "            pat_str=pattern,\n",
    "            mergeable_ranks=mergeable_ranks, # dict[bytes, int] (token bytes -> merge priority rank)\n",
    "            special_tokens=special_tokens, # dict[str, int] (special token name -> token id)\n",
    "        )\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, tokenizer_dir):\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            enc = pickle.load(f)\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tiktoken_name):\n",
    "        # https://github.com/openai/tiktoken/blob/eedc8563/tiktoken_ext/openai_public.py\n",
    "        enc = tiktoken.get_encoding(tiktoken_name)\n",
    "        # tiktoken calls the special document delimiter token \"<|endoftext|>\"\n",
    "        # yes this is confusing because this token is almost always PREPENDED to the beginning of the document\n",
    "        # it most often is used to signal the start of a new sequence to the LLM during inference etc.\n",
    "        # so in nanoChat we always use \"<|bos|>\" short for \"beginning of sequence\", but historically it is often called \"<|endoftext|>\".\n",
    "        return cls(enc, \"<|endoftext|>\")\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.enc.n_vocab\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        return self.enc.special_tokens_set\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        return self.enc.decode([id])\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def encode_special(self, text):\n",
    "        return self.enc.encode_single_token(text)\n",
    "\n",
    "    def get_bos_token_id(self):\n",
    "        return self.bos_token_id\n",
    "\n",
    "    def encode(self, text, prepend=None, append=None, num_threads=8):\n",
    "        # text can be either a string or a list of strings\n",
    "\n",
    "        if prepend is not None:\n",
    "            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n",
    "        if append is not None:\n",
    "            append_id = append if isinstance(append, int) else self.encode_special(append)\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            ids = self.enc.encode_ordinary(text)\n",
    "            if prepend is not None:\n",
    "                ids.insert(0, prepend_id) # TODO: slightly inefficient here? :( hmm\n",
    "            if append is not None:\n",
    "                ids.append(append_id)\n",
    "        elif isinstance(text, list):\n",
    "            ids = self.enc.encode_ordinary_batch(text, num_threads=num_threads)\n",
    "            if prepend is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.insert(0, prepend_id) # TODO: same\n",
    "            if append is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.append(append_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.enc.decode(ids)\n",
    "\n",
    "    def save(self, tokenizer_dir):\n",
    "        # save the encoding object to disk\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(self.enc, f)\n",
    "        print(f\"Saved tokenizer encoding to {pickle_path}\")\n",
    "\n",
    "    def render_conversation(self, conversation, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n",
    "        Returns:\n",
    "        - ids: list[int] is a list of token ids of this rendered conversation\n",
    "        - mask: list[int] of same length, mask = 1 for tokens that the Assistant is expected to train on.\n",
    "        \"\"\"\n",
    "        # ids, masks that we will return and a helper function to help build them up.\n",
    "        ids, mask = [], []\n",
    "        def add_tokens(token_ids, mask_val):\n",
    "            if isinstance(token_ids, int):\n",
    "                token_ids = [token_ids]\n",
    "            ids.extend(token_ids)\n",
    "            mask.extend([mask_val] * len(token_ids))\n",
    "\n",
    "        # sometimes the first message is a system message...\n",
    "        # => just merge it with the second (user) message\n",
    "        if conversation[\"messages\"][0][\"role\"] == \"system\":\n",
    "            # some conversation surgery is necessary here for now...\n",
    "            conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "            messages = conversation[\"messages\"]\n",
    "            assert messages[1][\"role\"] == \"user\", \"System message must be followed by a user message\"\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        else:\n",
    "            messages = conversation[\"messages\"]\n",
    "        assert len(messages) >= 1, f\"Conversation has less than 1 message: {messages}\"\n",
    "\n",
    "        # fetch all the special tokens we need\n",
    "        bos = self.get_bos_token_id()\n",
    "        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n",
    "        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n",
    "        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n",
    "        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n",
    "\n",
    "        # now we can tokenize the conversation\n",
    "        add_tokens(bos, 0)\n",
    "        for i, message in enumerate(messages):\n",
    "\n",
    "            # some sanity checking here around assumptions, to prevent footguns\n",
    "            must_be_from = \"user\" if i % 2 == 0 else \"assistant\"\n",
    "            assert message[\"role\"] == must_be_from, f\"Message {i} is from {message['role']} but should be from {must_be_from}\"\n",
    "\n",
    "            # content can be either a simple string or a list of parts (e.g. containing tool calls)\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            if message[\"role\"] == \"user\":\n",
    "                assert isinstance(content, str), \"User messages are simply expected to be strings\"\n",
    "                value_ids = self.encode(content)\n",
    "                add_tokens(user_start, 0)\n",
    "                add_tokens(value_ids, 0)\n",
    "                add_tokens(user_end, 0)\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                add_tokens(assistant_start, 0)\n",
    "                if isinstance(content, str):\n",
    "                    # simple string => simply add the tokens\n",
    "                    value_ids = self.encode(content)\n",
    "                    add_tokens(value_ids, 1)\n",
    "                elif isinstance(content, list):\n",
    "                    for part in content:\n",
    "                        value_ids = self.encode(part[\"text\"])\n",
    "                        if part[\"type\"] == \"text\":\n",
    "                            # string part => simply add the tokens\n",
    "                            add_tokens(value_ids, 1)\n",
    "                        elif part[\"type\"] == \"python\":\n",
    "                            # python tool call => add the tokens inside <|python_start|> and <|python_end|>\n",
    "                            add_tokens(python_start, 1)\n",
    "                            add_tokens(value_ids, 1)\n",
    "                            add_tokens(python_end, 1)\n",
    "                        elif part[\"type\"] == \"python_output\":\n",
    "                            # python output => add the tokens inside <|output_start|> and <|output_end|>\n",
    "                            # none of these tokens are supervised because the tokens come from Python at test time\n",
    "                            add_tokens(output_start, 0)\n",
    "                            add_tokens(value_ids, 0)\n",
    "                            add_tokens(output_end, 0)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown part type: {part['type']}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown content type: {type(content)}\")\n",
    "                add_tokens(assistant_end, 1)\n",
    "\n",
    "        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n",
    "        ids = ids[:max_tokens]\n",
    "        mask = mask[:max_tokens]\n",
    "        return ids, mask\n",
    "\n",
    "    def visualize_tokenization(self, ids, mask, with_token_id=False):\n",
    "        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n",
    "        RED = '\\033[91m'\n",
    "        GREEN = '\\033[92m'\n",
    "        RESET = '\\033[0m'\n",
    "        GRAY = '\\033[90m'\n",
    "        tokens = []\n",
    "        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n",
    "            token_str = self.decode([token_id])\n",
    "            color = GREEN if mask_val == 1 else RED\n",
    "            tokens.append(f\"{color}{token_str}{RESET}\")\n",
    "            if with_token_id:\n",
    "                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n",
    "        return '|'.join(tokens)\n",
    "\n",
    "    def render_for_completion(self, conversation):\n",
    "        \"\"\"\n",
    "        Used during Reinforcement Learning. In that setting, we want to\n",
    "        render the conversation priming the Assistant for a completion.\n",
    "        Unlike the Chat SFT case, we don't need to return the mask.\n",
    "        \"\"\"\n",
    "        # We have some surgery to do: we need to pop the last message (of the Assistant)\n",
    "        conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "        messages = conversation[\"messages\"]\n",
    "        assert messages[-1][\"role\"] == \"assistant\", \"Last message must be from the Assistant\"\n",
    "        messages.pop() # remove the last message (of the Assistant) inplace\n",
    "\n",
    "        # Now tokenize the conversation\n",
    "        ids, mask = self.render_conversation(conversation)\n",
    "\n",
    "        # Finally, to prime the Assistant for a completion, append the Assistant start token\n",
    "        assistant_start = self.encode_special(\"<|assistant_start|>\")\n",
    "        ids.append(assistant_start)\n",
    "        return ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# nanochat-specific convenience functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    from nanochat.common import get_base_dir\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    # return HuggingFaceTokenizer.from_directory(tokenizer_dir)\n",
    "    return RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_token_bytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bytes(device=\"cpu\"):\n",
    "    import torch\n",
    "    from nanochat.common import get_base_dir\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "    assert os.path.exists(token_bytes_path), f\"Token bytes not found at {token_bytes_path}? It gets written by tok_train.py\"\n",
    "    with open(token_bytes_path, \"rb\") as f:\n",
    "        token_bytes = torch.load(f, map_location=device)\n",
    "    return token_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ / â–¶ Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scripts/tok_train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From speedrun.sh:\n",
    "\n",
    "```python\n",
    "# train the tokenizer with vocab size 2**15 = 32768 on ~2B characters of data\n",
    "python -m scripts.tok_train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a tokenizer using our own BPE Tokenizer library.\n",
    "In the style of GPT-4 tokenizer.\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "#from nanochat.tokenizer import RustBPETokenizer\n",
    "from nanochat.common import get_base_dir\n",
    "#from nanochat.dataset import parquets_iter_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars: 2,000,000,000\n",
      "doc_cap: 10,000\n",
      "vocab_size: 32,768\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Parse command line arguments\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Train a BPE tokenizer')\n",
    "# parser.add_argument('--max-chars', type=int, default=2_000_000_000, help='Maximum characters to train on (default: 10B)')\n",
    "# parser.add_argument('--doc-cap', type=int, default=10_000, help='Maximum characters per document (default: 10,000)')\n",
    "# parser.add_argument('--vocab-size', type=int, default=32768, help='Vocabulary size (default: 32768 = 2^15)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    max_chars=2_000_000_000,\n",
    "    doc_cap=10_000,\n",
    "    vocab_size=32768,\n",
    ")\n",
    "\n",
    "print(f\"max_chars: {args.max_chars:,}\")\n",
    "print(f\"doc_cap: {args.doc_cap:,}\")\n",
    "print(f\"vocab_size: {args.vocab_size:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Text iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator():\n",
    "    \"\"\"\n",
    "    1) Flatten the batches into a single iterator\n",
    "    2) Crop every document to args.doc_cap characters\n",
    "    3) Break when we've seen args.max_chars characters\n",
    "    \"\"\"\n",
    "    nchars = 0\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "        for doc in batch:\n",
    "            doc_text = doc\n",
    "            if len(doc_text) > args.doc_cap:\n",
    "                doc_text = doc_text[:args.doc_cap]\n",
    "            nchars += len(doc_text)\n",
    "            yield doc_text\n",
    "            if nchars > args.max_chars:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_iter = text_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 89.58s\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Train the tokenizer\n",
    "t0 = time.time()\n",
    "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)\n",
    "t1 = time.time()\n",
    "train_time = t1 - t0\n",
    "print(f\"Training time: {train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer encoding to /root/.cache/nanochat/tokenizer/tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Save the tokenizer to disk\n",
    "base_dir = get_base_dir()\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "tokenizer.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Quick inline sanity check\n",
    "test_text = \"\"\"Hello world! This is a test.\n",
    "Numbers: 123, 4567, 89\n",
    "Contractions: I'm, you're, it's\n",
    "Special chars: @#$%^&*()\n",
    "Unicode: ä½ å¥½ä¸–ç•Œ ðŸŒ\"\"\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "assert decoded == test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_bytes to /root/.cache/nanochat/tokenizer/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# One more thing: we wish to cache a mapping from token id to number of bytes of that token\n",
    "# for efficient evaluation of bits per byte. Unlike the typical mean loss, this\n",
    "# allows us to report a loss that is invariant to the vocab size of the tokenizer.\n",
    "# The bits per byte on the validation set is then one of the primary metrics we care about.\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "special_set = set(tokenizer.get_special_tokens())\n",
    "token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n",
    "token_bytes = []\n",
    "for token_id in range(vocab_size):\n",
    "    token_str = token_strings[token_id] # the Python string representation of this token\n",
    "    if token_str in special_set:\n",
    "        token_bytes.append(0) # special characters are not counted\n",
    "    else:\n",
    "        id_bytes = len(token_str.encode(\"utf-8\")) # number of bytes that make up this token\n",
    "        token_bytes.append(id_bytes)\n",
    "token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n",
    "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "with open(token_bytes_path, \"wb\") as f:\n",
    "    torch.save(token_bytes, f)\n",
    "print(f\"Saved token_bytes to {token_bytes_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/nanochat/report/tokenizer-training.md'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log to report\n",
    "from nanochat.report import get_report\n",
    "token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)\n",
    "get_report().log(section=\"Tokenizer training\", data=[\n",
    "    vars(args), # argparse command line arguments\n",
    "    {\"train_time\": train_time},\n",
    "    {\"num_special_tokens\": len(special_set)},\n",
    "    {\n",
    "        \"token_bytes_min\": int(token_bytes_nonzero.min().item()),\n",
    "        \"token_bytes_max\": int(token_bytes_nonzero.max().item()),\n",
    "        \"token_bytes_mean\": token_bytes_nonzero.mean().item(),\n",
    "        \"token_bytes_std\": token_bytes_nonzero.std().item(),\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header.md  sft.md  tokenizer-training.md\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/nanochat/report/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tokenizer training\n",
       "timestamp: 2026-02-06 22:50:43\n",
       "\n",
       "- max_chars: 2,000,000,000\n",
       "- doc_cap: 10,000\n",
       "- vocab_size: 32,768\n",
       "- train_time: 89.5770\n",
       "- num_special_tokens: 9\n",
       "- token_bytes_min: 1\n",
       "- token_bytes_max: 19\n",
       "- token_bytes_mean: 6.6029\n",
       "- token_bytes_std: 2.8250\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import markdown from ipython\n",
    "from IPython.display import Markdown\n",
    "\n",
    "with open(\"/root/.cache/nanochat/report/tokenizer-training.md\", \"r\") as f:\n",
    "    report_markdown = f.read()\n",
    "\n",
    "Markdown(report_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scripts/tok_eval.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From speedrun.sh:\n",
    "\n",
    "```python\n",
    "# evaluate the tokenizer (report compression ratio etc.)\n",
    "python -m scripts.tok_eval\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate compression ratio of the tokenizer.\n",
    "\"\"\"\n",
    "\n",
    "#from nanochat.tokenizer import get_tokenizer, RustBPETokenizer\n",
    "#from nanochat.dataset import parquets_iter_batched\n",
    "\n",
    "# Random text I got from a random website this morning\n",
    "news_text = r\"\"\"\n",
    "(Washington, D.C., July 9, 2025)- Yesterday, Mexicoâ€™s National Service of Agro-Alimentary Health, Safety, and Quality (SENASICA) reported a new case of New World Screwworm (NWS) in Ixhuatlan de Madero, Veracruz in Mexico, which is approximately 160 miles northward of the current sterile fly dispersal grid, on the eastern side of the country and 370 miles south of the U.S./Mexico border. This new northward detection comes approximately two months after northern detections were reported in Oaxaca and Veracruz, less than 700 miles away from the U.S. border, which triggered the closure of our ports to Mexican cattle, bison, and horses on May 11, 2025.\n",
    "\n",
    "While USDA announced a risk-based phased port re-opening strategy for cattle, bison, and equine from Mexico beginning as early as July 7, 2025, this newly reported NWS case raises significant concern about the previously reported information shared by Mexican officials and severely compromises the outlined port reopening schedule of five ports from July 7-September 15. Therefore, in order to protect American livestock and our nationâ€™s food supply, Secretary Rollins has ordered the closure of livestock trade through southern ports of entry effective immediately.\n",
    "\n",
    "â€œThe United States has promised to be vigilant â€” and after detecting this new NWS case, we are pausing the planned port reopeningâ€™s to further quarantine and target this deadly pest in Mexico. We must see additional progress combatting NWS in Veracruz and other nearby Mexican states in order to reopen livestock ports along the Southern border,â€ said U.S. Secretary of Agriculture Brooke L. Rollins. â€œThanks to the aggressive monitoring by USDA staff in the U.S. and in Mexico, we have been able to take quick and decisive action to respond to the spread of this deadly pest.â€\n",
    "\"\"\".strip()\n",
    "\n",
    "# Random Korean text (to test non-English compression)\n",
    "korean_text = r\"\"\"\n",
    "ì •ì§í•œ ì‚¬ì‹¤ ìœ„ì—, ê³µì •í•œ ì‹œì„ ì„ ë”í•˜ë‹¤\n",
    "Herald Korea Times\n",
    "\n",
    "í—¤ëŸ´ë“œì½”ë¦¬ì•„íƒ€ìž„ì¦ˆëŠ” ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™” ë“± í•œêµ­ ì‚¬íšŒ ì „ë°˜ì˜ ì£¼ìš” ì´ìŠˆë¥¼ ì‹¬ë„ ìžˆê²Œ ë‹¤ë£¨ëŠ” ì¢…í•© ì˜¨ë¼ì¸ ì‹ ë¬¸ì‚¬ìž…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë‹¨ìˆœížˆ ë‰´ìŠ¤ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ì‹¤(Fact)ì— ê¸°ë°˜í•œ ì–‘ì¸¡ì˜ ì‹œê°ì„ ê· í˜• ìžˆê²Œ ì¡°ëª…í•˜ë©°, ë…ìž ì—¬ëŸ¬ë¶„ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìžˆëŠ” â€˜ì •ë³´ì˜ ê· í˜•â€™ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "í•œêµ­ ì–¸ë¡ ì˜ ì˜¤ëžœ ë¬¸ì œë¡œ ì§€ì ë˜ì–´ ì˜¨ ì •ì¹˜ì  íŽ¸í–¥, ì´ë…ì  ì™œê³¡ì—ì„œ ë²—ì–´ë‚˜\n",
    "ì˜¤ì§ ì •ì§í•¨ê³¼ ê³µì •í•¨ì„ ì›ì¹™ìœ¼ë¡œ ì‚¼ëŠ” ì–¸ë¡ ì„ ì§€í–¥í•©ë‹ˆë‹¤.\n",
    "ì–´ëŠ í•œìª½ì˜ ì£¼ìž¥ë§Œì„ í™•ëŒ€í•˜ê±°ë‚˜ ê°ì¶”ì§€ ì•Šê³ ,\n",
    "**ëª¨ë“  ìŸì ì— ëŒ€í•´ â€˜ë¬´ì—‡ì´ ìŸì ì¸ì§€â€™, â€˜ëˆ„ê°€ ë¬´ì—‡ì„ ì£¼ìž¥í•˜ëŠ”ì§€â€™, â€˜ì‚¬ì‹¤ì€ ë¬´ì—‡ì¸ì§€â€™**ë¥¼ ëª…í™•ížˆ ì „ë‹¬í•˜ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Random piece of code\n",
    "code_text = r\"\"\"\n",
    "class BasicTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # input text preprocessing\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count up the number of times every consecutive pair appears\n",
    "            stats = get_stats(ids)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\"\"\".strip()\n",
    "\n",
    "math_text = r\"\"\"\n",
    "\\documentclass[12pt]{article}\n",
    "\\usepackage{amsmath,amsthm,amssymb}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\n",
    "\\newtheorem{theorem}{Theorem}\n",
    "\\newtheorem*{remark}{Remark}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\begin{center}\n",
    "{\\Large A Cute Identity: The Sum of Cubes is a Square}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{theorem}\n",
    "For every integer $n \\ge 1$,\n",
    "\\[\n",
    "\\sum_{k=1}^{n} k^{3} \\;=\\; \\left(\\frac{n(n+1)}{2}\\right)^{2}.\n",
    "\\]\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}[Proof 1 (Induction)]\n",
    "Let $S(n) = \\sum_{k=1}^{n} k^3$. For $n=1$, $S(1)=1=(1\\cdot 2/2)^2$, so the base case holds.\n",
    "\n",
    "Assume $S(n)=\\big(\\tfrac{n(n+1)}{2}\\big)^2$ for some $n\\ge 1$.\n",
    "Then\n",
    "\\[\n",
    "S(n+1)\n",
    "= S(n) + (n+1)^3\n",
    "= \\left(\\frac{n(n+1)}{2}\\right)^2 + (n+1)^3.\n",
    "\\]\n",
    "Factor out $(n+1)^2$:\n",
    "\\[\n",
    "S(n+1)\n",
    "= (n+1)^2\\left( \\frac{n^2}{4} + (n+1) \\right)\n",
    "= (n+1)^2\\left( \\frac{n^2 + 4n + 4}{4} \\right)\n",
    "= (n+1)^2\\left( \\frac{(n+2)^2}{4} \\right).\n",
    "\\]\n",
    "Thus\n",
    "\\[\n",
    "S(n+1)=\\left(\\frac{(n+1)(n+2)}{2}\\right)^2,\n",
    "\\]\n",
    "which matches the claimed formula with $n$ replaced by $n+1$. By induction, the identity holds for all $n\\ge 1$.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{proof}[Proof 2 (Algebraic telescoping)]\n",
    "Recall the binomial identity\n",
    "\\[\n",
    "(k+1)^4 - k^4 = 4k^3 + 6k^2 + 4k + 1.\n",
    "\\]\n",
    "Summing both sides from $k=0$ to $n$ telescopes:\n",
    "\\[\n",
    "(n+1)^4 - 0^4\n",
    "= \\sum_{k=0}^{n}\\big(4k^3 + 6k^2 + 4k + 1\\big)\n",
    "= 4\\sum_{k=1}^{n}k^3 + 6\\sum_{k=1}^{n}k^2 + 4\\sum_{k=1}^{n}k + (n+1).\n",
    "\\]\n",
    "Using the standard sums\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\n",
    "\\quad\\text{and}\\quad\n",
    "\\sum_{k=1}^{n}k^2 = \\frac{n(n+1)(2n+1)}{6},\n",
    "\\]\n",
    "solve for $\\sum_{k=1}^{n}k^3$ to get\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k^3 = \\left(\\frac{n(n+1)}{2}\\right)^2.\n",
    "\\]\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{remark}\n",
    "Geometrically, the identity says: ``adding up $1^3,2^3,\\dots,n^3$ builds a perfect squareâ€™â€™â€”namely the square of the $n$th triangular number. This is why one sometimes calls it the \\emph{sum-of-cubes is a square} phenomenon.\n",
    "\\end{remark}\n",
    "\n",
    "\\end{document}\n",
    "\"\"\".strip()\n",
    "\n",
    "science_text = r\"\"\"\n",
    "Photosynthesis is a photochemical energy transduction process in which light-harvesting pigmentâ€“protein complexes within the thylakoid membranes of oxygenic phototrophs absorb photons and initiate charge separation at the reaction center, driving the linear electron transport chain from water to NADPâº via photosystem II, the cytochrome bâ‚†f complex, and photosystem I, concomitantly generating a trans-thylakoid proton motive force utilized by chloroplastic ATP synthase. The light-dependent reactions produce ATP and NADPH, which fuel the Calvinâ€“Bensonâ€“Bassham cycle in the stroma, wherein ribulose-1,5-bisphosphate is carboxylated by ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) to form 3-phosphoglycerate, subsequently reduced and regenerated through a series of enzymatic steps, enabling net assimilation of COâ‚‚ into triose phosphates and ultimately carbohydrates. This process is tightly regulated by photoprotective mechanisms, redox feedback, and metabolite flux, representing a central biochemical pathway coupling solar energy capture to the biosphereâ€™s primary productivity.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab sizes:\n",
      "GPT-2: 50257\n",
      "GPT-4: 100277\n",
      "Ours: 32768\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer was trained on data from earlier shards, so it has seen this data\n",
    "train_docs = next(parquets_iter_batched(split=\"train\"))\n",
    "train_text = \"\\n\".join(train_docs)\n",
    "val_docs = next(parquets_iter_batched(split=\"val\"))\n",
    "val_text = \"\\n\".join(val_docs)\n",
    "\n",
    "all_text = [\n",
    "    (\"news\", news_text),\n",
    "    (\"korean\", korean_text),\n",
    "    (\"code\", code_text),\n",
    "    (\"math\", math_text),\n",
    "    (\"science\", science_text),\n",
    "    (\"fwe-train\", train_text),\n",
    "]\n",
    "if val_text:\n",
    "    all_text.append((\"fwe-val\", val_text))\n",
    "\n",
    "# Try out current default compared to GPT-2 and GPT-4 tokenizers\n",
    "tokenizer_results = {}\n",
    "vocab_sizes = {}\n",
    "\n",
    "for tokenizer_name in [\"gpt2\", \"gpt4\", \"ours\"]:\n",
    "\n",
    "    if tokenizer_name == \"gpt2\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\") # gpt-2 base model tokenizer\n",
    "    elif tokenizer_name == \"gpt4\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\") # gpt-4 base model tokenizer\n",
    "    else:\n",
    "        tokenizer = get_tokenizer()\n",
    "\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "    tokenizer_results[tokenizer_name] = {}\n",
    "\n",
    "    for name, text in all_text:\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text\n",
    "\n",
    "        encoded_bytes = text.encode('utf-8')\n",
    "        ratio = len(encoded_bytes) / len(encoded)\n",
    "        tokenizer_results[tokenizer_name][name] = {\n",
    "            'bytes': len(encoded_bytes),\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }\n",
    "\n",
    "# ANSI color codes\n",
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "# Print vocab sizes\n",
    "print(f\"\\nVocab sizes:\")\n",
    "print(f\"GPT-2: {vocab_sizes['gpt2']}\")\n",
    "print(f\"GPT-4: {vocab_sizes['gpt4']}\")\n",
    "print(f\"Ours: {vocab_sizes['ours']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print_comparison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(baseline_name, baseline_results, ours_results, all_text):\n",
    "    \"\"\"Print comparison table between baseline tokenizer and ours.\"\"\"\n",
    "    print(f\"\\nComparison with {baseline_name}:\")\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"{'Text Type':<10} {'Bytes':<8} {baseline_name:<15} {'Ours':<15} {'Relative':<12} {'Better':<10}\")\n",
    "    print(f\"{'':10} {'':8} {'Tokens':<7} {'Ratio':<7} {'Tokens':<7} {'Ratio':<7} {'Diff %':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, text in all_text:\n",
    "        baseline_data = baseline_results[name]\n",
    "        ours_data = ours_results[name]\n",
    "\n",
    "        # Calculate relative difference (positive means ours is better, negative means worse)\n",
    "        # Using tokens: fewer tokens is better, so we calculate (baseline_tokens - ours_tokens) / baseline_tokens\n",
    "        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n",
    "\n",
    "        # Determine which has better compression (higher ratio = better)\n",
    "        if baseline_data['ratio'] > ours_data['ratio']:\n",
    "            baseline_color, ours_color = GREEN, RED\n",
    "            better = baseline_name\n",
    "            diff_color = RED\n",
    "        elif ours_data['ratio'] > baseline_data['ratio']:\n",
    "            baseline_color, ours_color = RED, GREEN\n",
    "            better = \"Ours\"\n",
    "            diff_color = GREEN\n",
    "        else:\n",
    "            baseline_color, ours_color = \"\", \"\"\n",
    "            better = \"Tie\"\n",
    "            diff_color = \"\"\n",
    "\n",
    "        print(f\"{name:<10} {baseline_data['bytes']:<8} \"\n",
    "              f\"{baseline_color}{baseline_data['tokens']:<7}{RESET} \"\n",
    "              f\"{baseline_color}{baseline_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['tokens']:<7}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{diff_color}{relative_diff:+7.1f}%{RESET}     \"\n",
    "              f\"{better:<10}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with GPT-2:\n",
      "===============================================================================================\n",
      "Text Type  Bytes    GPT-2           Ours            Relative     Better    \n",
      "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "news       1819     \u001b[91m404    \u001b[0m \u001b[91m4.50   \u001b[0m \u001b[92m403    \u001b[0m \u001b[92m4.51   \u001b[0m \u001b[92m   +0.2%\u001b[0m     Ours      \n",
      "korean     893      \u001b[92m745    \u001b[0m \u001b[92m1.20   \u001b[0m \u001b[91m797    \u001b[0m \u001b[91m1.12   \u001b[0m \u001b[91m   -7.0%\u001b[0m     GPT-2     \n",
      "code       1259     \u001b[92m576    \u001b[0m \u001b[92m2.19   \u001b[0m \u001b[91m620    \u001b[0m \u001b[91m2.03   \u001b[0m \u001b[91m   -7.6%\u001b[0m     GPT-2     \n",
      "math       1834     \u001b[92m936    \u001b[0m \u001b[92m1.96   \u001b[0m \u001b[91m1025   \u001b[0m \u001b[91m1.79   \u001b[0m \u001b[91m   -9.5%\u001b[0m     GPT-2     \n",
      "science    1112     \u001b[91m260    \u001b[0m \u001b[91m4.28   \u001b[0m \u001b[92m258    \u001b[0m \u001b[92m4.31   \u001b[0m \u001b[92m   +0.8%\u001b[0m     Ours      \n",
      "fwe-train  4208518  \u001b[91m900364 \u001b[0m \u001b[91m4.67   \u001b[0m \u001b[92m892476 \u001b[0m \u001b[92m4.72   \u001b[0m \u001b[92m   +0.9%\u001b[0m     Ours      \n",
      "fwe-val    4515835  \u001b[91m968625 \u001b[0m \u001b[91m4.66   \u001b[0m \u001b[92m963250 \u001b[0m \u001b[92m4.69   \u001b[0m \u001b[92m   +0.6%\u001b[0m     Ours      \n"
     ]
    }
   ],
   "source": [
    "# Print comparisons\n",
    "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with GPT-4:\n",
      "===============================================================================================\n",
      "Text Type  Bytes    GPT-4           Ours            Relative     Better    \n",
      "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "news       1819     \u001b[92m387    \u001b[0m \u001b[92m4.70   \u001b[0m \u001b[91m403    \u001b[0m \u001b[91m4.51   \u001b[0m \u001b[91m   -4.1%\u001b[0m     GPT-4     \n",
      "korean     893      \u001b[92m364    \u001b[0m \u001b[92m2.45   \u001b[0m \u001b[91m797    \u001b[0m \u001b[91m1.12   \u001b[0m \u001b[91m -119.0%\u001b[0m     GPT-4     \n",
      "code       1259     \u001b[92m309    \u001b[0m \u001b[92m4.07   \u001b[0m \u001b[91m620    \u001b[0m \u001b[91m2.03   \u001b[0m \u001b[91m -100.6%\u001b[0m     GPT-4     \n",
      "math       1834     \u001b[92m832    \u001b[0m \u001b[92m2.20   \u001b[0m \u001b[91m1025   \u001b[0m \u001b[91m1.79   \u001b[0m \u001b[91m  -23.2%\u001b[0m     GPT-4     \n",
      "science    1112     \u001b[92m249    \u001b[0m \u001b[92m4.47   \u001b[0m \u001b[91m258    \u001b[0m \u001b[91m4.31   \u001b[0m \u001b[91m   -3.6%\u001b[0m     GPT-4     \n",
      "fwe-train  4208518  \u001b[92m874799 \u001b[0m \u001b[92m4.81   \u001b[0m \u001b[91m892476 \u001b[0m \u001b[91m4.72   \u001b[0m \u001b[91m   -2.0%\u001b[0m     GPT-4     \n",
      "fwe-val    4515835  \u001b[92m943495 \u001b[0m \u001b[92m4.79   \u001b[0m \u001b[91m963250 \u001b[0m \u001b[91m4.69   \u001b[0m \u001b[91m   -2.1%\u001b[0m     GPT-4     \n"
     ]
    }
   ],
   "source": [
    "print_comparison(\"GPT-4\", tokenizer_results['gpt4'], tokenizer_results['ours'], all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/nanochat/report/tokenizer-evaluation.md'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log to report\n",
    "#from nanochat.report import get_report\n",
    "\n",
    "lines = []\n",
    "for baseline_name in [\"GPT-2\", \"GPT-4\"]:\n",
    "    baseline_key = baseline_name.lower().replace('-', '')\n",
    "    baseline_results = tokenizer_results[baseline_key]\n",
    "    ours_results = tokenizer_results['ours']\n",
    "    lines.append(f\"### Comparison with {baseline_name}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"| Text Type | Bytes | \" + baseline_name + \" Tokens | \" + baseline_name + \" Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\")\n",
    "    lines.append(\"|-----------|-------|--------------|--------------|-------------|------------|-----------------|\")\n",
    "    for name, text in all_text:\n",
    "        baseline_data = baseline_results[name]\n",
    "        ours_data = ours_results[name]\n",
    "        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n",
    "        lines.append(f\"| {name} | {baseline_data['bytes']} | {baseline_data['tokens']} | {baseline_data['ratio']:.2f} | {ours_data['tokens']} | {ours_data['ratio']:.2f} | {relative_diff:+.1f}% |\")\n",
    "    lines.append(\"\")\n",
    "report_markdown = \"\\n\".join(lines)\n",
    "get_report().log(section=\"Tokenizer evaluation\", data=[\n",
    "    report_markdown,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Comparison with GPT-2\n",
       "\n",
       "| Text Type | Bytes | GPT-2 Tokens | GPT-2 Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\n",
       "|-----------|-------|--------------|--------------|-------------|------------|-----------------|\n",
       "| news | 1819 | 404 | 4.50 | 403 | 4.51 | +0.2% |\n",
       "| korean | 893 | 745 | 1.20 | 797 | 1.12 | -7.0% |\n",
       "| code | 1259 | 576 | 2.19 | 620 | 2.03 | -7.6% |\n",
       "| math | 1834 | 936 | 1.96 | 1025 | 1.79 | -9.5% |\n",
       "| science | 1112 | 260 | 4.28 | 258 | 4.31 | +0.8% |\n",
       "| fwe-train | 4208518 | 900364 | 4.67 | 892476 | 4.72 | +0.9% |\n",
       "| fwe-val | 4515835 | 968625 | 4.66 | 963250 | 4.69 | +0.6% |\n",
       "\n",
       "### Comparison with GPT-4\n",
       "\n",
       "| Text Type | Bytes | GPT-4 Tokens | GPT-4 Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\n",
       "|-----------|-------|--------------|--------------|-------------|------------|-----------------|\n",
       "| news | 1819 | 387 | 4.70 | 403 | 4.51 | -4.1% |\n",
       "| korean | 893 | 364 | 2.45 | 797 | 1.12 | -119.0% |\n",
       "| code | 1259 | 309 | 4.07 | 620 | 2.03 | -100.6% |\n",
       "| math | 1834 | 832 | 2.20 | 1025 | 1.79 | -23.2% |\n",
       "| science | 1112 | 249 | 4.47 | 258 | 4.31 | -3.6% |\n",
       "| fwe-train | 4208518 | 874799 | 4.81 | 892476 | 4.72 | -2.0% |\n",
       "| fwe-val | 4515835 | 943495 | 4.79 | 963250 | 4.69 | -2.1% |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import markdown from ipython\n",
    "from IPython.display import Markdown\n",
    "Markdown(report_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/dataloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distributed dataloaders for pretraining.\n",
    "\n",
    "BOS-aligned bestfit:\n",
    "   - Every row starts with BOS token\n",
    "   - Documents packed using best-fit algorithm to minimize cropping\n",
    "   - When no document fits remaining space, crops a document to fill exactly\n",
    "   - 100% utilization (no padding), ~35% tokens cropped at T=2048\n",
    "\n",
    "Compared to the original tokenizing_distributed_data_loader:\n",
    "BOS-aligned loses ~35% of tokens to cropping, but ensures that\n",
    "there are fewer \"confusing\" tokens in the train/val batches as every token can\n",
    "now attend back to the BOS token and sees the full context of the document.\n",
    "\n",
    "Fallback to the original if you have very limited data AND long documents:\n",
    "https://github.com/karpathy/nanochat/blob/3c3a3d7/nanochat/dataloader.py#L78-L117\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from nanochat.common import get_dist_info\n",
    "#from nanochat.dataset import list_parquet_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_document_batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _document_batches(split, resume_state_dict, tokenizer_batch_size):\n",
    "    \"\"\"\n",
    "    Infinite iterator over document batches (list of text strings) from parquet files.\n",
    "\n",
    "    Handles DDP sharding and approximate resume. Each yield is (text_batch, (pq_idx, rg_idx, epoch))\n",
    "    where text_batch is a list of document strings, indices track position for resumption,\n",
    "    and epoch counts how many times we've cycled through the dataset (starts at 1).\n",
    "    \"\"\"\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "\n",
    "    parquet_paths = list_parquet_files()\n",
    "    assert len(parquet_paths) != 0, \"No dataset parquet files found, did you run dataset.py?\"\n",
    "    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n",
    "\n",
    "    resume_pq_idx = resume_state_dict[\"pq_idx\"] if resume_state_dict is not None else 0\n",
    "    resume_rg_idx = resume_state_dict[\"rg_idx\"] if resume_state_dict is not None else None\n",
    "    resume_epoch = resume_state_dict.get(\"epoch\", 1) if resume_state_dict is not None else 1\n",
    "    first_pass = True\n",
    "    pq_idx = resume_pq_idx\n",
    "    epoch = resume_epoch\n",
    "\n",
    "    while True:  # iterate infinitely (multi-epoch)\n",
    "        pq_idx = resume_pq_idx if first_pass else 0\n",
    "        while pq_idx < len(parquet_paths):\n",
    "            filepath = parquet_paths[pq_idx]\n",
    "            pf = pq.ParquetFile(filepath)\n",
    "            # Start from resume point if resuming on same file, otherwise from DDP rank\n",
    "            if first_pass and (resume_rg_idx is not None) and (pq_idx == resume_pq_idx):\n",
    "                base_idx = resume_rg_idx // ddp_world_size\n",
    "                base_idx += 1  # advance by 1 so we don't repeat data after resuming\n",
    "                rg_idx = base_idx * ddp_world_size + ddp_rank\n",
    "                if rg_idx >= pf.num_row_groups:\n",
    "                    pq_idx += 1\n",
    "                    continue\n",
    "                resume_rg_idx = None  # only do this once\n",
    "            else:\n",
    "                rg_idx = ddp_rank\n",
    "            while rg_idx < pf.num_row_groups:\n",
    "                rg = pf.read_row_group(rg_idx)\n",
    "                batch = rg.column('text').to_pylist()\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size], (pq_idx, rg_idx, epoch)\n",
    "                rg_idx += ddp_world_size\n",
    "            pq_idx += 1\n",
    "        first_pass = False\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizing_distributed_data_loader_with_state_bos_bestfit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_distributed_data_loader_with_state_bos_bestfit(\n",
    "    tokenizer, B, T, split,\n",
    "    tokenizer_threads=4, tokenizer_batch_size=128,\n",
    "    device=\"cuda\", resume_state_dict=None,\n",
    "    buffer_size=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    BOS-aligned dataloader with Best-Fit Cropping.\n",
    "\n",
    "    Reduces token waste compared to simple greedy cropping by searching a buffer\n",
    "    for documents that fit well, while maintaining 100% utilization (no padding).\n",
    "\n",
    "    Algorithm for each row:\n",
    "    1. From buffered docs, pick the LARGEST doc that fits entirely\n",
    "    2. Repeat until no doc fits\n",
    "    3. When nothing fits, crop a doc to fill remaining space exactly\n",
    "\n",
    "    Key properties:\n",
    "    - Every row starts with BOS\n",
    "    - 100% utilization (no padding, every token is trained on)\n",
    "    - Approximately 35% of all tokens are discarded due to cropping\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    row_capacity = T + 1\n",
    "    batches = _document_batches(split, resume_state_dict, tokenizer_batch_size)\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "    doc_buffer = []\n",
    "    pq_idx, rg_idx, epoch = 0, 0, 1\n",
    "\n",
    "    def refill_buffer():\n",
    "        nonlocal pq_idx, rg_idx, epoch\n",
    "        doc_batch, (pq_idx, rg_idx, epoch) = next(batches)\n",
    "        token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "        for tokens in token_lists:\n",
    "            doc_buffer.append(tokens)\n",
    "\n",
    "    # Pre-allocate buffers once: layout is [inputs (B*T) | targets (B*T)]\n",
    "    # This gives us contiguous views and a single HtoD transfer\n",
    "    use_cuda = device == \"cuda\"\n",
    "    row_buffer = torch.empty((B, row_capacity), dtype=torch.long) # for building rows without creating Python lists\n",
    "    cpu_buffer = torch.empty(2 * B * T, dtype=torch.long, pin_memory=use_cuda) # staging area (CPU)\n",
    "    gpu_buffer = torch.empty(2 * B * T, dtype=torch.long, device=device) # on-device buffer\n",
    "    cpu_inputs = cpu_buffer[:B * T].view(B, T) # a few views into these buffers just for convenience\n",
    "    cpu_targets = cpu_buffer[B * T:].view(B, T)\n",
    "    inputs = gpu_buffer[:B * T].view(B, T)\n",
    "    targets = gpu_buffer[B * T:].view(B, T)\n",
    "\n",
    "    while True:\n",
    "        for row_idx in range(B):\n",
    "            pos = 0\n",
    "            while pos < row_capacity:\n",
    "                # Ensure buffer has documents\n",
    "                while len(doc_buffer) < buffer_size:\n",
    "                    refill_buffer()\n",
    "\n",
    "                remaining = row_capacity - pos\n",
    "\n",
    "                # Find largest doc that fits entirely\n",
    "                best_idx = -1\n",
    "                best_len = 0\n",
    "                for i, doc in enumerate(doc_buffer):\n",
    "                    doc_len = len(doc)\n",
    "                    if doc_len <= remaining and doc_len > best_len:\n",
    "                        best_idx = i\n",
    "                        best_len = doc_len\n",
    "\n",
    "                if best_idx >= 0:\n",
    "                    doc = doc_buffer.pop(best_idx)\n",
    "                    doc_len = len(doc)\n",
    "                    row_buffer[row_idx, pos:pos + doc_len] = torch.tensor(doc, dtype=torch.long)\n",
    "                    pos += doc_len\n",
    "                else:\n",
    "                    # No doc fits - crop shortest in buffer to fill remaining and minimize waste\n",
    "                    shortest_idx = min(range(len(doc_buffer)), key=lambda i: len(doc_buffer[i]))\n",
    "                    doc = doc_buffer.pop(shortest_idx)\n",
    "                    row_buffer[row_idx, pos:pos + remaining] = torch.tensor(doc[:remaining], dtype=torch.long)\n",
    "                    pos += remaining\n",
    "\n",
    "        # Copy to pinned CPU buffer, then single HtoD transfer\n",
    "        cpu_inputs.copy_(row_buffer[:, :-1])\n",
    "        cpu_targets.copy_(row_buffer[:, 1:])\n",
    "\n",
    "        state_dict = {\"pq_idx\": pq_idx, \"rg_idx\": rg_idx, \"epoch\": epoch}\n",
    "\n",
    "        # Single HtoD copy into persistent GPU buffer and yield\n",
    "        gpu_buffer.copy_(cpu_buffer, non_blocking=use_cuda)\n",
    "        yield inputs, targets, state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizing_distributed_data_loader_bos_bestfit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_distributed_data_loader_bos_bestfit(*args, **kwargs):\n",
    "    \"\"\"Helper that omits state_dict from yields.\"\"\"\n",
    "    for inputs, targets, state_dict in tokenizing_distributed_data_loader_with_state_bos_bestfit(*args, **kwargs):\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/optim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A nice and efficient mixed AdamW/Muon Combined Optimizer.\n",
    "Usually the embeddings and scalars go into AdamW, and the matrix parameters go into Muon.\n",
    "Two versions are provided (MuonAdamW, DistMuonAdamW), for single GPU and distributed.\n",
    "\n",
    "Addapted from: https://github.com/KellerJordan/modded-nanogpt\n",
    "Further contributions from @karpathy and @chrisjmccormick.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `adamw_step_fused`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGood old AdamW optimizer, fused kernel.\\nhttps://arxiv.org/abs/1711.05101\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Good old AdamW optimizer, fused kernel.\n",
    "https://arxiv.org/abs/1711.05101\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(dynamic=False, fullgraph=True)\n",
    "def adamw_step_fused(\n",
    "    p: Tensor,              # (32768, 768) - parameter tensor\n",
    "    grad: Tensor,           # (32768, 768) - gradient, same shape as p\n",
    "    exp_avg: Tensor,        # (32768, 768) - first moment, same shape as p\n",
    "    exp_avg_sq: Tensor,     # (32768, 768) - second moment, same shape as p\n",
    "    step_t: Tensor,         # () - 0-D CPU tensor, step count\n",
    "    lr_t: Tensor,           # () - 0-D CPU tensor, learning rate\n",
    "    beta1_t: Tensor,        # () - 0-D CPU tensor, beta1\n",
    "    beta2_t: Tensor,        # () - 0-D CPU tensor, beta2\n",
    "    eps_t: Tensor,          # () - 0-D CPU tensor, epsilon\n",
    "    wd_t: Tensor,           # () - 0-D CPU tensor, weight decay\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fused AdamW step: weight_decay -> momentum_update -> bias_correction -> param_update\n",
    "    All in one compiled graph to eliminate Python overhead between ops.\n",
    "    The 0-D CPU tensors avoid recompilation when hyperparameter values change.\n",
    "    \"\"\"\n",
    "    # Weight decay (decoupled, applied before the update)\n",
    "    p.mul_(1 - lr_t * wd_t)\n",
    "    # Update running averages (lerp_ is cleaner and fuses well)\n",
    "    exp_avg.lerp_(grad, 1 - beta1_t)\n",
    "    exp_avg_sq.lerp_(grad.square(), 1 - beta2_t)\n",
    "    # Bias corrections\n",
    "    bias1 = 1 - beta1_t ** step_t\n",
    "    bias2 = 1 - beta2_t ** step_t\n",
    "    # Compute update and apply\n",
    "    denom = (exp_avg_sq / bias2).sqrt() + eps_t\n",
    "    step_size = lr_t / bias1\n",
    "    p.add_(exp_avg / denom, alpha=-step_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Muon optimizer adapted and simplified from modded-nanogpt.\n",
    "https://github.com/KellerJordan/modded-nanogpt\n",
    "\n",
    "Background:\n",
    "Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "\n",
    "Here, an alternative to Newton-Schulz iteration with potentially better convergence properties:\n",
    "Polar Express Sign Method for orthogonalization.\n",
    "https://arxiv.org/pdf/2505.16932\n",
    "by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.\n",
    "\n",
    "NorMuon variance reduction: per-neuron/column adaptive learning rate that normalizes\n",
    "update scales after orthogonalization (Muon's output has non-uniform scales across neurons).\n",
    "https://arxiv.org/pdf/2510.05491\n",
    "\n",
    "Some of the changes in nanochat implementation:\n",
    "- Uses a simpler, more general approach to parameter grouping and stacking\n",
    "- Uses a single fused kernel for the momentum -> polar_express -> variance_reduction -> update step\n",
    "- Makes no assumptions about model architecture (e.g. that attention weights are fused into QKVO format)\n",
    "\"\"\"\n",
    "\n",
    "# Coefficients for Polar Express (computed for num_iters=5, safety_factor=2e-2, cushion=2)\n",
    "# From https://arxiv.org/pdf/2505.16932\n",
    "polar_express_coeffs = [\n",
    "    (8.156554524902461, -22.48329292557795, 15.878769915207462),\n",
    "    (4.042929935166739, -2.808917465908714, 0.5000178451051316),\n",
    "    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),\n",
    "    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),\n",
    "    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `muon_step_fused`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(dynamic=False, fullgraph=True)\n",
    "def muon_step_fused(\n",
    "    stacked_grads: Tensor,          # (12, 768, 3072) - stacked gradients\n",
    "    stacked_params: Tensor,         # (12, 768, 3072) - stacked parameters\n",
    "    momentum_buffer: Tensor,        # (12, 768, 3072) - first moment buffer\n",
    "    second_momentum_buffer: Tensor, # (12, 768, 1) or (12, 1, 3072) - factored second moment\n",
    "    momentum_t: Tensor,             # () - 0-D CPU tensor, momentum coefficient\n",
    "    lr_t: Tensor,                   # () - 0-D CPU tensor, learning rate\n",
    "    wd_t: Tensor,                   # () - 0-D CPU tensor, weight decay\n",
    "    beta2_t: Tensor,                # () - 0-D CPU tensor, beta2 for second moment\n",
    "    ns_steps: int,                  # 5 - number of Newton-Schulz/Polar Express iterations\n",
    "    red_dim: int,                   # -1 or -2 - reduction dimension for variance\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fused Muon step: momentum -> polar_express -> variance_reduction -> cautious_update\n",
    "    All in one compiled graph to eliminate Python overhead between ops.\n",
    "    Some of the constants are 0-D CPU tensors to avoid recompilation when values change.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nesterov momentum\n",
    "    momentum = momentum_t.to(stacked_grads.dtype)\n",
    "    momentum_buffer.lerp_(stacked_grads, 1 - momentum)\n",
    "    g = stacked_grads.lerp_(momentum_buffer, momentum)\n",
    "\n",
    "    # Polar express\n",
    "    X = g.bfloat16()\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) * 1.02 + 1e-6)\n",
    "    if g.size(-2) > g.size(-1): # Tall matrix\n",
    "        for a, b, c in polar_express_coeffs[:ns_steps]:\n",
    "            A = X.mT @ X\n",
    "            B = b * A + c * (A @ A)\n",
    "            X = a * X + X @ B\n",
    "    else: # Wide matrix (original math)\n",
    "        for a, b, c in polar_express_coeffs[:ns_steps]:\n",
    "            A = X @ X.mT\n",
    "            B = b * A + c * (A @ A)\n",
    "            X = a * X + B @ X\n",
    "    g = X\n",
    "\n",
    "    # Variance reduction\n",
    "    beta2 = beta2_t.to(g.dtype)\n",
    "    v_mean = g.float().square().mean(dim=red_dim, keepdim=True)\n",
    "    red_dim_size = g.size(red_dim)\n",
    "    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True) * red_dim_size\n",
    "    v_norm = v_norm_sq.sqrt()\n",
    "    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)\n",
    "    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt()\n",
    "    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()\n",
    "    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt()\n",
    "    final_scale = step_size * (v_norm / v_norm_new.clamp_min(1e-10))\n",
    "    g = g * final_scale.to(g.dtype)\n",
    "\n",
    "    # Cautious weight decay + parameter update\n",
    "    lr = lr_t.to(g.dtype)\n",
    "    wd = wd_t.to(g.dtype)\n",
    "    mask = (g * stacked_params) >= 0\n",
    "    stacked_params.sub_(lr * g + lr * wd * stacked_params * mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MuonAdamW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Single GPU version of the MuonAdamW optimizer.\n",
    "# Used mostly for reference, debugging and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuonAdamW(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Combined optimizer: Muon for 2D matrix params, AdamW for others, single GPU version.\n",
    "\n",
    "    AdamW - Fused AdamW optimizer step.\n",
    "\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - The Muon optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        param_groups: List of dicts, each containing:\n",
    "            - 'params': List of parameters\n",
    "            - 'kind': 'adamw' or 'muon'\n",
    "            - For AdamW groups: 'lr', 'betas', 'eps', 'weight_decay'\n",
    "            - For Muon groups: 'lr', 'momentum', 'ns_steps', 'beta2', 'weight_decay'\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups: list[dict]):\n",
    "        super().__init__(param_groups, defaults={})\n",
    "        # 0-D CPU tensors to avoid torch.compile recompilation when values change\n",
    "        # AdamW tensors\n",
    "        self._adamw_step_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_lr_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_beta1_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_beta2_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_eps_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_wd_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        # Muon tensors\n",
    "        self._muon_momentum_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_lr_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_wd_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_beta2_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "    def _step_adamw(self, group: dict) -> None:\n",
    "        \"\"\"\n",
    "        AdamW update for each param in the group individually.\n",
    "        Lazy init the state, fill in all 0-D tensors, call the fused kernel.\n",
    "        \"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            grad = p.grad\n",
    "            state = self.state[p]\n",
    "\n",
    "            # State init\n",
    "            if not state:\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "            exp_avg = state['exp_avg']\n",
    "            exp_avg_sq = state['exp_avg_sq']\n",
    "            state['step'] += 1\n",
    "\n",
    "            # Fill 0-D tensors with current values\n",
    "            self._adamw_step_t.fill_(state['step'])\n",
    "            self._adamw_lr_t.fill_(group['lr'])\n",
    "            self._adamw_beta1_t.fill_(group['betas'][0])\n",
    "            self._adamw_beta2_t.fill_(group['betas'][1])\n",
    "            self._adamw_eps_t.fill_(group['eps'])\n",
    "            self._adamw_wd_t.fill_(group['weight_decay'])\n",
    "\n",
    "            # Fused update: weight_decay -> momentum -> bias_correction -> param_update\n",
    "            adamw_step_fused(\n",
    "                p, grad, exp_avg, exp_avg_sq,\n",
    "                self._adamw_step_t, self._adamw_lr_t, self._adamw_beta1_t,\n",
    "                self._adamw_beta2_t, self._adamw_eps_t, self._adamw_wd_t,\n",
    "            )\n",
    "\n",
    "    def _step_muon(self, group: dict) -> None:\n",
    "        \"\"\"\n",
    "        Muon update for all params in the group (stacked for efficiency).\n",
    "        Lazy init the state, fill in all 0-D tensors, call the fused kernel.\n",
    "        \"\"\"\n",
    "        params: list[Tensor] = group['params']\n",
    "        if not params:\n",
    "            return\n",
    "\n",
    "        # Get or create group-level buffers (stored in first param's state for convenience)\n",
    "        p = params[0]\n",
    "        state = self.state[p]\n",
    "        num_params = len(params)\n",
    "        shape, device, dtype = p.shape, p.device, p.dtype\n",
    "\n",
    "        # Momentum for every individual parameter\n",
    "        if \"momentum_buffer\" not in state:\n",
    "            state[\"momentum_buffer\"] = torch.zeros(num_params, *shape, dtype=dtype, device=device)\n",
    "        momentum_buffer = state[\"momentum_buffer\"]\n",
    "\n",
    "        # Second momentum buffer is factored, either per-row or per-column\n",
    "        if \"second_momentum_buffer\" not in state:\n",
    "            state_shape = (num_params, shape[-2], 1) if shape[-2] >= shape[-1] else (num_params, 1, shape[-1])\n",
    "            state[\"second_momentum_buffer\"] = torch.zeros(state_shape, dtype=dtype, device=device)\n",
    "        second_momentum_buffer = state[\"second_momentum_buffer\"]\n",
    "        red_dim = -1 if shape[-2] >= shape[-1] else -2\n",
    "\n",
    "        # Stack grads and params (NOTE: this assumes all params have the same shape)\n",
    "        stacked_grads = torch.stack([p.grad for p in params])\n",
    "        stacked_params = torch.stack(params)\n",
    "\n",
    "        # Fill all the 0-D tensors with current values\n",
    "        self._muon_momentum_t.fill_(group[\"momentum\"])\n",
    "        self._muon_beta2_t.fill_(group[\"beta2\"] if group[\"beta2\"] is not None else 0.0)\n",
    "        self._muon_lr_t.fill_(group[\"lr\"] * max(1.0, shape[-2] / shape[-1])**0.5)\n",
    "        self._muon_wd_t.fill_(group[\"weight_decay\"])\n",
    "\n",
    "        # Single fused kernel: momentum -> polar_express -> variance_reduction -> update\n",
    "        muon_step_fused(\n",
    "            stacked_grads,\n",
    "            stacked_params,\n",
    "            momentum_buffer,\n",
    "            second_momentum_buffer,\n",
    "            self._muon_momentum_t,\n",
    "            self._muon_lr_t,\n",
    "            self._muon_wd_t,\n",
    "            self._muon_beta2_t,\n",
    "            group[\"ns_steps\"],\n",
    "            red_dim,\n",
    "        )\n",
    "\n",
    "        # Copy back to original params\n",
    "        torch._foreach_copy_(params, list(stacked_params.unbind(0)))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if group['kind'] == 'adamw':\n",
    "                self._step_adamw(group)\n",
    "            elif group['kind'] == 'muon':\n",
    "                self._step_muon(group)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer kind: {group['kind']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Distributed version of the MuonAdamW optimizer.\n",
    "# Used for training on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistMuonAdamW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistMuonAdamW(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Combined distributed optimizer: Muon for 2D matrix params, AdamW for others.\n",
    "\n",
    "    See MuonAdamW for the algorithmic details of each optimizer. This class adds\n",
    "    distributed communication to enable multi-GPU training without PyTorch DDP.\n",
    "\n",
    "    Design Goals:\n",
    "    - Overlap communication with computation (async ops)\n",
    "    - Minimize memory by sharding optimizer states across ranks (ZeRO-2 style)\n",
    "    - Batch small tensors into single comm ops where possible\n",
    "\n",
    "    Communication Pattern (3-phase async):\n",
    "    We use a 3-phase structure to maximize overlap between communication and compute:\n",
    "\n",
    "        Phase 1: Launch all async reduce ops\n",
    "            - Kick off all reduce_scatter/all_reduce operations\n",
    "            - Don't wait - let them run in background while we continue\n",
    "\n",
    "        Phase 2: Wait for reduces, compute updates, launch gathers\n",
    "            - For each group: wait for its reduce, compute the update, launch gather\n",
    "            - By processing groups in order, earlier gathers run while later computes happen\n",
    "\n",
    "        Phase 3: Wait for gathers, copy back\n",
    "            - Wait for all gathers to complete\n",
    "            - Copy updated params back to original tensors (Muon only)\n",
    "\n",
    "    AdamW Communication (ZeRO-2 style):\n",
    "    - Small params (<1024 elements): all_reduce gradients, update full param on each rank.\n",
    "      Optimizer state is replicated but these params are tiny (scalars, biases).\n",
    "    - Large params: reduce_scatter gradients so each rank gets 1/N of the grad, update\n",
    "      only that slice, then all_gather the updated slices. Optimizer state (exp_avg,\n",
    "      exp_avg_sq) is sharded - each rank only stores state for its slice.\n",
    "      Requires param.shape[0] divisible by world_size.\n",
    "\n",
    "    Muon Communication (stacked + chunked):\n",
    "    - All params in a Muon group must have the same shape (caller's responsibility).\n",
    "    - Stack all K params into a single (K, *shape) tensor for efficient comm.\n",
    "    - Divide K params across N ranks: each rank \"owns\" ceil(K/N) params.\n",
    "    - reduce_scatter the stacked grads so each rank gets its chunk.\n",
    "    - Each rank computes Muon update only for params it owns.\n",
    "    - all_gather the updated params back to all ranks.\n",
    "    - Optimizer state (momentum_buffer, second_momentum_buffer) is sharded by chunk.\n",
    "    - Padding: if K doesn't divide evenly, we zero-pad to (ceil(K/N) * N) for comm,\n",
    "      then ignore the padding when copying back.\n",
    "\n",
    "    Buffer Reuse:\n",
    "    - For Muon, we allocate stacked_grads for reduce_scatter input, then reuse the\n",
    "      same buffer as the output for all_gather (stacked_params). This saves memory\n",
    "      since we don't need both buffers simultaneously.\n",
    "\n",
    "    Arguments:\n",
    "        param_groups: List of dicts, each containing:\n",
    "            - 'params': List of parameters\n",
    "            - 'kind': 'adamw' or 'muon'\n",
    "            - For AdamW groups: 'lr', 'betas', 'eps', 'weight_decay'\n",
    "            - For Muon groups: 'lr', 'momentum', 'ns_steps', 'beta2', 'weight_decay'\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups: list[dict]):\n",
    "        super().__init__(param_groups, defaults={})\n",
    "        # 0-D CPU tensors to avoid torch.compile recompilation when values change\n",
    "        self._adamw_step_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_lr_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_beta1_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_beta2_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_eps_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._adamw_wd_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_momentum_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_lr_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_wd_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "        self._muon_beta2_t = torch.tensor(0.0, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "    def _reduce_adamw(self, group: dict, world_size: int) -> dict:\n",
    "        \"\"\"Launch async reduce ops for AdamW group. Returns info dict with per-param infos.\"\"\"\n",
    "        param_infos = {}\n",
    "        for p in group['params']:\n",
    "            grad = p.grad\n",
    "            if p.numel() < 1024:\n",
    "                # Small params: all_reduce (no scatter/gather needed)\n",
    "                future = dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future()\n",
    "                param_infos[p] = dict(future=future, grad_slice=grad, is_small=True)\n",
    "            else:\n",
    "                # Large params: reduce_scatter\n",
    "                assert grad.shape[0] % world_size == 0, f\"AdamW reduce_scatter requires shape[0] ({grad.shape[0]}) divisible by world_size ({world_size})\"\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                future = dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future()\n",
    "                param_infos[p] = dict(future=future, grad_slice=grad_slice, is_small=False)\n",
    "        return dict(param_infos=param_infos)\n",
    "\n",
    "    def _reduce_muon(self, group: dict, world_size: int) -> dict:\n",
    "        \"\"\"Launch async reduce op for Muon group. Returns info dict.\"\"\"\n",
    "        params = group['params']\n",
    "        chunk_size = (len(params) + world_size - 1) // world_size\n",
    "        padded_num_params = chunk_size * world_size\n",
    "        p = params[0]\n",
    "        shape, device, dtype = p.shape, p.device, p.dtype\n",
    "\n",
    "        # Stack grads and zero-pad to padded_num_params\n",
    "        grad_stack = torch.stack([p.grad for p in params])\n",
    "        stacked_grads = torch.empty(padded_num_params, *shape, dtype=dtype, device=device)\n",
    "        stacked_grads[:len(params)].copy_(grad_stack)\n",
    "        if len(params) < padded_num_params:\n",
    "            stacked_grads[len(params):].zero_()\n",
    "\n",
    "        # Reduce_scatter to get this rank's chunk\n",
    "        grad_chunk = torch.empty(chunk_size, *shape, dtype=dtype, device=device)\n",
    "        future = dist.reduce_scatter_tensor(grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True).get_future()\n",
    "\n",
    "        return dict(future=future, grad_chunk=grad_chunk, stacked_grads=stacked_grads, chunk_size=chunk_size)\n",
    "\n",
    "    def _compute_adamw(self, group: dict, info: dict, gather_list: list, rank: int, world_size: int) -> None:\n",
    "        \"\"\"Wait for reduce, compute AdamW updates, launch gathers for large params.\"\"\"\n",
    "        param_infos = info['param_infos']\n",
    "        for p in group['params']:\n",
    "            pinfo = param_infos[p]\n",
    "            pinfo['future'].wait()\n",
    "            grad_slice = pinfo['grad_slice']\n",
    "            state = self.state[p]\n",
    "\n",
    "            # For small params, operate on full param; for large, operate on slice\n",
    "            if pinfo['is_small']:\n",
    "                p_slice = p\n",
    "            else:\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "\n",
    "            # State init\n",
    "            if not state:\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "            state['step'] += 1\n",
    "\n",
    "            # Fill 0-D tensors and run fused kernel\n",
    "            self._adamw_step_t.fill_(state['step'])\n",
    "            self._adamw_lr_t.fill_(group['lr'])\n",
    "            self._adamw_beta1_t.fill_(group['betas'][0])\n",
    "            self._adamw_beta2_t.fill_(group['betas'][1])\n",
    "            self._adamw_eps_t.fill_(group['eps'])\n",
    "            self._adamw_wd_t.fill_(group['weight_decay'])\n",
    "            adamw_step_fused(\n",
    "                p_slice, grad_slice, state['exp_avg'], state['exp_avg_sq'],\n",
    "                self._adamw_step_t, self._adamw_lr_t, self._adamw_beta1_t,\n",
    "                self._adamw_beta2_t, self._adamw_eps_t, self._adamw_wd_t,\n",
    "            )\n",
    "\n",
    "            # Large params need all_gather\n",
    "            if not pinfo['is_small']:\n",
    "                future = dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future()\n",
    "                gather_list.append(dict(future=future, params=None))\n",
    "\n",
    "    def _compute_muon(self, group: dict, info: dict, gather_list: list, rank: int) -> None:\n",
    "        \"\"\"Wait for reduce, compute Muon updates, launch gather.\"\"\"\n",
    "        info['future'].wait()\n",
    "        params = group['params']\n",
    "        chunk_size = info['chunk_size']\n",
    "        grad_chunk = info['grad_chunk']\n",
    "        p = params[0]\n",
    "        shape, device, dtype = p.shape, p.device, p.dtype\n",
    "\n",
    "        # How many params does this rank own?\n",
    "        start_idx = rank * chunk_size\n",
    "        num_owned = min(chunk_size, max(0, len(params) - start_idx))\n",
    "\n",
    "        # Get or create group-level state\n",
    "        state = self.state[p]\n",
    "        if \"momentum_buffer\" not in state:\n",
    "            state[\"momentum_buffer\"] = torch.zeros(chunk_size, *shape, dtype=dtype, device=device)\n",
    "        if \"second_momentum_buffer\" not in state:\n",
    "            state_shape = (chunk_size, shape[-2], 1) if shape[-2] >= shape[-1] else (chunk_size, 1, shape[-1])\n",
    "            state[\"second_momentum_buffer\"] = torch.zeros(state_shape, dtype=dtype, device=device)\n",
    "        red_dim = -1 if shape[-2] >= shape[-1] else -2\n",
    "\n",
    "        # Build output buffer for all_gather\n",
    "        updated_params = torch.empty(chunk_size, *shape, dtype=dtype, device=device)\n",
    "\n",
    "        if num_owned > 0:\n",
    "            owned_params = [params[start_idx + i] for i in range(num_owned)]\n",
    "            stacked_owned = torch.stack(owned_params)\n",
    "\n",
    "            # Fill 0-D tensors and run fused kernel\n",
    "            self._muon_momentum_t.fill_(group[\"momentum\"])\n",
    "            self._muon_beta2_t.fill_(group[\"beta2\"])\n",
    "            self._muon_lr_t.fill_(group[\"lr\"] * max(1.0, shape[-2] / shape[-1])**0.5)\n",
    "            self._muon_wd_t.fill_(group[\"weight_decay\"])\n",
    "            muon_step_fused(\n",
    "                grad_chunk[:num_owned], stacked_owned,\n",
    "                state[\"momentum_buffer\"][:num_owned], state[\"second_momentum_buffer\"][:num_owned],\n",
    "                self._muon_momentum_t, self._muon_lr_t, self._muon_wd_t, self._muon_beta2_t,\n",
    "                group[\"ns_steps\"], red_dim,\n",
    "            )\n",
    "            updated_params[:num_owned].copy_(stacked_owned)\n",
    "\n",
    "        if num_owned < chunk_size:\n",
    "            updated_params[num_owned:].zero_()\n",
    "\n",
    "        # Reuse stacked_grads buffer for all_gather output\n",
    "        stacked_params = info[\"stacked_grads\"]\n",
    "        future = dist.all_gather_into_tensor(stacked_params, updated_params, async_op=True).get_future()\n",
    "        gather_list.append(dict(future=future, stacked_params=stacked_params, params=params))\n",
    "\n",
    "    def _finish_gathers(self, gather_list: list) -> None:\n",
    "        \"\"\"Wait for all gathers and copy Muon params back.\"\"\"\n",
    "        for info in gather_list:\n",
    "            info[\"future\"].wait()\n",
    "            if info[\"params\"] is not None:\n",
    "                # Muon: copy from stacked buffer back to individual params\n",
    "                torch._foreach_copy_(info[\"params\"], list(info[\"stacked_params\"][:len(info[\"params\"])].unbind(0)))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Phase 1: launch all async reduce ops\n",
    "        reduce_infos: list[dict] = []\n",
    "        for group in self.param_groups:\n",
    "            if group['kind'] == 'adamw':\n",
    "                reduce_infos.append(self._reduce_adamw(group, world_size))\n",
    "            elif group['kind'] == 'muon':\n",
    "                reduce_infos.append(self._reduce_muon(group, world_size))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer kind: {group['kind']}\")\n",
    "\n",
    "        # Phase 2: wait for reduces, compute updates, launch gathers\n",
    "        gather_list: list[dict] = []\n",
    "        for group, info in zip(self.param_groups, reduce_infos):\n",
    "            if group['kind'] == 'adamw':\n",
    "                self._compute_adamw(group, info, gather_list, rank, world_size)\n",
    "            elif group['kind'] == 'muon':\n",
    "                self._compute_muon(group, info, gather_list, rank)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer kind: {group['kind']}\")\n",
    "\n",
    "        # Phase 3: wait for gathers, copy back\n",
    "        self._finish_gathers(gather_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/gpt.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT model (rewrite, a lot simpler)\n",
    "Notable features:\n",
    "- rotary embeddings (and no positional embeddings)\n",
    "- QK norm\n",
    "- untied weights for token embedding and lm_head\n",
    "- relu^2 activation in MLP\n",
    "- norm after token embedding\n",
    "- no learnable params in rmsnorm\n",
    "- no bias in linear layers\n",
    "- Group-Query Attention (GQA) support for more efficient inference\n",
    "- Flash Attention 3 integration\n",
    "\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nanochat.common import get_dist_info, print0\n",
    "#from nanochat.optim import MuonAdamW, DistMuonAdamW\n",
    "\n",
    "# Our custom Flash Attention module that automatically uses FA3 on Hopper+ and SDPA fallback elsewhere\n",
    "#from nanochat.flash_attention import flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GPTConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    sequence_len: int = 2048\n",
    "    vocab_size: int = 32768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 6 # number of query heads\n",
    "    n_kv_head: int = 6 # number of key/value heads (GQA)\n",
    "    n_embd: int = 768\n",
    "    # Sliding window attention pattern string, tiled across layers. Final layer always L.\n",
    "    # Characters: L=long (full context), S=short (half context)\n",
    "    # Examples: \"L\"=all full context, \"SL\"=alternating, \"SSL\"=two short then one long\n",
    "    window_pattern: str = \"SSSL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # Purely functional rmsnorm with no learnable params\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`has_ve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_ve(layer_idx, n_layer):\n",
    "    \"\"\"Returns True if GPT layer should have Value Embedding (alternating, last layer always included).\"\"\"\n",
    "    return layer_idx % 2 == (n_layer - 1) % 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_rotary_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, cos, sin):\n",
    "    assert x.ndim == 4  # multihead attention\n",
    "    d = x.shape[3] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:] # split up last dim into two halves\n",
    "    y1 = x1 * cos + x2 * sin # rotate pairs of dims\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat([y1, y2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CausalSelfAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.n_head = config.n_head\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.ve_gate_channels = 32\n",
    "        self.ve_gate = nn.Linear(self.ve_gate_channels, self.n_kv_head, bias=False) if has_ve(layer_idx, config.n_layer) else None\n",
    "\n",
    "    def forward(self, x, ve, cos_sin, window_size, kv_cache):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Project the input to get queries, keys, and values\n",
    "        # Shape: (B, T, H, D) - FA3's native layout, no transpose needed!\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        # Value residual (ResFormer): mix in value embedding with input-dependent gate per head\n",
    "        if ve is not None:\n",
    "            ve = ve.view(B, T, self.n_kv_head, self.head_dim)\n",
    "            gate = 2 * torch.sigmoid(self.ve_gate(x[..., :self.ve_gate_channels]))  # (B, T, n_kv_head), range (0, 2)\n",
    "            v = v + gate.unsqueeze(-1) * ve\n",
    "\n",
    "        # Apply Rotary Embeddings to queries and keys to get relative positional encoding\n",
    "        cos, sin = cos_sin\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n",
    "        q, k = norm(q), norm(k) # QK norm\n",
    "\n",
    "        # Flash Attention (FA3 on Hopper+, PyTorch SDPA fallback elsewhere)\n",
    "        # window_size is (left, right) tuple: (N, 0) for causal, (-1, 0) for full context\n",
    "        if kv_cache is None:\n",
    "            # Training: causal attention with optional sliding window\n",
    "            y = flash_attn.flash_attn_func(q, k, v, causal=True, window_size=window_size)\n",
    "        else:\n",
    "            # Inference: use flash_attn_with_kvcache which handles cache management\n",
    "            k_cache, v_cache = kv_cache.get_layer_cache(self.layer_idx)\n",
    "            y = flash_attn.flash_attn_with_kvcache(\n",
    "                q, k_cache, v_cache,\n",
    "                k=k, v=v,\n",
    "                cache_seqlens=kv_cache.cache_seqlens,\n",
    "                causal=True,\n",
    "                window_size=window_size,\n",
    "            )\n",
    "            # Advance position after last layer processes\n",
    "            if self.layer_idx == kv_cache.n_layers - 1:\n",
    "                kv_cache.advance(T)\n",
    "\n",
    "        # Re-assemble the heads and project back to residual stream\n",
    "        y = y.contiguous().view(B, T, -1)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MLP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square()\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, ve, cos_sin, window_size, kv_cache):\n",
    "        x = x + self.attn(norm(x), ve, cos_sin, window_size, kv_cache)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config, pad_vocab_size_to=64):\n",
    "        \"\"\"\n",
    "        NOTE a major footgun: this __init__ function runs in meta device context (!!)\n",
    "        Therefore, any calculations inside here are shapes and dtypes only, no actual data.\n",
    "        => We actually initialize all data (parameters, buffers, etc.) in init_weights() instead.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Compute per-layer window sizes for sliding window attention\n",
    "        # window_size is (left, right) tuple: (-1, 0) for full context, (N, 0) for sliding window\n",
    "        self.window_sizes = self._compute_window_sizes(config)\n",
    "        # Pad vocab for efficiency (DDP, tensor cores). This is just an optimization - outputs are cropped in forward().\n",
    "        # https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings\n",
    "        padded_vocab_size = ((config.vocab_size + pad_vocab_size_to - 1) // pad_vocab_size_to) * pad_vocab_size_to\n",
    "        if padded_vocab_size != config.vocab_size:\n",
    "            print0(f\"Padding vocab_size from {config.vocab_size} to {padded_vocab_size} for efficiency\")\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"wte\": nn.Embedding(padded_vocab_size, config.n_embd),\n",
    "            \"h\": nn.ModuleList([Block(config, layer_idx) for layer_idx in range(config.n_layer)]),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, padded_vocab_size, bias=False)\n",
    "        # Per-layer learnable scalars (inspired by modded-nanogpt)\n",
    "        # resid_lambdas: scales the residual stream at each layer (init 1.0 = neutral)\n",
    "        # x0_lambdas: blends initial embedding back in at each layer (init 0.0 = disabled)\n",
    "        # Separate parameters so they can have different optimizer treatment\n",
    "        self.resid_lambdas = nn.Parameter(torch.ones(config.n_layer))   # fake init, real init in init_weights()\n",
    "        self.x0_lambdas = nn.Parameter(torch.zeros(config.n_layer))     # fake init, real init in init_weights()\n",
    "        # Value embeddings (ResFormer-style): alternating layers, last layer always included\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        kv_dim = config.n_kv_head * head_dim\n",
    "        self.value_embeds = nn.ModuleDict({str(i): nn.Embedding(padded_vocab_size, kv_dim) for i in range(config.n_layer) if has_ve(i, config.n_layer)})\n",
    "        # To support meta device initialization, we init the rotary embeddings here, but it's just \"fake\" meta tensors only.\n",
    "        # As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,\n",
    "        # so let's just over-compute them by 10X, but assert fail if we ever reach that amount.\n",
    "        # In the future we can dynamically grow the cache, for now it's fine.\n",
    "        self.rotary_seq_len = config.sequence_len * 10 # 10X over-compute should be enough, TODO make nicer?\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False) # persistent=False means it's not saved to the checkpoint\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the full model in this one function for maximum clarity.\n",
    "\n",
    "        wte (embedding):     normal, std=1.0\n",
    "        lm_head:             normal, std=0.001\n",
    "        for each block:\n",
    "            attn.c_q:        uniform, std=1/sqrt(n_embd)\n",
    "            attn.c_k:        uniform, std=1/sqrt(n_embd)\n",
    "            attn.c_v:        uniform, std=1/sqrt(n_embd)\n",
    "            attn.c_proj:     zeros\n",
    "            mlp.c_fc:        uniform, std=1/sqrt(n_embd)\n",
    "            mlp.c_proj:      zeros\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding and unembedding\n",
    "        torch.nn.init.normal_(self.transformer.wte.weight, mean=0.0, std=1.0)\n",
    "        torch.nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.001)\n",
    "\n",
    "        # Transformer blocks: uniform init with bound = sqrt(3) * std (same standard deviation as normal)\n",
    "        n_embd = self.config.n_embd\n",
    "        s = 3**0.5 * n_embd**-0.5 # sqrt(3) multiplier makes sure Uniform achieves the same std as Normal\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.uniform_(block.attn.c_q.weight, -s, s) # weights use Uniform to avoid outliers\n",
    "            torch.nn.init.uniform_(block.attn.c_k.weight, -s, s)\n",
    "            torch.nn.init.uniform_(block.attn.c_v.weight, -s, s)\n",
    "            torch.nn.init.zeros_(block.attn.c_proj.weight) # projections are zero\n",
    "            torch.nn.init.uniform_(block.mlp.c_fc.weight, -s, s)\n",
    "            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n",
    "\n",
    "        # Per-layer scalars\n",
    "        self.resid_lambdas.fill_(1.0)   # 1.0 => typical residual connections at init\n",
    "        self.x0_lambdas.fill_(0.1)      # 0.1 => small initial weight for skip connection to input embedding\n",
    "\n",
    "        # Value embeddings (init like c_v: uniform with same std)\n",
    "        for ve in self.value_embeds.values():\n",
    "            torch.nn.init.uniform_(ve.weight, -s, s)\n",
    "\n",
    "        # Gate weights init to zero so gates start at sigmoid(0) = 0.5, scaled by 2 -> 1.0 (neutral)\n",
    "        for block in self.transformer.h:\n",
    "            if block.attn.ve_gate is not None:\n",
    "                torch.nn.init.zeros_(block.attn.ve_gate.weight)\n",
    "\n",
    "        # Rotary embeddings\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        # Cast embeddings to bf16: optimizer can tolerate it and it saves memory\n",
    "        if self.transformer.wte.weight.device.type == \"cuda\":\n",
    "            self.transformer.wte.to(dtype=torch.bfloat16)\n",
    "            for ve in self.value_embeds.values():\n",
    "                ve.to(dtype=torch.bfloat16)\n",
    "\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        # TODO: bump base theta more? e.g. 100K is more common more recently\n",
    "        # autodetect the device from model embeddings\n",
    "        if device is None:\n",
    "            device = self.transformer.wte.weight.device\n",
    "        # stride the channels\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        # stride the time steps\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        # calculate the rotation frequencies at each (time, channel) pair\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16() # keep them in bfloat16\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :] # add batch and head dims for later broadcasting\n",
    "        return cos, sin\n",
    "\n",
    "    def _compute_window_sizes(self, config):\n",
    "        \"\"\"\n",
    "        Compute per-layer window sizes for sliding window attention.\n",
    "\n",
    "        Returns list of (left, right) tuples for FA3's window_size parameter:\n",
    "        - left: how many tokens before current position to attend to (-1 = unlimited)\n",
    "        - right: how many tokens after current position to attend to (0 for causal)\n",
    "\n",
    "        Pattern string is tiled across layers. Final layer always gets L (full context).\n",
    "        Characters: L=long (full context), S=short (half context)\n",
    "        \"\"\"\n",
    "        pattern = config.window_pattern.upper()\n",
    "        assert all(c in \"SL\" for c in pattern), f\"Invalid window_pattern: {pattern}. Use only S and L.\"\n",
    "        # Map characters to window sizes\n",
    "        long_window = config.sequence_len\n",
    "        short_window = long_window // 2\n",
    "        char_to_window = {\n",
    "            \"L\": (long_window, 0),\n",
    "            \"S\": (short_window, 0),\n",
    "        }\n",
    "        # Tile pattern across layers\n",
    "        window_sizes = []\n",
    "        for layer_idx in range(config.n_layer):\n",
    "            char = pattern[layer_idx % len(pattern)]\n",
    "            window_sizes.append(char_to_window[char])\n",
    "        # Final layer always gets full context\n",
    "        window_sizes[-1] = (long_window, 0)\n",
    "        return window_sizes\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.transformer.wte.weight.device\n",
    "\n",
    "    def estimate_flops(self):\n",
    "        \"\"\"\n",
    "        Return the estimated FLOPs per token for the model (forward + backward).\n",
    "        Each matmul weight parameter contributes 2 FLOPs (multiply *, accumulate +) in forward, and 2X that in backward => 2+4=6.\n",
    "        Cleanest explanation of this: https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4\n",
    "        On top of that, 12 * h * q * effective_seq_len accounts for key @ query matmul flops inside attention.\n",
    "        With sliding windows, effective_seq_len varies per layer (capped by window size).\n",
    "        Ref: https://arxiv.org/abs/2204.02311 (PaLM paper).\n",
    "        This is ~1% off from the exact formulas of Chinchilla paper, the difference is:\n",
    "        - Chinchilla counts the embedding layer as flops (? weird, it's just a lookup => we ignore)\n",
    "        - Chinchilla counts exp/sum/divide in attention softmax as flops (a little sus and very tiny => we ignore)\n",
    "        \"\"\"\n",
    "        nparams = sum(p.numel() for p in self.parameters())\n",
    "        # Exclude non-matmul params: embeddings and per-layer scalars\n",
    "        value_embeds_numel = sum(ve.weight.numel() for ve in self.value_embeds.values())\n",
    "        nparams_exclude = (self.transformer.wte.weight.numel() + value_embeds_numel +\n",
    "                          self.resid_lambdas.numel() + self.x0_lambdas.numel())\n",
    "        h, q, t = self.config.n_head, self.config.n_embd // self.config.n_head, self.config.sequence_len\n",
    "        # Sum attention FLOPs per layer, accounting for sliding window\n",
    "        attn_flops = 0\n",
    "        for window_size in self.window_sizes:\n",
    "            window = window_size[0]  # (left, right) tuple, we use left\n",
    "            effective_seq = t if window < 0 else min(window, t)\n",
    "            attn_flops += 12 * h * q * effective_seq\n",
    "        num_flops_per_token = 6 * (nparams - nparams_exclude) + attn_flops\n",
    "        return num_flops_per_token\n",
    "\n",
    "    def num_scaling_params(self):\n",
    "        \"\"\"\n",
    "        Return detailed parameter counts for scaling law analysis.\n",
    "        Different papers use different conventions:\n",
    "        - Kaplan et al. excluded embedding parameters\n",
    "        - Chinchilla included all parameters\n",
    "        Ref: https://arxiv.org/abs/2203.15556 (Chinchilla paper)\n",
    "        Ref: https://arxiv.org/abs/2001.08361 (Kaplan et al. original scaling laws paper)\n",
    "\n",
    "        Returns a dict with counts for each parameter group, so downstream analysis\n",
    "        can experiment with which combination gives the cleanest scaling laws.\n",
    "        \"\"\"\n",
    "        # Count each group separately (mirrors the grouping in setup_optimizers)\n",
    "        wte = sum(p.numel() for p in self.transformer.wte.parameters())\n",
    "        value_embeds = sum(p.numel() for p in self.value_embeds.parameters())\n",
    "        lm_head = sum(p.numel() for p in self.lm_head.parameters())\n",
    "        transformer_matrices = sum(p.numel() for p in self.transformer.h.parameters())\n",
    "        scalars = self.resid_lambdas.numel() + self.x0_lambdas.numel()\n",
    "        total = wte + value_embeds + lm_head + transformer_matrices + scalars\n",
    "        assert total == sum(p.numel() for p in self.parameters()), \"Parameter count mismatch\"\n",
    "        return {\n",
    "            'wte': wte,\n",
    "            'value_embeds': value_embeds,\n",
    "            'lm_head': lm_head,\n",
    "            'transformer_matrices': transformer_matrices,\n",
    "            'scalars': scalars,\n",
    "            'total': total,\n",
    "        }\n",
    "\n",
    "    def setup_optimizer(self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0, adam_betas=(0.8, 0.95), scalar_lr=0.5):\n",
    "        model_dim = self.config.n_embd\n",
    "        ddp, rank, local_rank, world_size = get_dist_info()\n",
    "\n",
    "        # Separate out all parameters into groups\n",
    "        matrix_params = list(self.transformer.h.parameters())\n",
    "        value_embeds_params = list(self.value_embeds.parameters())\n",
    "        embedding_params = list(self.transformer.wte.parameters())\n",
    "        lm_head_params = list(self.lm_head.parameters())\n",
    "        resid_params = [self.resid_lambdas]\n",
    "        x0_params = [self.x0_lambdas]\n",
    "        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params) + len(value_embeds_params) + len(resid_params) + len(x0_params)\n",
    "\n",
    "        # Scale the LR for the AdamW parameters by âˆ1/âˆšdmodel (tuned for 768 dim model)\n",
    "        dmodel_lr_scale = (model_dim / 768) ** -0.5\n",
    "        print0(f\"Scaling the LR for the AdamW parameters âˆ1/âˆš({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n",
    "\n",
    "        # Build param_groups with all required fields explicit\n",
    "        param_groups = [\n",
    "            # AdamW groups (embeddings, lm_head, scalars)\n",
    "            dict(kind='adamw', params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale, betas=adam_betas, eps=1e-10, weight_decay=0.0),\n",
    "            dict(kind='adamw', params=embedding_params, lr=embedding_lr * dmodel_lr_scale, betas=adam_betas, eps=1e-10, weight_decay=0.0),\n",
    "            dict(kind='adamw', params=value_embeds_params, lr=embedding_lr * dmodel_lr_scale, betas=adam_betas, eps=1e-10, weight_decay=0.0),\n",
    "            dict(kind='adamw', params=resid_params, lr=scalar_lr * 0.01, betas=adam_betas, eps=1e-10, weight_decay=0.0),\n",
    "            dict(kind='adamw', params=x0_params, lr=scalar_lr, betas=(0.96, 0.95), eps=1e-10, weight_decay=0.0),  # higher beta1 for x0\n",
    "        ]\n",
    "        # Muon groups (matrix params, grouped by shape for stacking)\n",
    "        for shape in sorted({p.shape for p in matrix_params}):\n",
    "            group_params = [p for p in matrix_params if p.shape == shape]\n",
    "            param_groups.append(dict(\n",
    "                kind='muon', params=group_params, lr=matrix_lr,\n",
    "                momentum=0.95, ns_steps=5, beta2=0.95, weight_decay=weight_decay,\n",
    "            ))\n",
    "\n",
    "        Factory = DistMuonAdamW if ddp else MuonAdamW\n",
    "        optimizer = Factory(param_groups)\n",
    "        for group in optimizer.param_groups:\n",
    "            group[\"initial_lr\"] = group[\"lr\"]\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        # Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim/2))\n",
    "        assert T <= self.cos.size(1), f\"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}\"\n",
    "        assert idx.device == self.cos.device, f\"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}\"\n",
    "        assert self.cos.dtype == torch.bfloat16, \"Rotary embeddings must be in bfloat16\"\n",
    "        # if kv cache exists, we need to offset the rotary embeddings to the current position in the cache\n",
    "        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "        cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T] # truncate cache to current sequence length\n",
    "\n",
    "        # Forward the trunk of the Transformer\n",
    "        x = self.transformer.wte(idx) # embed current token\n",
    "        x = norm(x)\n",
    "        x0 = x  # save initial normalized embedding for x0 residual\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            x = self.resid_lambdas[i] * x + self.x0_lambdas[i] * x0\n",
    "            ve = self.value_embeds[str(i)](idx) if str(i) in self.value_embeds else None\n",
    "            x = block(x, ve, cos_sin, self.window_sizes[i], kv_cache)\n",
    "        x = norm(x)\n",
    "\n",
    "        # Forward the lm_head (compute logits)\n",
    "        softcap = 15 # smoothly cap the logits to the range [-softcap, softcap]\n",
    "        logits = self.lm_head(x) # (B, T, padded_vocab_size) <- very big tensor, large amount of memory\n",
    "        logits = logits[..., :self.config.vocab_size] # slice to remove padding\n",
    "        logits = logits.float() # switch to fp32 for logit softcap and loss computation\n",
    "        logits = softcap * torch.tanh(logits / softcap) # squash the logits\n",
    "\n",
    "        if targets is not None:\n",
    "            # training: given the targets, compute and return the loss\n",
    "            # TODO experiment with chunked cross-entropy?\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n",
    "            return loss\n",
    "        else:\n",
    "            # inference: just return the logits directly\n",
    "            return logits\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"\n",
    "        Naive autoregressive streaming inference.\n",
    "        To make it super simple, let's assume:\n",
    "        - batch size is 1\n",
    "        - ids and the yielded tokens are simple Python lists and ints\n",
    "        \"\"\"\n",
    "        assert isinstance(tokens, list)\n",
    "        device = self.get_device()\n",
    "        rng = None\n",
    "        if temperature > 0:\n",
    "            rng = torch.Generator(device=device)\n",
    "            rng.manual_seed(seed)\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device) # add batch dim\n",
    "        for _ in range(max_tokens):\n",
    "            logits = self.forward(ids) # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            if temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "            else:\n",
    "                next_ids = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            ids = torch.cat((ids, next_ids), dim=1)\n",
    "            token = next_ids.item()\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`checkpoint_manager.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for saving and loading model/optim/state checkpoints.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from nanochat.common import get_base_dir\n",
    "#from nanochat.gpt import GPT, GPTConfig\n",
    "#from nanochat.tokenizer import get_tokenizer\n",
    "from nanochat.common import setup_default_logging\n",
    "\n",
    "# Set up logging\n",
    "setup_default_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "def log0(message):\n",
    "    if int(os.environ.get('RANK', 0)) == 0:\n",
    "        logger.info(message)\n",
    "\n",
    "def _patch_missing_config_keys(model_config_kwargs):\n",
    "    \"\"\"Add default values for new config keys missing in old checkpoints.\"\"\"\n",
    "    # Old models were trained with full context (no sliding window)\n",
    "    if \"window_pattern\" not in model_config_kwargs:\n",
    "        model_config_kwargs[\"window_pattern\"] = \"L\"\n",
    "        log0(f\"Patching missing window_pattern in model config to 'L'\")\n",
    "\n",
    "def _patch_missing_keys(model_data, model_config):\n",
    "    \"\"\"Add default values for new parameters that may be missing in old checkpoints.\"\"\"\n",
    "    n_layer = model_config.n_layer\n",
    "    # resid_lambdas defaults to 1.0 (identity scaling)\n",
    "    if \"resid_lambdas\" not in model_data:\n",
    "        model_data[\"resid_lambdas\"] = torch.ones(n_layer)\n",
    "        log0(f\"Patching missing resid_lambdas in model data to 1.0\")\n",
    "    # x0_lambdas defaults to 0.0 (disabled)\n",
    "    if \"x0_lambdas\" not in model_data:\n",
    "        model_data[\"x0_lambdas\"] = torch.zeros(n_layer)\n",
    "        log0(f\"Patching missing x0_lambdas in model data to 0.0\")\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, step, model_data, optimizer_data, meta_data, rank=0):\n",
    "    if rank == 0:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        # Save the model state parameters\n",
    "        model_path = os.path.join(checkpoint_dir, f\"model_{step:06d}.pt\")\n",
    "        torch.save(model_data, model_path)\n",
    "        logger.info(f\"Saved model parameters to: {model_path}\")\n",
    "        # Save the metadata dict as json\n",
    "        meta_path = os.path.join(checkpoint_dir, f\"meta_{step:06d}.json\")\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta_data, f, indent=2)\n",
    "        logger.info(f\"Saved metadata to: {meta_path}\")\n",
    "    # Note that optimizer state is sharded across ranks, so each rank must save its own.\n",
    "    if optimizer_data is not None:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        optimizer_path = os.path.join(checkpoint_dir, f\"optim_{step:06d}_rank{rank:d}.pt\")\n",
    "        torch.save(optimizer_data, optimizer_path)\n",
    "        logger.info(f\"Saved optimizer state to: {optimizer_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, step, device, load_optimizer=False, rank=0):\n",
    "    # Load the model state\n",
    "    model_path = os.path.join(checkpoint_dir, f\"model_{step:06d}.pt\")\n",
    "    model_data = torch.load(model_path, map_location=device)\n",
    "    # Load the optimizer state if requested\n",
    "    optimizer_data = None\n",
    "    if load_optimizer:\n",
    "        optimizer_path = os.path.join(checkpoint_dir, f\"optim_{step:06d}_rank{rank:d}.pt\")\n",
    "        optimizer_data = torch.load(optimizer_path, map_location=device)\n",
    "    # Load the metadata\n",
    "    meta_path = os.path.join(checkpoint_dir, f\"meta_{step:06d}.json\")\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta_data = json.load(f)\n",
    "    return model_data, optimizer_data, meta_data\n",
    "\n",
    "\n",
    "def build_model(checkpoint_dir, step, device, phase):\n",
    "    \"\"\"\n",
    "    A bunch of repetitive code to build a model from a given checkpoint.\n",
    "    Returns:\n",
    "    - base model - uncompiled, not wrapped in DDP\n",
    "    - tokenizer\n",
    "    - meta data saved during base model training\n",
    "    \"\"\"\n",
    "    assert phase in [\"train\", \"eval\"], f\"Invalid phase: {phase}\"\n",
    "    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, step, device, load_optimizer=False)\n",
    "    if device.type in {\"cpu\", \"mps\"}:\n",
    "        # Convert bfloat16 tensors to float for CPU inference\n",
    "        model_data = {\n",
    "            k: v.float() if v.dtype == torch.bfloat16 else v\n",
    "            for k, v in model_data.items()\n",
    "        }\n",
    "    # Hack: fix torch compile issue, which prepends all keys with _orig_mod.\n",
    "    model_data = {k.removeprefix(\"_orig_mod.\"): v for k, v in model_data.items()}\n",
    "    model_config_kwargs = meta_data[\"model_config\"]\n",
    "    _patch_missing_config_keys(model_config_kwargs)\n",
    "    log0(f\"Building model with config: {model_config_kwargs}\")\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    _patch_missing_keys(model_data, model_config)\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPT(model_config)\n",
    "    # Load the model state\n",
    "    model.to_empty(device=device)\n",
    "    model.init_weights() # note: this is dumb, but we need to init the rotary embeddings. TODO: fix model re-init\n",
    "    model.load_state_dict(model_data, strict=True, assign=True)\n",
    "    # Put the model in the right training phase / mode\n",
    "    if phase == \"eval\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    # Load the Tokenizer\n",
    "    tokenizer = get_tokenizer()\n",
    "    # Sanity check: compatibility between model and tokenizer\n",
    "    assert tokenizer.get_vocab_size() == model_config_kwargs[\"vocab_size\"], f\"Tokenizer vocab size {tokenizer.get_vocab_size()} does not match model config vocab size {model_config_kwargs['vocab_size']}\"\n",
    "    return model, tokenizer, meta_data\n",
    "\n",
    "\n",
    "def find_largest_model(checkpoints_dir):\n",
    "    # attempt to guess the model tag: take the biggest model available\n",
    "    model_tags = [f for f in os.listdir(checkpoints_dir) if os.path.isdir(os.path.join(checkpoints_dir, f))]\n",
    "    if not model_tags:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {checkpoints_dir}\")\n",
    "    # 1) normally all model tags are of the form d<number>, try that first:\n",
    "    candidates = []\n",
    "    for model_tag in model_tags:\n",
    "        match = re.match(r\"d(\\d+)\", model_tag)\n",
    "        if match:\n",
    "            model_depth = int(match.group(1))\n",
    "            candidates.append((model_depth, model_tag))\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        return candidates[0][1]\n",
    "    # 2) if that failed, take the most recently updated model:\n",
    "    model_tags.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoints_dir, x)), reverse=True)\n",
    "    return model_tags[0]\n",
    "\n",
    "\n",
    "def find_last_step(checkpoint_dir):\n",
    "    # Look into checkpoint_dir and find model_<step>.pt with the highest step\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"model_*.pt\"))\n",
    "    if not checkpoint_files:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "    last_step = int(max(os.path.basename(f).split(\"_\")[-1].split(\".\")[0] for f in checkpoint_files))\n",
    "    return last_step\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# convenience functions that take into account nanochat's directory structure\n",
    "\n",
    "def load_model_from_dir(checkpoints_dir, device, phase, model_tag=None, step=None):\n",
    "    if model_tag is None:\n",
    "        # guess the model tag by defaulting to the largest model\n",
    "        model_tag = find_largest_model(checkpoints_dir)\n",
    "        log0(f\"No model tag provided, guessing model tag: {model_tag}\")\n",
    "    checkpoint_dir = os.path.join(checkpoints_dir, model_tag)\n",
    "    if step is None:\n",
    "        # guess the step by defaulting to the last step\n",
    "        step = find_last_step(checkpoint_dir)\n",
    "    assert step is not None, f\"No checkpoints found in {checkpoint_dir}\"\n",
    "    # build the model\n",
    "    log0(f\"Loading model from {checkpoint_dir} with step {step}\")\n",
    "    model, tokenizer, meta_data = build_model(checkpoint_dir, step, device, phase)\n",
    "    return model, tokenizer, meta_data\n",
    "\n",
    "def load_model(source, *args, **kwargs):\n",
    "    model_dir = {\n",
    "        \"base\": \"base_checkpoints\",\n",
    "        \"sft\": \"chatsft_checkpoints\",\n",
    "        \"rl\": \"chatrl_checkpoints\",\n",
    "    }[source]\n",
    "    base_dir = get_base_dir()\n",
    "    checkpoints_dir = os.path.join(base_dir, model_dir)\n",
    "    return load_model_from_dir(checkpoints_dir, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nanochat/engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Engine for efficient inference of our models.\n",
    "\n",
    "Everything works around token sequences:\n",
    "- The user can send token sequences to the engine\n",
    "- The engine returns the next token\n",
    "\n",
    "Notes:\n",
    "- The engine knows nothing about tokenization, it's purely token id sequences.\n",
    "\n",
    "The whole thing is made as efficient as possible.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import signal\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from collections import deque\n",
    "from nanochat.common import compute_init, autodetect_device_type\n",
    "#from nanochat.checkpoint_manager import load_model\n",
    "from contextlib import nullcontext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Calculator tool helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timeout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timeout(duration, formula):\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise Exception(f\"'{formula}': timed out after {duration} seconds\")\n",
    "\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(duration)\n",
    "    yield\n",
    "    signal.alarm(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_with_timeout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_timeout(formula, max_time=3):\n",
    "    try:\n",
    "        with timeout(max_time, formula):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", SyntaxWarning)\n",
    "                return eval(formula, {\"__builtins__\": {}}, {})\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        # print(f\"Warning: Failed to eval {formula}, exception: {e}\") # it's ok ignore wrong calculator usage\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`use_calculator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_calculator(expr):\n",
    "    \"\"\"\n",
    "    Evaluate a Python expression safely.\n",
    "    Supports both math expressions and string operations like .count()\n",
    "    \"\"\"\n",
    "    # Remove commas from numbers\n",
    "    expr = expr.replace(\",\", \"\")\n",
    "\n",
    "    # Check if it's a pure math expression (old behavior)\n",
    "    if all([x in \"0123456789*+-/.() \" for x in expr]):\n",
    "        if \"**\" in expr:  # disallow power operator\n",
    "            return None\n",
    "        return eval_with_timeout(expr)\n",
    "\n",
    "    # Check if it's a string operation we support\n",
    "    # Allow: strings (single/double quotes), .count(), letters, numbers, spaces, parens\n",
    "    allowed_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\\\"()._ \"\n",
    "    if not all([x in allowed_chars for x in expr]):\n",
    "        return None\n",
    "\n",
    "    # Disallow dangerous patterns\n",
    "    dangerous_patterns = ['__', 'import', 'exec', 'eval', 'compile', 'open', 'file',\n",
    "                         'input', 'raw_input', 'globals', 'locals', 'vars', 'dir',\n",
    "                         'getattr', 'setattr', 'delattr', 'hasattr']\n",
    "    expr_lower = expr.lower()\n",
    "    if any(pattern in expr_lower for pattern in dangerous_patterns):\n",
    "        return None\n",
    "\n",
    "    # Only allow .count() method for now (can expand later)\n",
    "    if '.count(' not in expr:\n",
    "        return None\n",
    "\n",
    "    # Evaluate with timeout\n",
    "    return eval_with_timeout(expr)\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KVCache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    KV Cache designed for Flash Attention 3's flash_attn_with_kvcache API.\n",
    "\n",
    "    Key differences from FA2-style cache:\n",
    "    - Tensors are (B, T, H, D) not (B, H, T, D)\n",
    "    - FA3 updates the cache in-place during flash_attn_with_kvcache\n",
    "    - Position tracked per batch element via cache_seqlens tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, num_heads, seq_len, head_dim, num_layers, device, dtype):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = seq_len\n",
    "        self.n_layers = num_layers\n",
    "        self.n_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        # Pre-allocate cache tensors: (n_layers, B, T, H, D)\n",
    "        self.k_cache = torch.zeros(num_layers, batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)\n",
    "        self.v_cache = torch.zeros(num_layers, batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)\n",
    "        # Current sequence length per batch element (FA3 needs int32)\n",
    "        self.cache_seqlens = torch.zeros(batch_size, dtype=torch.int32, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset cache to empty state.\"\"\"\n",
    "        self.cache_seqlens.zero_()\n",
    "\n",
    "    def get_pos(self):\n",
    "        \"\"\"Get current position (assumes all batch elements at same position).\"\"\"\n",
    "        return self.cache_seqlens[0].item()\n",
    "\n",
    "    def get_layer_cache(self, layer_idx):\n",
    "        \"\"\"Return (k_cache, v_cache) views for a specific layer.\"\"\"\n",
    "        return self.k_cache[layer_idx], self.v_cache[layer_idx]\n",
    "\n",
    "    def advance(self, num_tokens):\n",
    "        \"\"\"Advance the cache position by num_tokens.\"\"\"\n",
    "        self.cache_seqlens += num_tokens\n",
    "\n",
    "    def prefill(self, other):\n",
    "        \"\"\"\n",
    "        Copy cached KV from another cache into this one.\n",
    "        Used when we do batch=1 prefill and then want to generate multiple samples in parallel.\n",
    "        \"\"\"\n",
    "        assert self.get_pos() == 0, \"Cannot prefill a non-empty KV cache\"\n",
    "        assert self.n_layers == other.n_layers and self.n_heads == other.n_heads and self.head_dim == other.head_dim\n",
    "        assert self.max_seq_len >= other.max_seq_len\n",
    "        other_pos = other.get_pos()\n",
    "        self.k_cache[:, :, :other_pos, :, :] = other.k_cache[:, :, :other_pos, :, :]\n",
    "        self.v_cache[:, :, :other_pos, :, :] = other.v_cache[:, :, :other_pos, :, :]\n",
    "        self.cache_seqlens.fill_(other_pos)\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample_next_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_next_token(logits, rng, temperature=1.0, top_k=None):\n",
    "    \"\"\"Sample a single next token from given logits of shape (B, vocab_size). Returns (B, 1).\"\"\"\n",
    "    assert temperature >= 0.0, \"temperature must be non-negative\"\n",
    "    if temperature == 0.0:\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    if top_k is not None and top_k > 0:\n",
    "        k = min(top_k, logits.size(-1))\n",
    "        vals, idx = torch.topk(logits, k, dim=-1)\n",
    "        vals = vals / temperature\n",
    "        probs = F.softmax(vals, dim=-1)\n",
    "        choice = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "        return idx.gather(1, choice)\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RowState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowState:\n",
    "    # Per-row state tracking during generation\n",
    "    def __init__(self, current_tokens=None):\n",
    "        self.current_tokens = current_tokens or [] # Current token sequence for this row\n",
    "        self.forced_tokens = deque() # Queue of tokens to force inject\n",
    "        self.in_python_block = False # Whether we are inside a python block\n",
    "        self.python_expr_tokens = [] # Tokens of the current python expression\n",
    "        self.completed = False # Whether this row has completed generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer # needed for tool use\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokens, num_samples=1, max_tokens=None, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"Same as generate, but does single prefill and then clones the KV cache.\"\"\"\n",
    "        assert isinstance(tokens, list) and isinstance(tokens[0], int), \"expecting list of ints\"\n",
    "        device = self.model.get_device()\n",
    "        # NOTE: setting the dtype here and in this way is an ugly hack.\n",
    "        # Currently the repo assumes that cuda -> bfloat16 and everything else -> float32.\n",
    "        # We need to know the dtype here to call __init__ on KVCache and pre-allocate its tensors.\n",
    "        # As a quick hack, we're making generate() function inherit and know about this repo-wise assumption.\n",
    "        # I think there has to be a bigger refactor to deal with device/dtype tracking across the codebase.\n",
    "        # In particular, the KVCache should allocate its tensors lazily\n",
    "        dtype = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
    "        rng = torch.Generator(device=device)\n",
    "        rng.manual_seed(seed)\n",
    "\n",
    "        # Get the special tokens we need to coordinate the tool use state machine\n",
    "        get_special = lambda s: self.tokenizer.encode_special(s)\n",
    "        python_start = get_special(\"<|python_start|>\")\n",
    "        python_end = get_special(\"<|python_end|>\")\n",
    "        output_start = get_special(\"<|output_start|>\")\n",
    "        output_end = get_special(\"<|output_end|>\")\n",
    "        assistant_end = get_special(\"<|assistant_end|>\") # if sampled, ends row\n",
    "        bos = self.tokenizer.get_bos_token_id() # if sampled, ends row\n",
    "\n",
    "        # 1) Run a batch 1 prefill of the prompt tokens\n",
    "        m = self.model.config\n",
    "        kv_model_kwargs = {\"num_heads\": m.n_kv_head, \"head_dim\": m.n_embd // m.n_head, \"num_layers\": m.n_layer}\n",
    "        kv_cache_prefill = KVCache(\n",
    "            batch_size=1,\n",
    "            seq_len=len(tokens),\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            **kv_model_kwargs,\n",
    "        )\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "        logits = self.model.forward(ids, kv_cache=kv_cache_prefill)\n",
    "        logits = logits[:, -1, :].expand(num_samples, -1)  # (num_samples, vocab_size)\n",
    "\n",
    "        # 2) Replicate the KV cache for each sample/row\n",
    "        kv_length_hint = (len(tokens) + max_tokens) if max_tokens is not None else self.model.config.sequence_len\n",
    "        kv_cache_decode = KVCache(\n",
    "            batch_size=num_samples,\n",
    "            seq_len=kv_length_hint,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            **kv_model_kwargs,\n",
    "        )\n",
    "        kv_cache_decode.prefill(kv_cache_prefill)\n",
    "        del kv_cache_prefill # no need to keep this memory around\n",
    "\n",
    "        # 3) Initialize states for each sample\n",
    "        row_states = [RowState(tokens.copy()) for _ in range(num_samples)]\n",
    "\n",
    "        # 4) Main generation loop\n",
    "        num_generated = 0\n",
    "        while True:\n",
    "            # Stop condition: we've reached max tokens\n",
    "            if max_tokens is not None and num_generated >= max_tokens:\n",
    "                break\n",
    "            # Stop condition: all rows are completed\n",
    "            if all(state.completed for state in row_states):\n",
    "                break\n",
    "\n",
    "            # Sample the next token for each row\n",
    "            next_ids = sample_next_token(logits, rng, temperature, top_k)  # (B, 1)\n",
    "            sampled_tokens = next_ids[:, 0].tolist()\n",
    "\n",
    "            # Process each row: choose the next token, update state, optional tool use\n",
    "            token_column = [] # contains the next token id along each row\n",
    "            token_masks = [] # contains the mask (was it sampled (1) or forced (0)?) along each row\n",
    "            for i, state in enumerate(row_states):\n",
    "                # Select the next token in this row\n",
    "                is_forced = len(state.forced_tokens) > 0 # are there tokens waiting to be forced in deque?\n",
    "                token_masks.append(0 if is_forced else 1) # mask is 0 if forced, 1 if sampled\n",
    "                next_token = state.forced_tokens.popleft() if is_forced else sampled_tokens[i]\n",
    "                token_column.append(next_token)\n",
    "                # Update the state of this row to include the next token\n",
    "                state.current_tokens.append(next_token)\n",
    "                # On <|assistant_end|> or <|bos|>, mark the row as completed\n",
    "                if next_token == assistant_end or next_token == bos:\n",
    "                    state.completed = True\n",
    "                # Handle tool logic\n",
    "                if next_token == python_start:\n",
    "                    state.in_python_block = True\n",
    "                    state.python_expr_tokens = []\n",
    "                elif next_token == python_end and state.in_python_block:\n",
    "                    state.in_python_block = False\n",
    "                    if state.python_expr_tokens:\n",
    "                        expr = self.tokenizer.decode(state.python_expr_tokens)\n",
    "                        result = use_calculator(expr)\n",
    "                        if result is not None:\n",
    "                            result_tokens = self.tokenizer.encode(str(result))\n",
    "                            state.forced_tokens.append(output_start)\n",
    "                            state.forced_tokens.extend(result_tokens)\n",
    "                            state.forced_tokens.append(output_end)\n",
    "                    state.python_expr_tokens = []\n",
    "                elif state.in_python_block:\n",
    "                    state.python_expr_tokens.append(next_token)\n",
    "\n",
    "            # Yield the token column\n",
    "            yield token_column, token_masks\n",
    "            num_generated += 1\n",
    "\n",
    "            # Prepare logits for next iteration\n",
    "            ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            logits = self.model.forward(ids, kv_cache=kv_cache_decode)[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "    def generate_batch(self, tokens, num_samples=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Non-streaming batch generation that just returns the final token sequences.\n",
    "        Returns a list of token sequences (list of lists of ints).\n",
    "        Terminal tokens (assistant_end, bos) are not included in the results.\n",
    "        \"\"\"\n",
    "        assistant_end = self.tokenizer.encode_special(\"<|assistant_end|>\")\n",
    "        bos = self.tokenizer.get_bos_token_id()\n",
    "        results = [tokens.copy() for _ in range(num_samples)]\n",
    "        masks = [[0] * len(tokens) for _ in range(num_samples)]\n",
    "        completed = [False] * num_samples\n",
    "        for token_column, token_masks in self.generate(tokens, num_samples, **kwargs):\n",
    "            for i, (token, mask) in enumerate(zip(token_column, token_masks)):\n",
    "                if not completed[i]:\n",
    "                    if token == assistant_end or token == bos:\n",
    "                        completed[i] = True\n",
    "                    else:\n",
    "                        results[i].append(token)\n",
    "                        masks[i].append(mask)\n",
    "            # Stop if all rows are completed\n",
    "            if all(completed):\n",
    "                break\n",
    "        return results, masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "if False:\n",
    "    \"\"\"\n",
    "    Quick inline test to make sure that the naive/slow model.generate function\n",
    "    is equivalent to the faster Engine.generate function here.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    # init compute\n",
    "    device_type = autodetect_device_type()\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "    autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "    # load the model and tokenizer\n",
    "    model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")\n",
    "    bos_token_id = tokenizer.get_bos_token_id()\n",
    "    # common hyperparameters\n",
    "    kwargs = dict(max_tokens=64, temperature=0.0)\n",
    "    # set the starting prompt\n",
    "    prompt_tokens = tokenizer.encode(\"The chemical formula of water is\", prepend=bos_token_id)\n",
    "    # generate the reference sequence using the model.generate() function\n",
    "    generated_tokens = []\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    stream = model.generate(prompt_tokens, **kwargs)\n",
    "    with autocast_ctx:\n",
    "        for token in stream:\n",
    "            generated_tokens.append(token)\n",
    "            chunk = tokenizer.decode([token])\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    print(f\"Reference time: {t1 - t0:.2f}s\")\n",
    "    reference_ids = generated_tokens\n",
    "    # generate tokens with Engine\n",
    "    generated_tokens = []\n",
    "    engine = Engine(model, tokenizer)\n",
    "    stream = engine.generate(prompt_tokens, num_samples=1, **kwargs) # note: runs in fp32\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    with autocast_ctx:\n",
    "        for token_column, token_masks in stream:\n",
    "            token = token_column[0] # only print out the first row\n",
    "            generated_tokens.append(token)\n",
    "            chunk = tokenizer.decode([token])\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    print(f\"Engine time: {t1 - t0:.2f}s\")\n",
    "    # compare the two sequences\n",
    "    for i in range(len(reference_ids)):\n",
    "        if reference_ids[i] != generated_tokens[i]:\n",
    "            print(f\"Mismatch at {i}: {reference_ids[i]} != {generated_tokens[i]}\")\n",
    "            break\n",
    "    print(f\"Match: {reference_ids == generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bits-per-byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A number of functions that help with evaluating a base model.\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bpb(model, batches, steps, token_bytes):\n",
    "    \"\"\"\n",
    "    Instead of the naive 'mean loss', this function returns the bits per byte (bpb),\n",
    "    which is a tokenization vocab size-independent metric, meaning you are still comparing\n",
    "    apples:apples if you change the vocab size. The way this works is that instead of just\n",
    "    calculating the average loss as usual, you calculate the sum loss, and independently\n",
    "    also the sum bytes (of all the target tokens), and divide. This normalizes the loss by\n",
    "    the number of bytes that the target tokens represent.\n",
    "\n",
    "    The added complexity is so that:\n",
    "    1) All \"normal\" tokens are normalized by the length of the token in bytes\n",
    "    2) No special tokens (e.g. <|bos|>) are included in the metric - they are masked out.\n",
    "    3) No actively masked tokens (using ignore_index of e.g. -1) are included in the metric.\n",
    "\n",
    "    In addition to evaluate_loss, we need the token_bytes tensor:\n",
    "    It is a 1D tensor of shape (vocab_size,), indicating the number of bytes for\n",
    "    each token id, or 0 if the token is to not be counted (e.g. special tokens).\n",
    "    \"\"\"\n",
    "    # record the losses\n",
    "    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "    total_bytes = torch.tensor(0, dtype=torch.int64, device=model.get_device())\n",
    "    batch_iter = iter(batches)\n",
    "    for _ in range(steps):\n",
    "        x, y = next(batch_iter)\n",
    "        loss2d = model(x, y, loss_reduction='none') # (B, T)\n",
    "        loss2d = loss2d.view(-1) # flatten\n",
    "        y = y.view(-1) # flatten\n",
    "        if (y.int() < 0).any(): # mps does not currently have kernel for < 0 for int64, only int32\n",
    "            # slightly more complex code path if some target tokens are ignore_index (e.g. -1)\n",
    "            # any target token < 0 is to be ignored: do NOT index token_bytes with negatives\n",
    "            valid = y >= 0\n",
    "            y_safe = torch.where(valid, y, torch.zeros_like(y))\n",
    "            # map valid targets to their byte length; ignored targets contribute 0 bytes\n",
    "            num_bytes2d = torch.where(\n",
    "                valid,\n",
    "                token_bytes[y_safe],\n",
    "                torch.zeros_like(y, dtype=token_bytes.dtype)\n",
    "            )\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "        else:\n",
    "            # fast path: no ignored targets, safe to index directly\n",
    "            num_bytes2d = token_bytes[y]\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "    # sum reduce across all ranks\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    if world_size > 1:\n",
    "        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n",
    "    # move both to cpu, calculate bpb and return\n",
    "    total_nats = total_nats.item()\n",
    "    total_bytes = total_bytes.item()\n",
    "    if total_bytes == 0:\n",
    "        return float('inf')\n",
    "    bpb = total_nats / (math.log(2) * total_bytes)\n",
    "    return bpb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scripts/base_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified evaluation script for base models.\n",
    "\n",
    "Supports three evaluation modes (comma-separated):\n",
    "  --eval core    : CORE metric (accuracy on ICL tasks)\n",
    "  --eval bpb     : Bits per byte on train/val splits\n",
    "  --eval sample  : Generate samples from the model\n",
    "\n",
    "Default is all three: --eval core,bpb,sample\n",
    "\n",
    "Examples:\n",
    "\n",
    "    # Evaluate a HuggingFace model (e.g. GPT-2 124M) using 8 GPUs\n",
    "    torchrun --nproc_per_node=8 -m scripts.base_eval --hf-path openai-community/gpt2\n",
    "\n",
    "    # Evaluate a nanochat model (e.g. d24) using 8 GPUs\n",
    "    torchrun --nproc_per_node=8 -m scripts.base_eval --model-tag d24 --device-batch-size=16\n",
    "\n",
    "    # Quick/approximate evaluation using a single GPU\n",
    "    python -m scripts.base_eval --model-tag d24 --device-batch-size=16 --max-per-task=100 --split-tokens=524288\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import random\n",
    "import zipfile\n",
    "import tempfile\n",
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "\n",
    "from nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, autodetect_device_type, download_file_with_lock\n",
    "# from nanochat.tokenizer import HuggingFaceTokenizer, get_token_bytes\n",
    "# from nanochat.checkpoint_manager import load_model\n",
    "from nanochat.core_eval import evaluate_task\n",
    "# from nanochat.dataloader import tokenizing_distributed_data_loader_bos_bestfit\n",
    "# from nanochat.loss_eval import evaluate_bpb\n",
    "# from nanochat.engine import Engine\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HuggingFace loading utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelWrapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    \"\"\"Lightweight wrapper to give HuggingFace models a nanochat-compatible interface.\"\"\"\n",
    "    def __init__(self, model, max_seq_len=None):\n",
    "        self.model = model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, input_ids, targets=None, loss_reduction='mean'):\n",
    "        logits = self.model(input_ids).logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=-1,\n",
    "            reduction=loss_reduction\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def get_device(self):\n",
    "        return next(self.model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_hf_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(hf_path: str, device):\n",
    "    \"\"\"Load a HuggingFace model and tokenizer.\"\"\"\n",
    "    print0(f\"Loading HuggingFace model from: {hf_path}\")\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    model = AutoModelForCausalLM.from_pretrained(hf_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    max_seq_len = 1024 if \"gpt2\" in hf_path else None\n",
    "    model = ModelWrapper(model, max_seq_len=max_seq_len)\n",
    "    tokenizer = HuggingFaceTokenizer.from_pretrained(hf_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_hf_token_bytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_token_bytes(tokenizer, device=\"cpu\"):\n",
    "    \"\"\"Compute token_bytes tensor for a HuggingFace tokenizer.\"\"\"\n",
    "    vocab_size = tokenizer.tokenizer.get_vocab_size()\n",
    "    token_bytes = torch.zeros(vocab_size, dtype=torch.int64, device=device)\n",
    "    for token_id in range(vocab_size):\n",
    "        token_str = tokenizer.tokenizer.decode([token_id])\n",
    "        token_bytes[token_id] = len(token_str.encode('utf-8'))\n",
    "    return token_bytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`place_eval_bundle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CORE evaluation\n",
    "\n",
    "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_eval_bundle(file_path):\n",
    "    \"\"\"Unzip eval_bundle.zip and place it in the base directory.\"\"\"\n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(tmpdir)\n",
    "        extracted_bundle_dir = os.path.join(tmpdir, \"eval_bundle\")\n",
    "        shutil.move(extracted_bundle_dir, eval_bundle_dir)\n",
    "    print0(f\"Placed eval_bundle directory at {eval_bundle_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `evaluate_core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_core(model, tokenizer, device, max_per_task=-1):\n",
    "    \"\"\"\n",
    "    Evaluate a base model on the CORE benchmark.\n",
    "    Returns dict with results, centered_results, and core_metric.\n",
    "    \"\"\"\n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    # Download the eval bundle if needed\n",
    "    if not os.path.exists(eval_bundle_dir):\n",
    "        download_file_with_lock(EVAL_BUNDLE_URL, \"eval_bundle.zip\", postprocess_fn=place_eval_bundle)\n",
    "\n",
    "    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "    eval_meta_data = os.path.join(eval_bundle_dir, \"eval_meta_data.csv\")\n",
    "\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    tasks = config['icl_tasks']\n",
    "\n",
    "    # Load random baseline values\n",
    "    random_baselines = {}\n",
    "    with open(eval_meta_data, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            task_name = row['Eval Task']\n",
    "            random_baseline = row['Random baseline']\n",
    "            random_baselines[task_name] = float(random_baseline)\n",
    "\n",
    "    # Evaluate each task\n",
    "    results = {}\n",
    "    centered_results = {}\n",
    "    for task in tasks:\n",
    "        start_time = time.time()\n",
    "        label = task['label']\n",
    "        task_meta = {\n",
    "            'task_type': task['icl_task_type'],\n",
    "            'dataset_uri': task['dataset_uri'],\n",
    "            'num_fewshot': task['num_fewshot'][0],\n",
    "            'continuation_delimiter': task.get('continuation_delimiter', ' ')\n",
    "        }\n",
    "        print0(f\"Evaluating: {label} ({task_meta['num_fewshot']}-shot, type: {task_meta['task_type']})... \", end='')\n",
    "\n",
    "        data_path = os.path.join(data_base_path, task_meta['dataset_uri'])\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "        # Shuffle for consistent subsampling when using max_per_task\n",
    "        shuffle_rng = random.Random(1337)\n",
    "        shuffle_rng.shuffle(data)\n",
    "        if max_per_task > 0:\n",
    "            data = data[:max_per_task]\n",
    "\n",
    "        accuracy = evaluate_task(model, tokenizer, data, device, task_meta)\n",
    "        results[label] = accuracy\n",
    "        random_baseline = random_baselines[label]\n",
    "        centered_result = (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)\n",
    "        centered_results[label] = centered_result\n",
    "        elapsed = time.time() - start_time\n",
    "        print0(f\"accuracy: {accuracy:.4f} | centered: {centered_result:.4f} | time: {elapsed:.2f}s\")\n",
    "\n",
    "    core_metric = sum(centered_results.values()) / len(centered_results)\n",
    "    out = {\n",
    "        \"results\": results,\n",
    "        \"centered_results\": centered_results,\n",
    "        \"core_metric\": core_metric\n",
    "    }\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scripts/base_train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From speedrun.sh:\n",
    "```python\n",
    "# d24 model (slightly overtrained is enough to beat GPT-2 => increase data:params ratio from compute optimal 10.5 (default) to 12)\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --target-param-data-ratio=8.5 --device-batch-size=16 --fp8 --run=$WANDB_RUN\n",
    "```\n",
    "\n",
    "But we're modifying this for d12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train model. From root directory of the project, run as:\n",
    "\n",
    "python -m scripts.base_train\n",
    "\n",
    "or distributed as:\n",
    "\n",
    "torchrun --nproc_per_node=8 -m scripts.base_train\n",
    "\n",
    "If you are only on CPU/Macbook, you'll want to train a much much smaller LLM. Example:\n",
    "python -m scripts.base_train --depth=4 --max-seq-len=512 --device-batch-size=1 --eval-tokens=512 --core-metric-every=-1 --total-batch-size=512 --num-iterations=20\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import asdict\n",
    "from contextlib import nullcontext, contextmanager\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "# from nanochat.gpt import GPT, GPTConfig\n",
    "# from nanochat.dataloader import tokenizing_distributed_data_loader_bos_bestfit, tokenizing_distributed_data_loader_with_state_bos_bestfit\n",
    "from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, print_banner, get_base_dir, autodetect_device_type, get_peak_flops\n",
    "# from nanochat.tokenizer import get_tokenizer, get_token_bytes\n",
    "# from nanochat.checkpoint_manager import save_checkpoint, load_checkpoint\n",
    "# from nanochat.loss_eval import evaluate_bpb\n",
    "# from nanochat.engine import Engine\n",
    "# from nanochat.flash_attention import HAS_FA3\n",
    "# from scripts.base_eval import evaluate_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "                                                      â–‘â–‘â–ˆâ–ˆâ–ˆ                â–‘â–‘â–ˆâ–ˆâ–ˆ\n",
      "     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "    â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘\n",
      "     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ â–‘â–‘â–‘  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ\n",
      "     â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ â–‘â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆ  â–‘â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ\n",
      "     â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "    â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â–‘â–‘â–‘â–‘â–‘\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_banner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original CLI args:\n",
    "\n",
    "```python\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLI arguments\n",
    "parser = argparse.ArgumentParser(description=\"Pretrain base model\")\n",
    "# Logging\n",
    "parser.add_argument(\"--run\", type=str, default=\"dummy\", help=\"wandb run name ('dummy' disables wandb logging)\")\n",
    "# Runtime\n",
    "parser.add_argument(\"--device-type\", type=str, default=\"\", help=\"cuda|cpu|mps (empty = autodetect)\")\n",
    "# FP8 training\n",
    "parser.add_argument(\"--fp8\", action=\"store_true\", help=\"enable FP8 training (requires H100+ GPU and torchao)\")\n",
    "parser.add_argument(\"--fp8-recipe\", type=str, default=\"tensorwise\", choices=[\"rowwise\", \"tensorwise\"], help=\"FP8 scaling recipe: tensorwise (faster, recommended) or rowwise (more accurate but slower)\")\n",
    "# Model architecture\n",
    "parser.add_argument(\"--depth\", type=int, default=20, help=\"depth of the Transformer model\")\n",
    "parser.add_argument(\"--aspect-ratio\", type=int, default=64, help=\"model_dim = depth * aspect_ratio\")\n",
    "parser.add_argument(\"--head-dim\", type=int, default=128, help=\"target head dimension for attention\")\n",
    "parser.add_argument(\"--max-seq-len\", type=int, default=2048, help=\"max context length\")\n",
    "parser.add_argument(\"--window-pattern\", type=str, default=\"SSSL\", help=\"sliding window pattern tiled across layers: L=full, S=half context (e.g. 'SSL')\")\n",
    "# Training horizon (only one used, in order of precedence)\n",
    "parser.add_argument(\"--num-iterations\", type=int, default=-1, help=\"explicit number of optimization steps (-1 = disable)\")\n",
    "parser.add_argument(\"--target-flops\", type=float, default=-1.0, help=\"calculate num_iterations to reach target_flops (-1 = disable)\")\n",
    "parser.add_argument(\"--target-param-data-ratio\", type=float, default=10.5, help=\"calculate num_iterations to maintain data:param ratio (Chinchilla=20, -1 = disable)\")\n",
    "# Optimization\n",
    "parser.add_argument(\"--device-batch-size\", type=int, default=32, help=\"per-device batch size. good number to reduce to 16,8,4,... if you OOM on VRAM.\")\n",
    "parser.add_argument(\"--total-batch-size\", type=int, default=-1, help=\"total batch size in tokens. decent numbers are e.g. 524288. (-1 = auto-compute optimal)\")\n",
    "parser.add_argument(\"--embedding-lr\", type=float, default=0.3, help=\"learning rate for embedding parameters (Adam)\")\n",
    "parser.add_argument(\"--unembedding-lr\", type=float, default=0.004, help=\"learning rate for unembedding parameters (Adam)\")\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=0.2, help=\"cautious weight decay for the Muon optimizer (for weights)\")\n",
    "parser.add_argument(\"--matrix-lr\", type=float, default=0.02, help=\"learning rate for matrix parameters (Muon)\")\n",
    "parser.add_argument(\"--scalar-lr\", type=float, default=0.5, help=\"learning rate for scalars (resid_lambdas, x0_lambdas)\")\n",
    "parser.add_argument(\"--adam-beta1\", type=float, default=0.8, help=\"Adam beta1 for embedding/unembedding\")\n",
    "parser.add_argument(\"--adam-beta2\", type=float, default=0.95, help=\"Adam beta2 for embedding/unembedding\")\n",
    "parser.add_argument(\"--warmup-ratio\", type=float, default=0.0, help=\"ratio of iterations for LR warmup\")\n",
    "parser.add_argument(\"--warmdown-ratio\", type=float, default=0.5, help=\"ratio of iterations for LR warmdown\")\n",
    "parser.add_argument(\"--final-lr-frac\", type=float, default=0.0, help=\"final LR as fraction of initial LR\")\n",
    "parser.add_argument(\"--resume-from-step\", type=int, default=-1, help=\"resume training from this step (-1 = disable)\")\n",
    "# Evaluation\n",
    "parser.add_argument(\"--eval-every\", type=int, default=250, help=\"evaluate val bpb every N steps (-1 = disable)\")\n",
    "parser.add_argument(\"--eval-tokens\", type=int, default=40*524288, help=\"number of tokens to evaluate val loss on\")\n",
    "parser.add_argument(\"--core-metric-every\", type=int, default=2000, help=\"evaluate CORE metric every N steps (-1 = disable)\")\n",
    "parser.add_argument(\"--core-metric-max-per-task\", type=int, default=500, help=\"examples per task for CORE metric\")\n",
    "parser.add_argument(\"--sample-every\", type=int, default=2000, help=\"sample from model every N steps (-1 = disable)\")\n",
    "parser.add_argument(\"--save-every\", type=int, default=-1, help=\"save checkpoints every N steps (-1 = only at end)\")\n",
    "# Output\n",
    "parser.add_argument(\"--model-tag\", type=str, default=None, help=\"override model tag for checkpoint directory name\")\n",
    "args = parser.parse_args()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    run = WANDB_RUN,\n",
    "    device_type = \"\",\n",
    "    #fp8 = True, # TODO - Condition on compute capability\n",
    "    fp8 = False,\n",
    "    fp8_recipe = \"tensorwise\",\n",
    "    depth = 12,\n",
    "    aspect_ratio = 64,\n",
    "    head_dim = 128,\n",
    "    max_seq_len = 2048,\n",
    "    window_pattern = \"SSSL\",\n",
    "    num_iterations = -1,\n",
    "    target_flops = -1.0,\n",
    "    target_param_data_ratio = 8.5, # Code defaults to 10.5, speedrun.sh says 8.5\n",
    "    device_batch_size = 16, # Code defaults to 32, speedrun.sh says 16.\n",
    "    total_batch_size = -1,\n",
    "    embedding_lr = 0.3,\n",
    "    unembedding_lr = 0.004,\n",
    "    weight_decay = 0.2,\n",
    "    matrix_lr = 0.02,\n",
    "    scalar_lr = 0.5,\n",
    "    adam_beta1 = 0.8,\n",
    "    adam_beta2 = 0.95,\n",
    "    warmup_ratio = 0.0,\n",
    "    warmdown_ratio = 0.5,\n",
    "    final_lr_frac = 0.0,\n",
    "    resume_from_step = -1,\n",
    "    eval_every = 250,\n",
    "    eval_tokens = 40*524288,\n",
    "    core_metric_every = -1, # Disable. Default is 2000, which is more steps than we'll run.\n",
    "    core_metric_max_per_task = 500,\n",
    "    sample_every = -1, # Disable. Default is 2000.\n",
    "    save_every = -1,\n",
    "    model_tag = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config = vars(args).copy()  # for logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB | Peak FLOPS (BF16): 3.12e+14\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Compute init and wandb logging\n",
    "\n",
    "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0\n",
    "if device_type == \"cuda\":\n",
    "    gpu_device_name = torch.cuda.get_device_name(0)\n",
    "    gpu_peak_flops = get_peak_flops(gpu_device_name)\n",
    "    print0(f\"GPU: {gpu_device_name} | Peak FLOPS (BF16): {gpu_peak_flops:.2e}\")\n",
    "else:\n",
    "    gpu_peak_flops = float('inf')  # MFU not meaningful for CPU/MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260206_171130-wxer12y6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chrismccormick/nanochat/runs/wxer12y6' target=\"_blank\">nanochat - d.768 - l.12 - v1.0</a></strong> to <a href='https://wandb.ai/chrismccormick/nanochat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chrismccormick/nanochat' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chrismccormick/nanochat/runs/wxer12y6' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat/runs/wxer12y6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb logging init\n",
    "use_dummy_wandb = args.run == \"dummy\" or not master_process\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=args.run, config=user_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using Flash Attention 3 (Hopper GPU detected), efficient, new and awesome.\n"
     ]
    }
   ],
   "source": [
    "# Flash Attention status\n",
    "if HAS_FA3:\n",
    "    print0(\"âœ“ Using Flash Attention 3 (Hopper GPU detected), efficient, new and awesome.\")\n",
    "else:\n",
    "    print0(\"!\" * 80)\n",
    "    print0(\"WARNING: Flash Attention 3 not available, using PyTorch SDPA fallback\")\n",
    "    print0(\"WARNING: Training will be less efficient without FA3\")\n",
    "    if args.window_pattern != \"L\":\n",
    "        print0(f\"WARNING: SDPA has no support for sliding window attention (window_pattern='{args.window_pattern}'). Your GPU utilization will be terrible.\")\n",
    "        print0(\"WARNING: Recommend using --window-pattern L for full context attention without alternating sliding window patterns.\")\n",
    "    print0(\"!\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32,768\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Tokenizer will be useful for evaluation and also we need the vocab size to init the model\n",
    "tokenizer = get_tokenizer()\n",
    "token_bytes = get_token_bytes(device=device)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print0(f\"Vocab size: {vocab_size:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_model_meta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config:\n",
      "{\n",
      "  \"sequence_len\": 2048,\n",
      "  \"vocab_size\": 32768,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_head\": 6,\n",
      "  \"n_kv_head\": 6,\n",
      "  \"n_embd\": 768,\n",
      "  \"window_pattern\": \"SSSL\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Model\n",
    "\n",
    "def build_model_meta(depth):\n",
    "    \"\"\"Build a model on meta device for a given depth (shapes/dtypes only, no data).\"\"\"\n",
    "    # Model dim is nudged up to nearest multiple of head_dim for clean division\n",
    "    # (FA3 requires head_dim divisible by 8, and this guarantees head_dim == args.head_dim exactly)\n",
    "    base_dim = depth * args.aspect_ratio\n",
    "    model_dim = ((base_dim + args.head_dim - 1) // args.head_dim) * args.head_dim\n",
    "    num_heads = model_dim // args.head_dim\n",
    "    config = GPTConfig(\n",
    "        sequence_len=args.max_seq_len, vocab_size=vocab_size,\n",
    "        n_layer=depth, n_head=num_heads, n_kv_head=num_heads, n_embd=model_dim,\n",
    "        window_pattern=args.window_pattern,\n",
    "    )\n",
    "    with torch.device(\"meta\"):\n",
    "        model_meta = GPT(config)\n",
    "    return model_meta\n",
    "\n",
    "# Build the model, move to device, init the weights\n",
    "model = build_model_meta(args.depth) # 1) Build on meta device (only shapes/dtypes, no data)\n",
    "model_config = model.config\n",
    "model_config_kwargs = asdict(model_config)\n",
    "print0(f\"Model config:\\n{json.dumps(model_config_kwargs, indent=2)}\")\n",
    "model.to_empty(device=device) # 2) All tensors get storage on target device but with uninitialized (garbage) data\n",
    "model.init_weights() # 3) All tensors get initialized\n",
    "\n",
    "# If we are resuming, overwrite the model parameters with those of the checkpoint\n",
    "base_dir = get_base_dir()\n",
    "output_dirname = args.model_tag if args.model_tag else f\"d{args.depth}\" # e.g. d12\n",
    "checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n",
    "resuming = args.resume_from_step != -1\n",
    "if resuming:\n",
    "    print0(f\"Resuming optimization from step {args.resume_from_step}\")\n",
    "    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, args.resume_from_step, device, load_optimizer=True, rank=ddp_rank)\n",
    "    model.load_state_dict(model_data, strict=True, assign=True)\n",
    "    del model_data # free up this memory after the copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FP8 training initialization and management (this has to be done before torch.compile)\n",
    "\n",
    "# Convert Linear layers to Float8Linear if --fp8 is set\n",
    "if args.fp8:\n",
    "    if device_type != \"cuda\":\n",
    "        print0(\"Warning: FP8 training requires CUDA, ignoring --fp8 flag\")\n",
    "    else:\n",
    "        from torchao.float8 import Float8LinearConfig, convert_to_float8_training\n",
    "        import torch.nn as nn\n",
    "\n",
    "        # Filter: only convert layers with dimensions divisible by 16 (FP8 hardware requirement)\n",
    "        def fp8_module_filter(mod: nn.Module, fqn: str) -> bool:\n",
    "            if not isinstance(mod, nn.Linear):\n",
    "                return False\n",
    "            # FP8 requires both in_features and out_features divisible by 16\n",
    "            if mod.in_features % 16 != 0 or mod.out_features % 16 != 0:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        fp8_config = Float8LinearConfig.from_recipe_name(args.fp8_recipe)\n",
    "        convert_to_float8_training(model, config=fp8_config, module_filter_fn=fp8_module_filter)\n",
    "        num_fp8_layers = sum(1 for m in model.modules() if 'Float8' in type(m).__name__)\n",
    "        num_skipped = sum(1 for m in model.modules() if isinstance(m, nn.Linear)) - num_fp8_layers\n",
    "        print0(f\"âœ“ FP8 training enabled ({args.fp8_recipe} scaling) - converted {num_fp8_layers} layers, skipped {num_skipped} (dims not divisible by 16)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`disable_fp8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager to temporarily disable FP8 so that model evaluation remains in BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def disable_fp8(model):\n",
    "    \"\"\"Temporarily swap Float8Linear modules with nn.Linear for BF16 evaluation.\n",
    "\n",
    "    CastConfig is a frozen dataclass, so we can't mutate scaling_type. Instead,\n",
    "    we swap out Float8Linear modules entirely and restore them after.\n",
    "    \"\"\"\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Find all Float8Linear modules and their locations\n",
    "    fp8_locations = []  # list of (parent_module, attr_name, fp8_module)\n",
    "    for name, module in model.named_modules():\n",
    "        if 'Float8' in type(module).__name__:\n",
    "            if '.' in name:\n",
    "                parent_name, attr_name = name.rsplit('.', 1)\n",
    "                parent = model.get_submodule(parent_name)\n",
    "            else:\n",
    "                parent = model\n",
    "                attr_name = name\n",
    "            fp8_locations.append((parent, attr_name, module))\n",
    "\n",
    "    if not fp8_locations:\n",
    "        yield  # No FP8 modules, nothing to do\n",
    "        return\n",
    "\n",
    "    # Swap Float8Linear -> nn.Linear (shares the same weight tensor, no copy)\n",
    "    for parent, attr_name, fp8_module in fp8_locations:\n",
    "        linear = nn.Linear(\n",
    "            fp8_module.in_features,\n",
    "            fp8_module.out_features,\n",
    "            bias=fp8_module.bias is not None,\n",
    "            device=fp8_module.weight.device,\n",
    "            dtype=fp8_module.weight.dtype,\n",
    "        )\n",
    "        linear.weight = fp8_module.weight  # share, don't copy\n",
    "        if fp8_module.bias is not None:\n",
    "            linear.bias = fp8_module.bias\n",
    "        setattr(parent, attr_name, linear)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore Float8Linear modules\n",
    "        for parent, attr_name, fp8_module in fp8_locations:\n",
    "            setattr(parent, attr_name, fp8_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Compile the model\n",
    "\n",
    "orig_model = model # original, uncompiled model, for saving raw model state_dict and for inference/evaluation (because the shapes may change shape)\n",
    "model = torch.compile(model, dynamic=False) # the inputs to model will never change shape so dynamic=False is safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter counts:\n",
      "wte                     : 25,165,824\n",
      "value_embeds            : 150,994,944\n",
      "lm_head                 : 25,165,824\n",
      "transformer_matrices    : 84,935,808\n",
      "scalars                 : 24\n",
      "total                   : 286,262,424\n",
      "Estimated FLOPs per token: 8.021676e+08\n",
      "Auto-computed optimal batch size: 524,288 tokens\n",
      "Calculated number of iterations from target data:param ratio: 1,785\n",
      "Total number of training tokens: 935,854,080\n",
      "Tokens : Scaling params ratio: 8.50\n",
      "Total training FLOPs estimate: 7.507118e+17\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Determine the optimization horizon based on the model size\n",
    "# The compute-optimal models satisfy the Tokens:Params ratio of --target-param-data-ratio (derived experimentally via scaling laws analysis).\n",
    "# We've already initialized the model so we have Params. Optimal Tokens is now simply target-param-data-ratio * Params\n",
    "\n",
    "# Get the parameter counts of the model\n",
    "param_counts = model.num_scaling_params()\n",
    "print0(f\"Parameter counts:\")\n",
    "for key, value in param_counts.items():\n",
    "    print0(f\"{key:24s}: {value:,}\")\n",
    "num_params = param_counts['total']\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\n",
    "\n",
    "# Scaling params: transformer matrices + lm_head (gives cleanest scaling laws, see dev/LOG.md Jan 27, 2026)\n",
    "get_scaling_params = lambda m: m.num_scaling_params()['transformer_matrices'] + m.num_scaling_params()['lm_head']\n",
    "num_scaling_params = get_scaling_params(model)\n",
    "target_tokens = int(args.target_param_data_ratio * num_scaling_params)\n",
    "\n",
    "# Auto-compute optimal batch size based on Power Lines paper (Bopt âˆ D^0.383), ref: https://arxiv.org/abs/2505.13738\n",
    "total_batch_size = args.total_batch_size\n",
    "if total_batch_size == -1:\n",
    "    d12_ref = build_model_meta(12) # d12 is where the optimal batch size was measured to be 2**19 tokens\n",
    "    d12_num_scaling_params = get_scaling_params(d12_ref)\n",
    "    D_REF = args.target_param_data_ratio * d12_num_scaling_params\n",
    "    B_REF = 2**19\n",
    "    batch_size_ratio = target_tokens / D_REF\n",
    "    total_batch_size = 2 ** round(math.log2(B_REF * batch_size_ratio ** 0.383)) # also clamp to power of 2\n",
    "    print0(f\"Auto-computed optimal batch size: {total_batch_size:,} tokens\")\n",
    "\n",
    "# Calculate number of iterations. Either it is given, or from target flops, or from target data:param ratio (in that order)\n",
    "assert args.num_iterations > 0 or args.target_param_data_ratio > 0 or args.target_flops > 0\n",
    "if args.num_iterations > 0:\n",
    "    # Override num_iterations to a specific value if given\n",
    "    num_iterations = args.num_iterations\n",
    "    print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\n",
    "elif args.target_flops > 0:\n",
    "    # Calculate the number of iterations from the target flops (used in scaling laws analysis, e.g. runs/scaling_laws.sh)\n",
    "    num_iterations = round(args.target_flops / (num_flops_per_token * total_batch_size))\n",
    "    print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")\n",
    "elif args.target_param_data_ratio > 0:\n",
    "    # Calculate the number of iterations from the target param data ratio (the most common use case)\n",
    "    num_iterations = target_tokens // total_batch_size\n",
    "    print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")\n",
    "else:\n",
    "    raise ValueError(\"No training horizon specified\")\n",
    "total_tokens = total_batch_size * num_iterations\n",
    "print0(f\"Total number of training tokens: {total_tokens:,}\")\n",
    "print0(f\"Tokens : Scaling params ratio: {total_batch_size * num_iterations / num_scaling_params:.2f}\") # Chinchilla is ~20\n",
    "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens / micro-batch / rank: 16 x 2048 = 32,768\n",
      "Tokens / micro-batch: 32,768\n",
      "Total batch size 524,288 => gradient accumulation steps: 16\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Optimizer / data / training length related hyperparameters\n",
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = args.device_batch_size * args.max_seq_len # tokens per iteration for a single rank\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks\n",
    "assert total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n",
    "print0(f\"Tokens / micro-batch / rank: {args.device_batch_size} x {args.max_seq_len} = {tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "# Batch size scaling for learning rates (hyperparameters were tuned at reference batch size 2^19)\n",
    "batch_lr_scale = 1.0\n",
    "reference_batch_size = 2**19\n",
    "batch_ratio = total_batch_size / reference_batch_size\n",
    "if batch_ratio != 1.0:\n",
    "    # SGD: linear scaling with batch size is standard (not used in nanochat)\n",
    "    # AdamW: sqrt scaling is standard\n",
    "    # Muon: sqrt scaling is an assumption - not fully studied, but it's a second-order-ish optimizer\n",
    "    batch_lr_scale = batch_ratio ** 0.5\n",
    "    print0(f\"Scaling LRs by {batch_lr_scale:.4f} for batch size {total_batch_size:,} (reference: {reference_batch_size:,})\")\n",
    "\n",
    "# Weight decay is tuned at d12 and its scaling seems to be \\propto 1/channels^2 (or equivalently, \\propto 1/depth^2 due to constant aspect ratio)\n",
    "weight_decay_scaled = args.weight_decay * (12 / args.depth)**2\n",
    "if args.depth != 12:\n",
    "    print0(f\"Scaling weight decay from {args.weight_decay:.6f} to {weight_decay_scaled:.6f} for depth {args.depth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters âˆ1/âˆš(768/768) = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Optimizer (combined MuonAdamW: Muon for matrix params, AdamW for rest)\n",
    "adam_betas = (args.adam_beta1, args.adam_beta2)\n",
    "optimizer = model.setup_optimizer(\n",
    "    unembedding_lr=args.unembedding_lr * batch_lr_scale,\n",
    "    embedding_lr=args.embedding_lr * batch_lr_scale,\n",
    "    matrix_lr=args.matrix_lr * batch_lr_scale,\n",
    "    weight_decay=weight_decay_scaled,\n",
    "    adam_betas=adam_betas,\n",
    "    scalar_lr=args.scalar_lr * batch_lr_scale,\n",
    ")\n",
    "\n",
    "if resuming:\n",
    "    optimizer.load_state_dict(optimizer_data)\n",
    "    del optimizer_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the DataLoaders for train/val\n",
    "dataloader_resume_state_dict = None if not resuming else meta_data[\"dataloader_state_dict\"]\n",
    "train_loader = tokenizing_distributed_data_loader_with_state_bos_bestfit(tokenizer, args.device_batch_size, args.max_seq_len, split=\"train\", device=device, resume_state_dict=dataloader_resume_state_dict)\n",
    "build_val_loader = lambda: tokenizing_distributed_data_loader_bos_bestfit(tokenizer, args.device_batch_size, args.max_seq_len, split=\"val\", device=device)\n",
    "x, y, dataloader_state_dict = next(train_loader) # kick off load of the very first batch of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Set up hyperparameter schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_lr_multiplier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(args.warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(args.warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * args.final_lr_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_muon_momentum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum scheduler for Muon optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_weight_decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay scheduler for Muon optimizer (linear to zero over the course of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_decay(it):\n",
    "    return weight_decay_scaled * (1 - it / num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Loop state (variables updated by the training loop)\n",
    "\n",
    "if not resuming:\n",
    "    step = 0\n",
    "    val_bpb = None # will be set if eval_every > 0\n",
    "    min_val_bpb = float(\"inf\")\n",
    "    smooth_train_loss = 0 # EMA of training loss\n",
    "    total_training_time = 0 # total wall-clock time of training\n",
    "else:\n",
    "    step = meta_data[\"step\"]\n",
    "    loop_state = meta_data[\"loop_state\"]\n",
    "    val_bpb = meta_data[\"val_bpb\"]\n",
    "    min_val_bpb = loop_state[\"min_val_bpb\"]\n",
    "    smooth_train_loss = loop_state[\"smooth_train_loss\"]\n",
    "    total_training_time = loop_state[\"total_training_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00000 | Validation bpb: 3.175621\n",
      "step 00000/01785 (0.00%) | loss: 10.397367 | lrm: 1.00 | dt: 55086.21ms | tok/sec: 9,517 | mfu: 2.45 | epoch: 1 | total time: 0.00m\n",
      "step 00001/01785 (0.06%) | loss: 10.649736 | lrm: 1.00 | dt: 2594.40ms | tok/sec: 202,084 | mfu: 51.96 | epoch: 1 | total time: 0.00m\n",
      "step 00002/01785 (0.11%) | loss: 10.492660 | lrm: 1.00 | dt: 2547.64ms | tok/sec: 205,793 | mfu: 52.91 | epoch: 1 | total time: 0.00m\n",
      "step 00003/01785 (0.17%) | loss: 10.320310 | lrm: 1.00 | dt: 2552.70ms | tok/sec: 205,385 | mfu: 52.81 | epoch: 1 | total time: 0.00m\n",
      "step 00004/01785 (0.22%) | loss: 9.911604 | lrm: 1.00 | dt: 2553.48ms | tok/sec: 205,322 | mfu: 52.79 | epoch: 1 | total time: 0.00m\n",
      "step 00005/01785 (0.28%) | loss: 9.567101 | lrm: 1.00 | dt: 2552.48ms | tok/sec: 205,403 | mfu: 52.81 | epoch: 1 | total time: 0.00m\n",
      "step 00006/01785 (0.34%) | loss: 9.208370 | lrm: 1.00 | dt: 2561.48ms | tok/sec: 204,681 | mfu: 52.62 | epoch: 1 | total time: 0.00m\n",
      "step 00007/01785 (0.39%) | loss: 9.015981 | lrm: 1.00 | dt: 2558.55ms | tok/sec: 204,915 | mfu: 52.68 | epoch: 1 | total time: 0.00m\n",
      "step 00008/01785 (0.45%) | loss: 8.744496 | lrm: 1.00 | dt: 2560.57ms | tok/sec: 204,754 | mfu: 52.64 | epoch: 1 | total time: 0.00m\n",
      "step 00009/01785 (0.50%) | loss: 8.503517 | lrm: 1.00 | dt: 2562.46ms | tok/sec: 204,603 | mfu: 52.60 | epoch: 1 | total time: 0.00m\n",
      "step 00010/01785 (0.56%) | loss: 8.263778 | lrm: 1.00 | dt: 2565.77ms | tok/sec: 204,339 | mfu: 52.54 | epoch: 1 | total time: 0.00m\n",
      "step 00011/01785 (0.62%) | loss: 8.042461 | lrm: 1.00 | dt: 2565.99ms | tok/sec: 204,321 | mfu: 52.53 | epoch: 1 | total time: 0.04m | eta: 75.9m\n",
      "step 00012/01785 (0.67%) | loss: 7.831134 | lrm: 1.00 | dt: 2567.03ms | tok/sec: 204,238 | mfu: 52.51 | epoch: 1 | total time: 0.09m | eta: 75.8m\n",
      "step 00013/01785 (0.73%) | loss: 7.665779 | lrm: 1.00 | dt: 2569.71ms | tok/sec: 204,026 | mfu: 52.46 | epoch: 1 | total time: 0.13m | eta: 75.8m\n",
      "step 00014/01785 (0.78%) | loss: 7.514195 | lrm: 1.00 | dt: 2570.95ms | tok/sec: 203,927 | mfu: 52.43 | epoch: 1 | total time: 0.17m | eta: 75.8m\n",
      "step 00015/01785 (0.84%) | loss: 7.372196 | lrm: 1.00 | dt: 2567.41ms | tok/sec: 204,209 | mfu: 52.50 | epoch: 1 | total time: 0.21m | eta: 75.8m\n",
      "step 00016/01785 (0.90%) | loss: 7.239813 | lrm: 1.00 | dt: 2573.68ms | tok/sec: 203,711 | mfu: 52.38 | epoch: 1 | total time: 0.26m | eta: 75.7m\n",
      "step 00017/01785 (0.95%) | loss: 7.127350 | lrm: 1.00 | dt: 2571.76ms | tok/sec: 203,863 | mfu: 52.41 | epoch: 1 | total time: 0.30m | eta: 75.7m\n",
      "step 00018/01785 (1.01%) | loss: 7.019814 | lrm: 1.00 | dt: 2575.84ms | tok/sec: 203,540 | mfu: 52.33 | epoch: 1 | total time: 0.34m | eta: 75.7m\n",
      "step 00019/01785 (1.06%) | loss: 6.923394 | lrm: 1.00 | dt: 2573.58ms | tok/sec: 203,719 | mfu: 52.38 | epoch: 1 | total time: 0.39m | eta: 75.7m\n",
      "step 00020/01785 (1.12%) | loss: 6.826338 | lrm: 1.00 | dt: 2577.31ms | tok/sec: 203,424 | mfu: 52.30 | epoch: 1 | total time: 0.43m | eta: 75.6m\n",
      "step 00021/01785 (1.18%) | loss: 6.734616 | lrm: 1.00 | dt: 2576.96ms | tok/sec: 203,451 | mfu: 52.31 | epoch: 1 | total time: 0.47m | eta: 75.6m\n",
      "step 00022/01785 (1.23%) | loss: 6.675089 | lrm: 1.00 | dt: 2573.26ms | tok/sec: 203,744 | mfu: 52.38 | epoch: 1 | total time: 0.51m | eta: 75.6m\n",
      "step 00023/01785 (1.29%) | loss: 6.606791 | lrm: 1.00 | dt: 2575.74ms | tok/sec: 203,548 | mfu: 52.33 | epoch: 1 | total time: 0.56m | eta: 75.5m\n",
      "step 00024/01785 (1.34%) | loss: 6.548916 | lrm: 1.00 | dt: 2580.47ms | tok/sec: 203,175 | mfu: 52.24 | epoch: 1 | total time: 0.60m | eta: 75.5m\n",
      "step 00025/01785 (1.40%) | loss: 6.492263 | lrm: 1.00 | dt: 2577.97ms | tok/sec: 203,372 | mfu: 52.29 | epoch: 1 | total time: 0.64m | eta: 75.5m\n",
      "step 00026/01785 (1.46%) | loss: 6.438181 | lrm: 1.00 | dt: 2580.20ms | tok/sec: 203,196 | mfu: 52.24 | epoch: 1 | total time: 0.69m | eta: 75.4m\n",
      "step 00027/01785 (1.51%) | loss: 6.394798 | lrm: 1.00 | dt: 2580.14ms | tok/sec: 203,201 | mfu: 52.24 | epoch: 1 | total time: 0.73m | eta: 75.4m\n",
      "step 00028/01785 (1.57%) | loss: 6.345111 | lrm: 1.00 | dt: 2579.76ms | tok/sec: 203,231 | mfu: 52.25 | epoch: 1 | total time: 0.77m | eta: 75.4m\n",
      "step 00029/01785 (1.62%) | loss: 6.305323 | lrm: 1.00 | dt: 2580.49ms | tok/sec: 203,173 | mfu: 52.24 | epoch: 1 | total time: 0.82m | eta: 75.4m\n",
      "step 00030/01785 (1.68%) | loss: 6.255086 | lrm: 1.00 | dt: 2583.03ms | tok/sec: 202,973 | mfu: 52.19 | epoch: 1 | total time: 0.86m | eta: 75.3m\n",
      "step 00031/01785 (1.74%) | loss: 6.216694 | lrm: 1.00 | dt: 2582.14ms | tok/sec: 203,044 | mfu: 52.20 | epoch: 1 | total time: 0.90m | eta: 75.3m\n",
      "step 00032/01785 (1.79%) | loss: 6.180630 | lrm: 1.00 | dt: 2581.61ms | tok/sec: 203,085 | mfu: 52.21 | epoch: 1 | total time: 0.94m | eta: 75.3m\n",
      "step 00033/01785 (1.85%) | loss: 6.141278 | lrm: 1.00 | dt: 2583.01ms | tok/sec: 202,975 | mfu: 52.19 | epoch: 1 | total time: 0.99m | eta: 75.2m\n",
      "step 00034/01785 (1.90%) | loss: 6.111456 | lrm: 1.00 | dt: 2582.40ms | tok/sec: 203,023 | mfu: 52.20 | epoch: 1 | total time: 1.03m | eta: 75.2m\n",
      "step 00035/01785 (1.96%) | loss: 6.077061 | lrm: 1.00 | dt: 2581.96ms | tok/sec: 203,058 | mfu: 52.21 | epoch: 1 | total time: 1.07m | eta: 75.1m\n",
      "step 00036/01785 (2.02%) | loss: 6.044368 | lrm: 1.00 | dt: 2585.30ms | tok/sec: 202,796 | mfu: 52.14 | epoch: 1 | total time: 1.12m | eta: 75.1m\n",
      "step 00037/01785 (2.07%) | loss: 6.014059 | lrm: 1.00 | dt: 2580.92ms | tok/sec: 203,139 | mfu: 52.23 | epoch: 1 | total time: 1.16m | eta: 75.1m\n",
      "step 00038/01785 (2.13%) | loss: 5.988390 | lrm: 1.00 | dt: 2581.84ms | tok/sec: 203,067 | mfu: 52.21 | epoch: 1 | total time: 1.20m | eta: 75.0m\n",
      "step 00039/01785 (2.18%) | loss: 5.950684 | lrm: 1.00 | dt: 2582.67ms | tok/sec: 203,002 | mfu: 52.19 | epoch: 1 | total time: 1.25m | eta: 75.0m\n",
      "step 00040/01785 (2.24%) | loss: 5.914816 | lrm: 1.00 | dt: 2583.77ms | tok/sec: 202,916 | mfu: 52.17 | epoch: 1 | total time: 1.29m | eta: 75.0m\n",
      "step 00041/01785 (2.30%) | loss: 5.888856 | lrm: 1.00 | dt: 2584.40ms | tok/sec: 202,866 | mfu: 52.16 | epoch: 1 | total time: 1.33m | eta: 74.9m\n",
      "step 00042/01785 (2.35%) | loss: 5.864255 | lrm: 1.00 | dt: 2584.19ms | tok/sec: 202,882 | mfu: 52.16 | epoch: 1 | total time: 1.37m | eta: 74.9m\n",
      "step 00043/01785 (2.41%) | loss: 5.824268 | lrm: 1.00 | dt: 2585.33ms | tok/sec: 202,793 | mfu: 52.14 | epoch: 1 | total time: 1.42m | eta: 74.9m\n",
      "step 00044/01785 (2.46%) | loss: 5.797060 | lrm: 1.00 | dt: 2585.04ms | tok/sec: 202,816 | mfu: 52.15 | epoch: 1 | total time: 1.46m | eta: 74.8m\n",
      "step 00045/01785 (2.52%) | loss: 5.766003 | lrm: 1.00 | dt: 2585.42ms | tok/sec: 202,786 | mfu: 52.14 | epoch: 1 | total time: 1.50m | eta: 74.8m\n",
      "step 00046/01785 (2.58%) | loss: 5.736933 | lrm: 1.00 | dt: 2585.66ms | tok/sec: 202,767 | mfu: 52.13 | epoch: 1 | total time: 1.55m | eta: 74.7m\n",
      "step 00047/01785 (2.63%) | loss: 5.715731 | lrm: 1.00 | dt: 2584.16ms | tok/sec: 202,885 | mfu: 52.16 | epoch: 1 | total time: 1.59m | eta: 74.7m\n",
      "step 00048/01785 (2.69%) | loss: 5.691963 | lrm: 1.00 | dt: 2586.19ms | tok/sec: 202,726 | mfu: 52.12 | epoch: 1 | total time: 1.63m | eta: 74.7m\n",
      "step 00049/01785 (2.75%) | loss: 5.656358 | lrm: 1.00 | dt: 2586.18ms | tok/sec: 202,726 | mfu: 52.12 | epoch: 1 | total time: 1.68m | eta: 74.6m\n",
      "step 00050/01785 (2.80%) | loss: 5.624596 | lrm: 1.00 | dt: 2585.43ms | tok/sec: 202,785 | mfu: 52.14 | epoch: 1 | total time: 1.72m | eta: 74.6m\n",
      "step 00051/01785 (2.86%) | loss: 5.593837 | lrm: 1.00 | dt: 2582.79ms | tok/sec: 202,993 | mfu: 52.19 | epoch: 1 | total time: 1.76m | eta: 74.5m\n",
      "step 00052/01785 (2.91%) | loss: 5.573337 | lrm: 1.00 | dt: 2582.66ms | tok/sec: 203,003 | mfu: 52.19 | epoch: 1 | total time: 1.81m | eta: 74.5m\n",
      "step 00053/01785 (2.97%) | loss: 5.547534 | lrm: 1.00 | dt: 2584.41ms | tok/sec: 202,866 | mfu: 52.16 | epoch: 1 | total time: 1.85m | eta: 74.5m\n",
      "step 00054/01785 (3.03%) | loss: 5.530648 | lrm: 1.00 | dt: 2586.20ms | tok/sec: 202,725 | mfu: 52.12 | epoch: 1 | total time: 1.89m | eta: 74.4m\n",
      "step 00055/01785 (3.08%) | loss: 5.499033 | lrm: 1.00 | dt: 2594.30ms | tok/sec: 202,092 | mfu: 51.96 | epoch: 1 | total time: 1.94m | eta: 74.4m\n",
      "step 00056/01785 (3.14%) | loss: 5.456082 | lrm: 1.00 | dt: 2585.94ms | tok/sec: 202,745 | mfu: 52.13 | epoch: 1 | total time: 1.98m | eta: 74.4m\n",
      "step 00057/01785 (3.19%) | loss: 5.435540 | lrm: 1.00 | dt: 2584.62ms | tok/sec: 202,849 | mfu: 52.15 | epoch: 1 | total time: 2.02m | eta: 74.3m\n",
      "step 00058/01785 (3.25%) | loss: 5.409301 | lrm: 1.00 | dt: 2584.57ms | tok/sec: 202,853 | mfu: 52.15 | epoch: 1 | total time: 2.06m | eta: 74.3m\n",
      "step 00059/01785 (3.31%) | loss: 5.393446 | lrm: 1.00 | dt: 2584.24ms | tok/sec: 202,879 | mfu: 52.16 | epoch: 1 | total time: 2.11m | eta: 74.2m\n",
      "step 00060/01785 (3.36%) | loss: 5.365246 | lrm: 1.00 | dt: 2593.88ms | tok/sec: 202,125 | mfu: 51.97 | epoch: 1 | total time: 2.15m | eta: 74.2m\n",
      "step 00061/01785 (3.42%) | loss: 5.346220 | lrm: 1.00 | dt: 2586.25ms | tok/sec: 202,721 | mfu: 52.12 | epoch: 1 | total time: 2.19m | eta: 74.2m\n",
      "step 00062/01785 (3.47%) | loss: 5.335412 | lrm: 1.00 | dt: 2584.47ms | tok/sec: 202,861 | mfu: 52.16 | epoch: 1 | total time: 2.24m | eta: 74.1m\n",
      "step 00063/01785 (3.53%) | loss: 5.310206 | lrm: 1.00 | dt: 2581.85ms | tok/sec: 203,066 | mfu: 52.21 | epoch: 1 | total time: 2.28m | eta: 74.1m\n",
      "step 00064/01785 (3.59%) | loss: 5.317289 | lrm: 1.00 | dt: 2582.01ms | tok/sec: 203,054 | mfu: 52.21 | epoch: 1 | total time: 2.32m | eta: 74.0m\n",
      "step 00065/01785 (3.64%) | loss: 5.295066 | lrm: 1.00 | dt: 2582.58ms | tok/sec: 203,009 | mfu: 52.19 | epoch: 1 | total time: 2.37m | eta: 74.0m\n",
      "step 00066/01785 (3.70%) | loss: 5.278878 | lrm: 1.00 | dt: 2583.42ms | tok/sec: 202,943 | mfu: 52.18 | epoch: 1 | total time: 2.41m | eta: 73.9m\n",
      "step 00067/01785 (3.75%) | loss: 5.261127 | lrm: 1.00 | dt: 2583.16ms | tok/sec: 202,963 | mfu: 52.18 | epoch: 1 | total time: 2.45m | eta: 73.9m\n",
      "step 00068/01785 (3.81%) | loss: 5.229780 | lrm: 1.00 | dt: 2588.15ms | tok/sec: 202,572 | mfu: 52.08 | epoch: 1 | total time: 2.50m | eta: 73.9m\n",
      "step 00069/01785 (3.87%) | loss: 5.212085 | lrm: 1.00 | dt: 2590.06ms | tok/sec: 202,423 | mfu: 52.04 | epoch: 1 | total time: 2.54m | eta: 73.8m\n",
      "step 00070/01785 (3.92%) | loss: 5.196084 | lrm: 1.00 | dt: 2585.34ms | tok/sec: 202,792 | mfu: 52.14 | epoch: 1 | total time: 2.58m | eta: 73.8m\n",
      "step 00071/01785 (3.98%) | loss: 5.163022 | lrm: 1.00 | dt: 2583.87ms | tok/sec: 202,908 | mfu: 52.17 | epoch: 1 | total time: 2.62m | eta: 73.7m\n",
      "step 00072/01785 (4.03%) | loss: 5.133232 | lrm: 1.00 | dt: 2585.22ms | tok/sec: 202,802 | mfu: 52.14 | epoch: 1 | total time: 2.67m | eta: 73.7m\n",
      "step 00073/01785 (4.09%) | loss: 5.110695 | lrm: 1.00 | dt: 2585.11ms | tok/sec: 202,810 | mfu: 52.14 | epoch: 1 | total time: 2.71m | eta: 73.7m\n",
      "step 00074/01785 (4.15%) | loss: 5.103200 | lrm: 1.00 | dt: 2582.30ms | tok/sec: 203,031 | mfu: 52.20 | epoch: 1 | total time: 2.75m | eta: 73.6m\n",
      "step 00075/01785 (4.20%) | loss: 5.078358 | lrm: 1.00 | dt: 2584.56ms | tok/sec: 202,853 | mfu: 52.15 | epoch: 1 | total time: 2.80m | eta: 73.6m\n",
      "step 00076/01785 (4.26%) | loss: 5.060111 | lrm: 1.00 | dt: 2586.22ms | tok/sec: 202,723 | mfu: 52.12 | epoch: 1 | total time: 2.84m | eta: 73.5m\n",
      "step 00077/01785 (4.31%) | loss: 5.036929 | lrm: 1.00 | dt: 2585.98ms | tok/sec: 202,742 | mfu: 52.13 | epoch: 1 | total time: 2.88m | eta: 73.5m\n",
      "step 00078/01785 (4.37%) | loss: 5.023877 | lrm: 1.00 | dt: 2583.53ms | tok/sec: 202,934 | mfu: 52.18 | epoch: 1 | total time: 2.93m | eta: 73.5m\n",
      "step 00079/01785 (4.43%) | loss: 5.007628 | lrm: 1.00 | dt: 2584.63ms | tok/sec: 202,848 | mfu: 52.15 | epoch: 1 | total time: 2.97m | eta: 73.4m\n",
      "step 00080/01785 (4.48%) | loss: 4.994585 | lrm: 1.00 | dt: 2582.99ms | tok/sec: 202,976 | mfu: 52.19 | epoch: 1 | total time: 3.01m | eta: 73.4m\n",
      "step 00081/01785 (4.54%) | loss: 4.974686 | lrm: 1.00 | dt: 2584.32ms | tok/sec: 202,872 | mfu: 52.16 | epoch: 1 | total time: 3.06m | eta: 73.3m\n",
      "step 00082/01785 (4.59%) | loss: 4.952435 | lrm: 1.00 | dt: 2593.03ms | tok/sec: 202,191 | mfu: 51.98 | epoch: 1 | total time: 3.10m | eta: 73.3m\n",
      "step 00083/01785 (4.65%) | loss: 4.937669 | lrm: 1.00 | dt: 2590.23ms | tok/sec: 202,410 | mfu: 52.04 | epoch: 1 | total time: 3.14m | eta: 73.2m\n",
      "step 00084/01785 (4.71%) | loss: 4.914514 | lrm: 1.00 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 52.18 | epoch: 1 | total time: 3.18m | eta: 73.2m\n",
      "step 00085/01785 (4.76%) | loss: 4.891530 | lrm: 1.00 | dt: 2583.42ms | tok/sec: 202,943 | mfu: 52.18 | epoch: 1 | total time: 3.23m | eta: 73.2m\n",
      "step 00086/01785 (4.82%) | loss: 4.880091 | lrm: 1.00 | dt: 2585.63ms | tok/sec: 202,769 | mfu: 52.13 | epoch: 1 | total time: 3.27m | eta: 73.1m\n",
      "step 00087/01785 (4.87%) | loss: 4.871983 | lrm: 1.00 | dt: 2595.12ms | tok/sec: 202,028 | mfu: 51.94 | epoch: 1 | total time: 3.31m | eta: 73.1m\n",
      "step 00088/01785 (4.93%) | loss: 4.846825 | lrm: 1.00 | dt: 2584.99ms | tok/sec: 202,819 | mfu: 52.15 | epoch: 1 | total time: 3.36m | eta: 73.0m\n",
      "step 00089/01785 (4.99%) | loss: 4.828897 | lrm: 1.00 | dt: 2582.91ms | tok/sec: 202,983 | mfu: 52.19 | epoch: 1 | total time: 3.40m | eta: 73.0m\n",
      "step 00090/01785 (5.04%) | loss: 4.811869 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 52.20 | epoch: 1 | total time: 3.44m | eta: 73.0m\n",
      "step 00091/01785 (5.10%) | loss: 4.805914 | lrm: 1.00 | dt: 2585.06ms | tok/sec: 202,814 | mfu: 52.14 | epoch: 1 | total time: 3.49m | eta: 72.9m\n",
      "step 00092/01785 (5.15%) | loss: 4.795882 | lrm: 1.00 | dt: 2594.41ms | tok/sec: 202,083 | mfu: 51.96 | epoch: 1 | total time: 3.53m | eta: 72.9m\n",
      "step 00093/01785 (5.21%) | loss: 4.764914 | lrm: 1.00 | dt: 2585.84ms | tok/sec: 202,753 | mfu: 52.13 | epoch: 1 | total time: 3.57m | eta: 72.8m\n",
      "step 00094/01785 (5.27%) | loss: 4.737667 | lrm: 1.00 | dt: 2582.53ms | tok/sec: 203,012 | mfu: 52.20 | epoch: 1 | total time: 3.62m | eta: 72.8m\n",
      "step 00095/01785 (5.32%) | loss: 4.718411 | lrm: 1.00 | dt: 2583.06ms | tok/sec: 202,971 | mfu: 52.18 | epoch: 1 | total time: 3.66m | eta: 72.7m\n",
      "step 00096/01785 (5.38%) | loss: 4.708818 | lrm: 1.00 | dt: 2584.36ms | tok/sec: 202,869 | mfu: 52.16 | epoch: 1 | total time: 3.70m | eta: 72.7m\n",
      "step 00097/01785 (5.43%) | loss: 4.701331 | lrm: 1.00 | dt: 2585.13ms | tok/sec: 202,809 | mfu: 52.14 | epoch: 1 | total time: 3.75m | eta: 72.7m\n",
      "step 00098/01785 (5.49%) | loss: 4.691323 | lrm: 1.00 | dt: 2587.37ms | tok/sec: 202,633 | mfu: 52.10 | epoch: 1 | total time: 3.79m | eta: 72.6m\n",
      "step 00099/01785 (5.55%) | loss: 4.681616 | lrm: 1.00 | dt: 2582.93ms | tok/sec: 202,981 | mfu: 52.19 | epoch: 1 | total time: 3.83m | eta: 72.6m\n",
      "step 00100/01785 (5.60%) | loss: 4.661498 | lrm: 1.00 | dt: 2583.31ms | tok/sec: 202,951 | mfu: 52.18 | epoch: 1 | total time: 3.87m | eta: 72.5m\n",
      "step 00101/01785 (5.66%) | loss: 4.641689 | lrm: 1.00 | dt: 2588.08ms | tok/sec: 202,578 | mfu: 52.08 | epoch: 1 | total time: 3.92m | eta: 72.5m\n",
      "step 00102/01785 (5.71%) | loss: 4.626679 | lrm: 1.00 | dt: 2584.91ms | tok/sec: 202,826 | mfu: 52.15 | epoch: 1 | total time: 3.96m | eta: 72.5m\n",
      "step 00103/01785 (5.77%) | loss: 4.612213 | lrm: 1.00 | dt: 2584.03ms | tok/sec: 202,895 | mfu: 52.17 | epoch: 1 | total time: 4.00m | eta: 72.4m\n",
      "step 00104/01785 (5.83%) | loss: 4.601862 | lrm: 1.00 | dt: 2584.08ms | tok/sec: 202,891 | mfu: 52.16 | epoch: 1 | total time: 4.05m | eta: 72.4m\n",
      "step 00105/01785 (5.88%) | loss: 4.582030 | lrm: 1.00 | dt: 2584.31ms | tok/sec: 202,873 | mfu: 52.16 | epoch: 1 | total time: 4.09m | eta: 72.3m\n",
      "step 00106/01785 (5.94%) | loss: 4.567698 | lrm: 1.00 | dt: 2590.37ms | tok/sec: 202,398 | mfu: 52.04 | epoch: 1 | total time: 4.13m | eta: 72.3m\n",
      "step 00107/01785 (5.99%) | loss: 4.545447 | lrm: 1.00 | dt: 2590.67ms | tok/sec: 202,375 | mfu: 52.03 | epoch: 1 | total time: 4.18m | eta: 72.2m\n",
      "step 00108/01785 (6.05%) | loss: 4.537791 | lrm: 1.00 | dt: 2583.48ms | tok/sec: 202,938 | mfu: 52.18 | epoch: 1 | total time: 4.22m | eta: 72.2m\n",
      "step 00109/01785 (6.11%) | loss: 4.518296 | lrm: 1.00 | dt: 2584.30ms | tok/sec: 202,874 | mfu: 52.16 | epoch: 1 | total time: 4.26m | eta: 72.2m\n",
      "step 00110/01785 (6.16%) | loss: 4.496911 | lrm: 1.00 | dt: 2594.38ms | tok/sec: 202,086 | mfu: 51.96 | epoch: 1 | total time: 4.31m | eta: 72.1m\n",
      "step 00111/01785 (6.22%) | loss: 4.489437 | lrm: 1.00 | dt: 2587.71ms | tok/sec: 202,607 | mfu: 52.09 | epoch: 1 | total time: 4.35m | eta: 72.1m\n",
      "step 00112/01785 (6.27%) | loss: 4.476647 | lrm: 1.00 | dt: 2583.22ms | tok/sec: 202,958 | mfu: 52.18 | epoch: 1 | total time: 4.39m | eta: 72.0m\n",
      "step 00113/01785 (6.33%) | loss: 4.456249 | lrm: 1.00 | dt: 2584.47ms | tok/sec: 202,860 | mfu: 52.16 | epoch: 1 | total time: 4.43m | eta: 72.0m\n",
      "step 00114/01785 (6.39%) | loss: 4.440585 | lrm: 1.00 | dt: 2592.84ms | tok/sec: 202,206 | mfu: 51.99 | epoch: 1 | total time: 4.48m | eta: 71.9m\n",
      "step 00115/01785 (6.44%) | loss: 4.436564 | lrm: 1.00 | dt: 2588.30ms | tok/sec: 202,560 | mfu: 52.08 | epoch: 1 | total time: 4.52m | eta: 71.9m\n",
      "step 00116/01785 (6.50%) | loss: 4.436216 | lrm: 1.00 | dt: 2583.64ms | tok/sec: 202,926 | mfu: 52.17 | epoch: 1 | total time: 4.56m | eta: 71.9m\n",
      "step 00117/01785 (6.55%) | loss: 4.417821 | lrm: 1.00 | dt: 2583.14ms | tok/sec: 202,965 | mfu: 52.18 | epoch: 1 | total time: 4.61m | eta: 71.8m\n",
      "step 00118/01785 (6.61%) | loss: 4.403728 | lrm: 1.00 | dt: 2582.75ms | tok/sec: 202,995 | mfu: 52.19 | epoch: 1 | total time: 4.65m | eta: 71.8m\n",
      "step 00119/01785 (6.67%) | loss: 4.387175 | lrm: 1.00 | dt: 2582.32ms | tok/sec: 203,029 | mfu: 52.20 | epoch: 1 | total time: 4.69m | eta: 71.7m\n",
      "step 00120/01785 (6.72%) | loss: 4.384733 | lrm: 1.00 | dt: 2583.33ms | tok/sec: 202,950 | mfu: 52.18 | epoch: 1 | total time: 4.74m | eta: 71.7m\n",
      "step 00121/01785 (6.78%) | loss: 4.373265 | lrm: 1.00 | dt: 2586.25ms | tok/sec: 202,721 | mfu: 52.12 | epoch: 1 | total time: 4.78m | eta: 71.6m\n",
      "step 00122/01785 (6.83%) | loss: 4.371981 | lrm: 1.00 | dt: 2588.16ms | tok/sec: 202,571 | mfu: 52.08 | epoch: 1 | total time: 4.82m | eta: 71.6m\n",
      "step 00123/01785 (6.89%) | loss: 4.373618 | lrm: 1.00 | dt: 2591.31ms | tok/sec: 202,325 | mfu: 52.02 | epoch: 1 | total time: 4.87m | eta: 71.6m\n",
      "step 00124/01785 (6.95%) | loss: 4.365823 | lrm: 1.00 | dt: 2586.64ms | tok/sec: 202,690 | mfu: 52.11 | epoch: 1 | total time: 4.91m | eta: 71.5m\n",
      "step 00125/01785 (7.00%) | loss: 4.355609 | lrm: 1.00 | dt: 2588.63ms | tok/sec: 202,535 | mfu: 52.07 | epoch: 1 | total time: 4.95m | eta: 71.5m\n",
      "step 00126/01785 (7.06%) | loss: 4.361061 | lrm: 1.00 | dt: 2585.14ms | tok/sec: 202,808 | mfu: 52.14 | epoch: 1 | total time: 5.00m | eta: 71.4m\n",
      "step 00127/01785 (7.11%) | loss: 4.344442 | lrm: 1.00 | dt: 2586.30ms | tok/sec: 202,717 | mfu: 52.12 | epoch: 1 | total time: 5.04m | eta: 71.4m\n",
      "step 00128/01785 (7.17%) | loss: 4.322094 | lrm: 1.00 | dt: 2588.48ms | tok/sec: 202,546 | mfu: 52.08 | epoch: 1 | total time: 5.08m | eta: 71.4m\n",
      "step 00129/01785 (7.23%) | loss: 4.303194 | lrm: 1.00 | dt: 2596.50ms | tok/sec: 201,921 | mfu: 51.91 | epoch: 1 | total time: 5.12m | eta: 71.3m\n",
      "step 00130/01785 (7.28%) | loss: 4.285953 | lrm: 1.00 | dt: 2587.54ms | tok/sec: 202,620 | mfu: 52.09 | epoch: 1 | total time: 5.17m | eta: 71.3m\n",
      "step 00131/01785 (7.34%) | loss: 4.272466 | lrm: 1.00 | dt: 2586.63ms | tok/sec: 202,691 | mfu: 52.11 | epoch: 1 | total time: 5.21m | eta: 71.2m\n",
      "step 00132/01785 (7.39%) | loss: 4.263689 | lrm: 1.00 | dt: 2602.83ms | tok/sec: 201,429 | mfu: 51.79 | epoch: 1 | total time: 5.25m | eta: 71.2m\n",
      "step 00133/01785 (7.45%) | loss: 4.264160 | lrm: 1.00 | dt: 2587.48ms | tok/sec: 202,625 | mfu: 52.10 | epoch: 1 | total time: 5.30m | eta: 71.1m\n",
      "step 00134/01785 (7.51%) | loss: 4.252122 | lrm: 1.00 | dt: 2597.36ms | tok/sec: 201,854 | mfu: 51.90 | epoch: 1 | total time: 5.34m | eta: 71.1m\n",
      "step 00135/01785 (7.56%) | loss: 4.250729 | lrm: 1.00 | dt: 2584.42ms | tok/sec: 202,864 | mfu: 52.16 | epoch: 1 | total time: 5.38m | eta: 71.1m\n",
      "step 00136/01785 (7.62%) | loss: 4.239899 | lrm: 1.00 | dt: 2594.62ms | tok/sec: 202,067 | mfu: 51.95 | epoch: 1 | total time: 5.43m | eta: 71.0m\n",
      "step 00137/01785 (7.68%) | loss: 4.239503 | lrm: 1.00 | dt: 2588.82ms | tok/sec: 202,520 | mfu: 52.07 | epoch: 1 | total time: 5.47m | eta: 71.0m\n",
      "step 00138/01785 (7.73%) | loss: 4.214988 | lrm: 1.00 | dt: 2584.65ms | tok/sec: 202,846 | mfu: 52.15 | epoch: 1 | total time: 5.51m | eta: 70.9m\n",
      "step 00139/01785 (7.79%) | loss: 4.218720 | lrm: 1.00 | dt: 2600.75ms | tok/sec: 201,591 | mfu: 51.83 | epoch: 1 | total time: 5.56m | eta: 70.9m\n",
      "step 00140/01785 (7.84%) | loss: 4.204035 | lrm: 1.00 | dt: 2590.15ms | tok/sec: 202,415 | mfu: 52.04 | epoch: 1 | total time: 5.60m | eta: 70.9m\n",
      "step 00141/01785 (7.90%) | loss: 4.194938 | lrm: 1.00 | dt: 2592.77ms | tok/sec: 202,211 | mfu: 51.99 | epoch: 1 | total time: 5.64m | eta: 70.8m\n",
      "step 00142/01785 (7.96%) | loss: 4.190222 | lrm: 1.00 | dt: 2585.60ms | tok/sec: 202,772 | mfu: 52.13 | epoch: 1 | total time: 5.69m | eta: 70.8m\n",
      "step 00143/01785 (8.01%) | loss: 4.171272 | lrm: 1.00 | dt: 2592.86ms | tok/sec: 202,204 | mfu: 51.99 | epoch: 1 | total time: 5.73m | eta: 70.7m\n",
      "step 00144/01785 (8.07%) | loss: 4.144896 | lrm: 1.00 | dt: 2589.62ms | tok/sec: 202,457 | mfu: 52.05 | epoch: 1 | total time: 5.77m | eta: 70.7m\n",
      "step 00145/01785 (8.12%) | loss: 4.144062 | lrm: 1.00 | dt: 2583.25ms | tok/sec: 202,956 | mfu: 52.18 | epoch: 1 | total time: 5.82m | eta: 70.6m\n",
      "step 00146/01785 (8.18%) | loss: 4.130175 | lrm: 1.00 | dt: 2584.34ms | tok/sec: 202,871 | mfu: 52.16 | epoch: 1 | total time: 5.86m | eta: 70.6m\n",
      "step 00147/01785 (8.24%) | loss: 4.128980 | lrm: 1.00 | dt: 2602.08ms | tok/sec: 201,488 | mfu: 51.80 | epoch: 1 | total time: 5.90m | eta: 70.6m\n",
      "step 00148/01785 (8.29%) | loss: 4.128746 | lrm: 1.00 | dt: 2583.78ms | tok/sec: 202,915 | mfu: 52.17 | epoch: 1 | total time: 5.94m | eta: 70.5m\n",
      "step 00149/01785 (8.35%) | loss: 4.114894 | lrm: 1.00 | dt: 2587.25ms | tok/sec: 202,642 | mfu: 52.10 | epoch: 1 | total time: 5.99m | eta: 70.5m\n",
      "step 00150/01785 (8.40%) | loss: 4.111080 | lrm: 1.00 | dt: 2584.96ms | tok/sec: 202,822 | mfu: 52.15 | epoch: 1 | total time: 6.03m | eta: 70.4m\n",
      "step 00151/01785 (8.46%) | loss: 4.106003 | lrm: 1.00 | dt: 2585.37ms | tok/sec: 202,790 | mfu: 52.14 | epoch: 1 | total time: 6.07m | eta: 70.4m\n",
      "step 00152/01785 (8.52%) | loss: 4.090130 | lrm: 1.00 | dt: 2589.09ms | tok/sec: 202,498 | mfu: 52.06 | epoch: 1 | total time: 6.12m | eta: 70.3m\n",
      "step 00153/01785 (8.57%) | loss: 4.086474 | lrm: 1.00 | dt: 2586.84ms | tok/sec: 202,675 | mfu: 52.11 | epoch: 1 | total time: 6.16m | eta: 70.3m\n",
      "step 00154/01785 (8.63%) | loss: 4.092785 | lrm: 1.00 | dt: 2591.93ms | tok/sec: 202,277 | mfu: 52.01 | epoch: 1 | total time: 6.20m | eta: 70.3m\n",
      "step 00155/01785 (8.68%) | loss: 4.076225 | lrm: 1.00 | dt: 2588.06ms | tok/sec: 202,579 | mfu: 52.08 | epoch: 1 | total time: 6.25m | eta: 70.2m\n",
      "step 00156/01785 (8.74%) | loss: 4.075940 | lrm: 1.00 | dt: 2585.59ms | tok/sec: 202,772 | mfu: 52.13 | epoch: 1 | total time: 6.29m | eta: 70.2m\n",
      "step 00157/01785 (8.80%) | loss: 4.066747 | lrm: 1.00 | dt: 2599.75ms | tok/sec: 201,668 | mfu: 51.85 | epoch: 1 | total time: 6.33m | eta: 70.1m\n",
      "step 00158/01785 (8.85%) | loss: 4.055088 | lrm: 1.00 | dt: 2583.95ms | tok/sec: 202,902 | mfu: 52.17 | epoch: 1 | total time: 6.38m | eta: 70.1m\n",
      "step 00159/01785 (8.91%) | loss: 4.050436 | lrm: 1.00 | dt: 2601.60ms | tok/sec: 201,525 | mfu: 51.81 | epoch: 1 | total time: 6.42m | eta: 70.1m\n",
      "step 00160/01785 (8.96%) | loss: 4.037744 | lrm: 1.00 | dt: 2585.66ms | tok/sec: 202,767 | mfu: 52.13 | epoch: 1 | total time: 6.46m | eta: 70.0m\n",
      "step 00161/01785 (9.02%) | loss: 4.036841 | lrm: 1.00 | dt: 2600.46ms | tok/sec: 201,613 | mfu: 51.84 | epoch: 1 | total time: 6.51m | eta: 70.0m\n",
      "step 00162/01785 (9.08%) | loss: 4.029912 | lrm: 1.00 | dt: 2585.54ms | tok/sec: 202,776 | mfu: 52.13 | epoch: 1 | total time: 6.55m | eta: 69.9m\n",
      "step 00163/01785 (9.13%) | loss: 4.021831 | lrm: 1.00 | dt: 2600.96ms | tok/sec: 201,574 | mfu: 51.83 | epoch: 1 | total time: 6.59m | eta: 69.9m\n",
      "step 00164/01785 (9.19%) | loss: 4.006150 | lrm: 1.00 | dt: 2584.47ms | tok/sec: 202,861 | mfu: 52.16 | epoch: 1 | total time: 6.64m | eta: 69.8m\n",
      "step 00165/01785 (9.24%) | loss: 4.003024 | lrm: 1.00 | dt: 2600.65ms | tok/sec: 201,598 | mfu: 51.83 | epoch: 1 | total time: 6.68m | eta: 69.8m\n",
      "step 00166/01785 (9.30%) | loss: 3.986605 | lrm: 1.00 | dt: 2587.68ms | tok/sec: 202,609 | mfu: 52.09 | epoch: 1 | total time: 6.72m | eta: 69.8m\n",
      "step 00167/01785 (9.36%) | loss: 3.985627 | lrm: 1.00 | dt: 2599.95ms | tok/sec: 201,653 | mfu: 51.85 | epoch: 1 | total time: 6.77m | eta: 69.7m\n",
      "step 00168/01785 (9.41%) | loss: 3.981893 | lrm: 1.00 | dt: 2589.19ms | tok/sec: 202,490 | mfu: 52.06 | epoch: 1 | total time: 6.81m | eta: 69.7m\n",
      "step 00169/01785 (9.47%) | loss: 3.970491 | lrm: 1.00 | dt: 2595.81ms | tok/sec: 201,974 | mfu: 51.93 | epoch: 1 | total time: 6.85m | eta: 69.6m\n",
      "step 00170/01785 (9.52%) | loss: 3.958958 | lrm: 1.00 | dt: 2584.01ms | tok/sec: 202,897 | mfu: 52.17 | epoch: 1 | total time: 6.89m | eta: 69.6m\n",
      "step 00171/01785 (9.58%) | loss: 3.944698 | lrm: 1.00 | dt: 2599.92ms | tok/sec: 201,655 | mfu: 51.85 | epoch: 1 | total time: 6.94m | eta: 69.6m\n",
      "step 00172/01785 (9.64%) | loss: 3.933129 | lrm: 1.00 | dt: 2587.21ms | tok/sec: 202,645 | mfu: 52.10 | epoch: 1 | total time: 6.98m | eta: 69.5m\n",
      "step 00173/01785 (9.69%) | loss: 3.917974 | lrm: 1.00 | dt: 2593.60ms | tok/sec: 202,146 | mfu: 51.97 | epoch: 1 | total time: 7.02m | eta: 69.5m\n",
      "step 00174/01785 (9.75%) | loss: 3.920909 | lrm: 1.00 | dt: 2591.33ms | tok/sec: 202,324 | mfu: 52.02 | epoch: 1 | total time: 7.07m | eta: 69.4m\n",
      "step 00175/01785 (9.80%) | loss: 3.922689 | lrm: 1.00 | dt: 2585.25ms | tok/sec: 202,799 | mfu: 52.14 | epoch: 1 | total time: 7.11m | eta: 69.4m\n",
      "step 00176/01785 (9.86%) | loss: 3.909105 | lrm: 1.00 | dt: 2588.19ms | tok/sec: 202,569 | mfu: 52.08 | epoch: 1 | total time: 7.15m | eta: 69.3m\n",
      "step 00177/01785 (9.92%) | loss: 3.907668 | lrm: 1.00 | dt: 2588.02ms | tok/sec: 202,582 | mfu: 52.08 | epoch: 1 | total time: 7.20m | eta: 69.3m\n",
      "step 00178/01785 (9.97%) | loss: 3.916631 | lrm: 1.00 | dt: 2591.48ms | tok/sec: 202,311 | mfu: 52.02 | epoch: 1 | total time: 7.24m | eta: 69.3m\n",
      "step 00179/01785 (10.03%) | loss: 3.917109 | lrm: 1.00 | dt: 2592.22ms | tok/sec: 202,254 | mfu: 52.00 | epoch: 1 | total time: 7.28m | eta: 69.2m\n",
      "step 00180/01785 (10.08%) | loss: 3.898302 | lrm: 1.00 | dt: 2599.74ms | tok/sec: 201,669 | mfu: 51.85 | epoch: 1 | total time: 7.33m | eta: 69.2m\n",
      "step 00181/01785 (10.14%) | loss: 3.893646 | lrm: 1.00 | dt: 2588.34ms | tok/sec: 202,557 | mfu: 52.08 | epoch: 1 | total time: 7.37m | eta: 69.1m\n",
      "step 00182/01785 (10.20%) | loss: 3.895039 | lrm: 1.00 | dt: 2592.28ms | tok/sec: 202,249 | mfu: 52.00 | epoch: 1 | total time: 7.41m | eta: 69.1m\n",
      "step 00183/01785 (10.25%) | loss: 3.901732 | lrm: 1.00 | dt: 2593.61ms | tok/sec: 202,146 | mfu: 51.97 | epoch: 1 | total time: 7.46m | eta: 69.0m\n",
      "step 00184/01785 (10.31%) | loss: 3.903900 | lrm: 1.00 | dt: 2587.17ms | tok/sec: 202,649 | mfu: 52.10 | epoch: 1 | total time: 7.50m | eta: 69.0m\n",
      "step 00185/01785 (10.36%) | loss: 3.902633 | lrm: 1.00 | dt: 2599.38ms | tok/sec: 201,697 | mfu: 51.86 | epoch: 1 | total time: 7.54m | eta: 69.0m\n",
      "step 00186/01785 (10.42%) | loss: 3.891586 | lrm: 1.00 | dt: 2584.99ms | tok/sec: 202,820 | mfu: 52.15 | epoch: 1 | total time: 7.59m | eta: 68.9m\n",
      "step 00187/01785 (10.48%) | loss: 3.882958 | lrm: 1.00 | dt: 2602.33ms | tok/sec: 201,468 | mfu: 51.80 | epoch: 1 | total time: 7.63m | eta: 68.9m\n",
      "step 00188/01785 (10.53%) | loss: 3.876473 | lrm: 1.00 | dt: 2596.04ms | tok/sec: 201,956 | mfu: 51.92 | epoch: 1 | total time: 7.67m | eta: 68.8m\n",
      "step 00189/01785 (10.59%) | loss: 3.872463 | lrm: 1.00 | dt: 2589.12ms | tok/sec: 202,496 | mfu: 52.06 | epoch: 1 | total time: 7.72m | eta: 68.8m\n",
      "step 00190/01785 (10.64%) | loss: 3.861645 | lrm: 1.00 | dt: 2603.98ms | tok/sec: 201,341 | mfu: 51.77 | epoch: 1 | total time: 7.76m | eta: 68.8m\n",
      "step 00191/01785 (10.70%) | loss: 3.845007 | lrm: 1.00 | dt: 2603.88ms | tok/sec: 201,348 | mfu: 51.77 | epoch: 1 | total time: 7.80m | eta: 68.7m\n",
      "step 00192/01785 (10.76%) | loss: 3.851364 | lrm: 1.00 | dt: 2603.58ms | tok/sec: 201,372 | mfu: 51.77 | epoch: 1 | total time: 7.85m | eta: 68.7m\n",
      "step 00193/01785 (10.81%) | loss: 3.832876 | lrm: 1.00 | dt: 2603.42ms | tok/sec: 201,384 | mfu: 51.78 | epoch: 1 | total time: 7.89m | eta: 68.6m\n",
      "step 00194/01785 (10.87%) | loss: 3.837819 | lrm: 1.00 | dt: 2586.93ms | tok/sec: 202,668 | mfu: 52.11 | epoch: 1 | total time: 7.93m | eta: 68.6m\n",
      "step 00195/01785 (10.92%) | loss: 3.840974 | lrm: 1.00 | dt: 2600.98ms | tok/sec: 201,572 | mfu: 51.83 | epoch: 1 | total time: 7.98m | eta: 68.5m\n",
      "step 00196/01785 (10.98%) | loss: 3.839834 | lrm: 1.00 | dt: 2587.57ms | tok/sec: 202,617 | mfu: 52.09 | epoch: 1 | total time: 8.02m | eta: 68.5m\n",
      "step 00197/01785 (11.04%) | loss: 3.832844 | lrm: 1.00 | dt: 2597.78ms | tok/sec: 201,821 | mfu: 51.89 | epoch: 1 | total time: 8.06m | eta: 68.5m\n",
      "step 00198/01785 (11.09%) | loss: 3.835610 | lrm: 1.00 | dt: 2602.08ms | tok/sec: 201,488 | mfu: 51.80 | epoch: 1 | total time: 8.11m | eta: 68.4m\n",
      "step 00199/01785 (11.15%) | loss: 3.831929 | lrm: 1.00 | dt: 2586.53ms | tok/sec: 202,699 | mfu: 52.12 | epoch: 1 | total time: 8.15m | eta: 68.4m\n",
      "step 00200/01785 (11.20%) | loss: 3.830843 | lrm: 1.00 | dt: 2605.79ms | tok/sec: 201,201 | mfu: 51.73 | epoch: 1 | total time: 8.19m | eta: 68.3m\n",
      "step 00201/01785 (11.26%) | loss: 3.818858 | lrm: 1.00 | dt: 2594.94ms | tok/sec: 202,042 | mfu: 51.95 | epoch: 1 | total time: 8.24m | eta: 68.3m\n",
      "step 00202/01785 (11.32%) | loss: 3.835175 | lrm: 1.00 | dt: 2593.46ms | tok/sec: 202,157 | mfu: 51.98 | epoch: 1 | total time: 8.28m | eta: 68.3m\n",
      "step 00203/01785 (11.37%) | loss: 3.830859 | lrm: 1.00 | dt: 2590.95ms | tok/sec: 202,353 | mfu: 52.03 | epoch: 1 | total time: 8.32m | eta: 68.2m\n",
      "step 00204/01785 (11.43%) | loss: 3.835475 | lrm: 1.00 | dt: 2590.55ms | tok/sec: 202,384 | mfu: 52.03 | epoch: 1 | total time: 8.36m | eta: 68.2m\n",
      "step 00205/01785 (11.48%) | loss: 3.826834 | lrm: 1.00 | dt: 2591.27ms | tok/sec: 202,328 | mfu: 52.02 | epoch: 1 | total time: 8.41m | eta: 68.1m\n",
      "step 00206/01785 (11.54%) | loss: 3.821679 | lrm: 1.00 | dt: 2588.47ms | tok/sec: 202,547 | mfu: 52.08 | epoch: 1 | total time: 8.45m | eta: 68.1m\n",
      "step 00207/01785 (11.60%) | loss: 3.808963 | lrm: 1.00 | dt: 2604.75ms | tok/sec: 201,281 | mfu: 51.75 | epoch: 1 | total time: 8.49m | eta: 68.0m\n",
      "step 00208/01785 (11.65%) | loss: 3.816117 | lrm: 1.00 | dt: 2589.44ms | tok/sec: 202,471 | mfu: 52.06 | epoch: 1 | total time: 8.54m | eta: 68.0m\n",
      "step 00209/01785 (11.71%) | loss: 3.805729 | lrm: 1.00 | dt: 2596.90ms | tok/sec: 201,890 | mfu: 51.91 | epoch: 1 | total time: 8.58m | eta: 68.0m\n",
      "step 00210/01785 (11.76%) | loss: 3.786439 | lrm: 1.00 | dt: 2586.71ms | tok/sec: 202,685 | mfu: 52.11 | epoch: 1 | total time: 8.62m | eta: 67.9m\n",
      "step 00211/01785 (11.82%) | loss: 3.790093 | lrm: 1.00 | dt: 2599.21ms | tok/sec: 201,710 | mfu: 51.86 | epoch: 1 | total time: 8.67m | eta: 67.9m\n",
      "step 00212/01785 (11.88%) | loss: 3.795103 | lrm: 1.00 | dt: 2585.35ms | tok/sec: 202,791 | mfu: 52.14 | epoch: 1 | total time: 8.71m | eta: 67.8m\n",
      "step 00213/01785 (11.93%) | loss: 3.797017 | lrm: 1.00 | dt: 2601.61ms | tok/sec: 201,524 | mfu: 51.81 | epoch: 1 | total time: 8.75m | eta: 67.8m\n",
      "step 00214/01785 (11.99%) | loss: 3.790109 | lrm: 1.00 | dt: 2588.68ms | tok/sec: 202,531 | mfu: 52.07 | epoch: 1 | total time: 8.80m | eta: 67.7m\n",
      "step 00215/01785 (12.04%) | loss: 3.793866 | lrm: 1.00 | dt: 2596.36ms | tok/sec: 201,931 | mfu: 51.92 | epoch: 1 | total time: 8.84m | eta: 67.7m\n",
      "step 00216/01785 (12.10%) | loss: 3.789805 | lrm: 1.00 | dt: 2587.05ms | tok/sec: 202,658 | mfu: 52.10 | epoch: 1 | total time: 8.88m | eta: 67.7m\n",
      "step 00217/01785 (12.16%) | loss: 3.783617 | lrm: 1.00 | dt: 2598.26ms | tok/sec: 201,784 | mfu: 51.88 | epoch: 1 | total time: 8.93m | eta: 67.6m\n",
      "step 00218/01785 (12.21%) | loss: 3.774809 | lrm: 1.00 | dt: 2589.07ms | tok/sec: 202,500 | mfu: 52.06 | epoch: 1 | total time: 8.97m | eta: 67.6m\n",
      "step 00219/01785 (12.27%) | loss: 3.765117 | lrm: 1.00 | dt: 2596.49ms | tok/sec: 201,922 | mfu: 51.92 | epoch: 1 | total time: 9.01m | eta: 67.5m\n",
      "step 00220/01785 (12.32%) | loss: 3.764761 | lrm: 1.00 | dt: 2584.44ms | tok/sec: 202,863 | mfu: 52.16 | epoch: 1 | total time: 9.06m | eta: 67.5m\n",
      "step 00221/01785 (12.38%) | loss: 3.768196 | lrm: 1.00 | dt: 2604.01ms | tok/sec: 201,338 | mfu: 51.77 | epoch: 1 | total time: 9.10m | eta: 67.4m\n",
      "step 00222/01785 (12.44%) | loss: 3.760856 | lrm: 1.00 | dt: 2603.45ms | tok/sec: 201,381 | mfu: 51.78 | epoch: 1 | total time: 9.14m | eta: 67.4m\n",
      "step 00223/01785 (12.49%) | loss: 3.760449 | lrm: 1.00 | dt: 2592.39ms | tok/sec: 202,240 | mfu: 52.00 | epoch: 1 | total time: 9.19m | eta: 67.4m\n",
      "step 00224/01785 (12.55%) | loss: 3.760503 | lrm: 1.00 | dt: 2593.70ms | tok/sec: 202,139 | mfu: 51.97 | epoch: 1 | total time: 9.23m | eta: 67.3m\n",
      "step 00225/01785 (12.61%) | loss: 3.753462 | lrm: 1.00 | dt: 2603.53ms | tok/sec: 201,375 | mfu: 51.77 | epoch: 1 | total time: 9.27m | eta: 67.3m\n",
      "step 00226/01785 (12.66%) | loss: 3.777864 | lrm: 1.00 | dt: 2585.58ms | tok/sec: 202,774 | mfu: 52.13 | epoch: 1 | total time: 9.32m | eta: 67.2m\n",
      "step 00227/01785 (12.72%) | loss: 3.770828 | lrm: 1.00 | dt: 2586.65ms | tok/sec: 202,690 | mfu: 52.11 | epoch: 1 | total time: 9.36m | eta: 67.2m\n",
      "step 00228/01785 (12.77%) | loss: 3.767684 | lrm: 1.00 | dt: 2586.92ms | tok/sec: 202,668 | mfu: 52.11 | epoch: 1 | total time: 9.40m | eta: 67.2m\n",
      "step 00229/01785 (12.83%) | loss: 3.777626 | lrm: 1.00 | dt: 2605.29ms | tok/sec: 201,239 | mfu: 51.74 | epoch: 1 | total time: 9.45m | eta: 67.1m\n",
      "step 00230/01785 (12.89%) | loss: 3.768181 | lrm: 1.00 | dt: 2594.36ms | tok/sec: 202,087 | mfu: 51.96 | epoch: 1 | total time: 9.49m | eta: 67.1m\n",
      "step 00231/01785 (12.94%) | loss: 3.760539 | lrm: 1.00 | dt: 2589.75ms | tok/sec: 202,447 | mfu: 52.05 | epoch: 1 | total time: 9.53m | eta: 67.0m\n",
      "step 00232/01785 (13.00%) | loss: 3.762359 | lrm: 1.00 | dt: 2587.99ms | tok/sec: 202,584 | mfu: 52.09 | epoch: 1 | total time: 9.58m | eta: 67.0m\n",
      "step 00233/01785 (13.05%) | loss: 3.768991 | lrm: 1.00 | dt: 2590.78ms | tok/sec: 202,367 | mfu: 52.03 | epoch: 1 | total time: 9.62m | eta: 66.9m\n",
      "step 00234/01785 (13.11%) | loss: 3.774165 | lrm: 1.00 | dt: 2596.80ms | tok/sec: 201,897 | mfu: 51.91 | epoch: 1 | total time: 9.66m | eta: 66.9m\n",
      "step 00235/01785 (13.17%) | loss: 3.782130 | lrm: 1.00 | dt: 2604.83ms | tok/sec: 201,275 | mfu: 51.75 | epoch: 1 | total time: 9.71m | eta: 66.9m\n",
      "step 00236/01785 (13.22%) | loss: 3.785241 | lrm: 1.00 | dt: 2603.24ms | tok/sec: 201,398 | mfu: 51.78 | epoch: 1 | total time: 9.75m | eta: 66.8m\n",
      "step 00237/01785 (13.28%) | loss: 3.783732 | lrm: 1.00 | dt: 2587.35ms | tok/sec: 202,635 | mfu: 52.10 | epoch: 1 | total time: 9.79m | eta: 66.8m\n",
      "step 00238/01785 (13.33%) | loss: 3.778838 | lrm: 1.00 | dt: 2598.27ms | tok/sec: 201,783 | mfu: 51.88 | epoch: 1 | total time: 9.83m | eta: 66.7m\n",
      "step 00239/01785 (13.39%) | loss: 3.772228 | lrm: 1.00 | dt: 2587.26ms | tok/sec: 202,642 | mfu: 52.10 | epoch: 1 | total time: 9.88m | eta: 66.7m\n",
      "step 00240/01785 (13.45%) | loss: 3.752929 | lrm: 1.00 | dt: 2598.36ms | tok/sec: 201,776 | mfu: 51.88 | epoch: 1 | total time: 9.92m | eta: 66.6m\n",
      "step 00241/01785 (13.50%) | loss: 3.754399 | lrm: 1.00 | dt: 2584.31ms | tok/sec: 202,873 | mfu: 52.16 | epoch: 1 | total time: 9.96m | eta: 66.6m\n",
      "step 00242/01785 (13.56%) | loss: 3.764823 | lrm: 1.00 | dt: 2594.40ms | tok/sec: 202,084 | mfu: 51.96 | epoch: 1 | total time: 10.01m | eta: 66.6m\n",
      "step 00243/01785 (13.61%) | loss: 3.752596 | lrm: 1.00 | dt: 2586.55ms | tok/sec: 202,697 | mfu: 52.11 | epoch: 1 | total time: 10.05m | eta: 66.5m\n",
      "step 00244/01785 (13.67%) | loss: 3.736374 | lrm: 1.00 | dt: 2585.30ms | tok/sec: 202,796 | mfu: 52.14 | epoch: 1 | total time: 10.09m | eta: 66.5m\n",
      "step 00245/01785 (13.73%) | loss: 3.733748 | lrm: 1.00 | dt: 2601.25ms | tok/sec: 201,552 | mfu: 51.82 | epoch: 1 | total time: 10.14m | eta: 66.4m\n",
      "step 00246/01785 (13.78%) | loss: 3.722919 | lrm: 1.00 | dt: 2584.23ms | tok/sec: 202,879 | mfu: 52.16 | epoch: 1 | total time: 10.18m | eta: 66.4m\n",
      "step 00247/01785 (13.84%) | loss: 3.713028 | lrm: 1.00 | dt: 2586.44ms | tok/sec: 202,706 | mfu: 52.12 | epoch: 1 | total time: 10.22m | eta: 66.3m\n",
      "step 00248/01785 (13.89%) | loss: 3.713734 | lrm: 1.00 | dt: 2592.48ms | tok/sec: 202,234 | mfu: 52.00 | epoch: 1 | total time: 10.27m | eta: 66.3m\n",
      "step 00249/01785 (13.95%) | loss: 3.699923 | lrm: 1.00 | dt: 2585.80ms | tok/sec: 202,756 | mfu: 52.13 | epoch: 1 | total time: 10.31m | eta: 66.3m\n",
      "Step 00250 | Validation bpb: 1.131198\n",
      "step 00250/01785 (14.01%) | loss: 3.704363 | lrm: 1.00 | dt: 2594.43ms | tok/sec: 202,082 | mfu: 51.96 | epoch: 1 | total time: 10.35m | eta: 66.2m\n",
      "step 00251/01785 (14.06%) | loss: 3.706807 | lrm: 1.00 | dt: 2584.64ms | tok/sec: 202,847 | mfu: 52.15 | epoch: 1 | total time: 10.40m | eta: 66.2m\n",
      "step 00252/01785 (14.12%) | loss: 3.702364 | lrm: 1.00 | dt: 2603.97ms | tok/sec: 201,341 | mfu: 51.77 | epoch: 1 | total time: 10.44m | eta: 66.1m\n",
      "step 00253/01785 (14.17%) | loss: 3.702957 | lrm: 1.00 | dt: 2586.17ms | tok/sec: 202,727 | mfu: 52.12 | epoch: 1 | total time: 10.48m | eta: 66.1m\n",
      "step 00254/01785 (14.23%) | loss: 3.725752 | lrm: 1.00 | dt: 2604.01ms | tok/sec: 201,338 | mfu: 51.77 | epoch: 1 | total time: 10.53m | eta: 66.0m\n",
      "step 00255/01785 (14.29%) | loss: 3.723825 | lrm: 1.00 | dt: 2603.35ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 1 | total time: 10.57m | eta: 66.0m\n",
      "step 00256/01785 (14.34%) | loss: 3.725100 | lrm: 1.00 | dt: 2604.74ms | tok/sec: 201,282 | mfu: 51.75 | epoch: 1 | total time: 10.61m | eta: 66.0m\n",
      "step 00257/01785 (14.40%) | loss: 3.730336 | lrm: 1.00 | dt: 2589.75ms | tok/sec: 202,447 | mfu: 52.05 | epoch: 1 | total time: 10.66m | eta: 65.9m\n",
      "step 00258/01785 (14.45%) | loss: 3.715026 | lrm: 1.00 | dt: 2598.90ms | tok/sec: 201,734 | mfu: 51.87 | epoch: 1 | total time: 10.70m | eta: 65.9m\n",
      "step 00259/01785 (14.51%) | loss: 3.711724 | lrm: 1.00 | dt: 2587.47ms | tok/sec: 202,625 | mfu: 52.10 | epoch: 1 | total time: 10.74m | eta: 65.8m\n",
      "step 00260/01785 (14.57%) | loss: 3.704000 | lrm: 1.00 | dt: 2603.92ms | tok/sec: 201,345 | mfu: 51.77 | epoch: 1 | total time: 10.79m | eta: 65.8m\n",
      "step 00261/01785 (14.62%) | loss: 3.707206 | lrm: 1.00 | dt: 2588.17ms | tok/sec: 202,571 | mfu: 52.08 | epoch: 1 | total time: 10.83m | eta: 65.7m\n",
      "step 00262/01785 (14.68%) | loss: 3.708738 | lrm: 1.00 | dt: 2593.16ms | tok/sec: 202,180 | mfu: 51.98 | epoch: 1 | total time: 10.87m | eta: 65.7m\n",
      "step 00263/01785 (14.73%) | loss: 3.702866 | lrm: 1.00 | dt: 2592.05ms | tok/sec: 202,267 | mfu: 52.00 | epoch: 1 | total time: 10.92m | eta: 65.7m\n",
      "step 00264/01785 (14.79%) | loss: 3.711291 | lrm: 1.00 | dt: 2590.28ms | tok/sec: 202,406 | mfu: 52.04 | epoch: 1 | total time: 10.96m | eta: 65.6m\n",
      "step 00265/01785 (14.85%) | loss: 3.708531 | lrm: 1.00 | dt: 2606.58ms | tok/sec: 201,140 | mfu: 51.71 | epoch: 1 | total time: 11.00m | eta: 65.6m\n",
      "step 00266/01785 (14.90%) | loss: 3.703061 | lrm: 1.00 | dt: 2605.89ms | tok/sec: 201,193 | mfu: 51.73 | epoch: 1 | total time: 11.05m | eta: 65.5m\n",
      "step 00267/01785 (14.96%) | loss: 3.711452 | lrm: 1.00 | dt: 2603.76ms | tok/sec: 201,358 | mfu: 51.77 | epoch: 1 | total time: 11.09m | eta: 65.5m\n",
      "step 00268/01785 (15.01%) | loss: 3.707361 | lrm: 1.00 | dt: 2604.11ms | tok/sec: 201,330 | mfu: 51.76 | epoch: 1 | total time: 11.13m | eta: 65.5m\n",
      "step 00269/01785 (15.07%) | loss: 3.705603 | lrm: 1.00 | dt: 2597.90ms | tok/sec: 201,811 | mfu: 51.89 | epoch: 1 | total time: 11.18m | eta: 65.4m\n",
      "step 00270/01785 (15.13%) | loss: 3.702538 | lrm: 1.00 | dt: 2590.79ms | tok/sec: 202,366 | mfu: 52.03 | epoch: 1 | total time: 11.22m | eta: 65.4m\n",
      "step 00271/01785 (15.18%) | loss: 3.696118 | lrm: 1.00 | dt: 2604.19ms | tok/sec: 201,324 | mfu: 51.76 | epoch: 1 | total time: 11.26m | eta: 65.3m\n",
      "step 00272/01785 (15.24%) | loss: 3.691361 | lrm: 1.00 | dt: 2604.76ms | tok/sec: 201,280 | mfu: 51.75 | epoch: 1 | total time: 11.31m | eta: 65.3m\n",
      "step 00273/01785 (15.29%) | loss: 3.677887 | lrm: 1.00 | dt: 2604.25ms | tok/sec: 201,320 | mfu: 51.76 | epoch: 1 | total time: 11.35m | eta: 65.2m\n",
      "step 00274/01785 (15.35%) | loss: 3.669924 | lrm: 1.00 | dt: 2605.02ms | tok/sec: 201,260 | mfu: 51.75 | epoch: 1 | total time: 11.39m | eta: 65.2m\n",
      "step 00275/01785 (15.41%) | loss: 3.654671 | lrm: 1.00 | dt: 2604.21ms | tok/sec: 201,323 | mfu: 51.76 | epoch: 1 | total time: 11.44m | eta: 65.2m\n",
      "step 00276/01785 (15.46%) | loss: 3.660504 | lrm: 1.00 | dt: 2603.62ms | tok/sec: 201,368 | mfu: 51.77 | epoch: 1 | total time: 11.48m | eta: 65.1m\n",
      "step 00277/01785 (15.52%) | loss: 3.670878 | lrm: 1.00 | dt: 2603.40ms | tok/sec: 201,385 | mfu: 51.78 | epoch: 1 | total time: 11.52m | eta: 65.1m\n",
      "step 00278/01785 (15.57%) | loss: 3.667497 | lrm: 1.00 | dt: 2604.05ms | tok/sec: 201,335 | mfu: 51.76 | epoch: 1 | total time: 11.57m | eta: 65.0m\n",
      "step 00279/01785 (15.63%) | loss: 3.681468 | lrm: 1.00 | dt: 2604.62ms | tok/sec: 201,291 | mfu: 51.75 | epoch: 1 | total time: 11.61m | eta: 65.0m\n",
      "step 00280/01785 (15.69%) | loss: 3.681955 | lrm: 1.00 | dt: 2603.54ms | tok/sec: 201,375 | mfu: 51.77 | epoch: 1 | total time: 11.65m | eta: 65.0m\n",
      "step 00281/01785 (15.74%) | loss: 3.681632 | lrm: 1.00 | dt: 2605.51ms | tok/sec: 201,222 | mfu: 51.74 | epoch: 1 | total time: 11.70m | eta: 64.9m\n",
      "step 00282/01785 (15.80%) | loss: 3.678052 | lrm: 1.00 | dt: 2605.76ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 1 | total time: 11.74m | eta: 64.9m\n",
      "step 00283/01785 (15.85%) | loss: 3.667843 | lrm: 1.00 | dt: 2605.37ms | tok/sec: 201,233 | mfu: 51.74 | epoch: 1 | total time: 11.78m | eta: 64.8m\n",
      "step 00284/01785 (15.91%) | loss: 3.670464 | lrm: 1.00 | dt: 2607.87ms | tok/sec: 201,040 | mfu: 51.69 | epoch: 1 | total time: 11.83m | eta: 64.8m\n",
      "step 00285/01785 (15.97%) | loss: 3.662357 | lrm: 1.00 | dt: 2608.90ms | tok/sec: 200,960 | mfu: 51.67 | epoch: 1 | total time: 11.87m | eta: 64.7m\n",
      "step 00286/01785 (16.02%) | loss: 3.665272 | lrm: 1.00 | dt: 2609.22ms | tok/sec: 200,936 | mfu: 51.66 | epoch: 1 | total time: 11.91m | eta: 64.7m\n",
      "step 00287/01785 (16.08%) | loss: 3.675941 | lrm: 1.00 | dt: 2608.76ms | tok/sec: 200,972 | mfu: 51.67 | epoch: 1 | total time: 11.96m | eta: 64.7m\n",
      "step 00288/01785 (16.13%) | loss: 3.665037 | lrm: 1.00 | dt: 2611.89ms | tok/sec: 200,731 | mfu: 51.61 | epoch: 1 | total time: 12.00m | eta: 64.6m\n",
      "step 00289/01785 (16.19%) | loss: 3.663091 | lrm: 1.00 | dt: 2595.70ms | tok/sec: 201,983 | mfu: 51.93 | epoch: 1 | total time: 12.04m | eta: 64.6m\n",
      "step 00290/01785 (16.25%) | loss: 3.664145 | lrm: 1.00 | dt: 2594.62ms | tok/sec: 202,067 | mfu: 51.95 | epoch: 1 | total time: 12.09m | eta: 64.5m\n",
      "step 00291/01785 (16.30%) | loss: 3.659361 | lrm: 1.00 | dt: 2603.25ms | tok/sec: 201,397 | mfu: 51.78 | epoch: 1 | total time: 12.13m | eta: 64.5m\n",
      "step 00292/01785 (16.36%) | loss: 3.652521 | lrm: 1.00 | dt: 2603.61ms | tok/sec: 201,369 | mfu: 51.77 | epoch: 1 | total time: 12.17m | eta: 64.5m\n",
      "step 00293/01785 (16.41%) | loss: 3.643827 | lrm: 1.00 | dt: 2605.09ms | tok/sec: 201,255 | mfu: 51.74 | epoch: 1 | total time: 12.22m | eta: 64.4m\n",
      "step 00294/01785 (16.47%) | loss: 3.648678 | lrm: 1.00 | dt: 2596.73ms | tok/sec: 201,903 | mfu: 51.91 | epoch: 1 | total time: 12.26m | eta: 64.4m\n",
      "step 00295/01785 (16.53%) | loss: 3.655162 | lrm: 1.00 | dt: 2589.27ms | tok/sec: 202,485 | mfu: 52.06 | epoch: 1 | total time: 12.30m | eta: 64.3m\n",
      "step 00296/01785 (16.58%) | loss: 3.645835 | lrm: 1.00 | dt: 2603.21ms | tok/sec: 201,400 | mfu: 51.78 | epoch: 1 | total time: 12.35m | eta: 64.3m\n",
      "step 00297/01785 (16.64%) | loss: 3.643449 | lrm: 1.00 | dt: 2603.57ms | tok/sec: 201,372 | mfu: 51.77 | epoch: 1 | total time: 12.39m | eta: 64.2m\n",
      "step 00298/01785 (16.69%) | loss: 3.639658 | lrm: 1.00 | dt: 2594.98ms | tok/sec: 202,039 | mfu: 51.95 | epoch: 1 | total time: 12.43m | eta: 64.2m\n",
      "step 00299/01785 (16.75%) | loss: 3.649420 | lrm: 1.00 | dt: 2593.90ms | tok/sec: 202,123 | mfu: 51.97 | epoch: 1 | total time: 12.48m | eta: 64.2m\n",
      "step 00300/01785 (16.81%) | loss: 3.645430 | lrm: 1.00 | dt: 2603.20ms | tok/sec: 201,401 | mfu: 51.78 | epoch: 1 | total time: 12.52m | eta: 64.1m\n",
      "step 00301/01785 (16.86%) | loss: 3.664478 | lrm: 1.00 | dt: 2603.12ms | tok/sec: 201,407 | mfu: 51.78 | epoch: 1 | total time: 12.56m | eta: 64.1m\n",
      "step 00302/01785 (16.92%) | loss: 3.649698 | lrm: 1.00 | dt: 2603.60ms | tok/sec: 201,370 | mfu: 51.77 | epoch: 1 | total time: 12.61m | eta: 64.0m\n",
      "step 00303/01785 (16.97%) | loss: 3.657060 | lrm: 1.00 | dt: 2596.18ms | tok/sec: 201,946 | mfu: 51.92 | epoch: 1 | total time: 12.65m | eta: 64.0m\n",
      "step 00304/01785 (17.03%) | loss: 3.655262 | lrm: 1.00 | dt: 2588.90ms | tok/sec: 202,513 | mfu: 52.07 | epoch: 1 | total time: 12.69m | eta: 63.9m\n",
      "step 00305/01785 (17.09%) | loss: 3.646882 | lrm: 1.00 | dt: 2605.35ms | tok/sec: 201,235 | mfu: 51.74 | epoch: 1 | total time: 12.74m | eta: 63.9m\n",
      "step 00306/01785 (17.14%) | loss: 3.650931 | lrm: 1.00 | dt: 2602.73ms | tok/sec: 201,437 | mfu: 51.79 | epoch: 1 | total time: 12.78m | eta: 63.9m\n",
      "step 00307/01785 (17.20%) | loss: 3.647180 | lrm: 1.00 | dt: 2594.88ms | tok/sec: 202,047 | mfu: 51.95 | epoch: 1 | total time: 12.82m | eta: 63.8m\n",
      "step 00308/01785 (17.25%) | loss: 3.655224 | lrm: 1.00 | dt: 2591.83ms | tok/sec: 202,284 | mfu: 52.01 | epoch: 1 | total time: 12.87m | eta: 63.8m\n",
      "step 00309/01785 (17.31%) | loss: 3.651454 | lrm: 1.00 | dt: 2604.83ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 12.91m | eta: 63.7m\n",
      "step 00310/01785 (17.37%) | loss: 3.644303 | lrm: 1.00 | dt: 2607.08ms | tok/sec: 201,101 | mfu: 51.70 | epoch: 1 | total time: 12.95m | eta: 63.7m\n",
      "step 00311/01785 (17.42%) | loss: 3.637146 | lrm: 1.00 | dt: 2594.40ms | tok/sec: 202,084 | mfu: 51.96 | epoch: 1 | total time: 13.00m | eta: 63.6m\n",
      "step 00312/01785 (17.48%) | loss: 3.627315 | lrm: 1.00 | dt: 2596.79ms | tok/sec: 201,898 | mfu: 51.91 | epoch: 1 | total time: 13.04m | eta: 63.6m\n",
      "step 00313/01785 (17.54%) | loss: 3.629573 | lrm: 1.00 | dt: 2592.70ms | tok/sec: 202,216 | mfu: 51.99 | epoch: 1 | total time: 13.08m | eta: 63.6m\n",
      "step 00314/01785 (17.59%) | loss: 3.627978 | lrm: 1.00 | dt: 2600.90ms | tok/sec: 201,579 | mfu: 51.83 | epoch: 1 | total time: 13.13m | eta: 63.5m\n",
      "step 00315/01785 (17.65%) | loss: 3.620487 | lrm: 1.00 | dt: 2597.50ms | tok/sec: 201,843 | mfu: 51.89 | epoch: 1 | total time: 13.17m | eta: 63.5m\n",
      "step 00316/01785 (17.70%) | loss: 3.599224 | lrm: 1.00 | dt: 2591.34ms | tok/sec: 202,322 | mfu: 52.02 | epoch: 1 | total time: 13.21m | eta: 63.4m\n",
      "step 00317/01785 (17.76%) | loss: 3.597731 | lrm: 1.00 | dt: 2603.23ms | tok/sec: 201,399 | mfu: 51.78 | epoch: 1 | total time: 13.26m | eta: 63.4m\n",
      "step 00318/01785 (17.82%) | loss: 3.595175 | lrm: 1.00 | dt: 2605.27ms | tok/sec: 201,241 | mfu: 51.74 | epoch: 1 | total time: 13.30m | eta: 63.3m\n",
      "step 00319/01785 (17.87%) | loss: 3.593277 | lrm: 1.00 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 13.34m | eta: 63.3m\n",
      "step 00320/01785 (17.93%) | loss: 3.586261 | lrm: 1.00 | dt: 2605.22ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 13.39m | eta: 63.3m\n",
      "step 00321/01785 (17.98%) | loss: 3.598024 | lrm: 1.00 | dt: 2604.87ms | tok/sec: 201,272 | mfu: 51.75 | epoch: 1 | total time: 13.43m | eta: 63.2m\n",
      "step 00322/01785 (18.04%) | loss: 3.596514 | lrm: 1.00 | dt: 2602.10ms | tok/sec: 201,486 | mfu: 51.80 | epoch: 1 | total time: 13.47m | eta: 63.2m\n",
      "step 00323/01785 (18.10%) | loss: 3.605220 | lrm: 1.00 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 1 | total time: 13.52m | eta: 63.1m\n",
      "step 00324/01785 (18.15%) | loss: 3.602571 | lrm: 1.00 | dt: 2603.53ms | tok/sec: 201,375 | mfu: 51.77 | epoch: 1 | total time: 13.56m | eta: 63.1m\n",
      "step 00325/01785 (18.21%) | loss: 3.602957 | lrm: 1.00 | dt: 2604.43ms | tok/sec: 201,306 | mfu: 51.76 | epoch: 1 | total time: 13.60m | eta: 63.1m\n",
      "step 00326/01785 (18.26%) | loss: 3.602645 | lrm: 1.00 | dt: 2596.51ms | tok/sec: 201,920 | mfu: 51.91 | epoch: 1 | total time: 13.65m | eta: 63.0m\n",
      "step 00327/01785 (18.32%) | loss: 3.601954 | lrm: 1.00 | dt: 2590.61ms | tok/sec: 202,380 | mfu: 52.03 | epoch: 1 | total time: 13.69m | eta: 63.0m\n",
      "step 00328/01785 (18.38%) | loss: 3.603418 | lrm: 1.00 | dt: 2602.81ms | tok/sec: 201,431 | mfu: 51.79 | epoch: 1 | total time: 13.73m | eta: 62.9m\n",
      "step 00329/01785 (18.43%) | loss: 3.613877 | lrm: 1.00 | dt: 2604.05ms | tok/sec: 201,335 | mfu: 51.76 | epoch: 1 | total time: 13.78m | eta: 62.9m\n",
      "step 00330/01785 (18.49%) | loss: 3.611421 | lrm: 1.00 | dt: 2589.97ms | tok/sec: 202,429 | mfu: 52.05 | epoch: 1 | total time: 13.82m | eta: 62.8m\n",
      "step 00331/01785 (18.54%) | loss: 3.603970 | lrm: 1.00 | dt: 2598.68ms | tok/sec: 201,751 | mfu: 51.87 | epoch: 1 | total time: 13.86m | eta: 62.8m\n",
      "step 00332/01785 (18.60%) | loss: 3.606584 | lrm: 1.00 | dt: 2603.72ms | tok/sec: 201,360 | mfu: 51.77 | epoch: 1 | total time: 13.91m | eta: 62.8m\n",
      "step 00333/01785 (18.66%) | loss: 3.616336 | lrm: 1.00 | dt: 2604.54ms | tok/sec: 201,297 | mfu: 51.75 | epoch: 1 | total time: 13.95m | eta: 62.7m\n",
      "step 00334/01785 (18.71%) | loss: 3.609953 | lrm: 1.00 | dt: 2587.15ms | tok/sec: 202,650 | mfu: 52.10 | epoch: 1 | total time: 13.99m | eta: 62.7m\n",
      "step 00335/01785 (18.77%) | loss: 3.609374 | lrm: 1.00 | dt: 2601.46ms | tok/sec: 201,535 | mfu: 51.82 | epoch: 1 | total time: 14.04m | eta: 62.6m\n",
      "step 00336/01785 (18.82%) | loss: 3.604554 | lrm: 1.00 | dt: 2586.23ms | tok/sec: 202,723 | mfu: 52.12 | epoch: 1 | total time: 14.08m | eta: 62.6m\n",
      "step 00337/01785 (18.88%) | loss: 3.593403 | lrm: 1.00 | dt: 2587.54ms | tok/sec: 202,620 | mfu: 52.09 | epoch: 1 | total time: 14.12m | eta: 62.5m\n",
      "step 00338/01785 (18.94%) | loss: 3.580414 | lrm: 1.00 | dt: 2601.04ms | tok/sec: 201,568 | mfu: 51.82 | epoch: 1 | total time: 14.17m | eta: 62.5m\n",
      "step 00339/01785 (18.99%) | loss: 3.580406 | lrm: 1.00 | dt: 2589.96ms | tok/sec: 202,430 | mfu: 52.05 | epoch: 1 | total time: 14.21m | eta: 62.5m\n",
      "step 00340/01785 (19.05%) | loss: 3.588145 | lrm: 1.00 | dt: 2589.22ms | tok/sec: 202,488 | mfu: 52.06 | epoch: 1 | total time: 14.25m | eta: 62.4m\n",
      "step 00341/01785 (19.10%) | loss: 3.586180 | lrm: 1.00 | dt: 2592.36ms | tok/sec: 202,243 | mfu: 52.00 | epoch: 1 | total time: 14.30m | eta: 62.4m\n",
      "step 00342/01785 (19.16%) | loss: 3.589903 | lrm: 1.00 | dt: 2591.65ms | tok/sec: 202,299 | mfu: 52.01 | epoch: 1 | total time: 14.34m | eta: 62.3m\n",
      "step 00343/01785 (19.22%) | loss: 3.583381 | lrm: 1.00 | dt: 2589.49ms | tok/sec: 202,467 | mfu: 52.06 | epoch: 1 | total time: 14.38m | eta: 62.3m\n",
      "step 00344/01785 (19.27%) | loss: 3.576526 | lrm: 1.00 | dt: 2599.31ms | tok/sec: 201,702 | mfu: 51.86 | epoch: 1 | total time: 14.43m | eta: 62.2m\n",
      "step 00345/01785 (19.33%) | loss: 3.579558 | lrm: 1.00 | dt: 2603.38ms | tok/sec: 201,387 | mfu: 51.78 | epoch: 1 | total time: 14.47m | eta: 62.2m\n",
      "step 00346/01785 (19.38%) | loss: 3.573656 | lrm: 1.00 | dt: 2589.13ms | tok/sec: 202,496 | mfu: 52.06 | epoch: 1 | total time: 14.51m | eta: 62.2m\n",
      "step 00347/01785 (19.44%) | loss: 3.570482 | lrm: 1.00 | dt: 2598.56ms | tok/sec: 201,761 | mfu: 51.87 | epoch: 1 | total time: 14.56m | eta: 62.1m\n",
      "step 00348/01785 (19.50%) | loss: 3.568151 | lrm: 1.00 | dt: 2603.92ms | tok/sec: 201,345 | mfu: 51.77 | epoch: 1 | total time: 14.60m | eta: 62.1m\n",
      "step 00349/01785 (19.55%) | loss: 3.576719 | lrm: 1.00 | dt: 2594.68ms | tok/sec: 202,062 | mfu: 51.95 | epoch: 1 | total time: 14.64m | eta: 62.0m\n",
      "step 00350/01785 (19.61%) | loss: 3.591408 | lrm: 1.00 | dt: 2591.25ms | tok/sec: 202,330 | mfu: 52.02 | epoch: 1 | total time: 14.69m | eta: 62.0m\n",
      "step 00351/01785 (19.66%) | loss: 3.586113 | lrm: 1.00 | dt: 2605.16ms | tok/sec: 201,249 | mfu: 51.74 | epoch: 1 | total time: 14.73m | eta: 61.9m\n",
      "step 00352/01785 (19.72%) | loss: 3.585169 | lrm: 1.00 | dt: 2604.66ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 14.77m | eta: 61.9m\n",
      "step 00353/01785 (19.78%) | loss: 3.595054 | lrm: 1.00 | dt: 2604.44ms | tok/sec: 201,305 | mfu: 51.76 | epoch: 1 | total time: 14.82m | eta: 61.9m\n",
      "step 00354/01785 (19.83%) | loss: 3.608332 | lrm: 1.00 | dt: 2594.01ms | tok/sec: 202,114 | mfu: 51.96 | epoch: 1 | total time: 14.86m | eta: 61.8m\n",
      "step 00355/01785 (19.89%) | loss: 3.597501 | lrm: 1.00 | dt: 2591.86ms | tok/sec: 202,282 | mfu: 52.01 | epoch: 1 | total time: 14.90m | eta: 61.8m\n",
      "step 00356/01785 (19.94%) | loss: 3.596573 | lrm: 1.00 | dt: 2587.24ms | tok/sec: 202,643 | mfu: 52.10 | epoch: 1 | total time: 14.94m | eta: 61.7m\n",
      "step 00357/01785 (20.00%) | loss: 3.592321 | lrm: 1.00 | dt: 2601.34ms | tok/sec: 201,545 | mfu: 51.82 | epoch: 1 | total time: 14.99m | eta: 61.7m\n",
      "step 00358/01785 (20.06%) | loss: 3.600901 | lrm: 1.00 | dt: 2591.06ms | tok/sec: 202,345 | mfu: 52.02 | epoch: 1 | total time: 15.03m | eta: 61.6m\n",
      "step 00359/01785 (20.11%) | loss: 3.595896 | lrm: 1.00 | dt: 2594.51ms | tok/sec: 202,076 | mfu: 51.95 | epoch: 1 | total time: 15.07m | eta: 61.6m\n",
      "step 00360/01785 (20.17%) | loss: 3.609649 | lrm: 1.00 | dt: 2589.69ms | tok/sec: 202,451 | mfu: 52.05 | epoch: 1 | total time: 15.12m | eta: 61.6m\n",
      "step 00361/01785 (20.22%) | loss: 3.614450 | lrm: 1.00 | dt: 2598.16ms | tok/sec: 201,791 | mfu: 51.88 | epoch: 1 | total time: 15.16m | eta: 61.5m\n",
      "step 00362/01785 (20.28%) | loss: 3.613724 | lrm: 1.00 | dt: 2589.07ms | tok/sec: 202,500 | mfu: 52.06 | epoch: 1 | total time: 15.20m | eta: 61.5m\n",
      "step 00363/01785 (20.34%) | loss: 3.619963 | lrm: 1.00 | dt: 2601.24ms | tok/sec: 201,553 | mfu: 51.82 | epoch: 1 | total time: 15.25m | eta: 61.4m\n",
      "step 00364/01785 (20.39%) | loss: 3.612390 | lrm: 1.00 | dt: 2589.53ms | tok/sec: 202,464 | mfu: 52.05 | epoch: 1 | total time: 15.29m | eta: 61.4m\n",
      "step 00365/01785 (20.45%) | loss: 3.603444 | lrm: 1.00 | dt: 2604.17ms | tok/sec: 201,326 | mfu: 51.76 | epoch: 1 | total time: 15.33m | eta: 61.3m\n",
      "step 00366/01785 (20.50%) | loss: 3.597500 | lrm: 1.00 | dt: 2595.43ms | tok/sec: 202,004 | mfu: 51.94 | epoch: 1 | total time: 15.38m | eta: 61.3m\n",
      "step 00367/01785 (20.56%) | loss: 3.592916 | lrm: 1.00 | dt: 2592.71ms | tok/sec: 202,216 | mfu: 51.99 | epoch: 1 | total time: 15.42m | eta: 61.3m\n",
      "step 00368/01785 (20.62%) | loss: 3.590865 | lrm: 1.00 | dt: 2588.29ms | tok/sec: 202,561 | mfu: 52.08 | epoch: 1 | total time: 15.46m | eta: 61.2m\n",
      "step 00369/01785 (20.67%) | loss: 3.575734 | lrm: 1.00 | dt: 2592.32ms | tok/sec: 202,246 | mfu: 52.00 | epoch: 1 | total time: 15.51m | eta: 61.2m\n",
      "step 00370/01785 (20.73%) | loss: 3.571252 | lrm: 1.00 | dt: 2596.24ms | tok/sec: 201,940 | mfu: 51.92 | epoch: 1 | total time: 15.55m | eta: 61.1m\n",
      "step 00371/01785 (20.78%) | loss: 3.551570 | lrm: 1.00 | dt: 2604.67ms | tok/sec: 201,287 | mfu: 51.75 | epoch: 1 | total time: 15.59m | eta: 61.1m\n",
      "step 00372/01785 (20.84%) | loss: 3.558572 | lrm: 1.00 | dt: 2604.14ms | tok/sec: 201,328 | mfu: 51.76 | epoch: 1 | total time: 15.64m | eta: 61.0m\n",
      "step 00373/01785 (20.90%) | loss: 3.552407 | lrm: 1.00 | dt: 2595.44ms | tok/sec: 202,003 | mfu: 51.94 | epoch: 1 | total time: 15.68m | eta: 61.0m\n",
      "step 00374/01785 (20.95%) | loss: 3.562907 | lrm: 1.00 | dt: 2591.88ms | tok/sec: 202,281 | mfu: 52.01 | epoch: 1 | total time: 15.72m | eta: 61.0m\n",
      "step 00375/01785 (21.01%) | loss: 3.560556 | lrm: 1.00 | dt: 2604.73ms | tok/sec: 201,283 | mfu: 51.75 | epoch: 1 | total time: 15.77m | eta: 60.9m\n",
      "step 00376/01785 (21.06%) | loss: 3.559698 | lrm: 1.00 | dt: 2605.58ms | tok/sec: 201,217 | mfu: 51.73 | epoch: 1 | total time: 15.81m | eta: 60.9m\n",
      "step 00377/01785 (21.12%) | loss: 3.550746 | lrm: 1.00 | dt: 2603.45ms | tok/sec: 201,381 | mfu: 51.78 | epoch: 1 | total time: 15.85m | eta: 60.8m\n",
      "step 00378/01785 (21.18%) | loss: 3.562766 | lrm: 1.00 | dt: 2592.77ms | tok/sec: 202,211 | mfu: 51.99 | epoch: 1 | total time: 15.90m | eta: 60.8m\n",
      "step 00379/01785 (21.23%) | loss: 3.565532 | lrm: 1.00 | dt: 2593.36ms | tok/sec: 202,165 | mfu: 51.98 | epoch: 1 | total time: 15.94m | eta: 60.7m\n",
      "step 00380/01785 (21.29%) | loss: 3.560320 | lrm: 1.00 | dt: 2604.39ms | tok/sec: 201,309 | mfu: 51.76 | epoch: 1 | total time: 15.98m | eta: 60.7m\n",
      "step 00381/01785 (21.34%) | loss: 3.562316 | lrm: 1.00 | dt: 2592.86ms | tok/sec: 202,204 | mfu: 51.99 | epoch: 1 | total time: 16.03m | eta: 60.7m\n",
      "step 00382/01785 (21.40%) | loss: 3.568160 | lrm: 1.00 | dt: 2591.91ms | tok/sec: 202,278 | mfu: 52.01 | epoch: 1 | total time: 16.07m | eta: 60.6m\n",
      "step 00383/01785 (21.46%) | loss: 3.592638 | lrm: 1.00 | dt: 2592.19ms | tok/sec: 202,256 | mfu: 52.00 | epoch: 1 | total time: 16.11m | eta: 60.6m\n",
      "step 00384/01785 (21.51%) | loss: 3.602856 | lrm: 1.00 | dt: 2593.60ms | tok/sec: 202,146 | mfu: 51.97 | epoch: 1 | total time: 16.16m | eta: 60.5m\n",
      "step 00385/01785 (21.57%) | loss: 3.607718 | lrm: 1.00 | dt: 2587.78ms | tok/sec: 202,601 | mfu: 52.09 | epoch: 1 | total time: 16.20m | eta: 60.5m\n",
      "step 00386/01785 (21.62%) | loss: 3.607681 | lrm: 1.00 | dt: 2600.37ms | tok/sec: 201,620 | mfu: 51.84 | epoch: 1 | total time: 16.24m | eta: 60.4m\n",
      "step 00387/01785 (21.68%) | loss: 3.600469 | lrm: 1.00 | dt: 2591.10ms | tok/sec: 202,341 | mfu: 52.02 | epoch: 1 | total time: 16.29m | eta: 60.4m\n",
      "step 00388/01785 (21.74%) | loss: 3.607983 | lrm: 1.00 | dt: 2600.06ms | tok/sec: 201,644 | mfu: 51.84 | epoch: 1 | total time: 16.33m | eta: 60.3m\n",
      "step 00389/01785 (21.79%) | loss: 3.605715 | lrm: 1.00 | dt: 2590.35ms | tok/sec: 202,400 | mfu: 52.04 | epoch: 1 | total time: 16.37m | eta: 60.3m\n",
      "step 00390/01785 (21.85%) | loss: 3.582764 | lrm: 1.00 | dt: 2604.99ms | tok/sec: 201,263 | mfu: 51.75 | epoch: 1 | total time: 16.42m | eta: 60.3m\n",
      "step 00391/01785 (21.90%) | loss: 3.577368 | lrm: 1.00 | dt: 2590.79ms | tok/sec: 202,366 | mfu: 52.03 | epoch: 1 | total time: 16.46m | eta: 60.2m\n",
      "step 00392/01785 (21.96%) | loss: 3.556301 | lrm: 1.00 | dt: 2592.14ms | tok/sec: 202,260 | mfu: 52.00 | epoch: 1 | total time: 16.50m | eta: 60.2m\n",
      "step 00393/01785 (22.02%) | loss: 3.564437 | lrm: 1.00 | dt: 2610.75ms | tok/sec: 200,819 | mfu: 51.63 | epoch: 1 | total time: 16.55m | eta: 60.1m\n",
      "step 00394/01785 (22.07%) | loss: 3.546598 | lrm: 1.00 | dt: 2587.92ms | tok/sec: 202,590 | mfu: 52.09 | epoch: 1 | total time: 16.59m | eta: 60.1m\n",
      "step 00395/01785 (22.13%) | loss: 3.542106 | lrm: 1.00 | dt: 2605.60ms | tok/sec: 201,215 | mfu: 51.73 | epoch: 1 | total time: 16.63m | eta: 60.0m\n",
      "step 00396/01785 (22.18%) | loss: 3.534574 | lrm: 1.00 | dt: 2603.23ms | tok/sec: 201,399 | mfu: 51.78 | epoch: 1 | total time: 16.68m | eta: 60.0m\n",
      "step 00397/01785 (22.24%) | loss: 3.538656 | lrm: 1.00 | dt: 2603.71ms | tok/sec: 201,362 | mfu: 51.77 | epoch: 1 | total time: 16.72m | eta: 60.0m\n",
      "step 00398/01785 (22.30%) | loss: 3.541163 | lrm: 1.00 | dt: 2594.91ms | tok/sec: 202,044 | mfu: 51.95 | epoch: 1 | total time: 16.76m | eta: 59.9m\n",
      "step 00399/01785 (22.35%) | loss: 3.552053 | lrm: 1.00 | dt: 2589.69ms | tok/sec: 202,452 | mfu: 52.05 | epoch: 1 | total time: 16.81m | eta: 59.9m\n",
      "step 00400/01785 (22.41%) | loss: 3.543788 | lrm: 1.00 | dt: 2604.26ms | tok/sec: 201,319 | mfu: 51.76 | epoch: 1 | total time: 16.85m | eta: 59.8m\n",
      "step 00401/01785 (22.46%) | loss: 3.534234 | lrm: 1.00 | dt: 2605.29ms | tok/sec: 201,239 | mfu: 51.74 | epoch: 1 | total time: 16.89m | eta: 59.8m\n",
      "step 00402/01785 (22.52%) | loss: 3.534680 | lrm: 1.00 | dt: 2602.83ms | tok/sec: 201,429 | mfu: 51.79 | epoch: 1 | total time: 16.94m | eta: 59.8m\n",
      "step 00403/01785 (22.58%) | loss: 3.532829 | lrm: 1.00 | dt: 2605.33ms | tok/sec: 201,236 | mfu: 51.74 | epoch: 1 | total time: 16.98m | eta: 59.7m\n",
      "step 00404/01785 (22.63%) | loss: 3.529439 | lrm: 1.00 | dt: 2604.35ms | tok/sec: 201,312 | mfu: 51.76 | epoch: 1 | total time: 17.02m | eta: 59.7m\n",
      "step 00405/01785 (22.69%) | loss: 3.517817 | lrm: 1.00 | dt: 2604.45ms | tok/sec: 201,304 | mfu: 51.76 | epoch: 1 | total time: 17.07m | eta: 59.6m\n",
      "step 00406/01785 (22.75%) | loss: 3.505664 | lrm: 1.00 | dt: 2589.99ms | tok/sec: 202,428 | mfu: 52.05 | epoch: 1 | total time: 17.11m | eta: 59.6m\n",
      "step 00407/01785 (22.80%) | loss: 3.505141 | lrm: 1.00 | dt: 2598.88ms | tok/sec: 201,736 | mfu: 51.87 | epoch: 1 | total time: 17.15m | eta: 59.5m\n",
      "step 00408/01785 (22.86%) | loss: 3.528669 | lrm: 1.00 | dt: 2603.99ms | tok/sec: 201,340 | mfu: 51.77 | epoch: 1 | total time: 17.20m | eta: 59.5m\n",
      "step 00409/01785 (22.91%) | loss: 3.533403 | lrm: 1.00 | dt: 2594.54ms | tok/sec: 202,073 | mfu: 51.95 | epoch: 1 | total time: 17.24m | eta: 59.5m\n",
      "step 00410/01785 (22.97%) | loss: 3.538539 | lrm: 1.00 | dt: 2592.26ms | tok/sec: 202,251 | mfu: 52.00 | epoch: 1 | total time: 17.28m | eta: 59.4m\n",
      "step 00411/01785 (23.03%) | loss: 3.540269 | lrm: 1.00 | dt: 2603.06ms | tok/sec: 201,412 | mfu: 51.78 | epoch: 1 | total time: 17.33m | eta: 59.4m\n",
      "step 00412/01785 (23.08%) | loss: 3.543480 | lrm: 1.00 | dt: 2584.72ms | tok/sec: 202,841 | mfu: 52.15 | epoch: 1 | total time: 17.37m | eta: 59.3m\n",
      "step 00413/01785 (23.14%) | loss: 3.526642 | lrm: 1.00 | dt: 2605.79ms | tok/sec: 201,201 | mfu: 51.73 | epoch: 1 | total time: 17.41m | eta: 59.3m\n",
      "step 00414/01785 (23.19%) | loss: 3.529160 | lrm: 1.00 | dt: 2586.85ms | tok/sec: 202,674 | mfu: 52.11 | epoch: 1 | total time: 17.46m | eta: 59.2m\n",
      "step 00415/01785 (23.25%) | loss: 3.526930 | lrm: 1.00 | dt: 2604.22ms | tok/sec: 201,322 | mfu: 51.76 | epoch: 1 | total time: 17.50m | eta: 59.2m\n",
      "step 00416/01785 (23.31%) | loss: 3.521830 | lrm: 1.00 | dt: 2588.93ms | tok/sec: 202,511 | mfu: 52.07 | epoch: 1 | total time: 17.54m | eta: 59.2m\n",
      "step 00417/01785 (23.36%) | loss: 3.512799 | lrm: 1.00 | dt: 2593.66ms | tok/sec: 202,142 | mfu: 51.97 | epoch: 1 | total time: 17.59m | eta: 59.1m\n",
      "step 00418/01785 (23.42%) | loss: 3.522899 | lrm: 1.00 | dt: 2602.32ms | tok/sec: 201,469 | mfu: 51.80 | epoch: 1 | total time: 17.63m | eta: 59.1m\n",
      "step 00419/01785 (23.47%) | loss: 3.520176 | lrm: 1.00 | dt: 2597.56ms | tok/sec: 201,838 | mfu: 51.89 | epoch: 1 | total time: 17.67m | eta: 59.0m\n",
      "step 00420/01785 (23.53%) | loss: 3.511384 | lrm: 1.00 | dt: 2591.98ms | tok/sec: 202,273 | mfu: 52.01 | epoch: 1 | total time: 17.72m | eta: 59.0m\n",
      "step 00421/01785 (23.59%) | loss: 3.509022 | lrm: 1.00 | dt: 2598.00ms | tok/sec: 201,804 | mfu: 51.88 | epoch: 1 | total time: 17.76m | eta: 58.9m\n",
      "step 00422/01785 (23.64%) | loss: 3.510753 | lrm: 1.00 | dt: 2591.28ms | tok/sec: 202,327 | mfu: 52.02 | epoch: 1 | total time: 17.80m | eta: 58.9m\n",
      "step 00423/01785 (23.70%) | loss: 3.503267 | lrm: 1.00 | dt: 2597.46ms | tok/sec: 201,846 | mfu: 51.90 | epoch: 1 | total time: 17.84m | eta: 58.8m\n",
      "step 00424/01785 (23.75%) | loss: 3.499649 | lrm: 1.00 | dt: 2605.96ms | tok/sec: 201,187 | mfu: 51.73 | epoch: 1 | total time: 17.89m | eta: 58.8m\n",
      "step 00425/01785 (23.81%) | loss: 3.493003 | lrm: 1.00 | dt: 2598.38ms | tok/sec: 201,775 | mfu: 51.88 | epoch: 1 | total time: 17.93m | eta: 58.8m\n",
      "step 00426/01785 (23.87%) | loss: 3.505193 | lrm: 1.00 | dt: 2588.52ms | tok/sec: 202,543 | mfu: 52.07 | epoch: 1 | total time: 17.97m | eta: 58.7m\n",
      "step 00427/01785 (23.92%) | loss: 3.495438 | lrm: 1.00 | dt: 2602.82ms | tok/sec: 201,430 | mfu: 51.79 | epoch: 1 | total time: 18.02m | eta: 58.7m\n",
      "step 00428/01785 (23.98%) | loss: 3.494840 | lrm: 1.00 | dt: 2604.05ms | tok/sec: 201,335 | mfu: 51.76 | epoch: 1 | total time: 18.06m | eta: 58.6m\n",
      "step 00429/01785 (24.03%) | loss: 3.485736 | lrm: 1.00 | dt: 2604.97ms | tok/sec: 201,264 | mfu: 51.75 | epoch: 1 | total time: 18.10m | eta: 58.6m\n",
      "step 00430/01785 (24.09%) | loss: 3.479214 | lrm: 1.00 | dt: 2604.22ms | tok/sec: 201,322 | mfu: 51.76 | epoch: 1 | total time: 18.15m | eta: 58.5m\n",
      "step 00431/01785 (24.15%) | loss: 3.482945 | lrm: 1.00 | dt: 2603.88ms | tok/sec: 201,348 | mfu: 51.77 | epoch: 1 | total time: 18.19m | eta: 58.5m\n",
      "step 00432/01785 (24.20%) | loss: 3.477717 | lrm: 1.00 | dt: 2594.10ms | tok/sec: 202,107 | mfu: 51.96 | epoch: 1 | total time: 18.23m | eta: 58.5m\n",
      "step 00433/01785 (24.26%) | loss: 3.466212 | lrm: 1.00 | dt: 2593.21ms | tok/sec: 202,177 | mfu: 51.98 | epoch: 1 | total time: 18.28m | eta: 58.4m\n",
      "step 00434/01785 (24.31%) | loss: 3.472720 | lrm: 1.00 | dt: 2603.24ms | tok/sec: 201,398 | mfu: 51.78 | epoch: 1 | total time: 18.32m | eta: 58.4m\n",
      "step 00435/01785 (24.37%) | loss: 3.477859 | lrm: 1.00 | dt: 2593.74ms | tok/sec: 202,136 | mfu: 51.97 | epoch: 1 | total time: 18.36m | eta: 58.3m\n",
      "step 00436/01785 (24.43%) | loss: 3.485980 | lrm: 1.00 | dt: 2593.05ms | tok/sec: 202,189 | mfu: 51.98 | epoch: 1 | total time: 18.41m | eta: 58.3m\n",
      "step 00437/01785 (24.48%) | loss: 3.492941 | lrm: 1.00 | dt: 2603.68ms | tok/sec: 201,364 | mfu: 51.77 | epoch: 1 | total time: 18.45m | eta: 58.2m\n",
      "step 00438/01785 (24.54%) | loss: 3.494127 | lrm: 1.00 | dt: 2604.04ms | tok/sec: 201,336 | mfu: 51.76 | epoch: 1 | total time: 18.49m | eta: 58.2m\n",
      "step 00439/01785 (24.59%) | loss: 3.487323 | lrm: 1.00 | dt: 2594.45ms | tok/sec: 202,080 | mfu: 51.96 | epoch: 1 | total time: 18.54m | eta: 58.2m\n",
      "step 00440/01785 (24.65%) | loss: 3.494271 | lrm: 1.00 | dt: 2596.16ms | tok/sec: 201,947 | mfu: 51.92 | epoch: 1 | total time: 18.58m | eta: 58.1m\n",
      "step 00441/01785 (24.71%) | loss: 3.473740 | lrm: 1.00 | dt: 2589.96ms | tok/sec: 202,431 | mfu: 52.05 | epoch: 1 | total time: 18.62m | eta: 58.1m\n",
      "step 00442/01785 (24.76%) | loss: 3.480693 | lrm: 1.00 | dt: 2604.23ms | tok/sec: 201,321 | mfu: 51.76 | epoch: 1 | total time: 18.67m | eta: 58.0m\n",
      "step 00443/01785 (24.82%) | loss: 3.488650 | lrm: 1.00 | dt: 2597.66ms | tok/sec: 201,830 | mfu: 51.89 | epoch: 1 | total time: 18.71m | eta: 58.0m\n",
      "step 00444/01785 (24.87%) | loss: 3.486142 | lrm: 1.00 | dt: 2601.61ms | tok/sec: 201,524 | mfu: 51.81 | epoch: 1 | total time: 18.75m | eta: 57.9m\n",
      "step 00445/01785 (24.93%) | loss: 3.479927 | lrm: 1.00 | dt: 2594.93ms | tok/sec: 202,043 | mfu: 51.95 | epoch: 1 | total time: 18.80m | eta: 57.9m\n",
      "step 00446/01785 (24.99%) | loss: 3.481898 | lrm: 1.00 | dt: 2595.95ms | tok/sec: 201,963 | mfu: 51.93 | epoch: 1 | total time: 18.84m | eta: 57.9m\n",
      "step 00447/01785 (25.04%) | loss: 3.491951 | lrm: 1.00 | dt: 2594.39ms | tok/sec: 202,085 | mfu: 51.96 | epoch: 1 | total time: 18.88m | eta: 57.8m\n",
      "step 00448/01785 (25.10%) | loss: 3.486534 | lrm: 1.00 | dt: 2604.10ms | tok/sec: 201,331 | mfu: 51.76 | epoch: 1 | total time: 18.93m | eta: 57.8m\n",
      "step 00449/01785 (25.15%) | loss: 3.481957 | lrm: 1.00 | dt: 2604.89ms | tok/sec: 201,270 | mfu: 51.75 | epoch: 1 | total time: 18.97m | eta: 57.7m\n",
      "step 00450/01785 (25.21%) | loss: 3.488956 | lrm: 1.00 | dt: 2604.03ms | tok/sec: 201,336 | mfu: 51.76 | epoch: 1 | total time: 19.01m | eta: 57.7m\n",
      "step 00451/01785 (25.27%) | loss: 3.481375 | lrm: 1.00 | dt: 2604.61ms | tok/sec: 201,291 | mfu: 51.75 | epoch: 1 | total time: 19.06m | eta: 57.6m\n",
      "step 00452/01785 (25.32%) | loss: 3.483965 | lrm: 1.00 | dt: 2605.33ms | tok/sec: 201,236 | mfu: 51.74 | epoch: 1 | total time: 19.10m | eta: 57.6m\n",
      "step 00453/01785 (25.38%) | loss: 3.493822 | lrm: 1.00 | dt: 2604.05ms | tok/sec: 201,335 | mfu: 51.76 | epoch: 1 | total time: 19.14m | eta: 57.6m\n",
      "step 00454/01785 (25.43%) | loss: 3.499943 | lrm: 1.00 | dt: 2590.50ms | tok/sec: 202,388 | mfu: 52.04 | epoch: 1 | total time: 19.19m | eta: 57.5m\n",
      "step 00455/01785 (25.49%) | loss: 3.499101 | lrm: 1.00 | dt: 2596.15ms | tok/sec: 201,947 | mfu: 51.92 | epoch: 1 | total time: 19.23m | eta: 57.5m\n",
      "step 00456/01785 (25.55%) | loss: 3.511458 | lrm: 1.00 | dt: 2593.16ms | tok/sec: 202,180 | mfu: 51.98 | epoch: 1 | total time: 19.27m | eta: 57.4m\n",
      "step 00457/01785 (25.60%) | loss: 3.497582 | lrm: 1.00 | dt: 2593.37ms | tok/sec: 202,164 | mfu: 51.98 | epoch: 1 | total time: 19.32m | eta: 57.4m\n",
      "step 00458/01785 (25.66%) | loss: 3.499404 | lrm: 1.00 | dt: 2589.36ms | tok/sec: 202,477 | mfu: 52.06 | epoch: 1 | total time: 19.36m | eta: 57.3m\n",
      "step 00459/01785 (25.71%) | loss: 3.484716 | lrm: 1.00 | dt: 2597.61ms | tok/sec: 201,834 | mfu: 51.89 | epoch: 1 | total time: 19.40m | eta: 57.3m\n",
      "step 00460/01785 (25.77%) | loss: 3.481632 | lrm: 1.00 | dt: 2604.56ms | tok/sec: 201,296 | mfu: 51.75 | epoch: 1 | total time: 19.45m | eta: 57.3m\n",
      "step 00461/01785 (25.83%) | loss: 3.478741 | lrm: 1.00 | dt: 2605.44ms | tok/sec: 201,228 | mfu: 51.74 | epoch: 1 | total time: 19.49m | eta: 57.2m\n",
      "step 00462/01785 (25.88%) | loss: 3.477765 | lrm: 1.00 | dt: 2588.04ms | tok/sec: 202,580 | mfu: 52.08 | epoch: 1 | total time: 19.53m | eta: 57.2m\n",
      "step 00463/01785 (25.94%) | loss: 3.486236 | lrm: 1.00 | dt: 2598.64ms | tok/sec: 201,754 | mfu: 51.87 | epoch: 1 | total time: 19.58m | eta: 57.1m\n",
      "step 00464/01785 (25.99%) | loss: 3.494762 | lrm: 1.00 | dt: 2588.47ms | tok/sec: 202,547 | mfu: 52.08 | epoch: 1 | total time: 19.62m | eta: 57.1m\n",
      "step 00465/01785 (26.05%) | loss: 3.492129 | lrm: 1.00 | dt: 2601.99ms | tok/sec: 201,495 | mfu: 51.81 | epoch: 1 | total time: 19.66m | eta: 57.0m\n",
      "step 00466/01785 (26.11%) | loss: 3.489892 | lrm: 1.00 | dt: 2587.53ms | tok/sec: 202,621 | mfu: 52.09 | epoch: 1 | total time: 19.71m | eta: 57.0m\n",
      "step 00467/01785 (26.16%) | loss: 3.473280 | lrm: 1.00 | dt: 2605.93ms | tok/sec: 201,190 | mfu: 51.73 | epoch: 1 | total time: 19.75m | eta: 57.0m\n",
      "step 00468/01785 (26.22%) | loss: 3.465687 | lrm: 1.00 | dt: 2589.11ms | tok/sec: 202,497 | mfu: 52.06 | epoch: 1 | total time: 19.79m | eta: 56.9m\n",
      "step 00469/01785 (26.27%) | loss: 3.457492 | lrm: 1.00 | dt: 2590.93ms | tok/sec: 202,355 | mfu: 52.03 | epoch: 1 | total time: 19.84m | eta: 56.9m\n",
      "step 00470/01785 (26.33%) | loss: 3.455959 | lrm: 1.00 | dt: 2592.33ms | tok/sec: 202,245 | mfu: 52.00 | epoch: 1 | total time: 19.88m | eta: 56.8m\n",
      "step 00471/01785 (26.39%) | loss: 3.455926 | lrm: 1.00 | dt: 2595.00ms | tok/sec: 202,037 | mfu: 51.94 | epoch: 1 | total time: 19.92m | eta: 56.8m\n",
      "step 00472/01785 (26.44%) | loss: 3.448845 | lrm: 1.00 | dt: 2599.93ms | tok/sec: 201,654 | mfu: 51.85 | epoch: 1 | total time: 19.97m | eta: 56.7m\n",
      "step 00473/01785 (26.50%) | loss: 3.467239 | lrm: 1.00 | dt: 2604.33ms | tok/sec: 201,314 | mfu: 51.76 | epoch: 1 | total time: 20.01m | eta: 56.7m\n",
      "step 00474/01785 (26.55%) | loss: 3.470453 | lrm: 1.00 | dt: 2605.59ms | tok/sec: 201,216 | mfu: 51.73 | epoch: 1 | total time: 20.05m | eta: 56.7m\n",
      "step 00475/01785 (26.61%) | loss: 3.473603 | lrm: 1.00 | dt: 2605.05ms | tok/sec: 201,257 | mfu: 51.74 | epoch: 1 | total time: 20.10m | eta: 56.6m\n",
      "step 00476/01785 (26.67%) | loss: 3.474054 | lrm: 1.00 | dt: 2606.51ms | tok/sec: 201,145 | mfu: 51.72 | epoch: 1 | total time: 20.14m | eta: 56.6m\n",
      "step 00477/01785 (26.72%) | loss: 3.479199 | lrm: 1.00 | dt: 2595.86ms | tok/sec: 201,970 | mfu: 51.93 | epoch: 1 | total time: 20.18m | eta: 56.5m\n",
      "step 00478/01785 (26.78%) | loss: 3.454005 | lrm: 1.00 | dt: 2588.79ms | tok/sec: 202,522 | mfu: 52.07 | epoch: 1 | total time: 20.23m | eta: 56.5m\n",
      "step 00479/01785 (26.83%) | loss: 3.457279 | lrm: 1.00 | dt: 2606.55ms | tok/sec: 201,142 | mfu: 51.71 | epoch: 1 | total time: 20.27m | eta: 56.4m\n",
      "step 00480/01785 (26.89%) | loss: 3.472281 | lrm: 1.00 | dt: 2604.71ms | tok/sec: 201,284 | mfu: 51.75 | epoch: 1 | total time: 20.31m | eta: 56.4m\n",
      "step 00481/01785 (26.95%) | loss: 3.476868 | lrm: 1.00 | dt: 2604.39ms | tok/sec: 201,309 | mfu: 51.76 | epoch: 1 | total time: 20.36m | eta: 56.4m\n",
      "step 00482/01785 (27.00%) | loss: 3.482572 | lrm: 1.00 | dt: 2604.23ms | tok/sec: 201,321 | mfu: 51.76 | epoch: 1 | total time: 20.40m | eta: 56.3m\n",
      "step 00483/01785 (27.06%) | loss: 3.475462 | lrm: 1.00 | dt: 2592.10ms | tok/sec: 202,263 | mfu: 52.00 | epoch: 1 | total time: 20.44m | eta: 56.3m\n",
      "step 00484/01785 (27.11%) | loss: 3.480315 | lrm: 1.00 | dt: 2592.77ms | tok/sec: 202,211 | mfu: 51.99 | epoch: 1 | total time: 20.49m | eta: 56.2m\n",
      "step 00485/01785 (27.17%) | loss: 3.479957 | lrm: 1.00 | dt: 2605.02ms | tok/sec: 201,260 | mfu: 51.75 | epoch: 1 | total time: 20.53m | eta: 56.2m\n",
      "step 00486/01785 (27.23%) | loss: 3.475989 | lrm: 1.00 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 20.57m | eta: 56.1m\n",
      "step 00487/01785 (27.28%) | loss: 3.478339 | lrm: 1.00 | dt: 2604.51ms | tok/sec: 201,299 | mfu: 51.76 | epoch: 1 | total time: 20.62m | eta: 56.1m\n",
      "step 00488/01785 (27.34%) | loss: 3.473569 | lrm: 1.00 | dt: 2604.28ms | tok/sec: 201,318 | mfu: 51.76 | epoch: 1 | total time: 20.66m | eta: 56.1m\n",
      "step 00489/01785 (27.39%) | loss: 3.479861 | lrm: 1.00 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 1 | total time: 20.70m | eta: 56.0m\n",
      "step 00490/01785 (27.45%) | loss: 3.479294 | lrm: 1.00 | dt: 2603.82ms | tok/sec: 201,353 | mfu: 51.77 | epoch: 1 | total time: 20.75m | eta: 56.0m\n",
      "step 00491/01785 (27.51%) | loss: 3.476216 | lrm: 1.00 | dt: 2588.81ms | tok/sec: 202,520 | mfu: 52.07 | epoch: 1 | total time: 20.79m | eta: 55.9m\n",
      "step 00492/01785 (27.56%) | loss: 3.479475 | lrm: 1.00 | dt: 2604.63ms | tok/sec: 201,290 | mfu: 51.75 | epoch: 1 | total time: 20.83m | eta: 55.9m\n",
      "step 00493/01785 (27.62%) | loss: 3.481630 | lrm: 1.00 | dt: 2590.29ms | tok/sec: 202,405 | mfu: 52.04 | epoch: 1 | total time: 20.88m | eta: 55.8m\n",
      "step 00494/01785 (27.68%) | loss: 3.478897 | lrm: 1.00 | dt: 2607.08ms | tok/sec: 201,101 | mfu: 51.70 | epoch: 1 | total time: 20.92m | eta: 55.8m\n",
      "step 00495/01785 (27.73%) | loss: 3.475022 | lrm: 1.00 | dt: 2592.14ms | tok/sec: 202,260 | mfu: 52.00 | epoch: 1 | total time: 20.96m | eta: 55.8m\n",
      "step 00496/01785 (27.79%) | loss: 3.471261 | lrm: 1.00 | dt: 2593.49ms | tok/sec: 202,155 | mfu: 51.98 | epoch: 1 | total time: 21.01m | eta: 55.7m\n",
      "step 00497/01785 (27.84%) | loss: 3.481628 | lrm: 1.00 | dt: 2593.34ms | tok/sec: 202,166 | mfu: 51.98 | epoch: 1 | total time: 21.05m | eta: 55.7m\n",
      "step 00498/01785 (27.90%) | loss: 3.473552 | lrm: 1.00 | dt: 2600.24ms | tok/sec: 201,630 | mfu: 51.84 | epoch: 1 | total time: 21.09m | eta: 55.6m\n",
      "step 00499/01785 (27.96%) | loss: 3.468074 | lrm: 1.00 | dt: 2604.88ms | tok/sec: 201,271 | mfu: 51.75 | epoch: 1 | total time: 21.14m | eta: 55.6m\n",
      "Step 00500 | Validation bpb: 1.047455\n",
      "step 00500/01785 (28.01%) | loss: 3.449995 | lrm: 1.00 | dt: 2579.78ms | tok/sec: 203,230 | mfu: 52.25 | epoch: 1 | total time: 21.18m | eta: 55.5m\n",
      "step 00501/01785 (28.07%) | loss: 3.448461 | lrm: 1.00 | dt: 2577.43ms | tok/sec: 203,414 | mfu: 52.30 | epoch: 1 | total time: 21.22m | eta: 55.5m\n",
      "step 00502/01785 (28.12%) | loss: 3.440283 | lrm: 1.00 | dt: 2578.49ms | tok/sec: 203,331 | mfu: 52.28 | epoch: 1 | total time: 21.27m | eta: 55.5m\n",
      "step 00503/01785 (28.18%) | loss: 3.444780 | lrm: 1.00 | dt: 2585.49ms | tok/sec: 202,780 | mfu: 52.14 | epoch: 1 | total time: 21.31m | eta: 55.4m\n",
      "step 00504/01785 (28.24%) | loss: 3.448465 | lrm: 1.00 | dt: 2580.23ms | tok/sec: 203,194 | mfu: 52.24 | epoch: 1 | total time: 21.35m | eta: 55.4m\n",
      "step 00505/01785 (28.29%) | loss: 3.449979 | lrm: 1.00 | dt: 2584.84ms | tok/sec: 202,831 | mfu: 52.15 | epoch: 1 | total time: 21.39m | eta: 55.3m\n",
      "step 00506/01785 (28.35%) | loss: 3.451351 | lrm: 1.00 | dt: 2586.31ms | tok/sec: 202,716 | mfu: 52.12 | epoch: 1 | total time: 21.44m | eta: 55.3m\n",
      "step 00507/01785 (28.40%) | loss: 3.441383 | lrm: 1.00 | dt: 2586.67ms | tok/sec: 202,688 | mfu: 52.11 | epoch: 1 | total time: 21.48m | eta: 55.2m\n",
      "step 00508/01785 (28.46%) | loss: 3.433244 | lrm: 1.00 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 52.18 | epoch: 1 | total time: 21.52m | eta: 55.2m\n",
      "step 00509/01785 (28.52%) | loss: 3.436008 | lrm: 1.00 | dt: 2590.15ms | tok/sec: 202,415 | mfu: 52.04 | epoch: 1 | total time: 21.57m | eta: 55.1m\n",
      "step 00510/01785 (28.57%) | loss: 3.429742 | lrm: 1.00 | dt: 2584.23ms | tok/sec: 202,879 | mfu: 52.16 | epoch: 1 | total time: 21.61m | eta: 55.1m\n",
      "step 00511/01785 (28.63%) | loss: 3.435494 | lrm: 1.00 | dt: 2584.50ms | tok/sec: 202,858 | mfu: 52.16 | epoch: 1 | total time: 21.65m | eta: 55.1m\n",
      "step 00512/01785 (28.68%) | loss: 3.434562 | lrm: 1.00 | dt: 2582.84ms | tok/sec: 202,988 | mfu: 52.19 | epoch: 1 | total time: 21.70m | eta: 55.0m\n",
      "step 00513/01785 (28.74%) | loss: 3.424186 | lrm: 1.00 | dt: 2582.01ms | tok/sec: 203,053 | mfu: 52.21 | epoch: 1 | total time: 21.74m | eta: 55.0m\n",
      "step 00514/01785 (28.80%) | loss: 3.431395 | lrm: 1.00 | dt: 2583.57ms | tok/sec: 202,931 | mfu: 52.17 | epoch: 1 | total time: 21.78m | eta: 54.9m\n",
      "step 00515/01785 (28.85%) | loss: 3.429764 | lrm: 1.00 | dt: 2583.87ms | tok/sec: 202,907 | mfu: 52.17 | epoch: 1 | total time: 21.83m | eta: 54.9m\n",
      "step 00516/01785 (28.91%) | loss: 3.439927 | lrm: 1.00 | dt: 2583.43ms | tok/sec: 202,942 | mfu: 52.18 | epoch: 1 | total time: 21.87m | eta: 54.8m\n",
      "step 00517/01785 (28.96%) | loss: 3.435185 | lrm: 1.00 | dt: 2588.47ms | tok/sec: 202,547 | mfu: 52.08 | epoch: 1 | total time: 21.91m | eta: 54.8m\n",
      "step 00518/01785 (29.02%) | loss: 3.424574 | lrm: 1.00 | dt: 2596.64ms | tok/sec: 201,909 | mfu: 51.91 | epoch: 1 | total time: 21.96m | eta: 54.8m\n",
      "step 00519/01785 (29.08%) | loss: 3.425338 | lrm: 1.00 | dt: 2584.67ms | tok/sec: 202,845 | mfu: 52.15 | epoch: 1 | total time: 22.00m | eta: 54.7m\n",
      "step 00520/01785 (29.13%) | loss: 3.410503 | lrm: 1.00 | dt: 2585.70ms | tok/sec: 202,764 | mfu: 52.13 | epoch: 1 | total time: 22.04m | eta: 54.7m\n",
      "step 00521/01785 (29.19%) | loss: 3.406566 | lrm: 1.00 | dt: 2586.69ms | tok/sec: 202,686 | mfu: 52.11 | epoch: 1 | total time: 22.08m | eta: 54.6m\n",
      "step 00522/01785 (29.24%) | loss: 3.405497 | lrm: 1.00 | dt: 2597.57ms | tok/sec: 201,838 | mfu: 51.89 | epoch: 1 | total time: 22.13m | eta: 54.6m\n",
      "step 00523/01785 (29.30%) | loss: 3.413472 | lrm: 1.00 | dt: 2587.07ms | tok/sec: 202,657 | mfu: 52.10 | epoch: 1 | total time: 22.17m | eta: 54.5m\n",
      "step 00524/01785 (29.36%) | loss: 3.413023 | lrm: 1.00 | dt: 2585.18ms | tok/sec: 202,805 | mfu: 52.14 | epoch: 1 | total time: 22.21m | eta: 54.5m\n",
      "step 00525/01785 (29.41%) | loss: 3.416991 | lrm: 1.00 | dt: 2586.99ms | tok/sec: 202,663 | mfu: 52.11 | epoch: 1 | total time: 22.26m | eta: 54.5m\n",
      "step 00526/01785 (29.47%) | loss: 3.410746 | lrm: 1.00 | dt: 2587.28ms | tok/sec: 202,640 | mfu: 52.10 | epoch: 1 | total time: 22.30m | eta: 54.4m\n",
      "step 00527/01785 (29.52%) | loss: 3.395362 | lrm: 1.00 | dt: 2588.96ms | tok/sec: 202,509 | mfu: 52.07 | epoch: 1 | total time: 22.34m | eta: 54.4m\n",
      "step 00528/01785 (29.58%) | loss: 3.400386 | lrm: 1.00 | dt: 2588.57ms | tok/sec: 202,539 | mfu: 52.07 | epoch: 1 | total time: 22.39m | eta: 54.3m\n",
      "step 00529/01785 (29.64%) | loss: 3.400859 | lrm: 1.00 | dt: 2587.03ms | tok/sec: 202,660 | mfu: 52.10 | epoch: 1 | total time: 22.43m | eta: 54.3m\n",
      "step 00530/01785 (29.69%) | loss: 3.401319 | lrm: 1.00 | dt: 2598.39ms | tok/sec: 201,774 | mfu: 51.88 | epoch: 1 | total time: 22.47m | eta: 54.2m\n",
      "step 00531/01785 (29.75%) | loss: 3.403246 | lrm: 1.00 | dt: 2587.87ms | tok/sec: 202,594 | mfu: 52.09 | epoch: 1 | total time: 22.52m | eta: 54.2m\n",
      "step 00532/01785 (29.80%) | loss: 3.414134 | lrm: 1.00 | dt: 2586.81ms | tok/sec: 202,677 | mfu: 52.11 | epoch: 1 | total time: 22.56m | eta: 54.2m\n",
      "step 00533/01785 (29.86%) | loss: 3.424202 | lrm: 1.00 | dt: 2601.21ms | tok/sec: 201,555 | mfu: 51.82 | epoch: 1 | total time: 22.60m | eta: 54.1m\n",
      "step 00534/01785 (29.92%) | loss: 3.430299 | lrm: 1.00 | dt: 2586.12ms | tok/sec: 202,731 | mfu: 52.12 | epoch: 1 | total time: 22.65m | eta: 54.1m\n",
      "step 00535/01785 (29.97%) | loss: 3.428682 | lrm: 1.00 | dt: 2601.18ms | tok/sec: 201,557 | mfu: 51.82 | epoch: 1 | total time: 22.69m | eta: 54.0m\n",
      "step 00536/01785 (30.03%) | loss: 3.426910 | lrm: 1.00 | dt: 2585.46ms | tok/sec: 202,783 | mfu: 52.14 | epoch: 1 | total time: 22.73m | eta: 54.0m\n",
      "step 00537/01785 (30.08%) | loss: 3.422179 | lrm: 1.00 | dt: 2601.24ms | tok/sec: 201,553 | mfu: 51.82 | epoch: 1 | total time: 22.78m | eta: 53.9m\n",
      "step 00538/01785 (30.14%) | loss: 3.412679 | lrm: 1.00 | dt: 2587.69ms | tok/sec: 202,608 | mfu: 52.09 | epoch: 1 | total time: 22.82m | eta: 53.9m\n",
      "step 00539/01785 (30.20%) | loss: 3.414114 | lrm: 1.00 | dt: 2601.38ms | tok/sec: 201,542 | mfu: 51.82 | epoch: 1 | total time: 22.86m | eta: 53.8m\n",
      "step 00540/01785 (30.25%) | loss: 3.413471 | lrm: 1.00 | dt: 2596.01ms | tok/sec: 201,959 | mfu: 51.92 | epoch: 1 | total time: 22.91m | eta: 53.8m\n",
      "step 00541/01785 (30.31%) | loss: 3.397428 | lrm: 1.00 | dt: 2592.66ms | tok/sec: 202,220 | mfu: 51.99 | epoch: 1 | total time: 22.95m | eta: 53.8m\n",
      "step 00542/01785 (30.36%) | loss: 3.411480 | lrm: 1.00 | dt: 2591.93ms | tok/sec: 202,276 | mfu: 52.01 | epoch: 1 | total time: 22.99m | eta: 53.7m\n",
      "step 00543/01785 (30.42%) | loss: 3.406707 | lrm: 1.00 | dt: 2595.04ms | tok/sec: 202,034 | mfu: 51.94 | epoch: 1 | total time: 23.03m | eta: 53.7m\n",
      "step 00544/01785 (30.48%) | loss: 3.417183 | lrm: 1.00 | dt: 2585.93ms | tok/sec: 202,746 | mfu: 52.13 | epoch: 1 | total time: 23.08m | eta: 53.6m\n",
      "step 00545/01785 (30.53%) | loss: 3.413734 | lrm: 1.00 | dt: 2603.73ms | tok/sec: 201,360 | mfu: 51.77 | epoch: 1 | total time: 23.12m | eta: 53.6m\n",
      "step 00546/01785 (30.59%) | loss: 3.412457 | lrm: 1.00 | dt: 2588.53ms | tok/sec: 202,542 | mfu: 52.07 | epoch: 1 | total time: 23.16m | eta: 53.5m\n",
      "step 00547/01785 (30.64%) | loss: 3.408809 | lrm: 1.00 | dt: 2597.57ms | tok/sec: 201,837 | mfu: 51.89 | epoch: 1 | total time: 23.21m | eta: 53.5m\n",
      "step 00548/01785 (30.70%) | loss: 3.408008 | lrm: 1.00 | dt: 2588.53ms | tok/sec: 202,542 | mfu: 52.07 | epoch: 1 | total time: 23.25m | eta: 53.5m\n",
      "step 00549/01785 (30.76%) | loss: 3.405750 | lrm: 1.00 | dt: 2602.31ms | tok/sec: 201,470 | mfu: 51.80 | epoch: 1 | total time: 23.29m | eta: 53.4m\n",
      "step 00550/01785 (30.81%) | loss: 3.408833 | lrm: 1.00 | dt: 2608.49ms | tok/sec: 200,993 | mfu: 51.68 | epoch: 1 | total time: 23.34m | eta: 53.4m\n",
      "step 00551/01785 (30.87%) | loss: 3.405258 | lrm: 1.00 | dt: 2595.19ms | tok/sec: 202,023 | mfu: 51.94 | epoch: 1 | total time: 23.38m | eta: 53.3m\n",
      "step 00552/01785 (30.92%) | loss: 3.408499 | lrm: 1.00 | dt: 2596.50ms | tok/sec: 201,920 | mfu: 51.91 | epoch: 1 | total time: 23.42m | eta: 53.3m\n",
      "step 00553/01785 (30.98%) | loss: 3.418875 | lrm: 1.00 | dt: 2588.96ms | tok/sec: 202,508 | mfu: 52.07 | epoch: 1 | total time: 23.47m | eta: 53.2m\n",
      "step 00554/01785 (31.04%) | loss: 3.401555 | lrm: 1.00 | dt: 2596.37ms | tok/sec: 201,931 | mfu: 51.92 | epoch: 1 | total time: 23.51m | eta: 53.2m\n",
      "step 00555/01785 (31.09%) | loss: 3.400584 | lrm: 1.00 | dt: 2596.29ms | tok/sec: 201,937 | mfu: 51.92 | epoch: 1 | total time: 23.55m | eta: 53.2m\n",
      "step 00556/01785 (31.15%) | loss: 3.396399 | lrm: 1.00 | dt: 2602.12ms | tok/sec: 201,485 | mfu: 51.80 | epoch: 1 | total time: 23.60m | eta: 53.1m\n",
      "step 00557/01785 (31.20%) | loss: 3.410413 | lrm: 1.00 | dt: 2586.18ms | tok/sec: 202,727 | mfu: 52.12 | epoch: 1 | total time: 23.64m | eta: 53.1m\n",
      "step 00558/01785 (31.26%) | loss: 3.416359 | lrm: 1.00 | dt: 2602.96ms | tok/sec: 201,420 | mfu: 51.79 | epoch: 1 | total time: 23.68m | eta: 53.0m\n",
      "step 00559/01785 (31.32%) | loss: 3.412064 | lrm: 1.00 | dt: 2605.00ms | tok/sec: 201,261 | mfu: 51.75 | epoch: 1 | total time: 23.73m | eta: 53.0m\n",
      "step 00560/01785 (31.37%) | loss: 3.414849 | lrm: 1.00 | dt: 2590.42ms | tok/sec: 202,394 | mfu: 52.04 | epoch: 1 | total time: 23.77m | eta: 52.9m\n",
      "step 00561/01785 (31.43%) | loss: 3.406394 | lrm: 1.00 | dt: 2597.79ms | tok/sec: 201,820 | mfu: 51.89 | epoch: 1 | total time: 23.81m | eta: 52.9m\n",
      "step 00562/01785 (31.48%) | loss: 3.406501 | lrm: 1.00 | dt: 2603.46ms | tok/sec: 201,381 | mfu: 51.78 | epoch: 1 | total time: 23.86m | eta: 52.9m\n",
      "step 00563/01785 (31.54%) | loss: 3.407095 | lrm: 1.00 | dt: 2591.13ms | tok/sec: 202,339 | mfu: 52.02 | epoch: 1 | total time: 23.90m | eta: 52.8m\n",
      "step 00564/01785 (31.60%) | loss: 3.395743 | lrm: 1.00 | dt: 2596.53ms | tok/sec: 201,918 | mfu: 51.91 | epoch: 1 | total time: 23.94m | eta: 52.8m\n",
      "step 00565/01785 (31.65%) | loss: 3.388876 | lrm: 1.00 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 23.99m | eta: 52.7m\n",
      "step 00566/01785 (31.71%) | loss: 3.390816 | lrm: 1.00 | dt: 2604.11ms | tok/sec: 201,331 | mfu: 51.76 | epoch: 1 | total time: 24.03m | eta: 52.7m\n",
      "step 00567/01785 (31.76%) | loss: 3.388317 | lrm: 1.00 | dt: 2604.50ms | tok/sec: 201,300 | mfu: 51.76 | epoch: 1 | total time: 24.07m | eta: 52.6m\n",
      "step 00568/01785 (31.82%) | loss: 3.396239 | lrm: 1.00 | dt: 2603.48ms | tok/sec: 201,379 | mfu: 51.78 | epoch: 1 | total time: 24.12m | eta: 52.6m\n",
      "step 00569/01785 (31.88%) | loss: 3.394726 | lrm: 1.00 | dt: 2594.77ms | tok/sec: 202,056 | mfu: 51.95 | epoch: 1 | total time: 24.16m | eta: 52.6m\n",
      "step 00570/01785 (31.93%) | loss: 3.395342 | lrm: 1.00 | dt: 2590.16ms | tok/sec: 202,414 | mfu: 52.04 | epoch: 1 | total time: 24.20m | eta: 52.5m\n",
      "step 00571/01785 (31.99%) | loss: 3.394716 | lrm: 1.00 | dt: 2602.99ms | tok/sec: 201,417 | mfu: 51.79 | epoch: 1 | total time: 24.25m | eta: 52.5m\n",
      "step 00572/01785 (32.04%) | loss: 3.393462 | lrm: 1.00 | dt: 2595.43ms | tok/sec: 202,004 | mfu: 51.94 | epoch: 1 | total time: 24.29m | eta: 52.4m\n",
      "step 00573/01785 (32.10%) | loss: 3.412192 | lrm: 1.00 | dt: 2589.90ms | tok/sec: 202,435 | mfu: 52.05 | epoch: 1 | total time: 24.33m | eta: 52.4m\n",
      "step 00574/01785 (32.16%) | loss: 3.414136 | lrm: 1.00 | dt: 2605.29ms | tok/sec: 201,239 | mfu: 51.74 | epoch: 1 | total time: 24.38m | eta: 52.3m\n",
      "step 00575/01785 (32.21%) | loss: 3.417333 | lrm: 1.00 | dt: 2606.09ms | tok/sec: 201,177 | mfu: 51.72 | epoch: 1 | total time: 24.42m | eta: 52.3m\n",
      "step 00576/01785 (32.27%) | loss: 3.420090 | lrm: 1.00 | dt: 2589.97ms | tok/sec: 202,430 | mfu: 52.05 | epoch: 1 | total time: 24.46m | eta: 52.3m\n",
      "step 00577/01785 (32.32%) | loss: 3.423876 | lrm: 1.00 | dt: 2603.96ms | tok/sec: 201,342 | mfu: 51.77 | epoch: 1 | total time: 24.51m | eta: 52.2m\n",
      "step 00578/01785 (32.38%) | loss: 3.406726 | lrm: 1.00 | dt: 2588.10ms | tok/sec: 202,576 | mfu: 52.08 | epoch: 1 | total time: 24.55m | eta: 52.2m\n",
      "step 00579/01785 (32.44%) | loss: 3.398379 | lrm: 1.00 | dt: 2602.68ms | tok/sec: 201,441 | mfu: 51.79 | epoch: 1 | total time: 24.59m | eta: 52.1m\n",
      "step 00580/01785 (32.49%) | loss: 3.387149 | lrm: 1.00 | dt: 2593.85ms | tok/sec: 202,127 | mfu: 51.97 | epoch: 1 | total time: 24.64m | eta: 52.1m\n",
      "step 00581/01785 (32.55%) | loss: 3.400288 | lrm: 1.00 | dt: 2597.86ms | tok/sec: 201,815 | mfu: 51.89 | epoch: 1 | total time: 24.68m | eta: 52.0m\n",
      "step 00582/01785 (32.61%) | loss: 3.405146 | lrm: 1.00 | dt: 2589.99ms | tok/sec: 202,428 | mfu: 52.05 | epoch: 1 | total time: 24.72m | eta: 52.0m\n",
      "step 00583/01785 (32.66%) | loss: 3.405466 | lrm: 1.00 | dt: 2586.43ms | tok/sec: 202,707 | mfu: 52.12 | epoch: 1 | total time: 24.77m | eta: 52.0m\n",
      "step 00584/01785 (32.72%) | loss: 3.415080 | lrm: 1.00 | dt: 2601.13ms | tok/sec: 201,561 | mfu: 51.82 | epoch: 1 | total time: 24.81m | eta: 51.9m\n",
      "step 00585/01785 (32.77%) | loss: 3.412606 | lrm: 1.00 | dt: 2605.95ms | tok/sec: 201,188 | mfu: 51.73 | epoch: 1 | total time: 24.85m | eta: 51.9m\n",
      "step 00586/01785 (32.83%) | loss: 3.398246 | lrm: 1.00 | dt: 2589.90ms | tok/sec: 202,435 | mfu: 52.05 | epoch: 1 | total time: 24.90m | eta: 51.8m\n",
      "step 00587/01785 (32.89%) | loss: 3.404557 | lrm: 1.00 | dt: 2596.40ms | tok/sec: 201,928 | mfu: 51.92 | epoch: 1 | total time: 24.94m | eta: 51.8m\n",
      "step 00588/01785 (32.94%) | loss: 3.404805 | lrm: 1.00 | dt: 2605.26ms | tok/sec: 201,242 | mfu: 51.74 | epoch: 1 | total time: 24.98m | eta: 51.7m\n",
      "step 00589/01785 (33.00%) | loss: 3.401200 | lrm: 1.00 | dt: 2589.01ms | tok/sec: 202,504 | mfu: 52.07 | epoch: 1 | total time: 25.03m | eta: 51.7m\n",
      "step 00590/01785 (33.05%) | loss: 3.392163 | lrm: 1.00 | dt: 2598.48ms | tok/sec: 201,766 | mfu: 51.88 | epoch: 1 | total time: 25.07m | eta: 51.7m\n",
      "step 00591/01785 (33.11%) | loss: 3.392442 | lrm: 1.00 | dt: 2593.91ms | tok/sec: 202,122 | mfu: 51.97 | epoch: 1 | total time: 25.11m | eta: 51.6m\n",
      "step 00592/01785 (33.17%) | loss: 3.393027 | lrm: 1.00 | dt: 2591.80ms | tok/sec: 202,286 | mfu: 52.01 | epoch: 1 | total time: 25.16m | eta: 51.6m\n",
      "step 00593/01785 (33.22%) | loss: 3.401576 | lrm: 1.00 | dt: 2604.11ms | tok/sec: 201,331 | mfu: 51.76 | epoch: 1 | total time: 25.20m | eta: 51.5m\n",
      "step 00594/01785 (33.28%) | loss: 3.405575 | lrm: 1.00 | dt: 2586.91ms | tok/sec: 202,669 | mfu: 52.11 | epoch: 1 | total time: 25.24m | eta: 51.5m\n",
      "step 00595/01785 (33.33%) | loss: 3.395208 | lrm: 1.00 | dt: 2603.11ms | tok/sec: 201,408 | mfu: 51.78 | epoch: 1 | total time: 25.29m | eta: 51.4m\n",
      "step 00596/01785 (33.39%) | loss: 3.382420 | lrm: 1.00 | dt: 2603.35ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 1 | total time: 25.33m | eta: 51.4m\n",
      "step 00597/01785 (33.45%) | loss: 3.388803 | lrm: 1.00 | dt: 2593.98ms | tok/sec: 202,117 | mfu: 51.97 | epoch: 1 | total time: 25.37m | eta: 51.3m\n",
      "step 00598/01785 (33.50%) | loss: 3.385288 | lrm: 1.00 | dt: 2593.11ms | tok/sec: 202,184 | mfu: 51.98 | epoch: 1 | total time: 25.42m | eta: 51.3m\n",
      "step 00599/01785 (33.56%) | loss: 3.382871 | lrm: 1.00 | dt: 2604.84ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 25.46m | eta: 51.3m\n",
      "step 00600/01785 (33.61%) | loss: 3.387034 | lrm: 1.00 | dt: 2591.64ms | tok/sec: 202,299 | mfu: 52.01 | epoch: 1 | total time: 25.50m | eta: 51.2m\n",
      "step 00601/01785 (33.67%) | loss: 3.379223 | lrm: 1.00 | dt: 2595.56ms | tok/sec: 201,994 | mfu: 51.93 | epoch: 1 | total time: 25.55m | eta: 51.2m\n",
      "step 00602/01785 (33.73%) | loss: 3.378236 | lrm: 1.00 | dt: 2595.27ms | tok/sec: 202,016 | mfu: 51.94 | epoch: 1 | total time: 25.59m | eta: 51.1m\n",
      "step 00603/01785 (33.78%) | loss: 3.373546 | lrm: 1.00 | dt: 2598.64ms | tok/sec: 201,755 | mfu: 51.87 | epoch: 1 | total time: 25.63m | eta: 51.1m\n",
      "step 00604/01785 (33.84%) | loss: 3.370657 | lrm: 1.00 | dt: 2590.36ms | tok/sec: 202,399 | mfu: 52.04 | epoch: 1 | total time: 25.67m | eta: 51.0m\n",
      "step 00605/01785 (33.89%) | loss: 3.364588 | lrm: 1.00 | dt: 2605.24ms | tok/sec: 201,244 | mfu: 51.74 | epoch: 1 | total time: 25.72m | eta: 51.0m\n",
      "step 00606/01785 (33.95%) | loss: 3.346358 | lrm: 1.00 | dt: 2592.74ms | tok/sec: 202,213 | mfu: 51.99 | epoch: 1 | total time: 25.76m | eta: 51.0m\n",
      "step 00607/01785 (34.01%) | loss: 3.347587 | lrm: 1.00 | dt: 2591.75ms | tok/sec: 202,290 | mfu: 52.01 | epoch: 1 | total time: 25.80m | eta: 50.9m\n",
      "step 00608/01785 (34.06%) | loss: 3.352312 | lrm: 1.00 | dt: 2589.79ms | tok/sec: 202,443 | mfu: 52.05 | epoch: 1 | total time: 25.85m | eta: 50.9m\n",
      "step 00609/01785 (34.12%) | loss: 3.351701 | lrm: 1.00 | dt: 2603.88ms | tok/sec: 201,348 | mfu: 51.77 | epoch: 1 | total time: 25.89m | eta: 50.8m\n",
      "step 00610/01785 (34.17%) | loss: 3.368520 | lrm: 1.00 | dt: 2604.47ms | tok/sec: 201,303 | mfu: 51.76 | epoch: 1 | total time: 25.93m | eta: 50.8m\n",
      "step 00611/01785 (34.23%) | loss: 3.365200 | lrm: 1.00 | dt: 2589.16ms | tok/sec: 202,493 | mfu: 52.06 | epoch: 1 | total time: 25.98m | eta: 50.7m\n",
      "step 00612/01785 (34.29%) | loss: 3.352350 | lrm: 1.00 | dt: 2597.97ms | tok/sec: 201,806 | mfu: 51.89 | epoch: 1 | total time: 26.02m | eta: 50.7m\n",
      "step 00613/01785 (34.34%) | loss: 3.373470 | lrm: 1.00 | dt: 2605.58ms | tok/sec: 201,217 | mfu: 51.73 | epoch: 1 | total time: 26.06m | eta: 50.7m\n",
      "step 00614/01785 (34.40%) | loss: 3.366465 | lrm: 1.00 | dt: 2604.93ms | tok/sec: 201,267 | mfu: 51.75 | epoch: 1 | total time: 26.11m | eta: 50.6m\n",
      "step 00615/01785 (34.45%) | loss: 3.376470 | lrm: 1.00 | dt: 2593.23ms | tok/sec: 202,175 | mfu: 51.98 | epoch: 1 | total time: 26.15m | eta: 50.6m\n",
      "step 00616/01785 (34.51%) | loss: 3.378361 | lrm: 1.00 | dt: 2593.67ms | tok/sec: 202,141 | mfu: 51.97 | epoch: 1 | total time: 26.19m | eta: 50.5m\n",
      "step 00617/01785 (34.57%) | loss: 3.386322 | lrm: 1.00 | dt: 2603.44ms | tok/sec: 201,382 | mfu: 51.78 | epoch: 1 | total time: 26.24m | eta: 50.5m\n",
      "step 00618/01785 (34.62%) | loss: 3.383446 | lrm: 1.00 | dt: 2603.89ms | tok/sec: 201,347 | mfu: 51.77 | epoch: 1 | total time: 26.28m | eta: 50.4m\n",
      "step 00619/01785 (34.68%) | loss: 3.389045 | lrm: 1.00 | dt: 2597.09ms | tok/sec: 201,875 | mfu: 51.90 | epoch: 1 | total time: 26.32m | eta: 50.4m\n",
      "step 00620/01785 (34.73%) | loss: 3.383423 | lrm: 1.00 | dt: 2587.25ms | tok/sec: 202,643 | mfu: 52.10 | epoch: 1 | total time: 26.37m | eta: 50.4m\n",
      "step 00621/01785 (34.79%) | loss: 3.384249 | lrm: 1.00 | dt: 2603.34ms | tok/sec: 201,390 | mfu: 51.78 | epoch: 1 | total time: 26.41m | eta: 50.3m\n",
      "step 00622/01785 (34.85%) | loss: 3.383861 | lrm: 1.00 | dt: 2591.67ms | tok/sec: 202,297 | mfu: 52.01 | epoch: 1 | total time: 26.45m | eta: 50.3m\n",
      "step 00623/01785 (34.90%) | loss: 3.375326 | lrm: 1.00 | dt: 2595.41ms | tok/sec: 202,005 | mfu: 51.94 | epoch: 1 | total time: 26.50m | eta: 50.2m\n",
      "step 00624/01785 (34.96%) | loss: 3.389003 | lrm: 1.00 | dt: 2604.59ms | tok/sec: 201,293 | mfu: 51.75 | epoch: 1 | total time: 26.54m | eta: 50.2m\n",
      "step 00625/01785 (35.01%) | loss: 3.399471 | lrm: 1.00 | dt: 2604.21ms | tok/sec: 201,323 | mfu: 51.76 | epoch: 1 | total time: 26.58m | eta: 50.1m\n",
      "step 00626/01785 (35.07%) | loss: 3.401125 | lrm: 1.00 | dt: 2590.33ms | tok/sec: 202,401 | mfu: 52.04 | epoch: 1 | total time: 26.63m | eta: 50.1m\n",
      "step 00627/01785 (35.13%) | loss: 3.411040 | lrm: 1.00 | dt: 2600.07ms | tok/sec: 201,643 | mfu: 51.84 | epoch: 1 | total time: 26.67m | eta: 50.1m\n",
      "step 00628/01785 (35.18%) | loss: 3.396074 | lrm: 1.00 | dt: 2608.15ms | tok/sec: 201,018 | mfu: 51.68 | epoch: 1 | total time: 26.71m | eta: 50.0m\n",
      "step 00629/01785 (35.24%) | loss: 3.389535 | lrm: 1.00 | dt: 2589.02ms | tok/sec: 202,504 | mfu: 52.06 | epoch: 1 | total time: 26.76m | eta: 50.0m\n",
      "step 00630/01785 (35.29%) | loss: 3.397261 | lrm: 1.00 | dt: 2587.86ms | tok/sec: 202,594 | mfu: 52.09 | epoch: 1 | total time: 26.80m | eta: 49.9m\n",
      "step 00631/01785 (35.35%) | loss: 3.402314 | lrm: 1.00 | dt: 2599.58ms | tok/sec: 201,681 | mfu: 51.85 | epoch: 1 | total time: 26.84m | eta: 49.9m\n",
      "step 00632/01785 (35.41%) | loss: 3.381641 | lrm: 1.00 | dt: 2594.60ms | tok/sec: 202,068 | mfu: 51.95 | epoch: 1 | total time: 26.89m | eta: 49.8m\n",
      "step 00633/01785 (35.46%) | loss: 3.390702 | lrm: 1.00 | dt: 2597.80ms | tok/sec: 201,820 | mfu: 51.89 | epoch: 1 | total time: 26.93m | eta: 49.8m\n",
      "step 00634/01785 (35.52%) | loss: 3.401190 | lrm: 1.00 | dt: 2590.13ms | tok/sec: 202,417 | mfu: 52.04 | epoch: 1 | total time: 26.97m | eta: 49.8m\n",
      "step 00635/01785 (35.57%) | loss: 3.406786 | lrm: 1.00 | dt: 2596.47ms | tok/sec: 201,923 | mfu: 51.92 | epoch: 1 | total time: 27.02m | eta: 49.7m\n",
      "step 00636/01785 (35.63%) | loss: 3.407907 | lrm: 1.00 | dt: 2592.13ms | tok/sec: 202,261 | mfu: 52.00 | epoch: 1 | total time: 27.06m | eta: 49.7m\n",
      "step 00637/01785 (35.69%) | loss: 3.403279 | lrm: 1.00 | dt: 2605.05ms | tok/sec: 201,258 | mfu: 51.74 | epoch: 1 | total time: 27.10m | eta: 49.6m\n",
      "step 00638/01785 (35.74%) | loss: 3.401806 | lrm: 1.00 | dt: 2602.25ms | tok/sec: 201,475 | mfu: 51.80 | epoch: 1 | total time: 27.15m | eta: 49.6m\n",
      "step 00639/01785 (35.80%) | loss: 3.415313 | lrm: 1.00 | dt: 2591.78ms | tok/sec: 202,288 | mfu: 52.01 | epoch: 1 | total time: 27.19m | eta: 49.5m\n",
      "step 00640/01785 (35.85%) | loss: 3.412692 | lrm: 1.00 | dt: 2593.15ms | tok/sec: 202,181 | mfu: 51.98 | epoch: 1 | total time: 27.23m | eta: 49.5m\n",
      "step 00641/01785 (35.91%) | loss: 3.414086 | lrm: 1.00 | dt: 2606.08ms | tok/sec: 201,178 | mfu: 51.72 | epoch: 1 | total time: 27.28m | eta: 49.5m\n",
      "step 00642/01785 (35.97%) | loss: 3.398889 | lrm: 1.00 | dt: 2604.36ms | tok/sec: 201,312 | mfu: 51.76 | epoch: 1 | total time: 27.32m | eta: 49.4m\n",
      "step 00643/01785 (36.02%) | loss: 3.385633 | lrm: 1.00 | dt: 2604.89ms | tok/sec: 201,270 | mfu: 51.75 | epoch: 1 | total time: 27.36m | eta: 49.4m\n",
      "step 00644/01785 (36.08%) | loss: 3.379077 | lrm: 1.00 | dt: 2603.94ms | tok/sec: 201,344 | mfu: 51.77 | epoch: 1 | total time: 27.41m | eta: 49.3m\n",
      "step 00645/01785 (36.13%) | loss: 3.373106 | lrm: 1.00 | dt: 2588.14ms | tok/sec: 202,572 | mfu: 52.08 | epoch: 1 | total time: 27.45m | eta: 49.3m\n",
      "step 00646/01785 (36.19%) | loss: 3.374415 | lrm: 1.00 | dt: 2598.90ms | tok/sec: 201,734 | mfu: 51.87 | epoch: 1 | total time: 27.49m | eta: 49.2m\n",
      "step 00647/01785 (36.25%) | loss: 3.374792 | lrm: 1.00 | dt: 2606.13ms | tok/sec: 201,174 | mfu: 51.72 | epoch: 1 | total time: 27.54m | eta: 49.2m\n",
      "step 00648/01785 (36.30%) | loss: 3.373843 | lrm: 1.00 | dt: 2592.92ms | tok/sec: 202,199 | mfu: 51.99 | epoch: 1 | total time: 27.58m | eta: 49.2m\n",
      "step 00649/01785 (36.36%) | loss: 3.368944 | lrm: 1.00 | dt: 2594.61ms | tok/sec: 202,068 | mfu: 51.95 | epoch: 1 | total time: 27.62m | eta: 49.1m\n",
      "step 00650/01785 (36.41%) | loss: 3.376410 | lrm: 1.00 | dt: 2604.53ms | tok/sec: 201,298 | mfu: 51.75 | epoch: 1 | total time: 27.67m | eta: 49.1m\n",
      "step 00651/01785 (36.47%) | loss: 3.381113 | lrm: 1.00 | dt: 2604.52ms | tok/sec: 201,299 | mfu: 51.76 | epoch: 1 | total time: 27.71m | eta: 49.0m\n",
      "step 00652/01785 (36.53%) | loss: 3.371745 | lrm: 1.00 | dt: 2590.11ms | tok/sec: 202,418 | mfu: 52.04 | epoch: 1 | total time: 27.75m | eta: 49.0m\n",
      "step 00653/01785 (36.58%) | loss: 3.374177 | lrm: 1.00 | dt: 2597.92ms | tok/sec: 201,810 | mfu: 51.89 | epoch: 1 | total time: 27.80m | eta: 48.9m\n",
      "step 00654/01785 (36.64%) | loss: 3.375514 | lrm: 1.00 | dt: 2588.78ms | tok/sec: 202,522 | mfu: 52.07 | epoch: 1 | total time: 27.84m | eta: 48.9m\n",
      "step 00655/01785 (36.69%) | loss: 3.377874 | lrm: 1.00 | dt: 2588.29ms | tok/sec: 202,561 | mfu: 52.08 | epoch: 1 | total time: 27.88m | eta: 48.8m\n",
      "step 00656/01785 (36.75%) | loss: 3.366262 | lrm: 1.00 | dt: 2602.75ms | tok/sec: 201,436 | mfu: 51.79 | epoch: 1 | total time: 27.93m | eta: 48.8m\n",
      "step 00657/01785 (36.81%) | loss: 3.364958 | lrm: 1.00 | dt: 2587.90ms | tok/sec: 202,592 | mfu: 52.09 | epoch: 1 | total time: 27.97m | eta: 48.8m\n",
      "step 00658/01785 (36.86%) | loss: 3.373871 | lrm: 1.00 | dt: 2592.57ms | tok/sec: 202,227 | mfu: 51.99 | epoch: 1 | total time: 28.01m | eta: 48.7m\n",
      "step 00659/01785 (36.92%) | loss: 3.384856 | lrm: 1.00 | dt: 2592.00ms | tok/sec: 202,271 | mfu: 52.01 | epoch: 1 | total time: 28.06m | eta: 48.7m\n",
      "step 00660/01785 (36.97%) | loss: 3.372632 | lrm: 1.00 | dt: 2589.56ms | tok/sec: 202,461 | mfu: 52.05 | epoch: 1 | total time: 28.10m | eta: 48.6m\n",
      "step 00661/01785 (37.03%) | loss: 3.370056 | lrm: 1.00 | dt: 2603.34ms | tok/sec: 201,390 | mfu: 51.78 | epoch: 1 | total time: 28.14m | eta: 48.6m\n",
      "step 00662/01785 (37.09%) | loss: 3.355326 | lrm: 1.00 | dt: 2606.03ms | tok/sec: 201,182 | mfu: 51.73 | epoch: 1 | total time: 28.19m | eta: 48.5m\n",
      "step 00663/01785 (37.14%) | loss: 3.347598 | lrm: 1.00 | dt: 2606.03ms | tok/sec: 201,182 | mfu: 51.73 | epoch: 1 | total time: 28.23m | eta: 48.5m\n",
      "step 00664/01785 (37.20%) | loss: 3.358076 | lrm: 1.00 | dt: 2595.58ms | tok/sec: 201,992 | mfu: 51.93 | epoch: 1 | total time: 28.27m | eta: 48.5m\n",
      "step 00665/01785 (37.25%) | loss: 3.353818 | lrm: 1.00 | dt: 2591.23ms | tok/sec: 202,331 | mfu: 52.02 | epoch: 1 | total time: 28.32m | eta: 48.4m\n",
      "step 00666/01785 (37.31%) | loss: 3.338600 | lrm: 1.00 | dt: 2605.07ms | tok/sec: 201,256 | mfu: 51.74 | epoch: 1 | total time: 28.36m | eta: 48.4m\n",
      "step 00667/01785 (37.37%) | loss: 3.344613 | lrm: 1.00 | dt: 2594.35ms | tok/sec: 202,088 | mfu: 51.96 | epoch: 1 | total time: 28.40m | eta: 48.3m\n",
      "step 00668/01785 (37.42%) | loss: 3.348778 | lrm: 1.00 | dt: 2592.83ms | tok/sec: 202,206 | mfu: 51.99 | epoch: 1 | total time: 28.45m | eta: 48.3m\n",
      "step 00669/01785 (37.48%) | loss: 3.352292 | lrm: 1.00 | dt: 2606.28ms | tok/sec: 201,163 | mfu: 51.72 | epoch: 1 | total time: 28.49m | eta: 48.2m\n",
      "step 00670/01785 (37.54%) | loss: 3.360247 | lrm: 1.00 | dt: 2604.65ms | tok/sec: 201,289 | mfu: 51.75 | epoch: 1 | total time: 28.53m | eta: 48.2m\n",
      "step 00671/01785 (37.59%) | loss: 3.348464 | lrm: 1.00 | dt: 2591.37ms | tok/sec: 202,320 | mfu: 52.02 | epoch: 1 | total time: 28.58m | eta: 48.2m\n",
      "step 00672/01785 (37.65%) | loss: 3.349349 | lrm: 1.00 | dt: 2595.75ms | tok/sec: 201,979 | mfu: 51.93 | epoch: 1 | total time: 28.62m | eta: 48.1m\n",
      "step 00673/01785 (37.70%) | loss: 3.338284 | lrm: 1.00 | dt: 2588.24ms | tok/sec: 202,565 | mfu: 52.08 | epoch: 1 | total time: 28.66m | eta: 48.1m\n",
      "step 00674/01785 (37.76%) | loss: 3.346965 | lrm: 1.00 | dt: 2600.66ms | tok/sec: 201,597 | mfu: 51.83 | epoch: 1 | total time: 28.71m | eta: 48.0m\n",
      "step 00675/01785 (37.82%) | loss: 3.354692 | lrm: 1.00 | dt: 2589.49ms | tok/sec: 202,467 | mfu: 52.06 | epoch: 1 | total time: 28.75m | eta: 48.0m\n",
      "step 00676/01785 (37.87%) | loss: 3.354132 | lrm: 1.00 | dt: 2598.16ms | tok/sec: 201,792 | mfu: 51.88 | epoch: 1 | total time: 28.79m | eta: 47.9m\n",
      "step 00677/01785 (37.93%) | loss: 3.364513 | lrm: 1.00 | dt: 2604.58ms | tok/sec: 201,294 | mfu: 51.75 | epoch: 1 | total time: 28.84m | eta: 47.9m\n",
      "step 00678/01785 (37.98%) | loss: 3.351318 | lrm: 1.00 | dt: 2590.08ms | tok/sec: 202,421 | mfu: 52.04 | epoch: 1 | total time: 28.88m | eta: 47.9m\n",
      "step 00679/01785 (38.04%) | loss: 3.346565 | lrm: 1.00 | dt: 2599.31ms | tok/sec: 201,702 | mfu: 51.86 | epoch: 1 | total time: 28.92m | eta: 47.8m\n",
      "step 00680/01785 (38.10%) | loss: 3.342939 | lrm: 1.00 | dt: 2593.07ms | tok/sec: 202,188 | mfu: 51.98 | epoch: 1 | total time: 28.96m | eta: 47.8m\n",
      "step 00681/01785 (38.15%) | loss: 3.334644 | lrm: 1.00 | dt: 2600.37ms | tok/sec: 201,620 | mfu: 51.84 | epoch: 1 | total time: 29.01m | eta: 47.7m\n",
      "step 00682/01785 (38.21%) | loss: 3.330903 | lrm: 1.00 | dt: 2590.11ms | tok/sec: 202,419 | mfu: 52.04 | epoch: 1 | total time: 29.05m | eta: 47.7m\n",
      "step 00683/01785 (38.26%) | loss: 3.324199 | lrm: 1.00 | dt: 2589.22ms | tok/sec: 202,489 | mfu: 52.06 | epoch: 1 | total time: 29.09m | eta: 47.6m\n",
      "step 00684/01785 (38.32%) | loss: 3.332562 | lrm: 1.00 | dt: 2595.17ms | tok/sec: 202,024 | mfu: 51.94 | epoch: 1 | total time: 29.14m | eta: 47.6m\n",
      "step 00685/01785 (38.38%) | loss: 3.337592 | lrm: 1.00 | dt: 2608.43ms | tok/sec: 200,997 | mfu: 51.68 | epoch: 1 | total time: 29.18m | eta: 47.6m\n",
      "step 00686/01785 (38.43%) | loss: 3.328891 | lrm: 1.00 | dt: 2588.98ms | tok/sec: 202,507 | mfu: 52.07 | epoch: 1 | total time: 29.22m | eta: 47.5m\n",
      "step 00687/01785 (38.49%) | loss: 3.326796 | lrm: 1.00 | dt: 2603.17ms | tok/sec: 201,403 | mfu: 51.78 | epoch: 1 | total time: 29.27m | eta: 47.5m\n",
      "step 00688/01785 (38.54%) | loss: 3.332462 | lrm: 1.00 | dt: 2594.09ms | tok/sec: 202,108 | mfu: 51.96 | epoch: 1 | total time: 29.31m | eta: 47.4m\n",
      "step 00689/01785 (38.60%) | loss: 3.338439 | lrm: 1.00 | dt: 2594.75ms | tok/sec: 202,057 | mfu: 51.95 | epoch: 1 | total time: 29.35m | eta: 47.4m\n",
      "step 00690/01785 (38.66%) | loss: 3.330002 | lrm: 1.00 | dt: 2604.56ms | tok/sec: 201,296 | mfu: 51.75 | epoch: 1 | total time: 29.40m | eta: 47.3m\n",
      "step 00691/01785 (38.71%) | loss: 3.331116 | lrm: 1.00 | dt: 2594.83ms | tok/sec: 202,050 | mfu: 51.95 | epoch: 1 | total time: 29.44m | eta: 47.3m\n",
      "step 00692/01785 (38.77%) | loss: 3.331981 | lrm: 1.00 | dt: 2592.93ms | tok/sec: 202,198 | mfu: 51.99 | epoch: 1 | total time: 29.48m | eta: 47.3m\n",
      "step 00693/01785 (38.82%) | loss: 3.330632 | lrm: 1.00 | dt: 2596.35ms | tok/sec: 201,932 | mfu: 51.92 | epoch: 1 | total time: 29.53m | eta: 47.2m\n",
      "step 00694/01785 (38.88%) | loss: 3.318353 | lrm: 1.00 | dt: 2590.34ms | tok/sec: 202,400 | mfu: 52.04 | epoch: 1 | total time: 29.57m | eta: 47.2m\n",
      "step 00695/01785 (38.94%) | loss: 3.320758 | lrm: 1.00 | dt: 2603.38ms | tok/sec: 201,387 | mfu: 51.78 | epoch: 1 | total time: 29.61m | eta: 47.1m\n",
      "step 00696/01785 (38.99%) | loss: 3.327483 | lrm: 1.00 | dt: 2605.52ms | tok/sec: 201,221 | mfu: 51.74 | epoch: 1 | total time: 29.66m | eta: 47.1m\n",
      "step 00697/01785 (39.05%) | loss: 3.338335 | lrm: 1.00 | dt: 2592.21ms | tok/sec: 202,255 | mfu: 52.00 | epoch: 1 | total time: 29.70m | eta: 47.0m\n",
      "step 00698/01785 (39.10%) | loss: 3.330374 | lrm: 1.00 | dt: 2596.52ms | tok/sec: 201,919 | mfu: 51.91 | epoch: 1 | total time: 29.74m | eta: 47.0m\n",
      "step 00699/01785 (39.16%) | loss: 3.354341 | lrm: 1.00 | dt: 2594.94ms | tok/sec: 202,042 | mfu: 51.95 | epoch: 1 | total time: 29.79m | eta: 47.0m\n",
      "step 00700/01785 (39.22%) | loss: 3.344019 | lrm: 1.00 | dt: 2593.34ms | tok/sec: 202,167 | mfu: 51.98 | epoch: 1 | total time: 29.83m | eta: 46.9m\n",
      "step 00701/01785 (39.27%) | loss: 3.355539 | lrm: 1.00 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 1 | total time: 29.87m | eta: 46.9m\n",
      "step 00702/01785 (39.33%) | loss: 3.347921 | lrm: 1.00 | dt: 2604.60ms | tok/sec: 201,293 | mfu: 51.75 | epoch: 1 | total time: 29.92m | eta: 46.8m\n",
      "step 00703/01785 (39.38%) | loss: 3.346503 | lrm: 1.00 | dt: 2606.05ms | tok/sec: 201,181 | mfu: 51.72 | epoch: 1 | total time: 29.96m | eta: 46.8m\n",
      "step 00704/01785 (39.44%) | loss: 3.352872 | lrm: 1.00 | dt: 2590.06ms | tok/sec: 202,423 | mfu: 52.04 | epoch: 1 | total time: 30.00m | eta: 46.7m\n",
      "step 00705/01785 (39.50%) | loss: 3.349890 | lrm: 1.00 | dt: 2599.76ms | tok/sec: 201,667 | mfu: 51.85 | epoch: 1 | total time: 30.05m | eta: 46.7m\n",
      "step 00706/01785 (39.55%) | loss: 3.338264 | lrm: 1.00 | dt: 2596.14ms | tok/sec: 201,948 | mfu: 51.92 | epoch: 1 | total time: 30.09m | eta: 46.6m\n",
      "step 00707/01785 (39.61%) | loss: 3.331080 | lrm: 1.00 | dt: 2595.68ms | tok/sec: 201,985 | mfu: 51.93 | epoch: 1 | total time: 30.13m | eta: 46.6m\n",
      "step 00708/01785 (39.66%) | loss: 3.338843 | lrm: 1.00 | dt: 2606.77ms | tok/sec: 201,125 | mfu: 51.71 | epoch: 1 | total time: 30.18m | eta: 46.6m\n",
      "step 00709/01785 (39.72%) | loss: 3.336509 | lrm: 1.00 | dt: 2590.94ms | tok/sec: 202,354 | mfu: 52.03 | epoch: 1 | total time: 30.22m | eta: 46.5m\n",
      "step 00710/01785 (39.78%) | loss: 3.346464 | lrm: 1.00 | dt: 2590.68ms | tok/sec: 202,374 | mfu: 52.03 | epoch: 1 | total time: 30.26m | eta: 46.5m\n",
      "step 00711/01785 (39.83%) | loss: 3.341875 | lrm: 1.00 | dt: 2591.81ms | tok/sec: 202,286 | mfu: 52.01 | epoch: 1 | total time: 30.31m | eta: 46.4m\n",
      "step 00712/01785 (39.89%) | loss: 3.359128 | lrm: 1.00 | dt: 2597.03ms | tok/sec: 201,879 | mfu: 51.90 | epoch: 1 | total time: 30.35m | eta: 46.4m\n",
      "step 00713/01785 (39.94%) | loss: 3.362704 | lrm: 1.00 | dt: 2593.31ms | tok/sec: 202,169 | mfu: 51.98 | epoch: 1 | total time: 30.39m | eta: 46.3m\n",
      "step 00714/01785 (40.00%) | loss: 3.360713 | lrm: 1.00 | dt: 2603.89ms | tok/sec: 201,348 | mfu: 51.77 | epoch: 1 | total time: 30.44m | eta: 46.3m\n",
      "step 00715/01785 (40.06%) | loss: 3.361104 | lrm: 1.00 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 30.48m | eta: 46.3m\n",
      "step 00716/01785 (40.11%) | loss: 3.350706 | lrm: 1.00 | dt: 2593.15ms | tok/sec: 202,181 | mfu: 51.98 | epoch: 1 | total time: 30.52m | eta: 46.2m\n",
      "step 00717/01785 (40.17%) | loss: 3.352749 | lrm: 1.00 | dt: 2593.74ms | tok/sec: 202,136 | mfu: 51.97 | epoch: 1 | total time: 30.57m | eta: 46.2m\n",
      "step 00718/01785 (40.22%) | loss: 3.359070 | lrm: 1.00 | dt: 2595.76ms | tok/sec: 201,978 | mfu: 51.93 | epoch: 1 | total time: 30.61m | eta: 46.1m\n",
      "step 00719/01785 (40.28%) | loss: 3.343656 | lrm: 1.00 | dt: 2588.90ms | tok/sec: 202,513 | mfu: 52.07 | epoch: 1 | total time: 30.65m | eta: 46.1m\n",
      "step 00720/01785 (40.34%) | loss: 3.338972 | lrm: 1.00 | dt: 2605.02ms | tok/sec: 201,260 | mfu: 51.75 | epoch: 1 | total time: 30.70m | eta: 46.0m\n",
      "step 00721/01785 (40.39%) | loss: 3.326551 | lrm: 1.00 | dt: 2603.62ms | tok/sec: 201,368 | mfu: 51.77 | epoch: 1 | total time: 30.74m | eta: 46.0m\n",
      "step 00722/01785 (40.45%) | loss: 3.324052 | lrm: 1.00 | dt: 2604.84ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 30.78m | eta: 46.0m\n",
      "step 00723/01785 (40.50%) | loss: 3.328233 | lrm: 1.00 | dt: 2602.77ms | tok/sec: 201,434 | mfu: 51.79 | epoch: 1 | total time: 30.83m | eta: 45.9m\n",
      "step 00724/01785 (40.56%) | loss: 3.329103 | lrm: 1.00 | dt: 2604.92ms | tok/sec: 201,268 | mfu: 51.75 | epoch: 1 | total time: 30.87m | eta: 45.9m\n",
      "step 00725/01785 (40.62%) | loss: 3.329387 | lrm: 1.00 | dt: 2591.12ms | tok/sec: 202,340 | mfu: 52.02 | epoch: 1 | total time: 30.91m | eta: 45.8m\n",
      "step 00726/01785 (40.67%) | loss: 3.326185 | lrm: 1.00 | dt: 2596.65ms | tok/sec: 201,909 | mfu: 51.91 | epoch: 1 | total time: 30.96m | eta: 45.8m\n",
      "step 00727/01785 (40.73%) | loss: 3.324607 | lrm: 1.00 | dt: 2593.52ms | tok/sec: 202,153 | mfu: 51.97 | epoch: 1 | total time: 31.00m | eta: 45.7m\n",
      "step 00728/01785 (40.78%) | loss: 3.310884 | lrm: 1.00 | dt: 2593.00ms | tok/sec: 202,193 | mfu: 51.98 | epoch: 1 | total time: 31.04m | eta: 45.7m\n",
      "step 00729/01785 (40.84%) | loss: 3.326028 | lrm: 1.00 | dt: 2604.06ms | tok/sec: 201,334 | mfu: 51.76 | epoch: 1 | total time: 31.09m | eta: 45.7m\n",
      "step 00730/01785 (40.90%) | loss: 3.323804 | lrm: 1.00 | dt: 2606.30ms | tok/sec: 201,162 | mfu: 51.72 | epoch: 1 | total time: 31.13m | eta: 45.6m\n",
      "step 00731/01785 (40.95%) | loss: 3.326612 | lrm: 1.00 | dt: 2606.59ms | tok/sec: 201,139 | mfu: 51.71 | epoch: 1 | total time: 31.17m | eta: 45.6m\n",
      "step 00732/01785 (41.01%) | loss: 3.327737 | lrm: 1.00 | dt: 2609.09ms | tok/sec: 200,946 | mfu: 51.66 | epoch: 1 | total time: 31.22m | eta: 45.5m\n",
      "step 00733/01785 (41.06%) | loss: 3.342115 | lrm: 1.00 | dt: 2590.08ms | tok/sec: 202,421 | mfu: 52.04 | epoch: 1 | total time: 31.26m | eta: 45.5m\n",
      "step 00734/01785 (41.12%) | loss: 3.337052 | lrm: 1.00 | dt: 2607.95ms | tok/sec: 201,034 | mfu: 51.69 | epoch: 1 | total time: 31.30m | eta: 45.4m\n",
      "step 00735/01785 (41.18%) | loss: 3.337720 | lrm: 1.00 | dt: 2592.23ms | tok/sec: 202,253 | mfu: 52.00 | epoch: 1 | total time: 31.35m | eta: 45.4m\n",
      "step 00736/01785 (41.23%) | loss: 3.340737 | lrm: 1.00 | dt: 2596.11ms | tok/sec: 201,951 | mfu: 51.92 | epoch: 1 | total time: 31.39m | eta: 45.4m\n",
      "step 00737/01785 (41.29%) | loss: 3.347393 | lrm: 1.00 | dt: 2608.36ms | tok/sec: 201,003 | mfu: 51.68 | epoch: 1 | total time: 31.43m | eta: 45.3m\n",
      "step 00738/01785 (41.34%) | loss: 3.357400 | lrm: 1.00 | dt: 2607.21ms | tok/sec: 201,091 | mfu: 51.70 | epoch: 1 | total time: 31.48m | eta: 45.3m\n",
      "step 00739/01785 (41.40%) | loss: 3.358880 | lrm: 1.00 | dt: 2595.79ms | tok/sec: 201,976 | mfu: 51.93 | epoch: 1 | total time: 31.52m | eta: 45.2m\n",
      "step 00740/01785 (41.46%) | loss: 3.337791 | lrm: 1.00 | dt: 2593.71ms | tok/sec: 202,138 | mfu: 51.97 | epoch: 1 | total time: 31.56m | eta: 45.2m\n",
      "step 00741/01785 (41.51%) | loss: 3.328309 | lrm: 1.00 | dt: 2603.33ms | tok/sec: 201,390 | mfu: 51.78 | epoch: 1 | total time: 31.61m | eta: 45.1m\n",
      "step 00742/01785 (41.57%) | loss: 3.324201 | lrm: 1.00 | dt: 2605.22ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 31.65m | eta: 45.1m\n",
      "step 00743/01785 (41.62%) | loss: 3.332817 | lrm: 1.00 | dt: 2603.36ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 1 | total time: 31.69m | eta: 45.1m\n",
      "step 00744/01785 (41.68%) | loss: 3.312300 | lrm: 1.00 | dt: 2605.92ms | tok/sec: 201,190 | mfu: 51.73 | epoch: 1 | total time: 31.74m | eta: 45.0m\n",
      "step 00745/01785 (41.74%) | loss: 3.312360 | lrm: 1.00 | dt: 2605.04ms | tok/sec: 201,258 | mfu: 51.74 | epoch: 1 | total time: 31.78m | eta: 45.0m\n",
      "step 00746/01785 (41.79%) | loss: 3.318314 | lrm: 1.00 | dt: 2605.66ms | tok/sec: 201,211 | mfu: 51.73 | epoch: 1 | total time: 31.82m | eta: 44.9m\n",
      "step 00747/01785 (41.85%) | loss: 3.310511 | lrm: 1.00 | dt: 2596.30ms | tok/sec: 201,936 | mfu: 51.92 | epoch: 1 | total time: 31.87m | eta: 44.9m\n",
      "step 00748/01785 (41.90%) | loss: 3.315748 | lrm: 1.00 | dt: 2590.89ms | tok/sec: 202,358 | mfu: 52.03 | epoch: 1 | total time: 31.91m | eta: 44.8m\n",
      "step 00749/01785 (41.96%) | loss: 3.313956 | lrm: 1.00 | dt: 2605.41ms | tok/sec: 201,230 | mfu: 51.74 | epoch: 1 | total time: 31.95m | eta: 44.8m\n",
      "Step 00750 | Validation bpb: 1.013474\n",
      "step 00750/01785 (42.02%) | loss: 3.319307 | lrm: 1.00 | dt: 2584.73ms | tok/sec: 202,840 | mfu: 52.15 | epoch: 1 | total time: 32.00m | eta: 44.8m\n",
      "step 00751/01785 (42.07%) | loss: 3.322196 | lrm: 1.00 | dt: 2585.25ms | tok/sec: 202,799 | mfu: 52.14 | epoch: 1 | total time: 32.04m | eta: 44.7m\n",
      "step 00752/01785 (42.13%) | loss: 3.324531 | lrm: 1.00 | dt: 2584.49ms | tok/sec: 202,859 | mfu: 52.16 | epoch: 1 | total time: 32.08m | eta: 44.7m\n",
      "step 00753/01785 (42.18%) | loss: 3.337396 | lrm: 1.00 | dt: 2587.17ms | tok/sec: 202,649 | mfu: 52.10 | epoch: 1 | total time: 32.13m | eta: 44.6m\n",
      "step 00754/01785 (42.24%) | loss: 3.333229 | lrm: 1.00 | dt: 2599.88ms | tok/sec: 201,658 | mfu: 51.85 | epoch: 1 | total time: 32.17m | eta: 44.6m\n",
      "step 00755/01785 (42.30%) | loss: 3.329885 | lrm: 1.00 | dt: 2585.73ms | tok/sec: 202,762 | mfu: 52.13 | epoch: 1 | total time: 32.21m | eta: 44.5m\n",
      "step 00756/01785 (42.35%) | loss: 3.332602 | lrm: 1.00 | dt: 2601.59ms | tok/sec: 201,526 | mfu: 51.81 | epoch: 1 | total time: 32.26m | eta: 44.5m\n",
      "step 00757/01785 (42.41%) | loss: 3.340933 | lrm: 1.00 | dt: 2587.94ms | tok/sec: 202,589 | mfu: 52.09 | epoch: 1 | total time: 32.30m | eta: 44.4m\n",
      "step 00758/01785 (42.46%) | loss: 3.337904 | lrm: 1.00 | dt: 2599.71ms | tok/sec: 201,671 | mfu: 51.85 | epoch: 1 | total time: 32.34m | eta: 44.4m\n",
      "step 00759/01785 (42.52%) | loss: 3.334054 | lrm: 1.00 | dt: 2603.65ms | tok/sec: 201,366 | mfu: 51.77 | epoch: 1 | total time: 32.39m | eta: 44.4m\n",
      "step 00760/01785 (42.58%) | loss: 3.353847 | lrm: 1.00 | dt: 2585.75ms | tok/sec: 202,760 | mfu: 52.13 | epoch: 1 | total time: 32.43m | eta: 44.3m\n",
      "step 00761/01785 (42.63%) | loss: 3.345641 | lrm: 1.00 | dt: 2603.03ms | tok/sec: 201,414 | mfu: 51.78 | epoch: 1 | total time: 32.47m | eta: 44.3m\n",
      "step 00762/01785 (42.69%) | loss: 3.337332 | lrm: 1.00 | dt: 2606.57ms | tok/sec: 201,140 | mfu: 51.71 | epoch: 1 | total time: 32.52m | eta: 44.2m\n",
      "step 00763/01785 (42.75%) | loss: 3.336343 | lrm: 1.00 | dt: 2606.24ms | tok/sec: 201,166 | mfu: 51.72 | epoch: 1 | total time: 32.56m | eta: 44.2m\n",
      "step 00764/01785 (42.80%) | loss: 3.330760 | lrm: 1.00 | dt: 2587.12ms | tok/sec: 202,653 | mfu: 52.10 | epoch: 1 | total time: 32.60m | eta: 44.1m\n",
      "step 00765/01785 (42.86%) | loss: 3.316457 | lrm: 1.00 | dt: 2603.78ms | tok/sec: 201,356 | mfu: 51.77 | epoch: 1 | total time: 32.65m | eta: 44.1m\n",
      "step 00766/01785 (42.91%) | loss: 3.317224 | lrm: 1.00 | dt: 2591.07ms | tok/sec: 202,344 | mfu: 52.02 | epoch: 1 | total time: 32.69m | eta: 44.1m\n",
      "step 00767/01785 (42.97%) | loss: 3.320987 | lrm: 1.00 | dt: 2589.68ms | tok/sec: 202,452 | mfu: 52.05 | epoch: 1 | total time: 32.73m | eta: 44.0m\n",
      "step 00768/01785 (43.03%) | loss: 3.323846 | lrm: 1.00 | dt: 2590.65ms | tok/sec: 202,376 | mfu: 52.03 | epoch: 1 | total time: 32.77m | eta: 44.0m\n",
      "step 00769/01785 (43.08%) | loss: 3.326812 | lrm: 1.00 | dt: 2591.78ms | tok/sec: 202,288 | mfu: 52.01 | epoch: 1 | total time: 32.82m | eta: 43.9m\n",
      "step 00770/01785 (43.14%) | loss: 3.317791 | lrm: 1.00 | dt: 2593.85ms | tok/sec: 202,127 | mfu: 51.97 | epoch: 1 | total time: 32.86m | eta: 43.9m\n",
      "step 00771/01785 (43.19%) | loss: 3.327249 | lrm: 1.00 | dt: 2586.44ms | tok/sec: 202,706 | mfu: 52.12 | epoch: 1 | total time: 32.90m | eta: 43.8m\n",
      "step 00772/01785 (43.25%) | loss: 3.331301 | lrm: 1.00 | dt: 2604.33ms | tok/sec: 201,313 | mfu: 51.76 | epoch: 1 | total time: 32.95m | eta: 43.8m\n",
      "step 00773/01785 (43.31%) | loss: 3.335505 | lrm: 1.00 | dt: 2587.73ms | tok/sec: 202,605 | mfu: 52.09 | epoch: 1 | total time: 32.99m | eta: 43.8m\n",
      "step 00774/01785 (43.36%) | loss: 3.328531 | lrm: 1.00 | dt: 2600.08ms | tok/sec: 201,643 | mfu: 51.84 | epoch: 1 | total time: 33.03m | eta: 43.7m\n",
      "step 00775/01785 (43.42%) | loss: 3.334291 | lrm: 1.00 | dt: 2584.50ms | tok/sec: 202,858 | mfu: 52.16 | epoch: 1 | total time: 33.08m | eta: 43.7m\n",
      "step 00776/01785 (43.47%) | loss: 3.332846 | lrm: 1.00 | dt: 2603.12ms | tok/sec: 201,407 | mfu: 51.78 | epoch: 1 | total time: 33.12m | eta: 43.6m\n",
      "step 00777/01785 (43.53%) | loss: 3.319833 | lrm: 1.00 | dt: 2585.65ms | tok/sec: 202,768 | mfu: 52.13 | epoch: 1 | total time: 33.16m | eta: 43.6m\n",
      "step 00778/01785 (43.59%) | loss: 3.326964 | lrm: 1.00 | dt: 2604.78ms | tok/sec: 201,279 | mfu: 51.75 | epoch: 1 | total time: 33.21m | eta: 43.5m\n",
      "step 00779/01785 (43.64%) | loss: 3.331643 | lrm: 1.00 | dt: 2588.73ms | tok/sec: 202,526 | mfu: 52.07 | epoch: 1 | total time: 33.25m | eta: 43.5m\n",
      "step 00780/01785 (43.70%) | loss: 3.333154 | lrm: 1.00 | dt: 2600.47ms | tok/sec: 201,612 | mfu: 51.84 | epoch: 1 | total time: 33.29m | eta: 43.5m\n",
      "step 00781/01785 (43.75%) | loss: 3.322381 | lrm: 1.00 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 33.34m | eta: 43.4m\n",
      "step 00782/01785 (43.81%) | loss: 3.322728 | lrm: 1.00 | dt: 2588.66ms | tok/sec: 202,532 | mfu: 52.07 | epoch: 1 | total time: 33.38m | eta: 43.4m\n",
      "step 00783/01785 (43.87%) | loss: 3.322183 | lrm: 1.00 | dt: 2604.25ms | tok/sec: 201,319 | mfu: 51.76 | epoch: 1 | total time: 33.42m | eta: 43.3m\n",
      "step 00784/01785 (43.92%) | loss: 3.334687 | lrm: 1.00 | dt: 2587.03ms | tok/sec: 202,660 | mfu: 52.11 | epoch: 1 | total time: 33.47m | eta: 43.3m\n",
      "step 00785/01785 (43.98%) | loss: 3.338991 | lrm: 1.00 | dt: 2602.49ms | tok/sec: 201,456 | mfu: 51.80 | epoch: 1 | total time: 33.51m | eta: 43.2m\n",
      "step 00786/01785 (44.03%) | loss: 3.330066 | lrm: 1.00 | dt: 2590.95ms | tok/sec: 202,353 | mfu: 52.03 | epoch: 1 | total time: 33.55m | eta: 43.2m\n",
      "step 00787/01785 (44.09%) | loss: 3.347507 | lrm: 1.00 | dt: 2601.73ms | tok/sec: 201,515 | mfu: 51.81 | epoch: 1 | total time: 33.60m | eta: 43.2m\n",
      "step 00788/01785 (44.15%) | loss: 3.350919 | lrm: 1.00 | dt: 2598.94ms | tok/sec: 201,731 | mfu: 51.87 | epoch: 1 | total time: 33.64m | eta: 43.1m\n",
      "step 00789/01785 (44.20%) | loss: 3.341875 | lrm: 1.00 | dt: 2587.45ms | tok/sec: 202,627 | mfu: 52.10 | epoch: 1 | total time: 33.68m | eta: 43.1m\n",
      "step 00790/01785 (44.26%) | loss: 3.345165 | lrm: 1.00 | dt: 2604.42ms | tok/sec: 201,306 | mfu: 51.76 | epoch: 1 | total time: 33.73m | eta: 43.0m\n",
      "step 00791/01785 (44.31%) | loss: 3.340744 | lrm: 1.00 | dt: 2588.00ms | tok/sec: 202,584 | mfu: 52.09 | epoch: 1 | total time: 33.77m | eta: 43.0m\n",
      "step 00792/01785 (44.37%) | loss: 3.327475 | lrm: 1.00 | dt: 2588.64ms | tok/sec: 202,534 | mfu: 52.07 | epoch: 1 | total time: 33.81m | eta: 42.9m\n",
      "step 00793/01785 (44.43%) | loss: 3.312336 | lrm: 1.00 | dt: 2606.26ms | tok/sec: 201,165 | mfu: 51.72 | epoch: 1 | total time: 33.86m | eta: 42.9m\n",
      "step 00794/01785 (44.48%) | loss: 3.299010 | lrm: 1.00 | dt: 2593.13ms | tok/sec: 202,183 | mfu: 51.98 | epoch: 1 | total time: 33.90m | eta: 42.8m\n",
      "step 00795/01785 (44.54%) | loss: 3.295837 | lrm: 1.00 | dt: 2592.26ms | tok/sec: 202,251 | mfu: 52.00 | epoch: 1 | total time: 33.94m | eta: 42.8m\n",
      "step 00796/01785 (44.59%) | loss: 3.308259 | lrm: 1.00 | dt: 2589.56ms | tok/sec: 202,462 | mfu: 52.05 | epoch: 1 | total time: 33.99m | eta: 42.8m\n",
      "step 00797/01785 (44.65%) | loss: 3.305059 | lrm: 1.00 | dt: 2591.21ms | tok/sec: 202,333 | mfu: 52.02 | epoch: 1 | total time: 34.03m | eta: 42.7m\n",
      "step 00798/01785 (44.71%) | loss: 3.302615 | lrm: 1.00 | dt: 2597.78ms | tok/sec: 201,821 | mfu: 51.89 | epoch: 1 | total time: 34.07m | eta: 42.7m\n",
      "step 00799/01785 (44.76%) | loss: 3.294935 | lrm: 1.00 | dt: 2591.06ms | tok/sec: 202,344 | mfu: 52.02 | epoch: 1 | total time: 34.12m | eta: 42.6m\n",
      "step 00800/01785 (44.82%) | loss: 3.277534 | lrm: 1.00 | dt: 2600.14ms | tok/sec: 201,638 | mfu: 51.84 | epoch: 1 | total time: 34.16m | eta: 42.6m\n",
      "step 00801/01785 (44.87%) | loss: 3.277248 | lrm: 1.00 | dt: 2605.08ms | tok/sec: 201,256 | mfu: 51.74 | epoch: 1 | total time: 34.20m | eta: 42.5m\n",
      "step 00802/01785 (44.93%) | loss: 3.282803 | lrm: 1.00 | dt: 2604.83ms | tok/sec: 201,275 | mfu: 51.75 | epoch: 1 | total time: 34.25m | eta: 42.5m\n",
      "step 00803/01785 (44.99%) | loss: 3.276521 | lrm: 1.00 | dt: 2591.53ms | tok/sec: 202,308 | mfu: 52.01 | epoch: 1 | total time: 34.29m | eta: 42.5m\n",
      "step 00804/01785 (45.04%) | loss: 3.280201 | lrm: 1.00 | dt: 2594.70ms | tok/sec: 202,061 | mfu: 51.95 | epoch: 1 | total time: 34.33m | eta: 42.4m\n",
      "step 00805/01785 (45.10%) | loss: 3.277383 | lrm: 1.00 | dt: 2603.67ms | tok/sec: 201,364 | mfu: 51.77 | epoch: 1 | total time: 34.38m | eta: 42.4m\n",
      "step 00806/01785 (45.15%) | loss: 3.294071 | lrm: 1.00 | dt: 2587.77ms | tok/sec: 202,602 | mfu: 52.09 | epoch: 1 | total time: 34.42m | eta: 42.3m\n",
      "step 00807/01785 (45.21%) | loss: 3.287563 | lrm: 1.00 | dt: 2602.17ms | tok/sec: 201,481 | mfu: 51.80 | epoch: 1 | total time: 34.46m | eta: 42.3m\n",
      "step 00808/01785 (45.27%) | loss: 3.287176 | lrm: 1.00 | dt: 2589.29ms | tok/sec: 202,483 | mfu: 52.06 | epoch: 1 | total time: 34.51m | eta: 42.2m\n",
      "step 00809/01785 (45.32%) | loss: 3.303536 | lrm: 1.00 | dt: 2597.85ms | tok/sec: 201,816 | mfu: 51.89 | epoch: 1 | total time: 34.55m | eta: 42.2m\n",
      "step 00810/01785 (45.38%) | loss: 3.300879 | lrm: 1.00 | dt: 2604.61ms | tok/sec: 201,292 | mfu: 51.75 | epoch: 1 | total time: 34.59m | eta: 42.2m\n",
      "step 00811/01785 (45.43%) | loss: 3.308934 | lrm: 1.00 | dt: 2589.99ms | tok/sec: 202,428 | mfu: 52.05 | epoch: 1 | total time: 34.63m | eta: 42.1m\n",
      "step 00812/01785 (45.49%) | loss: 3.307053 | lrm: 1.00 | dt: 2599.20ms | tok/sec: 201,710 | mfu: 51.86 | epoch: 1 | total time: 34.68m | eta: 42.1m\n",
      "step 00813/01785 (45.55%) | loss: 3.299629 | lrm: 1.00 | dt: 2604.95ms | tok/sec: 201,265 | mfu: 51.75 | epoch: 1 | total time: 34.72m | eta: 42.0m\n",
      "step 00814/01785 (45.60%) | loss: 3.288477 | lrm: 1.00 | dt: 2588.16ms | tok/sec: 202,571 | mfu: 52.08 | epoch: 1 | total time: 34.76m | eta: 42.0m\n",
      "step 00815/01785 (45.66%) | loss: 3.288098 | lrm: 1.00 | dt: 2601.60ms | tok/sec: 201,525 | mfu: 51.81 | epoch: 1 | total time: 34.81m | eta: 41.9m\n",
      "step 00816/01785 (45.71%) | loss: 3.274509 | lrm: 1.00 | dt: 2589.00ms | tok/sec: 202,506 | mfu: 52.07 | epoch: 1 | total time: 34.85m | eta: 41.9m\n",
      "step 00817/01785 (45.77%) | loss: 3.282572 | lrm: 1.00 | dt: 2603.11ms | tok/sec: 201,408 | mfu: 51.78 | epoch: 1 | total time: 34.89m | eta: 41.9m\n",
      "step 00818/01785 (45.83%) | loss: 3.272776 | lrm: 1.00 | dt: 2592.80ms | tok/sec: 202,209 | mfu: 51.99 | epoch: 1 | total time: 34.94m | eta: 41.8m\n",
      "step 00819/01785 (45.88%) | loss: 3.274555 | lrm: 1.00 | dt: 2590.33ms | tok/sec: 202,401 | mfu: 52.04 | epoch: 1 | total time: 34.98m | eta: 41.8m\n",
      "step 00820/01785 (45.94%) | loss: 3.274190 | lrm: 1.00 | dt: 2592.05ms | tok/sec: 202,267 | mfu: 52.00 | epoch: 1 | total time: 35.02m | eta: 41.7m\n",
      "step 00821/01785 (45.99%) | loss: 3.275560 | lrm: 1.00 | dt: 2593.78ms | tok/sec: 202,132 | mfu: 51.97 | epoch: 1 | total time: 35.07m | eta: 41.7m\n",
      "step 00822/01785 (46.05%) | loss: 3.272968 | lrm: 1.00 | dt: 2591.59ms | tok/sec: 202,303 | mfu: 52.01 | epoch: 1 | total time: 35.11m | eta: 41.6m\n",
      "step 00823/01785 (46.11%) | loss: 3.281057 | lrm: 1.00 | dt: 2602.59ms | tok/sec: 201,448 | mfu: 51.79 | epoch: 1 | total time: 35.15m | eta: 41.6m\n",
      "step 00824/01785 (46.16%) | loss: 3.272776 | lrm: 1.00 | dt: 2593.35ms | tok/sec: 202,165 | mfu: 51.98 | epoch: 1 | total time: 35.20m | eta: 41.6m\n",
      "step 00825/01785 (46.22%) | loss: 3.268521 | lrm: 1.00 | dt: 2591.64ms | tok/sec: 202,299 | mfu: 52.01 | epoch: 1 | total time: 35.24m | eta: 41.5m\n",
      "step 00826/01785 (46.27%) | loss: 3.272190 | lrm: 1.00 | dt: 2605.84ms | tok/sec: 201,197 | mfu: 51.73 | epoch: 1 | total time: 35.28m | eta: 41.5m\n",
      "step 00827/01785 (46.33%) | loss: 3.282264 | lrm: 1.00 | dt: 2606.98ms | tok/sec: 201,109 | mfu: 51.71 | epoch: 1 | total time: 35.33m | eta: 41.4m\n",
      "step 00828/01785 (46.39%) | loss: 3.283346 | lrm: 1.00 | dt: 2605.22ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 35.37m | eta: 41.4m\n",
      "step 00829/01785 (46.44%) | loss: 3.288803 | lrm: 1.00 | dt: 2595.16ms | tok/sec: 202,025 | mfu: 51.94 | epoch: 1 | total time: 35.41m | eta: 41.3m\n",
      "step 00830/01785 (46.50%) | loss: 3.305179 | lrm: 1.00 | dt: 2591.07ms | tok/sec: 202,344 | mfu: 52.02 | epoch: 1 | total time: 35.46m | eta: 41.3m\n",
      "step 00831/01785 (46.55%) | loss: 3.304499 | lrm: 1.00 | dt: 2605.34ms | tok/sec: 201,235 | mfu: 51.74 | epoch: 1 | total time: 35.50m | eta: 41.3m\n",
      "step 00832/01785 (46.61%) | loss: 3.312879 | lrm: 1.00 | dt: 2588.51ms | tok/sec: 202,544 | mfu: 52.08 | epoch: 1 | total time: 35.54m | eta: 41.2m\n",
      "step 00833/01785 (46.67%) | loss: 3.322304 | lrm: 1.00 | dt: 2599.68ms | tok/sec: 201,674 | mfu: 51.85 | epoch: 1 | total time: 35.59m | eta: 41.2m\n",
      "step 00834/01785 (46.72%) | loss: 3.310150 | lrm: 1.00 | dt: 2605.18ms | tok/sec: 201,248 | mfu: 51.74 | epoch: 1 | total time: 35.63m | eta: 41.1m\n",
      "step 00835/01785 (46.78%) | loss: 3.322505 | lrm: 1.00 | dt: 2592.25ms | tok/sec: 202,252 | mfu: 52.00 | epoch: 1 | total time: 35.67m | eta: 41.1m\n",
      "step 00836/01785 (46.83%) | loss: 3.323855 | lrm: 1.00 | dt: 2595.03ms | tok/sec: 202,035 | mfu: 51.94 | epoch: 1 | total time: 35.72m | eta: 41.0m\n",
      "step 00837/01785 (46.89%) | loss: 3.323620 | lrm: 1.00 | dt: 2591.15ms | tok/sec: 202,337 | mfu: 52.02 | epoch: 1 | total time: 35.76m | eta: 41.0m\n",
      "step 00838/01785 (46.95%) | loss: 3.323361 | lrm: 1.00 | dt: 2596.99ms | tok/sec: 201,883 | mfu: 51.91 | epoch: 1 | total time: 35.80m | eta: 40.9m\n",
      "step 00839/01785 (47.00%) | loss: 3.327514 | lrm: 1.00 | dt: 2591.36ms | tok/sec: 202,321 | mfu: 52.02 | epoch: 1 | total time: 35.85m | eta: 40.9m\n",
      "step 00840/01785 (47.06%) | loss: 3.330590 | lrm: 1.00 | dt: 2596.86ms | tok/sec: 201,893 | mfu: 51.91 | epoch: 1 | total time: 35.89m | eta: 40.9m\n",
      "step 00841/01785 (47.11%) | loss: 3.321295 | lrm: 1.00 | dt: 2586.62ms | tok/sec: 202,692 | mfu: 52.11 | epoch: 1 | total time: 35.93m | eta: 40.8m\n",
      "step 00842/01785 (47.17%) | loss: 3.311016 | lrm: 1.00 | dt: 2587.90ms | tok/sec: 202,592 | mfu: 52.09 | epoch: 1 | total time: 35.98m | eta: 40.8m\n",
      "step 00843/01785 (47.23%) | loss: 3.310368 | lrm: 1.00 | dt: 2598.73ms | tok/sec: 201,747 | mfu: 51.87 | epoch: 1 | total time: 36.02m | eta: 40.7m\n",
      "step 00844/01785 (47.28%) | loss: 3.307992 | lrm: 1.00 | dt: 2592.26ms | tok/sec: 202,251 | mfu: 52.00 | epoch: 1 | total time: 36.06m | eta: 40.7m\n",
      "step 00845/01785 (47.34%) | loss: 3.304750 | lrm: 1.00 | dt: 2588.72ms | tok/sec: 202,527 | mfu: 52.07 | epoch: 1 | total time: 36.11m | eta: 40.6m\n",
      "step 00846/01785 (47.39%) | loss: 3.297480 | lrm: 1.00 | dt: 2589.97ms | tok/sec: 202,430 | mfu: 52.05 | epoch: 1 | total time: 36.15m | eta: 40.6m\n",
      "step 00847/01785 (47.45%) | loss: 3.309365 | lrm: 1.00 | dt: 2591.38ms | tok/sec: 202,320 | mfu: 52.02 | epoch: 1 | total time: 36.19m | eta: 40.6m\n",
      "step 00848/01785 (47.51%) | loss: 3.308600 | lrm: 1.00 | dt: 2593.07ms | tok/sec: 202,188 | mfu: 51.98 | epoch: 1 | total time: 36.24m | eta: 40.5m\n",
      "step 00849/01785 (47.56%) | loss: 3.311502 | lrm: 1.00 | dt: 2604.91ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 1 | total time: 36.28m | eta: 40.5m\n",
      "step 00850/01785 (47.62%) | loss: 3.314788 | lrm: 1.00 | dt: 2587.94ms | tok/sec: 202,589 | mfu: 52.09 | epoch: 1 | total time: 36.32m | eta: 40.4m\n",
      "step 00851/01785 (47.68%) | loss: 3.309131 | lrm: 1.00 | dt: 2601.94ms | tok/sec: 201,498 | mfu: 51.81 | epoch: 1 | total time: 36.37m | eta: 40.4m\n",
      "step 00852/01785 (47.73%) | loss: 3.309316 | lrm: 1.00 | dt: 2587.53ms | tok/sec: 202,620 | mfu: 52.09 | epoch: 1 | total time: 36.41m | eta: 40.3m\n",
      "step 00853/01785 (47.79%) | loss: 3.300003 | lrm: 1.00 | dt: 2602.73ms | tok/sec: 201,438 | mfu: 51.79 | epoch: 1 | total time: 36.45m | eta: 40.3m\n",
      "step 00854/01785 (47.84%) | loss: 3.292278 | lrm: 1.00 | dt: 2605.41ms | tok/sec: 201,230 | mfu: 51.74 | epoch: 1 | total time: 36.50m | eta: 40.3m\n",
      "step 00855/01785 (47.90%) | loss: 3.299295 | lrm: 1.00 | dt: 2589.27ms | tok/sec: 202,484 | mfu: 52.06 | epoch: 1 | total time: 36.54m | eta: 40.2m\n",
      "step 00856/01785 (47.96%) | loss: 3.302700 | lrm: 1.00 | dt: 2598.48ms | tok/sec: 201,767 | mfu: 51.88 | epoch: 1 | total time: 36.58m | eta: 40.2m\n",
      "step 00857/01785 (48.01%) | loss: 3.299395 | lrm: 1.00 | dt: 2605.12ms | tok/sec: 201,252 | mfu: 51.74 | epoch: 1 | total time: 36.63m | eta: 40.1m\n",
      "step 00858/01785 (48.07%) | loss: 3.300188 | lrm: 1.00 | dt: 2603.27ms | tok/sec: 201,395 | mfu: 51.78 | epoch: 1 | total time: 36.67m | eta: 40.1m\n",
      "step 00859/01785 (48.12%) | loss: 3.294871 | lrm: 1.00 | dt: 2605.65ms | tok/sec: 201,212 | mfu: 51.73 | epoch: 1 | total time: 36.71m | eta: 40.0m\n",
      "step 00860/01785 (48.18%) | loss: 3.292925 | lrm: 1.00 | dt: 2586.76ms | tok/sec: 202,681 | mfu: 52.11 | epoch: 1 | total time: 36.76m | eta: 40.0m\n",
      "step 00861/01785 (48.24%) | loss: 3.306025 | lrm: 1.00 | dt: 2603.07ms | tok/sec: 201,411 | mfu: 51.78 | epoch: 1 | total time: 36.80m | eta: 40.0m\n",
      "step 00862/01785 (48.29%) | loss: 3.312833 | lrm: 1.00 | dt: 2605.54ms | tok/sec: 201,220 | mfu: 51.73 | epoch: 1 | total time: 36.84m | eta: 39.9m\n",
      "step 00863/01785 (48.35%) | loss: 3.312408 | lrm: 1.00 | dt: 2606.00ms | tok/sec: 201,184 | mfu: 51.73 | epoch: 1 | total time: 36.89m | eta: 39.9m\n",
      "step 00864/01785 (48.40%) | loss: 3.325912 | lrm: 1.00 | dt: 2604.29ms | tok/sec: 201,317 | mfu: 51.76 | epoch: 1 | total time: 36.93m | eta: 39.8m\n",
      "step 00865/01785 (48.46%) | loss: 3.313518 | lrm: 1.00 | dt: 2588.32ms | tok/sec: 202,559 | mfu: 52.08 | epoch: 1 | total time: 36.97m | eta: 39.8m\n",
      "step 00866/01785 (48.52%) | loss: 3.313775 | lrm: 1.00 | dt: 2601.51ms | tok/sec: 201,531 | mfu: 51.81 | epoch: 1 | total time: 37.02m | eta: 39.7m\n",
      "step 00867/01785 (48.57%) | loss: 3.308195 | lrm: 1.00 | dt: 2607.52ms | tok/sec: 201,067 | mfu: 51.70 | epoch: 1 | total time: 37.06m | eta: 39.7m\n",
      "step 00868/01785 (48.63%) | loss: 3.298545 | lrm: 1.00 | dt: 2588.92ms | tok/sec: 202,512 | mfu: 52.07 | epoch: 1 | total time: 37.10m | eta: 39.7m\n",
      "step 00869/01785 (48.68%) | loss: 3.309055 | lrm: 1.00 | dt: 2606.49ms | tok/sec: 201,146 | mfu: 51.72 | epoch: 1 | total time: 37.15m | eta: 39.6m\n",
      "step 00870/01785 (48.74%) | loss: 3.292188 | lrm: 1.00 | dt: 2588.52ms | tok/sec: 202,543 | mfu: 52.07 | epoch: 1 | total time: 37.19m | eta: 39.6m\n",
      "step 00871/01785 (48.80%) | loss: 3.294467 | lrm: 1.00 | dt: 2607.56ms | tok/sec: 201,064 | mfu: 51.69 | epoch: 1 | total time: 37.23m | eta: 39.5m\n",
      "step 00872/01785 (48.85%) | loss: 3.300040 | lrm: 1.00 | dt: 2592.93ms | tok/sec: 202,198 | mfu: 51.99 | epoch: 1 | total time: 37.28m | eta: 39.5m\n",
      "step 00873/01785 (48.91%) | loss: 3.294355 | lrm: 1.00 | dt: 2595.52ms | tok/sec: 201,996 | mfu: 51.93 | epoch: 1 | total time: 37.32m | eta: 39.4m\n",
      "step 00874/01785 (48.96%) | loss: 3.296890 | lrm: 1.00 | dt: 2589.71ms | tok/sec: 202,450 | mfu: 52.05 | epoch: 1 | total time: 37.36m | eta: 39.4m\n",
      "step 00875/01785 (49.02%) | loss: 3.295855 | lrm: 1.00 | dt: 2603.11ms | tok/sec: 201,408 | mfu: 51.78 | epoch: 1 | total time: 37.40m | eta: 39.4m\n",
      "step 00876/01785 (49.08%) | loss: 3.288195 | lrm: 1.00 | dt: 2604.16ms | tok/sec: 201,326 | mfu: 51.76 | epoch: 1 | total time: 37.45m | eta: 39.3m\n",
      "step 00877/01785 (49.13%) | loss: 3.276523 | lrm: 1.00 | dt: 2593.71ms | tok/sec: 202,137 | mfu: 51.97 | epoch: 1 | total time: 37.49m | eta: 39.3m\n",
      "step 00878/01785 (49.19%) | loss: 3.274271 | lrm: 1.00 | dt: 2593.15ms | tok/sec: 202,182 | mfu: 51.98 | epoch: 1 | total time: 37.53m | eta: 39.2m\n",
      "step 00879/01785 (49.24%) | loss: 3.269709 | lrm: 1.00 | dt: 2605.65ms | tok/sec: 201,212 | mfu: 51.73 | epoch: 1 | total time: 37.58m | eta: 39.2m\n",
      "step 00880/01785 (49.30%) | loss: 3.272816 | lrm: 1.00 | dt: 2605.90ms | tok/sec: 201,192 | mfu: 51.73 | epoch: 1 | total time: 37.62m | eta: 39.1m\n",
      "step 00881/01785 (49.36%) | loss: 3.265406 | lrm: 1.00 | dt: 2604.92ms | tok/sec: 201,268 | mfu: 51.75 | epoch: 1 | total time: 37.67m | eta: 39.1m\n",
      "step 00882/01785 (49.41%) | loss: 3.265440 | lrm: 1.00 | dt: 2587.50ms | tok/sec: 202,623 | mfu: 52.10 | epoch: 1 | total time: 37.71m | eta: 39.0m\n",
      "step 00883/01785 (49.47%) | loss: 3.262194 | lrm: 1.00 | dt: 2599.53ms | tok/sec: 201,685 | mfu: 51.85 | epoch: 1 | total time: 37.75m | eta: 39.0m\n",
      "step 00884/01785 (49.52%) | loss: 3.275092 | lrm: 1.00 | dt: 2591.78ms | tok/sec: 202,288 | mfu: 52.01 | epoch: 1 | total time: 37.79m | eta: 39.0m\n",
      "step 00885/01785 (49.58%) | loss: 3.282183 | lrm: 1.00 | dt: 2593.86ms | tok/sec: 202,126 | mfu: 51.97 | epoch: 1 | total time: 37.84m | eta: 38.9m\n",
      "step 00886/01785 (49.64%) | loss: 3.283106 | lrm: 1.00 | dt: 2585.98ms | tok/sec: 202,742 | mfu: 52.13 | epoch: 1 | total time: 37.88m | eta: 38.9m\n",
      "step 00887/01785 (49.69%) | loss: 3.282489 | lrm: 1.00 | dt: 2603.20ms | tok/sec: 201,401 | mfu: 51.78 | epoch: 1 | total time: 37.92m | eta: 38.8m\n",
      "step 00888/01785 (49.75%) | loss: 3.263278 | lrm: 1.00 | dt: 2586.75ms | tok/sec: 202,682 | mfu: 52.11 | epoch: 1 | total time: 37.97m | eta: 38.8m\n",
      "step 00889/01785 (49.80%) | loss: 3.274022 | lrm: 1.00 | dt: 2601.44ms | tok/sec: 201,537 | mfu: 51.82 | epoch: 1 | total time: 38.01m | eta: 38.7m\n",
      "step 00890/01785 (49.86%) | loss: 3.259061 | lrm: 1.00 | dt: 2586.35ms | tok/sec: 202,713 | mfu: 52.12 | epoch: 1 | total time: 38.05m | eta: 38.7m\n",
      "step 00891/01785 (49.92%) | loss: 3.262896 | lrm: 1.00 | dt: 2602.45ms | tok/sec: 201,459 | mfu: 51.80 | epoch: 1 | total time: 38.10m | eta: 38.7m\n",
      "step 00892/01785 (49.97%) | loss: 3.255684 | lrm: 1.00 | dt: 2586.92ms | tok/sec: 202,668 | mfu: 52.11 | epoch: 1 | total time: 38.14m | eta: 38.6m\n",
      "step 00893/01785 (50.03%) | loss: 3.262955 | lrm: 1.00 | dt: 2602.59ms | tok/sec: 201,448 | mfu: 51.79 | epoch: 1 | total time: 38.18m | eta: 38.6m\n",
      "step 00894/01785 (50.08%) | loss: 3.259058 | lrm: 1.00 | dt: 2599.78ms | tok/sec: 201,666 | mfu: 51.85 | epoch: 1 | total time: 38.23m | eta: 38.5m\n",
      "step 00895/01785 (50.14%) | loss: 3.265502 | lrm: 1.00 | dt: 2595.92ms | tok/sec: 201,966 | mfu: 51.93 | epoch: 1 | total time: 38.27m | eta: 38.5m\n",
      "step 00896/01785 (50.20%) | loss: 3.269676 | lrm: 1.00 | dt: 2588.43ms | tok/sec: 202,550 | mfu: 52.08 | epoch: 1 | total time: 38.31m | eta: 38.4m\n",
      "step 00897/01785 (50.25%) | loss: 3.269955 | lrm: 1.00 | dt: 2589.31ms | tok/sec: 202,481 | mfu: 52.06 | epoch: 1 | total time: 38.36m | eta: 38.4m\n",
      "step 00898/01785 (50.31%) | loss: 3.269101 | lrm: 0.99 | dt: 2591.70ms | tok/sec: 202,295 | mfu: 52.01 | epoch: 1 | total time: 38.40m | eta: 38.4m\n",
      "step 00899/01785 (50.36%) | loss: 3.275790 | lrm: 0.99 | dt: 2592.21ms | tok/sec: 202,254 | mfu: 52.00 | epoch: 1 | total time: 38.44m | eta: 38.3m\n",
      "step 00900/01785 (50.42%) | loss: 3.272053 | lrm: 0.99 | dt: 2606.02ms | tok/sec: 201,183 | mfu: 51.73 | epoch: 1 | total time: 38.49m | eta: 38.3m\n",
      "step 00901/01785 (50.48%) | loss: 3.262459 | lrm: 0.99 | dt: 2590.24ms | tok/sec: 202,409 | mfu: 52.04 | epoch: 1 | total time: 38.53m | eta: 38.2m\n",
      "step 00902/01785 (50.53%) | loss: 3.261376 | lrm: 0.99 | dt: 2599.78ms | tok/sec: 201,665 | mfu: 51.85 | epoch: 1 | total time: 38.57m | eta: 38.2m\n",
      "step 00903/01785 (50.59%) | loss: 3.261325 | lrm: 0.99 | dt: 2587.47ms | tok/sec: 202,625 | mfu: 52.10 | epoch: 1 | total time: 38.62m | eta: 38.1m\n",
      "step 00904/01785 (50.64%) | loss: 3.266459 | lrm: 0.99 | dt: 2603.36ms | tok/sec: 201,388 | mfu: 51.78 | epoch: 1 | total time: 38.66m | eta: 38.1m\n",
      "step 00905/01785 (50.70%) | loss: 3.262334 | lrm: 0.99 | dt: 2595.67ms | tok/sec: 201,985 | mfu: 51.93 | epoch: 1 | total time: 38.70m | eta: 38.1m\n",
      "step 00906/01785 (50.76%) | loss: 3.264603 | lrm: 0.99 | dt: 2591.03ms | tok/sec: 202,347 | mfu: 52.02 | epoch: 1 | total time: 38.75m | eta: 38.0m\n",
      "step 00907/01785 (50.81%) | loss: 3.268936 | lrm: 0.98 | dt: 2605.33ms | tok/sec: 201,236 | mfu: 51.74 | epoch: 1 | total time: 38.79m | eta: 38.0m\n",
      "step 00908/01785 (50.87%) | loss: 3.275097 | lrm: 0.98 | dt: 2586.70ms | tok/sec: 202,686 | mfu: 52.11 | epoch: 1 | total time: 38.83m | eta: 37.9m\n",
      "step 00909/01785 (50.92%) | loss: 3.278534 | lrm: 0.98 | dt: 2602.85ms | tok/sec: 201,428 | mfu: 51.79 | epoch: 1 | total time: 38.88m | eta: 37.9m\n",
      "step 00910/01785 (50.98%) | loss: 3.289065 | lrm: 0.98 | dt: 2605.28ms | tok/sec: 201,240 | mfu: 51.74 | epoch: 1 | total time: 38.92m | eta: 37.8m\n",
      "step 00911/01785 (51.04%) | loss: 3.284621 | lrm: 0.98 | dt: 2605.40ms | tok/sec: 201,231 | mfu: 51.74 | epoch: 1 | total time: 38.96m | eta: 37.8m\n",
      "step 00912/01785 (51.09%) | loss: 3.278426 | lrm: 0.98 | dt: 2605.35ms | tok/sec: 201,235 | mfu: 51.74 | epoch: 1 | total time: 39.01m | eta: 37.8m\n",
      "step 00913/01785 (51.15%) | loss: 3.274283 | lrm: 0.98 | dt: 2605.21ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 39.05m | eta: 37.7m\n",
      "step 00914/01785 (51.20%) | loss: 3.269403 | lrm: 0.98 | dt: 2604.66ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 39.09m | eta: 37.7m\n",
      "step 00915/01785 (51.26%) | loss: 3.272227 | lrm: 0.98 | dt: 2606.70ms | tok/sec: 201,130 | mfu: 51.71 | epoch: 1 | total time: 39.14m | eta: 37.6m\n",
      "step 00916/01785 (51.32%) | loss: 3.289669 | lrm: 0.97 | dt: 2605.63ms | tok/sec: 201,213 | mfu: 51.73 | epoch: 1 | total time: 39.18m | eta: 37.6m\n",
      "step 00917/01785 (51.37%) | loss: 3.282755 | lrm: 0.97 | dt: 2604.60ms | tok/sec: 201,293 | mfu: 51.75 | epoch: 1 | total time: 39.22m | eta: 37.5m\n",
      "step 00918/01785 (51.43%) | loss: 3.294490 | lrm: 0.97 | dt: 2590.16ms | tok/sec: 202,415 | mfu: 52.04 | epoch: 1 | total time: 39.27m | eta: 37.5m\n",
      "step 00919/01785 (51.48%) | loss: 3.297621 | lrm: 0.97 | dt: 2599.34ms | tok/sec: 201,700 | mfu: 51.86 | epoch: 1 | total time: 39.31m | eta: 37.5m\n",
      "step 00920/01785 (51.54%) | loss: 3.305973 | lrm: 0.97 | dt: 2588.72ms | tok/sec: 202,527 | mfu: 52.07 | epoch: 1 | total time: 39.35m | eta: 37.4m\n",
      "step 00921/01785 (51.60%) | loss: 3.295422 | lrm: 0.97 | dt: 2606.50ms | tok/sec: 201,146 | mfu: 51.72 | epoch: 1 | total time: 39.40m | eta: 37.4m\n",
      "step 00922/01785 (51.65%) | loss: 3.290092 | lrm: 0.97 | dt: 2607.95ms | tok/sec: 201,034 | mfu: 51.69 | epoch: 1 | total time: 39.44m | eta: 37.3m\n",
      "step 00923/01785 (51.71%) | loss: 3.279474 | lrm: 0.97 | dt: 2591.01ms | tok/sec: 202,349 | mfu: 52.02 | epoch: 1 | total time: 39.48m | eta: 37.3m\n",
      "step 00924/01785 (51.76%) | loss: 3.282272 | lrm: 0.97 | dt: 2592.57ms | tok/sec: 202,227 | mfu: 51.99 | epoch: 1 | total time: 39.53m | eta: 37.2m\n",
      "step 00925/01785 (51.82%) | loss: 3.284081 | lrm: 0.96 | dt: 2592.99ms | tok/sec: 202,194 | mfu: 51.99 | epoch: 1 | total time: 39.57m | eta: 37.2m\n",
      "step 00926/01785 (51.88%) | loss: 3.274744 | lrm: 0.96 | dt: 2600.69ms | tok/sec: 201,595 | mfu: 51.83 | epoch: 1 | total time: 39.61m | eta: 37.1m\n",
      "step 00927/01785 (51.93%) | loss: 3.275837 | lrm: 0.96 | dt: 2591.09ms | tok/sec: 202,342 | mfu: 52.02 | epoch: 1 | total time: 39.66m | eta: 37.1m\n",
      "step 00928/01785 (51.99%) | loss: 3.271709 | lrm: 0.96 | dt: 2595.56ms | tok/sec: 201,994 | mfu: 51.93 | epoch: 1 | total time: 39.70m | eta: 37.1m\n",
      "step 00929/01785 (52.04%) | loss: 3.275244 | lrm: 0.96 | dt: 2591.98ms | tok/sec: 202,273 | mfu: 52.01 | epoch: 1 | total time: 39.74m | eta: 37.0m\n",
      "step 00930/01785 (52.10%) | loss: 3.277424 | lrm: 0.96 | dt: 2603.65ms | tok/sec: 201,366 | mfu: 51.77 | epoch: 1 | total time: 39.79m | eta: 37.0m\n",
      "step 00931/01785 (52.16%) | loss: 3.276397 | lrm: 0.96 | dt: 2604.96ms | tok/sec: 201,265 | mfu: 51.75 | epoch: 1 | total time: 39.83m | eta: 36.9m\n",
      "step 00932/01785 (52.21%) | loss: 3.272380 | lrm: 0.96 | dt: 2586.34ms | tok/sec: 202,714 | mfu: 52.12 | epoch: 1 | total time: 39.87m | eta: 36.9m\n",
      "step 00933/01785 (52.27%) | loss: 3.273606 | lrm: 0.96 | dt: 2601.90ms | tok/sec: 201,501 | mfu: 51.81 | epoch: 1 | total time: 39.92m | eta: 36.8m\n",
      "step 00934/01785 (52.32%) | loss: 3.269783 | lrm: 0.95 | dt: 2586.89ms | tok/sec: 202,670 | mfu: 52.11 | epoch: 1 | total time: 39.96m | eta: 36.8m\n",
      "step 00935/01785 (52.38%) | loss: 3.272708 | lrm: 0.95 | dt: 2600.85ms | tok/sec: 201,583 | mfu: 51.83 | epoch: 1 | total time: 40.00m | eta: 36.8m\n",
      "step 00936/01785 (52.44%) | loss: 3.267077 | lrm: 0.95 | dt: 2603.35ms | tok/sec: 201,390 | mfu: 51.78 | epoch: 1 | total time: 40.05m | eta: 36.7m\n",
      "step 00937/01785 (52.49%) | loss: 3.264498 | lrm: 0.95 | dt: 2589.26ms | tok/sec: 202,485 | mfu: 52.06 | epoch: 1 | total time: 40.09m | eta: 36.7m\n",
      "step 00938/01785 (52.55%) | loss: 3.283632 | lrm: 0.95 | dt: 2600.71ms | tok/sec: 201,594 | mfu: 51.83 | epoch: 1 | total time: 40.13m | eta: 36.6m\n",
      "step 00939/01785 (52.61%) | loss: 3.278969 | lrm: 0.95 | dt: 2605.37ms | tok/sec: 201,233 | mfu: 51.74 | epoch: 1 | total time: 40.18m | eta: 36.6m\n",
      "step 00940/01785 (52.66%) | loss: 3.283284 | lrm: 0.95 | dt: 2604.91ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 1 | total time: 40.22m | eta: 36.5m\n",
      "step 00941/01785 (52.72%) | loss: 3.268695 | lrm: 0.95 | dt: 2604.55ms | tok/sec: 201,296 | mfu: 51.75 | epoch: 1 | total time: 40.26m | eta: 36.5m\n",
      "step 00942/01785 (52.77%) | loss: 3.272213 | lrm: 0.95 | dt: 2592.40ms | tok/sec: 202,240 | mfu: 52.00 | epoch: 1 | total time: 40.31m | eta: 36.5m\n",
      "step 00943/01785 (52.83%) | loss: 3.277130 | lrm: 0.94 | dt: 2596.54ms | tok/sec: 201,917 | mfu: 51.91 | epoch: 1 | total time: 40.35m | eta: 36.4m\n",
      "step 00944/01785 (52.89%) | loss: 3.265517 | lrm: 0.94 | dt: 2604.97ms | tok/sec: 201,264 | mfu: 51.75 | epoch: 1 | total time: 40.39m | eta: 36.4m\n",
      "step 00945/01785 (52.94%) | loss: 3.277923 | lrm: 0.94 | dt: 2588.29ms | tok/sec: 202,561 | mfu: 52.08 | epoch: 1 | total time: 40.44m | eta: 36.3m\n",
      "step 00946/01785 (53.00%) | loss: 3.267980 | lrm: 0.94 | dt: 2605.73ms | tok/sec: 201,205 | mfu: 51.73 | epoch: 1 | total time: 40.48m | eta: 36.3m\n",
      "step 00947/01785 (53.05%) | loss: 3.280384 | lrm: 0.94 | dt: 2588.40ms | tok/sec: 202,552 | mfu: 52.08 | epoch: 1 | total time: 40.52m | eta: 36.2m\n",
      "step 00948/01785 (53.11%) | loss: 3.271499 | lrm: 0.94 | dt: 2597.24ms | tok/sec: 201,863 | mfu: 51.90 | epoch: 1 | total time: 40.57m | eta: 36.2m\n",
      "step 00949/01785 (53.17%) | loss: 3.268044 | lrm: 0.94 | dt: 2593.09ms | tok/sec: 202,186 | mfu: 51.98 | epoch: 1 | total time: 40.61m | eta: 36.2m\n",
      "step 00950/01785 (53.22%) | loss: 3.259324 | lrm: 0.94 | dt: 2595.24ms | tok/sec: 202,018 | mfu: 51.94 | epoch: 1 | total time: 40.65m | eta: 36.1m\n",
      "step 00951/01785 (53.28%) | loss: 3.263606 | lrm: 0.93 | dt: 2596.95ms | tok/sec: 201,886 | mfu: 51.91 | epoch: 1 | total time: 40.70m | eta: 36.1m\n",
      "step 00952/01785 (53.33%) | loss: 3.265747 | lrm: 0.93 | dt: 2587.55ms | tok/sec: 202,619 | mfu: 52.09 | epoch: 1 | total time: 40.74m | eta: 36.0m\n",
      "step 00953/01785 (53.39%) | loss: 3.275393 | lrm: 0.93 | dt: 2590.81ms | tok/sec: 202,364 | mfu: 52.03 | epoch: 1 | total time: 40.78m | eta: 36.0m\n",
      "step 00954/01785 (53.45%) | loss: 3.279861 | lrm: 0.93 | dt: 2598.88ms | tok/sec: 201,736 | mfu: 51.87 | epoch: 1 | total time: 40.82m | eta: 35.9m\n",
      "step 00955/01785 (53.50%) | loss: 3.274083 | lrm: 0.93 | dt: 2585.92ms | tok/sec: 202,747 | mfu: 52.13 | epoch: 1 | total time: 40.87m | eta: 35.9m\n",
      "step 00956/01785 (53.56%) | loss: 3.267060 | lrm: 0.93 | dt: 2603.70ms | tok/sec: 201,362 | mfu: 51.77 | epoch: 1 | total time: 40.91m | eta: 35.9m\n",
      "step 00957/01785 (53.61%) | loss: 3.271902 | lrm: 0.93 | dt: 2585.80ms | tok/sec: 202,756 | mfu: 52.13 | epoch: 1 | total time: 40.95m | eta: 35.8m\n",
      "step 00958/01785 (53.67%) | loss: 3.264583 | lrm: 0.93 | dt: 2601.80ms | tok/sec: 201,509 | mfu: 51.81 | epoch: 1 | total time: 41.00m | eta: 35.8m\n",
      "step 00959/01785 (53.73%) | loss: 3.277195 | lrm: 0.93 | dt: 2586.82ms | tok/sec: 202,676 | mfu: 52.11 | epoch: 1 | total time: 41.04m | eta: 35.7m\n",
      "step 00960/01785 (53.78%) | loss: 3.284264 | lrm: 0.92 | dt: 2598.68ms | tok/sec: 201,751 | mfu: 51.87 | epoch: 1 | total time: 41.08m | eta: 35.7m\n",
      "step 00961/01785 (53.84%) | loss: 3.270927 | lrm: 0.92 | dt: 2585.41ms | tok/sec: 202,786 | mfu: 52.14 | epoch: 1 | total time: 41.13m | eta: 35.6m\n",
      "step 00962/01785 (53.89%) | loss: 3.261207 | lrm: 0.92 | dt: 2601.83ms | tok/sec: 201,507 | mfu: 51.81 | epoch: 1 | total time: 41.17m | eta: 35.6m\n",
      "step 00963/01785 (53.95%) | loss: 3.261059 | lrm: 0.92 | dt: 2584.38ms | tok/sec: 202,867 | mfu: 52.16 | epoch: 1 | total time: 41.21m | eta: 35.5m\n",
      "step 00964/01785 (54.01%) | loss: 3.271579 | lrm: 0.92 | dt: 2586.79ms | tok/sec: 202,678 | mfu: 52.11 | epoch: 1 | total time: 41.26m | eta: 35.5m\n",
      "step 00965/01785 (54.06%) | loss: 3.276935 | lrm: 0.92 | dt: 2600.44ms | tok/sec: 201,615 | mfu: 51.84 | epoch: 1 | total time: 41.30m | eta: 35.5m\n",
      "step 00966/01785 (54.12%) | loss: 3.281513 | lrm: 0.92 | dt: 2586.28ms | tok/sec: 202,719 | mfu: 52.12 | epoch: 1 | total time: 41.34m | eta: 35.4m\n",
      "step 00967/01785 (54.17%) | loss: 3.280486 | lrm: 0.92 | dt: 2599.01ms | tok/sec: 201,725 | mfu: 51.86 | epoch: 1 | total time: 41.39m | eta: 35.4m\n",
      "step 00968/01785 (54.23%) | loss: 3.268491 | lrm: 0.92 | dt: 2587.62ms | tok/sec: 202,614 | mfu: 52.09 | epoch: 1 | total time: 41.43m | eta: 35.3m\n",
      "step 00969/01785 (54.29%) | loss: 3.267200 | lrm: 0.91 | dt: 2601.28ms | tok/sec: 201,549 | mfu: 51.82 | epoch: 1 | total time: 41.47m | eta: 35.3m\n",
      "step 00970/01785 (54.34%) | loss: 3.266132 | lrm: 0.91 | dt: 2590.30ms | tok/sec: 202,404 | mfu: 52.04 | epoch: 1 | total time: 41.52m | eta: 35.2m\n",
      "step 00971/01785 (54.40%) | loss: 3.265457 | lrm: 0.91 | dt: 2601.31ms | tok/sec: 201,547 | mfu: 51.82 | epoch: 1 | total time: 41.56m | eta: 35.2m\n",
      "step 00972/01785 (54.45%) | loss: 3.273237 | lrm: 0.91 | dt: 2588.19ms | tok/sec: 202,569 | mfu: 52.08 | epoch: 1 | total time: 41.60m | eta: 35.2m\n",
      "step 00973/01785 (54.51%) | loss: 3.285542 | lrm: 0.91 | dt: 2589.18ms | tok/sec: 202,491 | mfu: 52.06 | epoch: 1 | total time: 41.65m | eta: 35.1m\n",
      "step 00974/01785 (54.57%) | loss: 3.266770 | lrm: 0.91 | dt: 2591.81ms | tok/sec: 202,286 | mfu: 52.01 | epoch: 1 | total time: 41.69m | eta: 35.1m\n",
      "step 00975/01785 (54.62%) | loss: 3.267780 | lrm: 0.91 | dt: 2602.79ms | tok/sec: 201,433 | mfu: 51.79 | epoch: 1 | total time: 41.73m | eta: 35.0m\n",
      "step 00976/01785 (54.68%) | loss: 3.246615 | lrm: 0.91 | dt: 2591.85ms | tok/sec: 202,283 | mfu: 52.01 | epoch: 1 | total time: 41.78m | eta: 35.0m\n",
      "step 00977/01785 (54.73%) | loss: 3.233264 | lrm: 0.91 | dt: 2595.55ms | tok/sec: 201,994 | mfu: 51.93 | epoch: 1 | total time: 41.82m | eta: 34.9m\n",
      "step 00978/01785 (54.79%) | loss: 3.227437 | lrm: 0.90 | dt: 2586.84ms | tok/sec: 202,675 | mfu: 52.11 | epoch: 1 | total time: 41.86m | eta: 34.9m\n",
      "step 00979/01785 (54.85%) | loss: 3.223393 | lrm: 0.90 | dt: 2589.44ms | tok/sec: 202,471 | mfu: 52.06 | epoch: 1 | total time: 41.91m | eta: 34.9m\n",
      "step 00980/01785 (54.90%) | loss: 3.210043 | lrm: 0.90 | dt: 2598.94ms | tok/sec: 201,731 | mfu: 51.87 | epoch: 1 | total time: 41.95m | eta: 34.8m\n",
      "step 00981/01785 (54.96%) | loss: 3.199395 | lrm: 0.90 | dt: 2587.19ms | tok/sec: 202,647 | mfu: 52.10 | epoch: 1 | total time: 41.99m | eta: 34.8m\n",
      "step 00982/01785 (55.01%) | loss: 3.211986 | lrm: 0.90 | dt: 2598.70ms | tok/sec: 201,750 | mfu: 51.87 | epoch: 1 | total time: 42.03m | eta: 34.7m\n",
      "step 00983/01785 (55.07%) | loss: 3.215064 | lrm: 0.90 | dt: 2587.79ms | tok/sec: 202,600 | mfu: 52.09 | epoch: 1 | total time: 42.08m | eta: 34.7m\n",
      "step 00984/01785 (55.13%) | loss: 3.220843 | lrm: 0.90 | dt: 2600.00ms | tok/sec: 201,649 | mfu: 51.85 | epoch: 1 | total time: 42.12m | eta: 34.6m\n",
      "step 00985/01785 (55.18%) | loss: 3.230214 | lrm: 0.90 | dt: 2596.14ms | tok/sec: 201,949 | mfu: 51.92 | epoch: 1 | total time: 42.16m | eta: 34.6m\n",
      "step 00986/01785 (55.24%) | loss: 3.212300 | lrm: 0.90 | dt: 2591.81ms | tok/sec: 202,286 | mfu: 52.01 | epoch: 1 | total time: 42.21m | eta: 34.6m\n",
      "step 00987/01785 (55.29%) | loss: 3.204237 | lrm: 0.89 | dt: 2591.13ms | tok/sec: 202,339 | mfu: 52.02 | epoch: 1 | total time: 42.25m | eta: 34.5m\n",
      "step 00988/01785 (55.35%) | loss: 3.207928 | lrm: 0.89 | dt: 2595.91ms | tok/sec: 201,966 | mfu: 51.93 | epoch: 1 | total time: 42.29m | eta: 34.5m\n",
      "step 00989/01785 (55.41%) | loss: 3.208819 | lrm: 0.89 | dt: 2588.40ms | tok/sec: 202,552 | mfu: 52.08 | epoch: 1 | total time: 42.34m | eta: 34.4m\n",
      "step 00990/01785 (55.46%) | loss: 3.206240 | lrm: 0.89 | dt: 2602.25ms | tok/sec: 201,475 | mfu: 51.80 | epoch: 1 | total time: 42.38m | eta: 34.4m\n",
      "step 00991/01785 (55.52%) | loss: 3.204119 | lrm: 0.89 | dt: 2588.51ms | tok/sec: 202,544 | mfu: 52.08 | epoch: 1 | total time: 42.42m | eta: 34.3m\n",
      "step 00992/01785 (55.57%) | loss: 3.195476 | lrm: 0.89 | dt: 2601.79ms | tok/sec: 201,510 | mfu: 51.81 | epoch: 1 | total time: 42.47m | eta: 34.3m\n",
      "step 00993/01785 (55.63%) | loss: 3.213322 | lrm: 0.89 | dt: 2591.05ms | tok/sec: 202,345 | mfu: 52.02 | epoch: 1 | total time: 42.51m | eta: 34.3m\n",
      "step 00994/01785 (55.69%) | loss: 3.207063 | lrm: 0.89 | dt: 2596.17ms | tok/sec: 201,946 | mfu: 51.92 | epoch: 1 | total time: 42.55m | eta: 34.2m\n",
      "step 00995/01785 (55.74%) | loss: 3.215896 | lrm: 0.89 | dt: 2589.41ms | tok/sec: 202,474 | mfu: 52.06 | epoch: 1 | total time: 42.60m | eta: 34.2m\n",
      "step 00996/01785 (55.80%) | loss: 3.210530 | lrm: 0.88 | dt: 2600.66ms | tok/sec: 201,598 | mfu: 51.83 | epoch: 1 | total time: 42.64m | eta: 34.1m\n",
      "step 00997/01785 (55.85%) | loss: 3.232833 | lrm: 0.88 | dt: 2605.91ms | tok/sec: 201,192 | mfu: 51.73 | epoch: 1 | total time: 42.68m | eta: 34.1m\n",
      "step 00998/01785 (55.91%) | loss: 3.233704 | lrm: 0.88 | dt: 2588.19ms | tok/sec: 202,569 | mfu: 52.08 | epoch: 1 | total time: 42.73m | eta: 34.0m\n",
      "step 00999/01785 (55.97%) | loss: 3.223795 | lrm: 0.88 | dt: 2606.40ms | tok/sec: 201,154 | mfu: 51.72 | epoch: 1 | total time: 42.77m | eta: 34.0m\n",
      "Step 01000 | Validation bpb: 0.987937\n",
      "step 01000/01785 (56.02%) | loss: 3.216955 | lrm: 0.88 | dt: 2602.51ms | tok/sec: 201,454 | mfu: 51.80 | epoch: 1 | total time: 42.81m | eta: 33.9m\n",
      "step 01001/01785 (56.08%) | loss: 3.217588 | lrm: 0.88 | dt: 2583.75ms | tok/sec: 202,917 | mfu: 52.17 | epoch: 1 | total time: 42.86m | eta: 33.9m\n",
      "step 01002/01785 (56.13%) | loss: 3.238337 | lrm: 0.88 | dt: 2603.95ms | tok/sec: 201,343 | mfu: 51.77 | epoch: 1 | total time: 42.90m | eta: 33.9m\n",
      "step 01003/01785 (56.19%) | loss: 3.240559 | lrm: 0.88 | dt: 2588.27ms | tok/sec: 202,563 | mfu: 52.08 | epoch: 1 | total time: 42.94m | eta: 33.8m\n",
      "step 01004/01785 (56.25%) | loss: 3.241421 | lrm: 0.88 | dt: 2584.65ms | tok/sec: 202,846 | mfu: 52.15 | epoch: 1 | total time: 42.99m | eta: 33.8m\n",
      "step 01005/01785 (56.30%) | loss: 3.239616 | lrm: 0.87 | dt: 2588.46ms | tok/sec: 202,548 | mfu: 52.08 | epoch: 1 | total time: 43.03m | eta: 33.7m\n",
      "step 01006/01785 (56.36%) | loss: 3.238842 | lrm: 0.87 | dt: 2589.36ms | tok/sec: 202,478 | mfu: 52.06 | epoch: 1 | total time: 43.07m | eta: 33.7m\n",
      "step 01007/01785 (56.41%) | loss: 3.231215 | lrm: 0.87 | dt: 2586.97ms | tok/sec: 202,664 | mfu: 52.11 | epoch: 1 | total time: 43.12m | eta: 33.6m\n",
      "step 01008/01785 (56.47%) | loss: 3.237064 | lrm: 0.87 | dt: 2589.00ms | tok/sec: 202,506 | mfu: 52.07 | epoch: 1 | total time: 43.16m | eta: 33.6m\n",
      "step 01009/01785 (56.53%) | loss: 3.234130 | lrm: 0.87 | dt: 2587.25ms | tok/sec: 202,642 | mfu: 52.10 | epoch: 1 | total time: 43.20m | eta: 33.6m\n",
      "step 01010/01785 (56.58%) | loss: 3.239473 | lrm: 0.87 | dt: 2595.82ms | tok/sec: 201,974 | mfu: 51.93 | epoch: 1 | total time: 43.25m | eta: 33.5m\n",
      "step 01011/01785 (56.64%) | loss: 3.234624 | lrm: 0.87 | dt: 2586.53ms | tok/sec: 202,699 | mfu: 52.12 | epoch: 1 | total time: 43.29m | eta: 33.5m\n",
      "step 01012/01785 (56.69%) | loss: 3.237257 | lrm: 0.87 | dt: 2583.30ms | tok/sec: 202,953 | mfu: 52.18 | epoch: 1 | total time: 43.33m | eta: 33.4m\n",
      "step 01013/01785 (56.75%) | loss: 3.233294 | lrm: 0.87 | dt: 2603.18ms | tok/sec: 201,402 | mfu: 51.78 | epoch: 1 | total time: 43.37m | eta: 33.4m\n",
      "step 01014/01785 (56.81%) | loss: 3.228290 | lrm: 0.86 | dt: 2585.13ms | tok/sec: 202,809 | mfu: 52.14 | epoch: 1 | total time: 43.42m | eta: 33.3m\n",
      "step 01015/01785 (56.86%) | loss: 3.225669 | lrm: 0.86 | dt: 2601.51ms | tok/sec: 201,532 | mfu: 51.81 | epoch: 1 | total time: 43.46m | eta: 33.3m\n",
      "step 01016/01785 (56.92%) | loss: 3.226330 | lrm: 0.86 | dt: 2604.90ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 1 | total time: 43.50m | eta: 33.3m\n",
      "step 01017/01785 (56.97%) | loss: 3.227302 | lrm: 0.86 | dt: 2604.91ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 1 | total time: 43.55m | eta: 33.2m\n",
      "step 01018/01785 (57.03%) | loss: 3.224336 | lrm: 0.86 | dt: 2588.40ms | tok/sec: 202,552 | mfu: 52.08 | epoch: 1 | total time: 43.59m | eta: 33.2m\n",
      "step 01019/01785 (57.09%) | loss: 3.226848 | lrm: 0.86 | dt: 2602.64ms | tok/sec: 201,444 | mfu: 51.79 | epoch: 1 | total time: 43.63m | eta: 33.1m\n",
      "step 01020/01785 (57.14%) | loss: 3.232981 | lrm: 0.86 | dt: 2605.19ms | tok/sec: 201,247 | mfu: 51.74 | epoch: 1 | total time: 43.68m | eta: 33.1m\n",
      "step 01021/01785 (57.20%) | loss: 3.231193 | lrm: 0.86 | dt: 2606.24ms | tok/sec: 201,166 | mfu: 51.72 | epoch: 1 | total time: 43.72m | eta: 33.0m\n",
      "step 01022/01785 (57.25%) | loss: 3.235039 | lrm: 0.86 | dt: 2606.84ms | tok/sec: 201,119 | mfu: 51.71 | epoch: 1 | total time: 43.76m | eta: 33.0m\n",
      "step 01023/01785 (57.31%) | loss: 3.234807 | lrm: 0.85 | dt: 2603.48ms | tok/sec: 201,379 | mfu: 51.78 | epoch: 1 | total time: 43.81m | eta: 33.0m\n",
      "step 01024/01785 (57.37%) | loss: 3.243803 | lrm: 0.85 | dt: 2592.70ms | tok/sec: 202,216 | mfu: 51.99 | epoch: 1 | total time: 43.85m | eta: 32.9m\n",
      "step 01025/01785 (57.42%) | loss: 3.243416 | lrm: 0.85 | dt: 2597.09ms | tok/sec: 201,875 | mfu: 51.90 | epoch: 1 | total time: 43.89m | eta: 32.9m\n",
      "step 01026/01785 (57.48%) | loss: 3.237709 | lrm: 0.85 | dt: 2588.39ms | tok/sec: 202,553 | mfu: 52.08 | epoch: 1 | total time: 43.94m | eta: 32.8m\n",
      "step 01027/01785 (57.54%) | loss: 3.245433 | lrm: 0.85 | dt: 2599.61ms | tok/sec: 201,679 | mfu: 51.85 | epoch: 1 | total time: 43.98m | eta: 32.8m\n",
      "step 01028/01785 (57.59%) | loss: 3.245719 | lrm: 0.85 | dt: 2604.73ms | tok/sec: 201,283 | mfu: 51.75 | epoch: 1 | total time: 44.02m | eta: 32.7m\n",
      "step 01029/01785 (57.65%) | loss: 3.232628 | lrm: 0.85 | dt: 2596.23ms | tok/sec: 201,941 | mfu: 51.92 | epoch: 1 | total time: 44.07m | eta: 32.7m\n",
      "step 01030/01785 (57.70%) | loss: 3.222817 | lrm: 0.85 | dt: 2591.68ms | tok/sec: 202,296 | mfu: 52.01 | epoch: 1 | total time: 44.11m | eta: 32.7m\n",
      "step 01031/01785 (57.76%) | loss: 3.216980 | lrm: 0.85 | dt: 2591.97ms | tok/sec: 202,273 | mfu: 52.01 | epoch: 1 | total time: 44.15m | eta: 32.6m\n",
      "step 01032/01785 (57.82%) | loss: 3.221045 | lrm: 0.84 | dt: 2602.28ms | tok/sec: 201,472 | mfu: 51.80 | epoch: 1 | total time: 44.20m | eta: 32.6m\n",
      "step 01033/01785 (57.87%) | loss: 3.217634 | lrm: 0.84 | dt: 2590.64ms | tok/sec: 202,377 | mfu: 52.03 | epoch: 1 | total time: 44.24m | eta: 32.5m\n",
      "step 01034/01785 (57.93%) | loss: 3.229250 | lrm: 0.84 | dt: 2591.39ms | tok/sec: 202,319 | mfu: 52.02 | epoch: 1 | total time: 44.28m | eta: 32.5m\n",
      "step 01035/01785 (57.98%) | loss: 3.237027 | lrm: 0.84 | dt: 2591.17ms | tok/sec: 202,336 | mfu: 52.02 | epoch: 1 | total time: 44.33m | eta: 32.4m\n",
      "step 01036/01785 (58.04%) | loss: 3.234375 | lrm: 0.84 | dt: 2590.91ms | tok/sec: 202,356 | mfu: 52.03 | epoch: 1 | total time: 44.37m | eta: 32.4m\n",
      "step 01037/01785 (58.10%) | loss: 3.224145 | lrm: 0.84 | dt: 2601.81ms | tok/sec: 201,508 | mfu: 51.81 | epoch: 1 | total time: 44.41m | eta: 32.3m\n",
      "step 01038/01785 (58.15%) | loss: 3.205375 | lrm: 0.84 | dt: 2589.60ms | tok/sec: 202,459 | mfu: 52.05 | epoch: 1 | total time: 44.46m | eta: 32.3m\n",
      "step 01039/01785 (58.21%) | loss: 3.203272 | lrm: 0.84 | dt: 2601.26ms | tok/sec: 201,551 | mfu: 51.82 | epoch: 1 | total time: 44.50m | eta: 32.3m\n",
      "step 01040/01785 (58.26%) | loss: 3.211456 | lrm: 0.84 | dt: 2605.47ms | tok/sec: 201,226 | mfu: 51.74 | epoch: 1 | total time: 44.54m | eta: 32.2m\n",
      "step 01041/01785 (58.32%) | loss: 3.205833 | lrm: 0.83 | dt: 2605.70ms | tok/sec: 201,208 | mfu: 51.73 | epoch: 1 | total time: 44.59m | eta: 32.2m\n",
      "step 01042/01785 (58.38%) | loss: 3.212479 | lrm: 0.83 | dt: 2596.58ms | tok/sec: 201,914 | mfu: 51.91 | epoch: 1 | total time: 44.63m | eta: 32.1m\n",
      "step 01043/01785 (58.43%) | loss: 3.211221 | lrm: 0.83 | dt: 2590.91ms | tok/sec: 202,356 | mfu: 52.03 | epoch: 1 | total time: 44.67m | eta: 32.1m\n",
      "step 01044/01785 (58.49%) | loss: 3.212851 | lrm: 0.83 | dt: 2605.16ms | tok/sec: 201,250 | mfu: 51.74 | epoch: 1 | total time: 44.72m | eta: 32.0m\n",
      "step 01045/01785 (58.54%) | loss: 3.220057 | lrm: 0.83 | dt: 2605.11ms | tok/sec: 201,253 | mfu: 51.74 | epoch: 1 | total time: 44.76m | eta: 32.0m\n",
      "step 01046/01785 (58.60%) | loss: 3.216527 | lrm: 0.83 | dt: 2603.12ms | tok/sec: 201,407 | mfu: 51.78 | epoch: 1 | total time: 44.80m | eta: 32.0m\n",
      "step 01047/01785 (58.66%) | loss: 3.212322 | lrm: 0.83 | dt: 2592.82ms | tok/sec: 202,207 | mfu: 51.99 | epoch: 1 | total time: 44.85m | eta: 31.9m\n",
      "step 01048/01785 (58.71%) | loss: 3.206245 | lrm: 0.83 | dt: 2592.72ms | tok/sec: 202,215 | mfu: 51.99 | epoch: 1 | total time: 44.89m | eta: 31.9m\n",
      "step 01049/01785 (58.77%) | loss: 3.203496 | lrm: 0.83 | dt: 2604.03ms | tok/sec: 201,337 | mfu: 51.76 | epoch: 1 | total time: 44.93m | eta: 31.8m\n",
      "step 01050/01785 (58.82%) | loss: 3.201777 | lrm: 0.82 | dt: 2605.76ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 1 | total time: 44.98m | eta: 31.8m\n",
      "step 01051/01785 (58.88%) | loss: 3.202526 | lrm: 0.82 | dt: 2604.66ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 45.02m | eta: 31.7m\n",
      "step 01052/01785 (58.94%) | loss: 3.206176 | lrm: 0.82 | dt: 2604.50ms | tok/sec: 201,300 | mfu: 51.76 | epoch: 1 | total time: 45.06m | eta: 31.7m\n",
      "step 01053/01785 (58.99%) | loss: 3.205909 | lrm: 0.82 | dt: 2604.65ms | tok/sec: 201,289 | mfu: 51.75 | epoch: 1 | total time: 45.11m | eta: 31.7m\n",
      "step 01054/01785 (59.05%) | loss: 3.206510 | lrm: 0.82 | dt: 2605.27ms | tok/sec: 201,241 | mfu: 51.74 | epoch: 1 | total time: 45.15m | eta: 31.6m\n",
      "step 01055/01785 (59.10%) | loss: 3.210526 | lrm: 0.82 | dt: 2597.95ms | tok/sec: 201,808 | mfu: 51.89 | epoch: 1 | total time: 45.19m | eta: 31.6m\n",
      "step 01056/01785 (59.16%) | loss: 3.212923 | lrm: 0.82 | dt: 2594.42ms | tok/sec: 202,082 | mfu: 51.96 | epoch: 1 | total time: 45.24m | eta: 31.5m\n",
      "step 01057/01785 (59.22%) | loss: 3.228420 | lrm: 0.82 | dt: 2590.82ms | tok/sec: 202,364 | mfu: 52.03 | epoch: 1 | total time: 45.28m | eta: 31.5m\n",
      "step 01058/01785 (59.27%) | loss: 3.228536 | lrm: 0.82 | dt: 2604.66ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 45.32m | eta: 31.4m\n",
      "step 01059/01785 (59.33%) | loss: 3.228983 | lrm: 0.81 | dt: 2591.04ms | tok/sec: 202,346 | mfu: 52.02 | epoch: 1 | total time: 45.37m | eta: 31.4m\n",
      "step 01060/01785 (59.38%) | loss: 3.232721 | lrm: 0.81 | dt: 2598.51ms | tok/sec: 201,764 | mfu: 51.87 | epoch: 1 | total time: 45.41m | eta: 31.4m\n",
      "step 01061/01785 (59.44%) | loss: 3.232874 | lrm: 0.81 | dt: 2598.40ms | tok/sec: 201,773 | mfu: 51.88 | epoch: 1 | total time: 45.45m | eta: 31.3m\n",
      "step 01062/01785 (59.50%) | loss: 3.234780 | lrm: 0.81 | dt: 2593.81ms | tok/sec: 202,130 | mfu: 51.97 | epoch: 1 | total time: 45.50m | eta: 31.3m\n",
      "step 01063/01785 (59.55%) | loss: 3.238189 | lrm: 0.81 | dt: 2604.95ms | tok/sec: 201,265 | mfu: 51.75 | epoch: 1 | total time: 45.54m | eta: 31.2m\n",
      "step 01064/01785 (59.61%) | loss: 3.242439 | lrm: 0.81 | dt: 2606.08ms | tok/sec: 201,178 | mfu: 51.72 | epoch: 1 | total time: 45.58m | eta: 31.2m\n",
      "step 01065/01785 (59.66%) | loss: 3.233050 | lrm: 0.81 | dt: 2596.07ms | tok/sec: 201,954 | mfu: 51.92 | epoch: 1 | total time: 45.63m | eta: 31.1m\n",
      "step 01066/01785 (59.72%) | loss: 3.240654 | lrm: 0.81 | dt: 2589.58ms | tok/sec: 202,460 | mfu: 52.05 | epoch: 1 | total time: 45.67m | eta: 31.1m\n",
      "step 01067/01785 (59.78%) | loss: 3.249848 | lrm: 0.80 | dt: 2602.90ms | tok/sec: 201,424 | mfu: 51.79 | epoch: 1 | total time: 45.71m | eta: 31.1m\n",
      "step 01068/01785 (59.83%) | loss: 3.235927 | lrm: 0.80 | dt: 2605.28ms | tok/sec: 201,240 | mfu: 51.74 | epoch: 1 | total time: 45.76m | eta: 31.0m\n",
      "step 01069/01785 (59.89%) | loss: 3.219632 | lrm: 0.80 | dt: 2604.84ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 45.80m | eta: 31.0m\n",
      "step 01070/01785 (59.94%) | loss: 3.221416 | lrm: 0.80 | dt: 2605.07ms | tok/sec: 201,256 | mfu: 51.74 | epoch: 1 | total time: 45.84m | eta: 30.9m\n",
      "step 01071/01785 (60.00%) | loss: 3.232161 | lrm: 0.80 | dt: 2604.61ms | tok/sec: 201,292 | mfu: 51.75 | epoch: 1 | total time: 45.89m | eta: 30.9m\n",
      "step 01072/01785 (60.06%) | loss: 3.229200 | lrm: 0.80 | dt: 2605.08ms | tok/sec: 201,256 | mfu: 51.74 | epoch: 1 | total time: 45.93m | eta: 30.8m\n",
      "step 01073/01785 (60.11%) | loss: 3.227083 | lrm: 0.80 | dt: 2605.39ms | tok/sec: 201,232 | mfu: 51.74 | epoch: 1 | total time: 45.97m | eta: 30.8m\n",
      "step 01074/01785 (60.17%) | loss: 3.223668 | lrm: 0.80 | dt: 2591.22ms | tok/sec: 202,332 | mfu: 52.02 | epoch: 1 | total time: 46.02m | eta: 30.8m\n",
      "step 01075/01785 (60.22%) | loss: 3.211909 | lrm: 0.80 | dt: 2599.18ms | tok/sec: 201,712 | mfu: 51.86 | epoch: 1 | total time: 46.06m | eta: 30.7m\n",
      "step 01076/01785 (60.28%) | loss: 3.214311 | lrm: 0.79 | dt: 2593.43ms | tok/sec: 202,159 | mfu: 51.98 | epoch: 1 | total time: 46.10m | eta: 30.7m\n",
      "step 01077/01785 (60.34%) | loss: 3.215445 | lrm: 0.79 | dt: 2593.53ms | tok/sec: 202,152 | mfu: 51.97 | epoch: 1 | total time: 46.15m | eta: 30.6m\n",
      "step 01078/01785 (60.39%) | loss: 3.209113 | lrm: 0.79 | dt: 2601.98ms | tok/sec: 201,495 | mfu: 51.81 | epoch: 1 | total time: 46.19m | eta: 30.6m\n",
      "step 01079/01785 (60.45%) | loss: 3.207625 | lrm: 0.79 | dt: 2588.73ms | tok/sec: 202,526 | mfu: 52.07 | epoch: 1 | total time: 46.23m | eta: 30.5m\n",
      "step 01080/01785 (60.50%) | loss: 3.212968 | lrm: 0.79 | dt: 2598.99ms | tok/sec: 201,727 | mfu: 51.87 | epoch: 1 | total time: 46.28m | eta: 30.5m\n",
      "step 01081/01785 (60.56%) | loss: 3.218160 | lrm: 0.79 | dt: 2589.17ms | tok/sec: 202,492 | mfu: 52.06 | epoch: 1 | total time: 46.32m | eta: 30.4m\n",
      "step 01082/01785 (60.62%) | loss: 3.214845 | lrm: 0.79 | dt: 2602.85ms | tok/sec: 201,428 | mfu: 51.79 | epoch: 1 | total time: 46.36m | eta: 30.4m\n",
      "step 01083/01785 (60.67%) | loss: 3.209593 | lrm: 0.79 | dt: 2588.78ms | tok/sec: 202,523 | mfu: 52.07 | epoch: 1 | total time: 46.41m | eta: 30.4m\n",
      "step 01084/01785 (60.73%) | loss: 3.206250 | lrm: 0.79 | dt: 2591.80ms | tok/sec: 202,287 | mfu: 52.01 | epoch: 1 | total time: 46.45m | eta: 30.3m\n",
      "step 01085/01785 (60.78%) | loss: 3.198723 | lrm: 0.78 | dt: 2598.74ms | tok/sec: 201,746 | mfu: 51.87 | epoch: 1 | total time: 46.49m | eta: 30.3m\n",
      "step 01086/01785 (60.84%) | loss: 3.207018 | lrm: 0.78 | dt: 2592.25ms | tok/sec: 202,252 | mfu: 52.00 | epoch: 1 | total time: 46.54m | eta: 30.2m\n",
      "step 01087/01785 (60.90%) | loss: 3.209533 | lrm: 0.78 | dt: 2592.57ms | tok/sec: 202,227 | mfu: 51.99 | epoch: 1 | total time: 46.58m | eta: 30.2m\n",
      "step 01088/01785 (60.95%) | loss: 3.219146 | lrm: 0.78 | dt: 2591.24ms | tok/sec: 202,331 | mfu: 52.02 | epoch: 1 | total time: 46.62m | eta: 30.1m\n",
      "step 01089/01785 (61.01%) | loss: 3.237414 | lrm: 0.78 | dt: 2585.73ms | tok/sec: 202,761 | mfu: 52.13 | epoch: 1 | total time: 46.67m | eta: 30.1m\n",
      "step 01090/01785 (61.06%) | loss: 3.232615 | lrm: 0.78 | dt: 2603.78ms | tok/sec: 201,356 | mfu: 51.77 | epoch: 1 | total time: 46.71m | eta: 30.1m\n",
      "step 01091/01785 (61.12%) | loss: 3.215842 | lrm: 0.78 | dt: 2593.04ms | tok/sec: 202,190 | mfu: 51.98 | epoch: 1 | total time: 46.75m | eta: 30.0m\n",
      "step 01092/01785 (61.18%) | loss: 3.212415 | lrm: 0.78 | dt: 2594.45ms | tok/sec: 202,080 | mfu: 51.96 | epoch: 1 | total time: 46.80m | eta: 30.0m\n",
      "step 01093/01785 (61.23%) | loss: 3.224017 | lrm: 0.78 | dt: 2585.47ms | tok/sec: 202,782 | mfu: 52.14 | epoch: 1 | total time: 46.84m | eta: 29.9m\n",
      "step 01094/01785 (61.29%) | loss: 3.226968 | lrm: 0.77 | dt: 2590.85ms | tok/sec: 202,361 | mfu: 52.03 | epoch: 1 | total time: 46.88m | eta: 29.9m\n",
      "step 01095/01785 (61.34%) | loss: 3.225010 | lrm: 0.77 | dt: 2604.72ms | tok/sec: 201,283 | mfu: 51.75 | epoch: 1 | total time: 46.93m | eta: 29.8m\n",
      "step 01096/01785 (61.40%) | loss: 3.231274 | lrm: 0.77 | dt: 2594.04ms | tok/sec: 202,112 | mfu: 51.96 | epoch: 1 | total time: 46.97m | eta: 29.8m\n",
      "step 01097/01785 (61.46%) | loss: 3.222030 | lrm: 0.77 | dt: 2594.16ms | tok/sec: 202,103 | mfu: 51.96 | epoch: 1 | total time: 47.01m | eta: 29.8m\n",
      "step 01098/01785 (61.51%) | loss: 3.220946 | lrm: 0.77 | dt: 2604.09ms | tok/sec: 201,332 | mfu: 51.76 | epoch: 1 | total time: 47.06m | eta: 29.7m\n",
      "step 01099/01785 (61.57%) | loss: 3.222654 | lrm: 0.77 | dt: 2604.75ms | tok/sec: 201,281 | mfu: 51.75 | epoch: 1 | total time: 47.10m | eta: 29.7m\n",
      "step 01100/01785 (61.62%) | loss: 3.212010 | lrm: 0.77 | dt: 2604.15ms | tok/sec: 201,327 | mfu: 51.76 | epoch: 1 | total time: 47.14m | eta: 29.6m\n",
      "step 01101/01785 (61.68%) | loss: 3.214188 | lrm: 0.77 | dt: 2604.84ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 47.19m | eta: 29.6m\n",
      "step 01102/01785 (61.74%) | loss: 3.208988 | lrm: 0.77 | dt: 2604.27ms | tok/sec: 201,318 | mfu: 51.76 | epoch: 1 | total time: 47.23m | eta: 29.5m\n",
      "step 01103/01785 (61.79%) | loss: 3.211231 | lrm: 0.76 | dt: 2605.01ms | tok/sec: 201,261 | mfu: 51.75 | epoch: 1 | total time: 47.27m | eta: 29.5m\n",
      "step 01104/01785 (61.85%) | loss: 3.211307 | lrm: 0.76 | dt: 2592.26ms | tok/sec: 202,251 | mfu: 52.00 | epoch: 1 | total time: 47.32m | eta: 29.5m\n",
      "step 01105/01785 (61.90%) | loss: 3.207261 | lrm: 0.76 | dt: 2595.07ms | tok/sec: 202,032 | mfu: 51.94 | epoch: 1 | total time: 47.36m | eta: 29.4m\n",
      "step 01106/01785 (61.96%) | loss: 3.206727 | lrm: 0.76 | dt: 2606.01ms | tok/sec: 201,184 | mfu: 51.73 | epoch: 1 | total time: 47.40m | eta: 29.4m\n",
      "step 01107/01785 (62.02%) | loss: 3.200424 | lrm: 0.76 | dt: 2606.79ms | tok/sec: 201,123 | mfu: 51.71 | epoch: 1 | total time: 47.45m | eta: 29.3m\n",
      "step 01108/01785 (62.07%) | loss: 3.201484 | lrm: 0.76 | dt: 2606.53ms | tok/sec: 201,143 | mfu: 51.72 | epoch: 1 | total time: 47.49m | eta: 29.3m\n",
      "step 01109/01785 (62.13%) | loss: 3.193309 | lrm: 0.76 | dt: 2589.54ms | tok/sec: 202,464 | mfu: 52.05 | epoch: 1 | total time: 47.53m | eta: 29.2m\n",
      "step 01110/01785 (62.18%) | loss: 3.199818 | lrm: 0.76 | dt: 2590.25ms | tok/sec: 202,408 | mfu: 52.04 | epoch: 1 | total time: 47.58m | eta: 29.2m\n",
      "step 01111/01785 (62.24%) | loss: 3.203155 | lrm: 0.76 | dt: 2603.71ms | tok/sec: 201,362 | mfu: 51.77 | epoch: 1 | total time: 47.62m | eta: 29.2m\n",
      "step 01112/01785 (62.30%) | loss: 3.205847 | lrm: 0.75 | dt: 2592.69ms | tok/sec: 202,217 | mfu: 51.99 | epoch: 1 | total time: 47.66m | eta: 29.1m\n",
      "step 01113/01785 (62.35%) | loss: 3.206872 | lrm: 0.75 | dt: 2591.89ms | tok/sec: 202,280 | mfu: 52.01 | epoch: 1 | total time: 47.70m | eta: 29.1m\n",
      "step 01114/01785 (62.41%) | loss: 3.214804 | lrm: 0.75 | dt: 2607.31ms | tok/sec: 201,083 | mfu: 51.70 | epoch: 1 | total time: 47.75m | eta: 29.0m\n",
      "step 01115/01785 (62.46%) | loss: 3.209046 | lrm: 0.75 | dt: 2603.79ms | tok/sec: 201,355 | mfu: 51.77 | epoch: 1 | total time: 47.79m | eta: 29.0m\n",
      "step 01116/01785 (62.52%) | loss: 3.199092 | lrm: 0.75 | dt: 2605.62ms | tok/sec: 201,214 | mfu: 51.73 | epoch: 1 | total time: 47.84m | eta: 28.9m\n",
      "step 01117/01785 (62.58%) | loss: 3.185680 | lrm: 0.75 | dt: 2603.85ms | tok/sec: 201,351 | mfu: 51.77 | epoch: 1 | total time: 47.88m | eta: 28.9m\n",
      "step 01118/01785 (62.63%) | loss: 3.189090 | lrm: 0.75 | dt: 2604.47ms | tok/sec: 201,302 | mfu: 51.76 | epoch: 1 | total time: 47.92m | eta: 28.8m\n",
      "step 01119/01785 (62.69%) | loss: 3.191261 | lrm: 0.75 | dt: 2604.59ms | tok/sec: 201,293 | mfu: 51.75 | epoch: 1 | total time: 47.97m | eta: 28.8m\n",
      "step 01120/01785 (62.75%) | loss: 3.192447 | lrm: 0.75 | dt: 2604.18ms | tok/sec: 201,325 | mfu: 51.76 | epoch: 1 | total time: 48.01m | eta: 28.8m\n",
      "step 01121/01785 (62.80%) | loss: 3.182072 | lrm: 0.74 | dt: 2594.94ms | tok/sec: 202,042 | mfu: 51.95 | epoch: 1 | total time: 48.05m | eta: 28.7m\n",
      "step 01122/01785 (62.86%) | loss: 3.176652 | lrm: 0.74 | dt: 2591.93ms | tok/sec: 202,276 | mfu: 52.01 | epoch: 1 | total time: 48.10m | eta: 28.7m\n",
      "step 01123/01785 (62.91%) | loss: 3.183367 | lrm: 0.74 | dt: 2604.14ms | tok/sec: 201,328 | mfu: 51.76 | epoch: 1 | total time: 48.14m | eta: 28.6m\n",
      "step 01124/01785 (62.97%) | loss: 3.180310 | lrm: 0.74 | dt: 2605.76ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 1 | total time: 48.18m | eta: 28.6m\n",
      "step 01125/01785 (63.03%) | loss: 3.188767 | lrm: 0.74 | dt: 2607.07ms | tok/sec: 201,102 | mfu: 51.70 | epoch: 1 | total time: 48.23m | eta: 28.5m\n",
      "step 01126/01785 (63.08%) | loss: 3.184899 | lrm: 0.74 | dt: 2605.31ms | tok/sec: 201,238 | mfu: 51.74 | epoch: 1 | total time: 48.27m | eta: 28.5m\n",
      "step 01127/01785 (63.14%) | loss: 3.179491 | lrm: 0.74 | dt: 2605.25ms | tok/sec: 201,242 | mfu: 51.74 | epoch: 1 | total time: 48.31m | eta: 28.5m\n",
      "step 01128/01785 (63.19%) | loss: 3.174522 | lrm: 0.74 | dt: 2604.63ms | tok/sec: 201,290 | mfu: 51.75 | epoch: 1 | total time: 48.36m | eta: 28.4m\n",
      "step 01129/01785 (63.25%) | loss: 3.161236 | lrm: 0.74 | dt: 2593.08ms | tok/sec: 202,187 | mfu: 51.98 | epoch: 1 | total time: 48.40m | eta: 28.4m\n",
      "step 01130/01785 (63.31%) | loss: 3.180871 | lrm: 0.73 | dt: 2592.33ms | tok/sec: 202,246 | mfu: 52.00 | epoch: 1 | total time: 48.44m | eta: 28.3m\n",
      "step 01131/01785 (63.36%) | loss: 3.188306 | lrm: 0.73 | dt: 2595.96ms | tok/sec: 201,963 | mfu: 51.93 | epoch: 1 | total time: 48.49m | eta: 28.3m\n",
      "step 01132/01785 (63.42%) | loss: 3.171397 | lrm: 0.73 | dt: 2591.94ms | tok/sec: 202,276 | mfu: 52.01 | epoch: 1 | total time: 48.53m | eta: 28.2m\n",
      "step 01133/01785 (63.47%) | loss: 3.166041 | lrm: 0.73 | dt: 2605.83ms | tok/sec: 201,197 | mfu: 51.73 | epoch: 1 | total time: 48.57m | eta: 28.2m\n",
      "step 01134/01785 (63.53%) | loss: 3.165635 | lrm: 0.73 | dt: 2595.64ms | tok/sec: 201,987 | mfu: 51.93 | epoch: 1 | total time: 48.62m | eta: 28.2m\n",
      "step 01135/01785 (63.59%) | loss: 3.160681 | lrm: 0.73 | dt: 2597.37ms | tok/sec: 201,853 | mfu: 51.90 | epoch: 1 | total time: 48.66m | eta: 28.1m\n",
      "step 01136/01785 (63.64%) | loss: 3.161490 | lrm: 0.73 | dt: 2596.55ms | tok/sec: 201,916 | mfu: 51.91 | epoch: 1 | total time: 48.70m | eta: 28.1m\n",
      "step 01137/01785 (63.70%) | loss: 3.170178 | lrm: 0.73 | dt: 2601.71ms | tok/sec: 201,516 | mfu: 51.81 | epoch: 1 | total time: 48.75m | eta: 28.0m\n",
      "step 01138/01785 (63.75%) | loss: 3.172956 | lrm: 0.73 | dt: 2593.38ms | tok/sec: 202,163 | mfu: 51.98 | epoch: 1 | total time: 48.79m | eta: 28.0m\n",
      "step 01139/01785 (63.81%) | loss: 3.180911 | lrm: 0.72 | dt: 2592.42ms | tok/sec: 202,238 | mfu: 52.00 | epoch: 1 | total time: 48.83m | eta: 27.9m\n",
      "step 01140/01785 (63.87%) | loss: 3.191933 | lrm: 0.72 | dt: 2605.39ms | tok/sec: 201,232 | mfu: 51.74 | epoch: 1 | total time: 48.88m | eta: 27.9m\n",
      "step 01141/01785 (63.92%) | loss: 3.194013 | lrm: 0.72 | dt: 2596.04ms | tok/sec: 201,956 | mfu: 51.92 | epoch: 1 | total time: 48.92m | eta: 27.9m\n",
      "step 01142/01785 (63.98%) | loss: 3.202691 | lrm: 0.72 | dt: 2593.80ms | tok/sec: 202,131 | mfu: 51.97 | epoch: 1 | total time: 48.96m | eta: 27.8m\n",
      "step 01143/01785 (64.03%) | loss: 3.197348 | lrm: 0.72 | dt: 2603.66ms | tok/sec: 201,365 | mfu: 51.77 | epoch: 1 | total time: 49.01m | eta: 27.8m\n",
      "step 01144/01785 (64.09%) | loss: 3.202946 | lrm: 0.72 | dt: 2596.77ms | tok/sec: 201,899 | mfu: 51.91 | epoch: 1 | total time: 49.05m | eta: 27.7m\n",
      "step 01145/01785 (64.15%) | loss: 3.201670 | lrm: 0.72 | dt: 2592.53ms | tok/sec: 202,230 | mfu: 51.99 | epoch: 1 | total time: 49.09m | eta: 27.7m\n",
      "step 01146/01785 (64.20%) | loss: 3.192358 | lrm: 0.72 | dt: 2603.87ms | tok/sec: 201,349 | mfu: 51.77 | epoch: 1 | total time: 49.14m | eta: 27.6m\n",
      "step 01147/01785 (64.26%) | loss: 3.183527 | lrm: 0.72 | dt: 2605.76ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 1 | total time: 49.18m | eta: 27.6m\n",
      "step 01148/01785 (64.31%) | loss: 3.186614 | lrm: 0.71 | dt: 2603.97ms | tok/sec: 201,341 | mfu: 51.77 | epoch: 1 | total time: 49.22m | eta: 27.6m\n",
      "step 01149/01785 (64.37%) | loss: 3.193532 | lrm: 0.71 | dt: 2605.25ms | tok/sec: 201,242 | mfu: 51.74 | epoch: 1 | total time: 49.27m | eta: 27.5m\n",
      "step 01150/01785 (64.43%) | loss: 3.196944 | lrm: 0.71 | dt: 2602.91ms | tok/sec: 201,423 | mfu: 51.79 | epoch: 1 | total time: 49.31m | eta: 27.5m\n",
      "step 01151/01785 (64.48%) | loss: 3.195775 | lrm: 0.71 | dt: 2604.47ms | tok/sec: 201,302 | mfu: 51.76 | epoch: 1 | total time: 49.35m | eta: 27.4m\n",
      "step 01152/01785 (64.54%) | loss: 3.192266 | lrm: 0.71 | dt: 2604.41ms | tok/sec: 201,307 | mfu: 51.76 | epoch: 1 | total time: 49.40m | eta: 27.4m\n",
      "step 01153/01785 (64.59%) | loss: 3.192808 | lrm: 0.71 | dt: 2605.93ms | tok/sec: 201,190 | mfu: 51.73 | epoch: 1 | total time: 49.44m | eta: 27.3m\n",
      "step 01154/01785 (64.65%) | loss: 3.190008 | lrm: 0.71 | dt: 2604.83ms | tok/sec: 201,275 | mfu: 51.75 | epoch: 1 | total time: 49.48m | eta: 27.3m\n",
      "step 01155/01785 (64.71%) | loss: 3.197395 | lrm: 0.71 | dt: 2606.59ms | tok/sec: 201,139 | mfu: 51.71 | epoch: 1 | total time: 49.53m | eta: 27.2m\n",
      "step 01156/01785 (64.76%) | loss: 3.190737 | lrm: 0.71 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 1 | total time: 49.57m | eta: 27.2m\n",
      "step 01157/01785 (64.82%) | loss: 3.170810 | lrm: 0.70 | dt: 2605.95ms | tok/sec: 201,188 | mfu: 51.73 | epoch: 1 | total time: 49.61m | eta: 27.2m\n",
      "step 01158/01785 (64.87%) | loss: 3.171108 | lrm: 0.70 | dt: 2604.12ms | tok/sec: 201,330 | mfu: 51.76 | epoch: 1 | total time: 49.66m | eta: 27.1m\n",
      "step 01159/01785 (64.93%) | loss: 3.157594 | lrm: 0.70 | dt: 2591.48ms | tok/sec: 202,312 | mfu: 52.02 | epoch: 1 | total time: 49.70m | eta: 27.1m\n",
      "step 01160/01785 (64.99%) | loss: 3.162340 | lrm: 0.70 | dt: 2602.83ms | tok/sec: 201,430 | mfu: 51.79 | epoch: 1 | total time: 49.74m | eta: 27.0m\n",
      "step 01161/01785 (65.04%) | loss: 3.172896 | lrm: 0.70 | dt: 2593.66ms | tok/sec: 202,141 | mfu: 51.97 | epoch: 1 | total time: 49.79m | eta: 27.0m\n",
      "step 01162/01785 (65.10%) | loss: 3.168447 | lrm: 0.70 | dt: 2600.44ms | tok/sec: 201,615 | mfu: 51.84 | epoch: 1 | total time: 49.83m | eta: 26.9m\n",
      "step 01163/01785 (65.15%) | loss: 3.164723 | lrm: 0.70 | dt: 2592.84ms | tok/sec: 202,205 | mfu: 51.99 | epoch: 1 | total time: 49.87m | eta: 26.9m\n",
      "step 01164/01785 (65.21%) | loss: 3.184565 | lrm: 0.70 | dt: 2595.67ms | tok/sec: 201,985 | mfu: 51.93 | epoch: 1 | total time: 49.92m | eta: 26.9m\n",
      "step 01165/01785 (65.27%) | loss: 3.173456 | lrm: 0.70 | dt: 2609.24ms | tok/sec: 200,935 | mfu: 51.66 | epoch: 1 | total time: 49.96m | eta: 26.8m\n",
      "step 01166/01785 (65.32%) | loss: 3.147493 | lrm: 0.69 | dt: 2593.92ms | tok/sec: 202,121 | mfu: 51.97 | epoch: 1 | total time: 50.00m | eta: 26.8m\n",
      "step 01167/01785 (65.38%) | loss: 3.155740 | lrm: 0.69 | dt: 2595.44ms | tok/sec: 202,003 | mfu: 51.94 | epoch: 1 | total time: 50.05m | eta: 26.7m\n",
      "step 01168/01785 (65.43%) | loss: 3.158847 | lrm: 0.69 | dt: 2604.53ms | tok/sec: 201,298 | mfu: 51.75 | epoch: 1 | total time: 50.09m | eta: 26.7m\n",
      "step 01169/01785 (65.49%) | loss: 3.159204 | lrm: 0.69 | dt: 2604.44ms | tok/sec: 201,305 | mfu: 51.76 | epoch: 1 | total time: 50.13m | eta: 26.6m\n",
      "step 01170/01785 (65.55%) | loss: 3.168827 | lrm: 0.69 | dt: 2603.53ms | tok/sec: 201,375 | mfu: 51.77 | epoch: 1 | total time: 50.18m | eta: 26.6m\n",
      "step 01171/01785 (65.60%) | loss: 3.169369 | lrm: 0.69 | dt: 2603.79ms | tok/sec: 201,355 | mfu: 51.77 | epoch: 1 | total time: 50.22m | eta: 26.6m\n",
      "step 01172/01785 (65.66%) | loss: 3.160701 | lrm: 0.69 | dt: 2589.25ms | tok/sec: 202,486 | mfu: 52.06 | epoch: 1 | total time: 50.26m | eta: 26.5m\n",
      "step 01173/01785 (65.71%) | loss: 3.156991 | lrm: 0.69 | dt: 2596.66ms | tok/sec: 201,908 | mfu: 51.91 | epoch: 1 | total time: 50.31m | eta: 26.5m\n",
      "step 01174/01785 (65.77%) | loss: 3.163824 | lrm: 0.68 | dt: 2604.23ms | tok/sec: 201,321 | mfu: 51.76 | epoch: 1 | total time: 50.35m | eta: 26.4m\n",
      "step 01175/01785 (65.83%) | loss: 3.164995 | lrm: 0.68 | dt: 2604.49ms | tok/sec: 201,301 | mfu: 51.76 | epoch: 1 | total time: 50.39m | eta: 26.4m\n",
      "step 01176/01785 (65.88%) | loss: 3.171618 | lrm: 0.68 | dt: 2605.63ms | tok/sec: 201,213 | mfu: 51.73 | epoch: 1 | total time: 50.44m | eta: 26.3m\n",
      "step 01177/01785 (65.94%) | loss: 3.170049 | lrm: 0.68 | dt: 2605.77ms | tok/sec: 201,202 | mfu: 51.73 | epoch: 1 | total time: 50.48m | eta: 26.3m\n",
      "step 01178/01785 (65.99%) | loss: 3.165641 | lrm: 0.68 | dt: 2590.29ms | tok/sec: 202,405 | mfu: 52.04 | epoch: 1 | total time: 50.52m | eta: 26.3m\n",
      "step 01179/01785 (66.05%) | loss: 3.155275 | lrm: 0.68 | dt: 2597.55ms | tok/sec: 201,839 | mfu: 51.89 | epoch: 1 | total time: 50.57m | eta: 26.2m\n",
      "step 01180/01785 (66.11%) | loss: 3.160182 | lrm: 0.68 | dt: 2604.66ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 50.61m | eta: 26.2m\n",
      "step 01181/01785 (66.16%) | loss: 3.172838 | lrm: 0.68 | dt: 2590.49ms | tok/sec: 202,389 | mfu: 52.04 | epoch: 1 | total time: 50.65m | eta: 26.1m\n",
      "step 01182/01785 (66.22%) | loss: 3.170217 | lrm: 0.68 | dt: 2596.69ms | tok/sec: 201,906 | mfu: 51.91 | epoch: 1 | total time: 50.70m | eta: 26.1m\n",
      "step 01183/01785 (66.27%) | loss: 3.172450 | lrm: 0.67 | dt: 2604.29ms | tok/sec: 201,316 | mfu: 51.76 | epoch: 1 | total time: 50.74m | eta: 26.0m\n",
      "step 01184/01785 (66.33%) | loss: 3.181268 | lrm: 0.67 | dt: 2589.41ms | tok/sec: 202,474 | mfu: 52.06 | epoch: 1 | total time: 50.78m | eta: 26.0m\n",
      "step 01185/01785 (66.39%) | loss: 3.169826 | lrm: 0.67 | dt: 2603.13ms | tok/sec: 201,406 | mfu: 51.78 | epoch: 1 | total time: 50.83m | eta: 26.0m\n",
      "step 01186/01785 (66.44%) | loss: 3.164473 | lrm: 0.67 | dt: 2596.25ms | tok/sec: 201,940 | mfu: 51.92 | epoch: 1 | total time: 50.87m | eta: 25.9m\n",
      "step 01187/01785 (66.50%) | loss: 3.158476 | lrm: 0.67 | dt: 2594.81ms | tok/sec: 202,052 | mfu: 51.95 | epoch: 1 | total time: 50.91m | eta: 25.9m\n",
      "step 01188/01785 (66.55%) | loss: 3.150261 | lrm: 0.67 | dt: 2592.80ms | tok/sec: 202,209 | mfu: 51.99 | epoch: 1 | total time: 50.96m | eta: 25.8m\n",
      "step 01189/01785 (66.61%) | loss: 3.144218 | lrm: 0.67 | dt: 2604.27ms | tok/sec: 201,318 | mfu: 51.76 | epoch: 1 | total time: 51.00m | eta: 25.8m\n",
      "step 01190/01785 (66.67%) | loss: 3.150803 | lrm: 0.67 | dt: 2593.69ms | tok/sec: 202,139 | mfu: 51.97 | epoch: 1 | total time: 51.04m | eta: 25.7m\n",
      "step 01191/01785 (66.72%) | loss: 3.154634 | lrm: 0.67 | dt: 2590.49ms | tok/sec: 202,389 | mfu: 52.04 | epoch: 1 | total time: 51.09m | eta: 25.7m\n",
      "step 01192/01785 (66.78%) | loss: 3.153187 | lrm: 0.66 | dt: 2605.22ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 51.13m | eta: 25.7m\n",
      "step 01193/01785 (66.83%) | loss: 3.149126 | lrm: 0.66 | dt: 2605.49ms | tok/sec: 201,224 | mfu: 51.74 | epoch: 1 | total time: 51.17m | eta: 25.6m\n",
      "step 01194/01785 (66.89%) | loss: 3.154105 | lrm: 0.66 | dt: 2595.52ms | tok/sec: 201,997 | mfu: 51.93 | epoch: 1 | total time: 51.22m | eta: 25.6m\n",
      "step 01195/01785 (66.95%) | loss: 3.141410 | lrm: 0.66 | dt: 2591.80ms | tok/sec: 202,287 | mfu: 52.01 | epoch: 1 | total time: 51.26m | eta: 25.5m\n",
      "step 01196/01785 (67.00%) | loss: 3.145445 | lrm: 0.66 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 1 | total time: 51.30m | eta: 25.5m\n",
      "step 01197/01785 (67.06%) | loss: 3.150733 | lrm: 0.66 | dt: 2594.40ms | tok/sec: 202,084 | mfu: 51.96 | epoch: 1 | total time: 51.35m | eta: 25.4m\n",
      "step 01198/01785 (67.11%) | loss: 3.130153 | lrm: 0.66 | dt: 2592.30ms | tok/sec: 202,248 | mfu: 52.00 | epoch: 1 | total time: 51.39m | eta: 25.4m\n",
      "step 01199/01785 (67.17%) | loss: 3.134102 | lrm: 0.66 | dt: 2594.27ms | tok/sec: 202,094 | mfu: 51.96 | epoch: 1 | total time: 51.43m | eta: 25.3m\n",
      "step 01200/01785 (67.23%) | loss: 3.142455 | lrm: 0.66 | dt: 2593.93ms | tok/sec: 202,121 | mfu: 51.97 | epoch: 1 | total time: 51.47m | eta: 25.3m\n",
      "step 01201/01785 (67.28%) | loss: 3.145747 | lrm: 0.65 | dt: 2603.93ms | tok/sec: 201,344 | mfu: 51.77 | epoch: 1 | total time: 51.52m | eta: 25.3m\n",
      "step 01202/01785 (67.34%) | loss: 3.139672 | lrm: 0.65 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 1 | total time: 51.56m | eta: 25.2m\n",
      "step 01203/01785 (67.39%) | loss: 3.141888 | lrm: 0.65 | dt: 2589.05ms | tok/sec: 202,501 | mfu: 52.06 | epoch: 1 | total time: 51.60m | eta: 25.2m\n",
      "step 01204/01785 (67.45%) | loss: 3.143715 | lrm: 0.65 | dt: 2597.76ms | tok/sec: 201,822 | mfu: 51.89 | epoch: 1 | total time: 51.65m | eta: 25.1m\n",
      "step 01205/01785 (67.51%) | loss: 3.140279 | lrm: 0.65 | dt: 2606.04ms | tok/sec: 201,181 | mfu: 51.72 | epoch: 1 | total time: 51.69m | eta: 25.1m\n",
      "step 01206/01785 (67.56%) | loss: 3.147775 | lrm: 0.65 | dt: 2602.92ms | tok/sec: 201,423 | mfu: 51.79 | epoch: 1 | total time: 51.73m | eta: 25.0m\n",
      "step 01207/01785 (67.62%) | loss: 3.141893 | lrm: 0.65 | dt: 2590.73ms | tok/sec: 202,370 | mfu: 52.03 | epoch: 1 | total time: 51.78m | eta: 25.0m\n",
      "step 01208/01785 (67.68%) | loss: 3.131695 | lrm: 0.65 | dt: 2596.69ms | tok/sec: 201,906 | mfu: 51.91 | epoch: 1 | total time: 51.82m | eta: 25.0m\n",
      "step 01209/01785 (67.73%) | loss: 3.140180 | lrm: 0.65 | dt: 2586.03ms | tok/sec: 202,738 | mfu: 52.13 | epoch: 1 | total time: 51.86m | eta: 24.9m\n",
      "step 01210/01785 (67.79%) | loss: 3.149018 | lrm: 0.64 | dt: 2604.35ms | tok/sec: 201,312 | mfu: 51.76 | epoch: 1 | total time: 51.91m | eta: 24.9m\n",
      "step 01211/01785 (67.84%) | loss: 3.151767 | lrm: 0.64 | dt: 2587.87ms | tok/sec: 202,594 | mfu: 52.09 | epoch: 1 | total time: 51.95m | eta: 24.8m\n",
      "step 01212/01785 (67.90%) | loss: 3.173856 | lrm: 0.64 | dt: 2600.20ms | tok/sec: 201,633 | mfu: 51.84 | epoch: 1 | total time: 51.99m | eta: 24.8m\n",
      "step 01213/01785 (67.96%) | loss: 3.171582 | lrm: 0.64 | dt: 2593.30ms | tok/sec: 202,170 | mfu: 51.98 | epoch: 1 | total time: 52.04m | eta: 24.7m\n",
      "step 01214/01785 (68.01%) | loss: 3.187233 | lrm: 0.64 | dt: 2587.85ms | tok/sec: 202,595 | mfu: 52.09 | epoch: 1 | total time: 52.08m | eta: 24.7m\n",
      "step 01215/01785 (68.07%) | loss: 3.178397 | lrm: 0.64 | dt: 2591.22ms | tok/sec: 202,332 | mfu: 52.02 | epoch: 1 | total time: 52.12m | eta: 24.7m\n",
      "step 01216/01785 (68.12%) | loss: 3.167336 | lrm: 0.64 | dt: 2591.61ms | tok/sec: 202,301 | mfu: 52.01 | epoch: 1 | total time: 52.17m | eta: 24.6m\n",
      "step 01217/01785 (68.18%) | loss: 3.169802 | lrm: 0.64 | dt: 2589.67ms | tok/sec: 202,453 | mfu: 52.05 | epoch: 1 | total time: 52.21m | eta: 24.6m\n",
      "step 01218/01785 (68.24%) | loss: 3.171788 | lrm: 0.64 | dt: 2589.81ms | tok/sec: 202,442 | mfu: 52.05 | epoch: 1 | total time: 52.25m | eta: 24.5m\n",
      "step 01219/01785 (68.29%) | loss: 3.179252 | lrm: 0.63 | dt: 2598.88ms | tok/sec: 201,736 | mfu: 51.87 | epoch: 1 | total time: 52.30m | eta: 24.5m\n",
      "step 01220/01785 (68.35%) | loss: 3.173960 | lrm: 0.63 | dt: 2590.75ms | tok/sec: 202,369 | mfu: 52.03 | epoch: 1 | total time: 52.34m | eta: 24.4m\n",
      "step 01221/01785 (68.40%) | loss: 3.181284 | lrm: 0.63 | dt: 2597.62ms | tok/sec: 201,834 | mfu: 51.89 | epoch: 1 | total time: 52.38m | eta: 24.4m\n",
      "step 01222/01785 (68.46%) | loss: 3.179160 | lrm: 0.63 | dt: 2602.38ms | tok/sec: 201,464 | mfu: 51.80 | epoch: 1 | total time: 52.43m | eta: 24.4m\n",
      "step 01223/01785 (68.52%) | loss: 3.183838 | lrm: 0.63 | dt: 2604.37ms | tok/sec: 201,310 | mfu: 51.76 | epoch: 1 | total time: 52.47m | eta: 24.3m\n",
      "step 01224/01785 (68.57%) | loss: 3.181154 | lrm: 0.63 | dt: 2606.21ms | tok/sec: 201,168 | mfu: 51.72 | epoch: 1 | total time: 52.51m | eta: 24.3m\n",
      "step 01225/01785 (68.63%) | loss: 3.175173 | lrm: 0.63 | dt: 2606.46ms | tok/sec: 201,149 | mfu: 51.72 | epoch: 1 | total time: 52.56m | eta: 24.2m\n",
      "step 01226/01785 (68.68%) | loss: 3.169926 | lrm: 0.63 | dt: 2602.87ms | tok/sec: 201,427 | mfu: 51.79 | epoch: 1 | total time: 52.60m | eta: 24.2m\n",
      "step 01227/01785 (68.74%) | loss: 3.180468 | lrm: 0.63 | dt: 2603.62ms | tok/sec: 201,368 | mfu: 51.77 | epoch: 1 | total time: 52.64m | eta: 24.1m\n",
      "step 01228/01785 (68.80%) | loss: 3.173455 | lrm: 0.62 | dt: 2587.92ms | tok/sec: 202,590 | mfu: 52.09 | epoch: 1 | total time: 52.69m | eta: 24.1m\n",
      "step 01229/01785 (68.85%) | loss: 3.174595 | lrm: 0.62 | dt: 2600.63ms | tok/sec: 201,600 | mfu: 51.83 | epoch: 1 | total time: 52.73m | eta: 24.1m\n",
      "step 01230/01785 (68.91%) | loss: 3.168521 | lrm: 0.62 | dt: 2605.87ms | tok/sec: 201,195 | mfu: 51.73 | epoch: 1 | total time: 52.77m | eta: 24.0m\n",
      "step 01231/01785 (68.96%) | loss: 3.166968 | lrm: 0.62 | dt: 2605.05ms | tok/sec: 201,258 | mfu: 51.74 | epoch: 1 | total time: 52.82m | eta: 24.0m\n",
      "step 01232/01785 (69.02%) | loss: 3.168561 | lrm: 0.62 | dt: 2594.57ms | tok/sec: 202,071 | mfu: 51.95 | epoch: 1 | total time: 52.86m | eta: 23.9m\n",
      "step 01233/01785 (69.08%) | loss: 3.167944 | lrm: 0.62 | dt: 2592.99ms | tok/sec: 202,194 | mfu: 51.99 | epoch: 1 | total time: 52.90m | eta: 23.9m\n",
      "step 01234/01785 (69.13%) | loss: 3.179823 | lrm: 0.62 | dt: 2604.67ms | tok/sec: 201,288 | mfu: 51.75 | epoch: 1 | total time: 52.95m | eta: 23.8m\n",
      "step 01235/01785 (69.19%) | loss: 3.159586 | lrm: 0.62 | dt: 2604.60ms | tok/sec: 201,292 | mfu: 51.75 | epoch: 1 | total time: 52.99m | eta: 23.8m\n",
      "step 01236/01785 (69.24%) | loss: 3.169294 | lrm: 0.62 | dt: 2594.01ms | tok/sec: 202,114 | mfu: 51.96 | epoch: 1 | total time: 53.03m | eta: 23.7m\n",
      "step 01237/01785 (69.30%) | loss: 3.162131 | lrm: 0.61 | dt: 2595.73ms | tok/sec: 201,980 | mfu: 51.93 | epoch: 1 | total time: 53.08m | eta: 23.7m\n",
      "step 01238/01785 (69.36%) | loss: 3.167477 | lrm: 0.61 | dt: 2608.37ms | tok/sec: 201,001 | mfu: 51.68 | epoch: 1 | total time: 53.12m | eta: 23.7m\n",
      "step 01239/01785 (69.41%) | loss: 3.166709 | lrm: 0.61 | dt: 2597.54ms | tok/sec: 201,839 | mfu: 51.89 | epoch: 1 | total time: 53.16m | eta: 23.6m\n",
      "step 01240/01785 (69.47%) | loss: 3.161789 | lrm: 0.61 | dt: 2595.32ms | tok/sec: 202,012 | mfu: 51.94 | epoch: 1 | total time: 53.21m | eta: 23.6m\n",
      "step 01241/01785 (69.52%) | loss: 3.158124 | lrm: 0.61 | dt: 2592.06ms | tok/sec: 202,267 | mfu: 52.00 | epoch: 1 | total time: 53.25m | eta: 23.5m\n",
      "step 01242/01785 (69.58%) | loss: 3.155776 | lrm: 0.61 | dt: 2592.68ms | tok/sec: 202,218 | mfu: 51.99 | epoch: 1 | total time: 53.29m | eta: 23.5m\n",
      "step 01243/01785 (69.64%) | loss: 3.162945 | lrm: 0.61 | dt: 2593.45ms | tok/sec: 202,158 | mfu: 51.98 | epoch: 1 | total time: 53.34m | eta: 23.4m\n",
      "step 01244/01785 (69.69%) | loss: 3.150281 | lrm: 0.61 | dt: 2599.98ms | tok/sec: 201,650 | mfu: 51.85 | epoch: 1 | total time: 53.38m | eta: 23.4m\n",
      "step 01245/01785 (69.75%) | loss: 3.152827 | lrm: 0.61 | dt: 2604.90ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 1 | total time: 53.42m | eta: 23.4m\n",
      "step 01246/01785 (69.80%) | loss: 3.146702 | lrm: 0.60 | dt: 2604.18ms | tok/sec: 201,325 | mfu: 51.76 | epoch: 1 | total time: 53.47m | eta: 23.3m\n",
      "step 01247/01785 (69.86%) | loss: 3.146971 | lrm: 0.60 | dt: 2596.44ms | tok/sec: 201,925 | mfu: 51.92 | epoch: 1 | total time: 53.51m | eta: 23.3m\n",
      "step 01248/01785 (69.92%) | loss: 3.146030 | lrm: 0.60 | dt: 2591.54ms | tok/sec: 202,307 | mfu: 52.01 | epoch: 1 | total time: 53.55m | eta: 23.2m\n",
      "step 01249/01785 (69.97%) | loss: 3.141560 | lrm: 0.60 | dt: 2605.92ms | tok/sec: 201,190 | mfu: 51.73 | epoch: 1 | total time: 53.60m | eta: 23.2m\n",
      "Step 01250 | Validation bpb: 0.959949\n",
      "step 01250/01785 (70.03%) | loss: 3.140701 | lrm: 0.60 | dt: 2597.23ms | tok/sec: 201,864 | mfu: 51.90 | epoch: 1 | total time: 53.64m | eta: 23.1m\n",
      "step 01251/01785 (70.08%) | loss: 3.144900 | lrm: 0.60 | dt: 2585.59ms | tok/sec: 202,773 | mfu: 52.13 | epoch: 1 | total time: 53.68m | eta: 23.1m\n",
      "step 01252/01785 (70.14%) | loss: 3.144953 | lrm: 0.60 | dt: 2585.01ms | tok/sec: 202,818 | mfu: 52.15 | epoch: 1 | total time: 53.73m | eta: 23.1m\n",
      "step 01253/01785 (70.20%) | loss: 3.140460 | lrm: 0.60 | dt: 2602.62ms | tok/sec: 201,446 | mfu: 51.79 | epoch: 1 | total time: 53.77m | eta: 23.0m\n",
      "step 01254/01785 (70.25%) | loss: 3.150447 | lrm: 0.60 | dt: 2587.98ms | tok/sec: 202,586 | mfu: 52.09 | epoch: 1 | total time: 53.81m | eta: 23.0m\n",
      "step 01255/01785 (70.31%) | loss: 3.154482 | lrm: 0.59 | dt: 2600.79ms | tok/sec: 201,587 | mfu: 51.83 | epoch: 1 | total time: 53.86m | eta: 22.9m\n",
      "step 01256/01785 (70.36%) | loss: 3.150419 | lrm: 0.59 | dt: 2585.19ms | tok/sec: 202,804 | mfu: 52.14 | epoch: 1 | total time: 53.90m | eta: 22.9m\n",
      "step 01257/01785 (70.42%) | loss: 3.158557 | lrm: 0.59 | dt: 2603.24ms | tok/sec: 201,398 | mfu: 51.78 | epoch: 1 | total time: 53.94m | eta: 22.8m\n",
      "step 01258/01785 (70.48%) | loss: 3.168350 | lrm: 0.59 | dt: 2585.35ms | tok/sec: 202,792 | mfu: 52.14 | epoch: 1 | total time: 53.99m | eta: 22.8m\n",
      "step 01259/01785 (70.53%) | loss: 3.174926 | lrm: 0.59 | dt: 2603.69ms | tok/sec: 201,363 | mfu: 51.77 | epoch: 1 | total time: 54.03m | eta: 22.8m\n",
      "step 01260/01785 (70.59%) | loss: 3.170770 | lrm: 0.59 | dt: 2586.76ms | tok/sec: 202,681 | mfu: 52.11 | epoch: 1 | total time: 54.07m | eta: 22.7m\n",
      "step 01261/01785 (70.64%) | loss: 3.166958 | lrm: 0.59 | dt: 2603.39ms | tok/sec: 201,386 | mfu: 51.78 | epoch: 1 | total time: 54.12m | eta: 22.7m\n",
      "step 01262/01785 (70.70%) | loss: 3.164494 | lrm: 0.59 | dt: 2604.74ms | tok/sec: 201,282 | mfu: 51.75 | epoch: 1 | total time: 54.16m | eta: 22.6m\n",
      "step 01263/01785 (70.76%) | loss: 3.166723 | lrm: 0.59 | dt: 2587.76ms | tok/sec: 202,603 | mfu: 52.09 | epoch: 1 | total time: 54.20m | eta: 22.6m\n",
      "step 01264/01785 (70.81%) | loss: 3.147997 | lrm: 0.58 | dt: 2591.23ms | tok/sec: 202,331 | mfu: 52.02 | epoch: 1 | total time: 54.24m | eta: 22.5m\n",
      "step 01265/01785 (70.87%) | loss: 3.149452 | lrm: 0.58 | dt: 2595.03ms | tok/sec: 202,035 | mfu: 51.94 | epoch: 1 | total time: 54.29m | eta: 22.5m\n",
      "step 01266/01785 (70.92%) | loss: 3.148709 | lrm: 0.58 | dt: 2604.30ms | tok/sec: 201,316 | mfu: 51.76 | epoch: 1 | total time: 54.33m | eta: 22.5m\n",
      "step 01267/01785 (70.98%) | loss: 3.133561 | lrm: 0.58 | dt: 2585.53ms | tok/sec: 202,777 | mfu: 52.14 | epoch: 1 | total time: 54.37m | eta: 22.4m\n",
      "step 01268/01785 (71.04%) | loss: 3.121855 | lrm: 0.58 | dt: 2605.04ms | tok/sec: 201,259 | mfu: 51.74 | epoch: 1 | total time: 54.42m | eta: 22.4m\n",
      "step 01269/01785 (71.09%) | loss: 3.128791 | lrm: 0.58 | dt: 2589.54ms | tok/sec: 202,463 | mfu: 52.05 | epoch: 1 | total time: 54.46m | eta: 22.3m\n",
      "step 01270/01785 (71.15%) | loss: 3.146507 | lrm: 0.58 | dt: 2601.44ms | tok/sec: 201,537 | mfu: 51.82 | epoch: 1 | total time: 54.50m | eta: 22.3m\n",
      "step 01271/01785 (71.20%) | loss: 3.151635 | lrm: 0.58 | dt: 2587.75ms | tok/sec: 202,603 | mfu: 52.09 | epoch: 1 | total time: 54.55m | eta: 22.2m\n",
      "step 01272/01785 (71.26%) | loss: 3.150773 | lrm: 0.58 | dt: 2588.82ms | tok/sec: 202,520 | mfu: 52.07 | epoch: 1 | total time: 54.59m | eta: 22.2m\n",
      "step 01273/01785 (71.32%) | loss: 3.147397 | lrm: 0.57 | dt: 2590.70ms | tok/sec: 202,373 | mfu: 52.03 | epoch: 1 | total time: 54.63m | eta: 22.1m\n",
      "step 01274/01785 (71.37%) | loss: 3.141514 | lrm: 0.57 | dt: 2591.00ms | tok/sec: 202,349 | mfu: 52.03 | epoch: 1 | total time: 54.68m | eta: 22.1m\n",
      "step 01275/01785 (71.43%) | loss: 3.140986 | lrm: 0.57 | dt: 2590.59ms | tok/sec: 202,381 | mfu: 52.03 | epoch: 1 | total time: 54.72m | eta: 22.1m\n",
      "step 01276/01785 (71.48%) | loss: 3.125858 | lrm: 0.57 | dt: 2595.58ms | tok/sec: 201,992 | mfu: 51.93 | epoch: 1 | total time: 54.76m | eta: 22.0m\n",
      "step 01277/01785 (71.54%) | loss: 3.118302 | lrm: 0.57 | dt: 2595.57ms | tok/sec: 201,993 | mfu: 51.93 | epoch: 1 | total time: 54.81m | eta: 22.0m\n",
      "step 01278/01785 (71.60%) | loss: 3.135014 | lrm: 0.57 | dt: 2588.46ms | tok/sec: 202,547 | mfu: 52.08 | epoch: 1 | total time: 54.85m | eta: 21.9m\n",
      "step 01279/01785 (71.65%) | loss: 3.137536 | lrm: 0.57 | dt: 2601.85ms | tok/sec: 201,505 | mfu: 51.81 | epoch: 1 | total time: 54.89m | eta: 21.9m\n",
      "step 01280/01785 (71.71%) | loss: 3.134612 | lrm: 0.57 | dt: 2587.76ms | tok/sec: 202,603 | mfu: 52.09 | epoch: 1 | total time: 54.94m | eta: 21.8m\n",
      "step 01281/01785 (71.76%) | loss: 3.128394 | lrm: 0.57 | dt: 2603.64ms | tok/sec: 201,367 | mfu: 51.77 | epoch: 1 | total time: 54.98m | eta: 21.8m\n",
      "step 01282/01785 (71.82%) | loss: 3.119274 | lrm: 0.56 | dt: 2589.44ms | tok/sec: 202,471 | mfu: 52.06 | epoch: 1 | total time: 55.02m | eta: 21.8m\n",
      "step 01283/01785 (71.88%) | loss: 3.112905 | lrm: 0.56 | dt: 2597.83ms | tok/sec: 201,817 | mfu: 51.89 | epoch: 1 | total time: 55.07m | eta: 21.7m\n",
      "step 01284/01785 (71.93%) | loss: 3.101387 | lrm: 0.56 | dt: 2587.78ms | tok/sec: 202,601 | mfu: 52.09 | epoch: 1 | total time: 55.11m | eta: 21.7m\n",
      "step 01285/01785 (71.99%) | loss: 3.104496 | lrm: 0.56 | dt: 2601.65ms | tok/sec: 201,520 | mfu: 51.81 | epoch: 1 | total time: 55.15m | eta: 21.6m\n",
      "step 01286/01785 (72.04%) | loss: 3.112955 | lrm: 0.56 | dt: 2592.26ms | tok/sec: 202,251 | mfu: 52.00 | epoch: 1 | total time: 55.20m | eta: 21.6m\n",
      "step 01287/01785 (72.10%) | loss: 3.122847 | lrm: 0.56 | dt: 2594.88ms | tok/sec: 202,047 | mfu: 51.95 | epoch: 1 | total time: 55.24m | eta: 21.5m\n",
      "step 01288/01785 (72.16%) | loss: 3.123979 | lrm: 0.56 | dt: 2591.45ms | tok/sec: 202,314 | mfu: 52.02 | epoch: 1 | total time: 55.28m | eta: 21.5m\n",
      "step 01289/01785 (72.21%) | loss: 3.131955 | lrm: 0.56 | dt: 2596.95ms | tok/sec: 201,885 | mfu: 51.91 | epoch: 1 | total time: 55.33m | eta: 21.5m\n",
      "step 01290/01785 (72.27%) | loss: 3.125024 | lrm: 0.55 | dt: 2595.25ms | tok/sec: 202,018 | mfu: 51.94 | epoch: 1 | total time: 55.37m | eta: 21.4m\n",
      "step 01291/01785 (72.32%) | loss: 3.138122 | lrm: 0.55 | dt: 2591.12ms | tok/sec: 202,340 | mfu: 52.02 | epoch: 1 | total time: 55.41m | eta: 21.4m\n",
      "step 01292/01785 (72.38%) | loss: 3.120134 | lrm: 0.55 | dt: 2604.87ms | tok/sec: 201,271 | mfu: 51.75 | epoch: 1 | total time: 55.46m | eta: 21.3m\n",
      "step 01293/01785 (72.44%) | loss: 3.124991 | lrm: 0.55 | dt: 2588.93ms | tok/sec: 202,511 | mfu: 52.07 | epoch: 1 | total time: 55.50m | eta: 21.3m\n",
      "step 01294/01785 (72.49%) | loss: 3.133758 | lrm: 0.55 | dt: 2600.56ms | tok/sec: 201,606 | mfu: 51.83 | epoch: 1 | total time: 55.54m | eta: 21.2m\n",
      "step 01295/01785 (72.55%) | loss: 3.127901 | lrm: 0.55 | dt: 2604.93ms | tok/sec: 201,267 | mfu: 51.75 | epoch: 1 | total time: 55.59m | eta: 21.2m\n",
      "step 01296/01785 (72.61%) | loss: 3.142990 | lrm: 0.55 | dt: 2586.82ms | tok/sec: 202,676 | mfu: 52.11 | epoch: 1 | total time: 55.63m | eta: 21.2m\n",
      "step 01297/01785 (72.66%) | loss: 3.154327 | lrm: 0.55 | dt: 2589.49ms | tok/sec: 202,468 | mfu: 52.06 | epoch: 1 | total time: 55.67m | eta: 21.1m\n",
      "step 01298/01785 (72.72%) | loss: 3.159114 | lrm: 0.55 | dt: 2605.01ms | tok/sec: 201,261 | mfu: 51.75 | epoch: 1 | total time: 55.72m | eta: 21.1m\n",
      "step 01299/01785 (72.77%) | loss: 3.146368 | lrm: 0.54 | dt: 2591.26ms | tok/sec: 202,329 | mfu: 52.02 | epoch: 1 | total time: 55.76m | eta: 21.0m\n",
      "step 01300/01785 (72.83%) | loss: 3.146716 | lrm: 0.54 | dt: 2591.38ms | tok/sec: 202,319 | mfu: 52.02 | epoch: 1 | total time: 55.80m | eta: 21.0m\n",
      "step 01301/01785 (72.89%) | loss: 3.152282 | lrm: 0.54 | dt: 2590.05ms | tok/sec: 202,423 | mfu: 52.04 | epoch: 1 | total time: 55.84m | eta: 20.9m\n",
      "step 01302/01785 (72.94%) | loss: 3.150714 | lrm: 0.54 | dt: 2588.55ms | tok/sec: 202,541 | mfu: 52.07 | epoch: 1 | total time: 55.89m | eta: 20.9m\n",
      "step 01303/01785 (73.00%) | loss: 3.150796 | lrm: 0.54 | dt: 2604.07ms | tok/sec: 201,334 | mfu: 51.76 | epoch: 1 | total time: 55.93m | eta: 20.8m\n",
      "step 01304/01785 (73.05%) | loss: 3.156545 | lrm: 0.54 | dt: 2589.18ms | tok/sec: 202,491 | mfu: 52.06 | epoch: 1 | total time: 55.97m | eta: 20.8m\n",
      "step 01305/01785 (73.11%) | loss: 3.139484 | lrm: 0.54 | dt: 2598.19ms | tok/sec: 201,789 | mfu: 51.88 | epoch: 1 | total time: 56.02m | eta: 20.8m\n",
      "step 01306/01785 (73.17%) | loss: 3.150479 | lrm: 0.54 | dt: 2604.79ms | tok/sec: 201,278 | mfu: 51.75 | epoch: 1 | total time: 56.06m | eta: 20.7m\n",
      "step 01307/01785 (73.22%) | loss: 3.144496 | lrm: 0.54 | dt: 2585.46ms | tok/sec: 202,783 | mfu: 52.14 | epoch: 1 | total time: 56.10m | eta: 20.7m\n",
      "step 01308/01785 (73.28%) | loss: 3.127452 | lrm: 0.53 | dt: 2603.62ms | tok/sec: 201,368 | mfu: 51.77 | epoch: 1 | total time: 56.15m | eta: 20.6m\n",
      "step 01309/01785 (73.33%) | loss: 3.129496 | lrm: 0.53 | dt: 2586.16ms | tok/sec: 202,728 | mfu: 52.12 | epoch: 1 | total time: 56.19m | eta: 20.6m\n",
      "step 01310/01785 (73.39%) | loss: 3.132821 | lrm: 0.53 | dt: 2601.86ms | tok/sec: 201,505 | mfu: 51.81 | epoch: 1 | total time: 56.23m | eta: 20.5m\n",
      "step 01311/01785 (73.45%) | loss: 3.119657 | lrm: 0.53 | dt: 2584.95ms | tok/sec: 202,823 | mfu: 52.15 | epoch: 1 | total time: 56.28m | eta: 20.5m\n",
      "step 01312/01785 (73.50%) | loss: 3.125134 | lrm: 0.53 | dt: 2604.39ms | tok/sec: 201,309 | mfu: 51.76 | epoch: 1 | total time: 56.32m | eta: 20.5m\n",
      "step 01313/01785 (73.56%) | loss: 3.124713 | lrm: 0.53 | dt: 2585.48ms | tok/sec: 202,781 | mfu: 52.14 | epoch: 1 | total time: 56.36m | eta: 20.4m\n",
      "step 01314/01785 (73.61%) | loss: 3.125773 | lrm: 0.53 | dt: 2604.02ms | tok/sec: 201,337 | mfu: 51.77 | epoch: 1 | total time: 56.41m | eta: 20.4m\n",
      "step 01315/01785 (73.67%) | loss: 3.129107 | lrm: 0.53 | dt: 2589.57ms | tok/sec: 202,461 | mfu: 52.05 | epoch: 1 | total time: 56.45m | eta: 20.3m\n",
      "step 01316/01785 (73.73%) | loss: 3.123809 | lrm: 0.53 | dt: 2599.14ms | tok/sec: 201,715 | mfu: 51.86 | epoch: 1 | total time: 56.49m | eta: 20.3m\n",
      "step 01317/01785 (73.78%) | loss: 3.118065 | lrm: 0.52 | dt: 2588.17ms | tok/sec: 202,571 | mfu: 52.08 | epoch: 1 | total time: 56.54m | eta: 20.2m\n",
      "step 01318/01785 (73.84%) | loss: 3.128667 | lrm: 0.52 | dt: 2599.21ms | tok/sec: 201,710 | mfu: 51.86 | epoch: 1 | total time: 56.58m | eta: 20.2m\n",
      "step 01319/01785 (73.89%) | loss: 3.112408 | lrm: 0.52 | dt: 2590.85ms | tok/sec: 202,361 | mfu: 52.03 | epoch: 1 | total time: 56.62m | eta: 20.2m\n",
      "step 01320/01785 (73.95%) | loss: 3.115367 | lrm: 0.52 | dt: 2597.75ms | tok/sec: 201,823 | mfu: 51.89 | epoch: 1 | total time: 56.67m | eta: 20.1m\n",
      "step 01321/01785 (74.01%) | loss: 3.120909 | lrm: 0.52 | dt: 2587.48ms | tok/sec: 202,625 | mfu: 52.10 | epoch: 1 | total time: 56.71m | eta: 20.1m\n",
      "step 01322/01785 (74.06%) | loss: 3.125372 | lrm: 0.52 | dt: 2604.75ms | tok/sec: 201,281 | mfu: 51.75 | epoch: 1 | total time: 56.75m | eta: 20.0m\n",
      "step 01323/01785 (74.12%) | loss: 3.116773 | lrm: 0.52 | dt: 2589.56ms | tok/sec: 202,462 | mfu: 52.05 | epoch: 1 | total time: 56.80m | eta: 20.0m\n",
      "step 01324/01785 (74.17%) | loss: 3.124407 | lrm: 0.52 | dt: 2603.57ms | tok/sec: 201,373 | mfu: 51.77 | epoch: 1 | total time: 56.84m | eta: 19.9m\n",
      "step 01325/01785 (74.23%) | loss: 3.129884 | lrm: 0.52 | dt: 2592.65ms | tok/sec: 202,221 | mfu: 51.99 | epoch: 1 | total time: 56.88m | eta: 19.9m\n",
      "step 01326/01785 (74.29%) | loss: 3.123888 | lrm: 0.51 | dt: 2592.71ms | tok/sec: 202,215 | mfu: 51.99 | epoch: 1 | total time: 56.93m | eta: 19.9m\n",
      "step 01327/01785 (74.34%) | loss: 3.120824 | lrm: 0.51 | dt: 2590.41ms | tok/sec: 202,396 | mfu: 52.04 | epoch: 1 | total time: 56.97m | eta: 19.8m\n",
      "step 01328/01785 (74.40%) | loss: 3.113143 | lrm: 0.51 | dt: 2588.51ms | tok/sec: 202,544 | mfu: 52.08 | epoch: 1 | total time: 57.01m | eta: 19.8m\n",
      "step 01329/01785 (74.45%) | loss: 3.102328 | lrm: 0.51 | dt: 2601.74ms | tok/sec: 201,514 | mfu: 51.81 | epoch: 1 | total time: 57.06m | eta: 19.7m\n",
      "step 01330/01785 (74.51%) | loss: 3.093285 | lrm: 0.51 | dt: 2585.77ms | tok/sec: 202,759 | mfu: 52.13 | epoch: 1 | total time: 57.10m | eta: 19.7m\n",
      "step 01331/01785 (74.57%) | loss: 3.086893 | lrm: 0.51 | dt: 2604.79ms | tok/sec: 201,278 | mfu: 51.75 | epoch: 1 | total time: 57.14m | eta: 19.6m\n",
      "step 01332/01785 (74.62%) | loss: 3.096271 | lrm: 0.51 | dt: 2587.88ms | tok/sec: 202,593 | mfu: 52.09 | epoch: 1 | total time: 57.19m | eta: 19.6m\n",
      "step 01333/01785 (74.68%) | loss: 3.100649 | lrm: 0.51 | dt: 2601.00ms | tok/sec: 201,571 | mfu: 51.83 | epoch: 1 | total time: 57.23m | eta: 19.6m\n",
      "step 01334/01785 (74.73%) | loss: 3.103181 | lrm: 0.51 | dt: 2586.08ms | tok/sec: 202,734 | mfu: 52.12 | epoch: 1 | total time: 57.27m | eta: 19.5m\n",
      "step 01335/01785 (74.79%) | loss: 3.101298 | lrm: 0.50 | dt: 2601.77ms | tok/sec: 201,512 | mfu: 51.81 | epoch: 1 | total time: 57.32m | eta: 19.5m\n",
      "step 01336/01785 (74.85%) | loss: 3.107888 | lrm: 0.50 | dt: 2586.98ms | tok/sec: 202,663 | mfu: 52.11 | epoch: 1 | total time: 57.36m | eta: 19.4m\n",
      "step 01337/01785 (74.90%) | loss: 3.105185 | lrm: 0.50 | dt: 2602.49ms | tok/sec: 201,455 | mfu: 51.80 | epoch: 1 | total time: 57.40m | eta: 19.4m\n",
      "step 01338/01785 (74.96%) | loss: 3.103854 | lrm: 0.50 | dt: 2606.53ms | tok/sec: 201,144 | mfu: 51.72 | epoch: 1 | total time: 57.45m | eta: 19.3m\n",
      "step 01339/01785 (75.01%) | loss: 3.100319 | lrm: 0.50 | dt: 2591.17ms | tok/sec: 202,336 | mfu: 52.02 | epoch: 1 | total time: 57.49m | eta: 19.3m\n",
      "step 01340/01785 (75.07%) | loss: 3.108787 | lrm: 0.50 | dt: 2597.26ms | tok/sec: 201,861 | mfu: 51.90 | epoch: 1 | total time: 57.53m | eta: 19.2m\n",
      "step 01341/01785 (75.13%) | loss: 3.103200 | lrm: 0.50 | dt: 2605.02ms | tok/sec: 201,260 | mfu: 51.75 | epoch: 1 | total time: 57.57m | eta: 19.2m\n",
      "step 01342/01785 (75.18%) | loss: 3.111273 | lrm: 0.50 | dt: 2604.02ms | tok/sec: 201,337 | mfu: 51.76 | epoch: 1 | total time: 57.62m | eta: 19.2m\n",
      "step 01343/01785 (75.24%) | loss: 3.099448 | lrm: 0.50 | dt: 2605.09ms | tok/sec: 201,255 | mfu: 51.74 | epoch: 1 | total time: 57.66m | eta: 19.1m\n",
      "step 01344/01785 (75.29%) | loss: 3.106945 | lrm: 0.49 | dt: 2588.28ms | tok/sec: 202,561 | mfu: 52.08 | epoch: 1 | total time: 57.70m | eta: 19.1m\n",
      "step 01345/01785 (75.35%) | loss: 3.100997 | lrm: 0.49 | dt: 2602.53ms | tok/sec: 201,453 | mfu: 51.79 | epoch: 1 | total time: 57.75m | eta: 19.0m\n",
      "step 01346/01785 (75.41%) | loss: 3.097493 | lrm: 0.49 | dt: 2606.38ms | tok/sec: 201,155 | mfu: 51.72 | epoch: 1 | total time: 57.79m | eta: 19.0m\n",
      "step 01347/01785 (75.46%) | loss: 3.085355 | lrm: 0.49 | dt: 2587.94ms | tok/sec: 202,589 | mfu: 52.09 | epoch: 1 | total time: 57.83m | eta: 18.9m\n",
      "step 01348/01785 (75.52%) | loss: 3.085354 | lrm: 0.49 | dt: 2606.26ms | tok/sec: 201,165 | mfu: 51.72 | epoch: 1 | total time: 57.88m | eta: 18.9m\n",
      "step 01349/01785 (75.57%) | loss: 3.089322 | lrm: 0.49 | dt: 2588.16ms | tok/sec: 202,571 | mfu: 52.08 | epoch: 1 | total time: 57.92m | eta: 18.9m\n",
      "step 01350/01785 (75.63%) | loss: 3.079870 | lrm: 0.49 | dt: 2607.46ms | tok/sec: 201,072 | mfu: 51.70 | epoch: 1 | total time: 57.96m | eta: 18.8m\n",
      "step 01351/01785 (75.69%) | loss: 3.080377 | lrm: 0.49 | dt: 2591.22ms | tok/sec: 202,332 | mfu: 52.02 | epoch: 1 | total time: 58.01m | eta: 18.8m\n",
      "step 01352/01785 (75.74%) | loss: 3.084907 | lrm: 0.49 | dt: 2590.93ms | tok/sec: 202,355 | mfu: 52.03 | epoch: 1 | total time: 58.05m | eta: 18.7m\n",
      "step 01353/01785 (75.80%) | loss: 3.085954 | lrm: 0.48 | dt: 2591.28ms | tok/sec: 202,328 | mfu: 52.02 | epoch: 1 | total time: 58.09m | eta: 18.7m\n",
      "step 01354/01785 (75.85%) | loss: 3.102712 | lrm: 0.48 | dt: 2588.77ms | tok/sec: 202,524 | mfu: 52.07 | epoch: 1 | total time: 58.14m | eta: 18.6m\n",
      "step 01355/01785 (75.91%) | loss: 3.105894 | lrm: 0.48 | dt: 2600.75ms | tok/sec: 201,591 | mfu: 51.83 | epoch: 1 | total time: 58.18m | eta: 18.6m\n",
      "step 01356/01785 (75.97%) | loss: 3.099026 | lrm: 0.48 | dt: 2590.03ms | tok/sec: 202,425 | mfu: 52.04 | epoch: 1 | total time: 58.22m | eta: 18.6m\n",
      "step 01357/01785 (76.02%) | loss: 3.105126 | lrm: 0.48 | dt: 2598.51ms | tok/sec: 201,764 | mfu: 51.87 | epoch: 1 | total time: 58.27m | eta: 18.5m\n",
      "step 01358/01785 (76.08%) | loss: 3.101521 | lrm: 0.48 | dt: 2606.09ms | tok/sec: 201,177 | mfu: 51.72 | epoch: 1 | total time: 58.31m | eta: 18.5m\n",
      "step 01359/01785 (76.13%) | loss: 3.101637 | lrm: 0.48 | dt: 2592.94ms | tok/sec: 202,198 | mfu: 51.99 | epoch: 1 | total time: 58.35m | eta: 18.4m\n",
      "step 01360/01785 (76.19%) | loss: 3.098521 | lrm: 0.48 | dt: 2594.68ms | tok/sec: 202,062 | mfu: 51.95 | epoch: 1 | total time: 58.40m | eta: 18.4m\n",
      "step 01361/01785 (76.25%) | loss: 3.093322 | lrm: 0.48 | dt: 2606.58ms | tok/sec: 201,140 | mfu: 51.71 | epoch: 1 | total time: 58.44m | eta: 18.3m\n",
      "step 01362/01785 (76.30%) | loss: 3.096354 | lrm: 0.47 | dt: 2595.93ms | tok/sec: 201,965 | mfu: 51.93 | epoch: 1 | total time: 58.48m | eta: 18.3m\n",
      "step 01363/01785 (76.36%) | loss: 3.097122 | lrm: 0.47 | dt: 2592.23ms | tok/sec: 202,253 | mfu: 52.00 | epoch: 1 | total time: 58.53m | eta: 18.3m\n",
      "step 01364/01785 (76.41%) | loss: 3.107448 | lrm: 0.47 | dt: 2606.22ms | tok/sec: 201,167 | mfu: 51.72 | epoch: 1 | total time: 58.57m | eta: 18.2m\n",
      "step 01365/01785 (76.47%) | loss: 3.113381 | lrm: 0.47 | dt: 2604.30ms | tok/sec: 201,316 | mfu: 51.76 | epoch: 1 | total time: 58.61m | eta: 18.2m\n",
      "step 01366/01785 (76.53%) | loss: 3.113660 | lrm: 0.47 | dt: 2592.14ms | tok/sec: 202,260 | mfu: 52.00 | epoch: 1 | total time: 58.66m | eta: 18.1m\n",
      "step 01367/01785 (76.58%) | loss: 3.122461 | lrm: 0.47 | dt: 2596.56ms | tok/sec: 201,916 | mfu: 51.91 | epoch: 1 | total time: 58.70m | eta: 18.1m\n",
      "step 01368/01785 (76.64%) | loss: 3.115745 | lrm: 0.47 | dt: 2604.85ms | tok/sec: 201,273 | mfu: 51.75 | epoch: 1 | total time: 58.74m | eta: 18.0m\n",
      "step 01369/01785 (76.69%) | loss: 3.107679 | lrm: 0.47 | dt: 2604.08ms | tok/sec: 201,333 | mfu: 51.76 | epoch: 1 | total time: 58.79m | eta: 18.0m\n",
      "step 01370/01785 (76.75%) | loss: 3.099961 | lrm: 0.47 | dt: 2604.89ms | tok/sec: 201,271 | mfu: 51.75 | epoch: 1 | total time: 58.83m | eta: 18.0m\n",
      "step 01371/01785 (76.81%) | loss: 3.096040 | lrm: 0.46 | dt: 2605.53ms | tok/sec: 201,221 | mfu: 51.73 | epoch: 1 | total time: 58.87m | eta: 17.9m\n",
      "step 01372/01785 (76.86%) | loss: 3.089790 | lrm: 0.46 | dt: 2590.51ms | tok/sec: 202,387 | mfu: 52.03 | epoch: 1 | total time: 58.92m | eta: 17.9m\n",
      "step 01373/01785 (76.92%) | loss: 3.085733 | lrm: 0.46 | dt: 2601.97ms | tok/sec: 201,496 | mfu: 51.81 | epoch: 1 | total time: 58.96m | eta: 17.8m\n",
      "step 01374/01785 (76.97%) | loss: 3.102190 | lrm: 0.46 | dt: 2588.51ms | tok/sec: 202,544 | mfu: 52.08 | epoch: 1 | total time: 59.00m | eta: 17.8m\n",
      "step 01375/01785 (77.03%) | loss: 3.103929 | lrm: 0.46 | dt: 2598.60ms | tok/sec: 201,758 | mfu: 51.87 | epoch: 1 | total time: 59.05m | eta: 17.7m\n",
      "step 01376/01785 (77.09%) | loss: 3.113953 | lrm: 0.46 | dt: 2591.80ms | tok/sec: 202,286 | mfu: 52.01 | epoch: 1 | total time: 59.09m | eta: 17.7m\n",
      "step 01377/01785 (77.14%) | loss: 3.122721 | lrm: 0.46 | dt: 2593.53ms | tok/sec: 202,152 | mfu: 51.97 | epoch: 1 | total time: 59.13m | eta: 17.6m\n",
      "step 01378/01785 (77.20%) | loss: 3.115733 | lrm: 0.46 | dt: 2592.38ms | tok/sec: 202,242 | mfu: 52.00 | epoch: 1 | total time: 59.18m | eta: 17.6m\n",
      "step 01379/01785 (77.25%) | loss: 3.108562 | lrm: 0.46 | dt: 2593.19ms | tok/sec: 202,178 | mfu: 51.98 | epoch: 1 | total time: 59.22m | eta: 17.6m\n",
      "step 01380/01785 (77.31%) | loss: 3.102704 | lrm: 0.45 | dt: 2588.76ms | tok/sec: 202,525 | mfu: 52.07 | epoch: 1 | total time: 59.26m | eta: 17.5m\n",
      "step 01381/01785 (77.37%) | loss: 3.099313 | lrm: 0.45 | dt: 2601.96ms | tok/sec: 201,497 | mfu: 51.81 | epoch: 1 | total time: 59.31m | eta: 17.5m\n",
      "step 01382/01785 (77.42%) | loss: 3.104873 | lrm: 0.45 | dt: 2593.94ms | tok/sec: 202,120 | mfu: 51.97 | epoch: 1 | total time: 59.35m | eta: 17.4m\n",
      "step 01383/01785 (77.48%) | loss: 3.104781 | lrm: 0.45 | dt: 2595.19ms | tok/sec: 202,023 | mfu: 51.94 | epoch: 1 | total time: 59.39m | eta: 17.4m\n",
      "step 01384/01785 (77.54%) | loss: 3.103668 | lrm: 0.45 | dt: 2588.64ms | tok/sec: 202,533 | mfu: 52.07 | epoch: 1 | total time: 59.44m | eta: 17.3m\n",
      "step 01385/01785 (77.59%) | loss: 3.111716 | lrm: 0.45 | dt: 2599.56ms | tok/sec: 201,683 | mfu: 51.85 | epoch: 1 | total time: 59.48m | eta: 17.3m\n",
      "step 01386/01785 (77.65%) | loss: 3.112134 | lrm: 0.45 | dt: 2604.32ms | tok/sec: 201,314 | mfu: 51.76 | epoch: 1 | total time: 59.52m | eta: 17.3m\n",
      "step 01387/01785 (77.70%) | loss: 3.121757 | lrm: 0.45 | dt: 2602.34ms | tok/sec: 201,467 | mfu: 51.80 | epoch: 1 | total time: 59.57m | eta: 17.2m\n",
      "step 01388/01785 (77.76%) | loss: 3.121913 | lrm: 0.45 | dt: 2605.59ms | tok/sec: 201,216 | mfu: 51.73 | epoch: 1 | total time: 59.61m | eta: 17.2m\n",
      "step 01389/01785 (77.82%) | loss: 3.112467 | lrm: 0.44 | dt: 2588.50ms | tok/sec: 202,544 | mfu: 52.08 | epoch: 1 | total time: 59.65m | eta: 17.1m\n",
      "step 01390/01785 (77.87%) | loss: 3.111061 | lrm: 0.44 | dt: 2601.12ms | tok/sec: 201,562 | mfu: 51.82 | epoch: 1 | total time: 59.70m | eta: 17.1m\n",
      "step 01391/01785 (77.93%) | loss: 3.116595 | lrm: 0.44 | dt: 2605.22ms | tok/sec: 201,245 | mfu: 51.74 | epoch: 1 | total time: 59.74m | eta: 17.0m\n",
      "step 01392/01785 (77.98%) | loss: 3.112543 | lrm: 0.44 | dt: 2603.99ms | tok/sec: 201,340 | mfu: 51.77 | epoch: 1 | total time: 59.78m | eta: 17.0m\n",
      "step 01393/01785 (78.04%) | loss: 3.111666 | lrm: 0.44 | dt: 2590.60ms | tok/sec: 202,380 | mfu: 52.03 | epoch: 1 | total time: 59.83m | eta: 17.0m\n",
      "step 01394/01785 (78.10%) | loss: 3.108358 | lrm: 0.44 | dt: 2597.18ms | tok/sec: 201,867 | mfu: 51.90 | epoch: 1 | total time: 59.87m | eta: 16.9m\n",
      "step 01395/01785 (78.15%) | loss: 3.102820 | lrm: 0.44 | dt: 2604.23ms | tok/sec: 201,321 | mfu: 51.76 | epoch: 1 | total time: 59.91m | eta: 16.9m\n",
      "step 01396/01785 (78.21%) | loss: 3.119389 | lrm: 0.44 | dt: 2592.56ms | tok/sec: 202,227 | mfu: 51.99 | epoch: 1 | total time: 59.96m | eta: 16.8m\n",
      "step 01397/01785 (78.26%) | loss: 3.114382 | lrm: 0.43 | dt: 2595.34ms | tok/sec: 202,011 | mfu: 51.94 | epoch: 1 | total time: 60.00m | eta: 16.8m\n",
      "step 01398/01785 (78.32%) | loss: 3.111469 | lrm: 0.43 | dt: 2605.76ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 1 | total time: 60.04m | eta: 16.7m\n",
      "step 01399/01785 (78.38%) | loss: 3.109218 | lrm: 0.43 | dt: 2592.09ms | tok/sec: 202,264 | mfu: 52.00 | epoch: 1 | total time: 60.09m | eta: 16.7m\n",
      "step 01400/01785 (78.43%) | loss: 3.115193 | lrm: 0.43 | dt: 2599.97ms | tok/sec: 201,651 | mfu: 51.85 | epoch: 1 | total time: 60.13m | eta: 16.7m\n",
      "step 01401/01785 (78.49%) | loss: 3.101772 | lrm: 0.43 | dt: 2589.62ms | tok/sec: 202,457 | mfu: 52.05 | epoch: 1 | total time: 60.17m | eta: 16.6m\n",
      "step 01402/01785 (78.54%) | loss: 3.101514 | lrm: 0.43 | dt: 2606.74ms | tok/sec: 201,128 | mfu: 51.71 | epoch: 1 | total time: 60.22m | eta: 16.6m\n",
      "step 01403/01785 (78.60%) | loss: 3.099519 | lrm: 0.43 | dt: 2590.83ms | tok/sec: 202,362 | mfu: 52.03 | epoch: 1 | total time: 60.26m | eta: 16.5m\n",
      "step 01404/01785 (78.66%) | loss: 3.092013 | lrm: 0.43 | dt: 2593.69ms | tok/sec: 202,139 | mfu: 51.97 | epoch: 1 | total time: 60.30m | eta: 16.5m\n",
      "step 01405/01785 (78.71%) | loss: 3.091175 | lrm: 0.43 | dt: 2591.60ms | tok/sec: 202,302 | mfu: 52.01 | epoch: 1 | total time: 60.35m | eta: 16.4m\n",
      "step 01406/01785 (78.77%) | loss: 3.080716 | lrm: 0.42 | dt: 2594.28ms | tok/sec: 202,094 | mfu: 51.96 | epoch: 1 | total time: 60.39m | eta: 16.4m\n",
      "step 01407/01785 (78.82%) | loss: 3.066715 | lrm: 0.42 | dt: 2595.08ms | tok/sec: 202,031 | mfu: 51.94 | epoch: 1 | total time: 60.43m | eta: 16.4m\n",
      "step 01408/01785 (78.88%) | loss: 3.074802 | lrm: 0.42 | dt: 2596.70ms | tok/sec: 201,905 | mfu: 51.91 | epoch: 1 | total time: 60.48m | eta: 16.3m\n",
      "step 01409/01785 (78.94%) | loss: 3.097657 | lrm: 0.42 | dt: 2589.93ms | tok/sec: 202,433 | mfu: 52.05 | epoch: 1 | total time: 60.52m | eta: 16.3m\n",
      "step 01410/01785 (78.99%) | loss: 3.091979 | lrm: 0.42 | dt: 2605.34ms | tok/sec: 201,236 | mfu: 51.74 | epoch: 1 | total time: 60.56m | eta: 16.2m\n",
      "step 01411/01785 (79.05%) | loss: 3.088318 | lrm: 0.42 | dt: 2604.84ms | tok/sec: 201,274 | mfu: 51.75 | epoch: 1 | total time: 60.61m | eta: 16.2m\n",
      "step 01412/01785 (79.10%) | loss: 3.097273 | lrm: 0.42 | dt: 2603.49ms | tok/sec: 201,378 | mfu: 51.78 | epoch: 1 | total time: 60.65m | eta: 16.1m\n",
      "step 01413/01785 (79.16%) | loss: 3.097344 | lrm: 0.42 | dt: 2604.17ms | tok/sec: 201,326 | mfu: 51.76 | epoch: 1 | total time: 60.69m | eta: 16.1m\n",
      "step 01414/01785 (79.22%) | loss: 3.099826 | lrm: 0.42 | dt: 2603.82ms | tok/sec: 201,353 | mfu: 51.77 | epoch: 1 | total time: 60.74m | eta: 16.0m\n",
      "step 01415/01785 (79.27%) | loss: 3.111907 | lrm: 0.41 | dt: 2591.12ms | tok/sec: 202,340 | mfu: 52.02 | epoch: 1 | total time: 60.78m | eta: 16.0m\n",
      "step 01416/01785 (79.33%) | loss: 3.106602 | lrm: 0.41 | dt: 2597.65ms | tok/sec: 201,831 | mfu: 51.89 | epoch: 1 | total time: 60.82m | eta: 16.0m\n",
      "step 01417/01785 (79.38%) | loss: 3.105127 | lrm: 0.41 | dt: 2605.01ms | tok/sec: 201,261 | mfu: 51.75 | epoch: 1 | total time: 60.87m | eta: 15.9m\n",
      "step 01418/01785 (79.44%) | loss: 3.106410 | lrm: 0.41 | dt: 2605.65ms | tok/sec: 201,211 | mfu: 51.73 | epoch: 1 | total time: 60.91m | eta: 15.9m\n",
      "step 01419/01785 (79.50%) | loss: 3.093477 | lrm: 0.41 | dt: 2587.90ms | tok/sec: 202,592 | mfu: 52.09 | epoch: 1 | total time: 60.95m | eta: 15.8m\n",
      "step 01420/01785 (79.55%) | loss: 3.079886 | lrm: 0.41 | dt: 2601.28ms | tok/sec: 201,549 | mfu: 51.82 | epoch: 1 | total time: 61.00m | eta: 15.8m\n",
      "step 01421/01785 (79.61%) | loss: 3.083797 | lrm: 0.41 | dt: 2590.37ms | tok/sec: 202,399 | mfu: 52.04 | epoch: 2 | total time: 61.04m | eta: 15.7m\n",
      "step 01422/01785 (79.66%) | loss: 3.089618 | lrm: 0.41 | dt: 2595.55ms | tok/sec: 201,994 | mfu: 51.93 | epoch: 2 | total time: 61.08m | eta: 15.7m\n",
      "step 01423/01785 (79.72%) | loss: 3.086791 | lrm: 0.41 | dt: 2604.36ms | tok/sec: 201,311 | mfu: 51.76 | epoch: 2 | total time: 61.13m | eta: 15.7m\n",
      "step 01424/01785 (79.78%) | loss: 3.080979 | lrm: 0.40 | dt: 2587.65ms | tok/sec: 202,611 | mfu: 52.09 | epoch: 2 | total time: 61.17m | eta: 15.6m\n",
      "step 01425/01785 (79.83%) | loss: 3.079680 | lrm: 0.40 | dt: 2593.02ms | tok/sec: 202,192 | mfu: 51.98 | epoch: 2 | total time: 61.21m | eta: 15.6m\n",
      "step 01426/01785 (79.89%) | loss: 3.079516 | lrm: 0.40 | dt: 2600.51ms | tok/sec: 201,609 | mfu: 51.83 | epoch: 2 | total time: 61.25m | eta: 15.5m\n",
      "step 01427/01785 (79.94%) | loss: 3.093078 | lrm: 0.40 | dt: 2588.91ms | tok/sec: 202,513 | mfu: 52.07 | epoch: 2 | total time: 61.30m | eta: 15.5m\n",
      "step 01428/01785 (80.00%) | loss: 3.085747 | lrm: 0.40 | dt: 2590.84ms | tok/sec: 202,362 | mfu: 52.03 | epoch: 2 | total time: 61.34m | eta: 15.4m\n",
      "step 01429/01785 (80.06%) | loss: 3.091090 | lrm: 0.40 | dt: 2590.07ms | tok/sec: 202,422 | mfu: 52.04 | epoch: 2 | total time: 61.38m | eta: 15.4m\n",
      "step 01430/01785 (80.11%) | loss: 3.085307 | lrm: 0.40 | dt: 2592.11ms | tok/sec: 202,263 | mfu: 52.00 | epoch: 2 | total time: 61.43m | eta: 15.4m\n",
      "step 01431/01785 (80.17%) | loss: 3.085018 | lrm: 0.40 | dt: 2590.77ms | tok/sec: 202,367 | mfu: 52.03 | epoch: 2 | total time: 61.47m | eta: 15.3m\n",
      "step 01432/01785 (80.22%) | loss: 3.090548 | lrm: 0.40 | dt: 2589.26ms | tok/sec: 202,485 | mfu: 52.06 | epoch: 2 | total time: 61.51m | eta: 15.3m\n",
      "step 01433/01785 (80.28%) | loss: 3.100269 | lrm: 0.39 | dt: 2599.32ms | tok/sec: 201,702 | mfu: 51.86 | epoch: 2 | total time: 61.56m | eta: 15.2m\n",
      "step 01434/01785 (80.34%) | loss: 3.095228 | lrm: 0.39 | dt: 2590.42ms | tok/sec: 202,395 | mfu: 52.04 | epoch: 2 | total time: 61.60m | eta: 15.2m\n",
      "step 01435/01785 (80.39%) | loss: 3.097792 | lrm: 0.39 | dt: 2599.67ms | tok/sec: 201,675 | mfu: 51.85 | epoch: 2 | total time: 61.64m | eta: 15.1m\n",
      "step 01436/01785 (80.45%) | loss: 3.103561 | lrm: 0.39 | dt: 2603.43ms | tok/sec: 201,383 | mfu: 51.78 | epoch: 2 | total time: 61.69m | eta: 15.1m\n",
      "step 01437/01785 (80.50%) | loss: 3.099917 | lrm: 0.39 | dt: 2589.85ms | tok/sec: 202,439 | mfu: 52.05 | epoch: 2 | total time: 61.73m | eta: 15.1m\n",
      "step 01438/01785 (80.56%) | loss: 3.086003 | lrm: 0.39 | dt: 2598.32ms | tok/sec: 201,779 | mfu: 51.88 | epoch: 2 | total time: 61.77m | eta: 15.0m\n",
      "step 01439/01785 (80.62%) | loss: 3.087675 | lrm: 0.39 | dt: 2591.14ms | tok/sec: 202,338 | mfu: 52.02 | epoch: 2 | total time: 61.82m | eta: 15.0m\n",
      "step 01440/01785 (80.67%) | loss: 3.078890 | lrm: 0.39 | dt: 2595.16ms | tok/sec: 202,025 | mfu: 51.94 | epoch: 2 | total time: 61.86m | eta: 14.9m\n",
      "step 01441/01785 (80.73%) | loss: 3.084156 | lrm: 0.39 | dt: 2585.56ms | tok/sec: 202,775 | mfu: 52.13 | epoch: 2 | total time: 61.90m | eta: 14.9m\n",
      "step 01442/01785 (80.78%) | loss: 3.077225 | lrm: 0.38 | dt: 2603.25ms | tok/sec: 201,397 | mfu: 51.78 | epoch: 2 | total time: 61.95m | eta: 14.8m\n",
      "step 01443/01785 (80.84%) | loss: 3.083514 | lrm: 0.38 | dt: 2604.64ms | tok/sec: 201,290 | mfu: 51.75 | epoch: 2 | total time: 61.99m | eta: 14.8m\n",
      "step 01444/01785 (80.90%) | loss: 3.083688 | lrm: 0.38 | dt: 2586.28ms | tok/sec: 202,719 | mfu: 52.12 | epoch: 2 | total time: 62.03m | eta: 14.8m\n",
      "step 01445/01785 (80.95%) | loss: 3.101823 | lrm: 0.38 | dt: 2602.89ms | tok/sec: 201,425 | mfu: 51.79 | epoch: 2 | total time: 62.08m | eta: 14.7m\n",
      "step 01446/01785 (81.01%) | loss: 3.079510 | lrm: 0.38 | dt: 2587.50ms | tok/sec: 202,623 | mfu: 52.10 | epoch: 2 | total time: 62.12m | eta: 14.7m\n",
      "step 01447/01785 (81.06%) | loss: 3.064085 | lrm: 0.38 | dt: 2600.85ms | tok/sec: 201,583 | mfu: 51.83 | epoch: 2 | total time: 62.16m | eta: 14.6m\n",
      "step 01448/01785 (81.12%) | loss: 3.054098 | lrm: 0.38 | dt: 2587.60ms | tok/sec: 202,615 | mfu: 52.09 | epoch: 2 | total time: 62.21m | eta: 14.6m\n",
      "step 01449/01785 (81.18%) | loss: 3.056444 | lrm: 0.38 | dt: 2600.66ms | tok/sec: 201,597 | mfu: 51.83 | epoch: 2 | total time: 62.25m | eta: 14.5m\n",
      "step 01450/01785 (81.23%) | loss: 3.048697 | lrm: 0.38 | dt: 2586.41ms | tok/sec: 202,708 | mfu: 52.12 | epoch: 2 | total time: 62.29m | eta: 14.5m\n",
      "step 01451/01785 (81.29%) | loss: 3.046475 | lrm: 0.37 | dt: 2601.33ms | tok/sec: 201,546 | mfu: 51.82 | epoch: 2 | total time: 62.34m | eta: 14.4m\n",
      "step 01452/01785 (81.34%) | loss: 3.038078 | lrm: 0.37 | dt: 2590.43ms | tok/sec: 202,394 | mfu: 52.04 | epoch: 2 | total time: 62.38m | eta: 14.4m\n",
      "step 01453/01785 (81.40%) | loss: 3.042126 | lrm: 0.37 | dt: 2586.67ms | tok/sec: 202,688 | mfu: 52.11 | epoch: 2 | total time: 62.42m | eta: 14.4m\n",
      "step 01454/01785 (81.46%) | loss: 3.048321 | lrm: 0.37 | dt: 2587.67ms | tok/sec: 202,610 | mfu: 52.09 | epoch: 2 | total time: 62.47m | eta: 14.3m\n",
      "step 01455/01785 (81.51%) | loss: 3.050661 | lrm: 0.37 | dt: 2589.98ms | tok/sec: 202,429 | mfu: 52.05 | epoch: 2 | total time: 62.51m | eta: 14.3m\n",
      "step 01456/01785 (81.57%) | loss: 3.050858 | lrm: 0.37 | dt: 2592.73ms | tok/sec: 202,214 | mfu: 51.99 | epoch: 2 | total time: 62.55m | eta: 14.2m\n",
      "step 01457/01785 (81.62%) | loss: 3.062115 | lrm: 0.37 | dt: 2590.99ms | tok/sec: 202,350 | mfu: 52.03 | epoch: 2 | total time: 62.59m | eta: 14.2m\n",
      "step 01458/01785 (81.68%) | loss: 3.054167 | lrm: 0.37 | dt: 2601.45ms | tok/sec: 201,536 | mfu: 51.82 | epoch: 2 | total time: 62.64m | eta: 14.1m\n",
      "step 01459/01785 (81.74%) | loss: 3.067513 | lrm: 0.37 | dt: 2590.99ms | tok/sec: 202,350 | mfu: 52.03 | epoch: 2 | total time: 62.68m | eta: 14.1m\n",
      "step 01460/01785 (81.79%) | loss: 3.063140 | lrm: 0.36 | dt: 2595.89ms | tok/sec: 201,968 | mfu: 51.93 | epoch: 2 | total time: 62.72m | eta: 14.1m\n",
      "step 01461/01785 (81.85%) | loss: 3.072596 | lrm: 0.36 | dt: 2587.24ms | tok/sec: 202,643 | mfu: 52.10 | epoch: 2 | total time: 62.77m | eta: 14.0m\n",
      "step 01462/01785 (81.90%) | loss: 3.076044 | lrm: 0.36 | dt: 2601.11ms | tok/sec: 201,563 | mfu: 51.82 | epoch: 2 | total time: 62.81m | eta: 14.0m\n",
      "step 01463/01785 (81.96%) | loss: 3.065944 | lrm: 0.36 | dt: 2603.25ms | tok/sec: 201,397 | mfu: 51.78 | epoch: 2 | total time: 62.85m | eta: 13.9m\n",
      "step 01464/01785 (82.02%) | loss: 3.061139 | lrm: 0.36 | dt: 2605.40ms | tok/sec: 201,231 | mfu: 51.74 | epoch: 2 | total time: 62.90m | eta: 13.9m\n",
      "step 01465/01785 (82.07%) | loss: 3.068287 | lrm: 0.36 | dt: 2588.17ms | tok/sec: 202,570 | mfu: 52.08 | epoch: 2 | total time: 62.94m | eta: 13.8m\n",
      "step 01466/01785 (82.13%) | loss: 3.072495 | lrm: 0.36 | dt: 2600.91ms | tok/sec: 201,578 | mfu: 51.83 | epoch: 2 | total time: 62.98m | eta: 13.8m\n",
      "step 01467/01785 (82.18%) | loss: 3.073602 | lrm: 0.36 | dt: 2603.58ms | tok/sec: 201,372 | mfu: 51.77 | epoch: 2 | total time: 63.03m | eta: 13.8m\n",
      "step 01468/01785 (82.24%) | loss: 3.068524 | lrm: 0.36 | dt: 2588.02ms | tok/sec: 202,582 | mfu: 52.08 | epoch: 2 | total time: 63.07m | eta: 13.7m\n",
      "step 01469/01785 (82.30%) | loss: 3.070304 | lrm: 0.35 | dt: 2602.45ms | tok/sec: 201,459 | mfu: 51.80 | epoch: 2 | total time: 63.11m | eta: 13.7m\n",
      "step 01470/01785 (82.35%) | loss: 3.072009 | lrm: 0.35 | dt: 2586.28ms | tok/sec: 202,719 | mfu: 52.12 | epoch: 2 | total time: 63.16m | eta: 13.6m\n",
      "step 01471/01785 (82.41%) | loss: 3.076368 | lrm: 0.35 | dt: 2602.26ms | tok/sec: 201,474 | mfu: 51.80 | epoch: 2 | total time: 63.20m | eta: 13.6m\n",
      "step 01472/01785 (82.46%) | loss: 3.079175 | lrm: 0.35 | dt: 2590.75ms | tok/sec: 202,369 | mfu: 52.03 | epoch: 2 | total time: 63.24m | eta: 13.5m\n",
      "step 01473/01785 (82.52%) | loss: 3.081140 | lrm: 0.35 | dt: 2596.48ms | tok/sec: 201,922 | mfu: 51.92 | epoch: 2 | total time: 63.29m | eta: 13.5m\n",
      "step 01474/01785 (82.58%) | loss: 3.067034 | lrm: 0.35 | dt: 2592.27ms | tok/sec: 202,250 | mfu: 52.00 | epoch: 2 | total time: 63.33m | eta: 13.5m\n",
      "step 01475/01785 (82.63%) | loss: 3.079135 | lrm: 0.35 | dt: 2595.30ms | tok/sec: 202,014 | mfu: 51.94 | epoch: 2 | total time: 63.37m | eta: 13.4m\n",
      "step 01476/01785 (82.69%) | loss: 3.084819 | lrm: 0.35 | dt: 2607.11ms | tok/sec: 201,099 | mfu: 51.70 | epoch: 2 | total time: 63.42m | eta: 13.4m\n",
      "step 01477/01785 (82.75%) | loss: 3.079932 | lrm: 0.35 | dt: 2591.68ms | tok/sec: 202,296 | mfu: 52.01 | epoch: 2 | total time: 63.46m | eta: 13.3m\n",
      "step 01478/01785 (82.80%) | loss: 3.081971 | lrm: 0.34 | dt: 2589.17ms | tok/sec: 202,492 | mfu: 52.06 | epoch: 2 | total time: 63.50m | eta: 13.3m\n",
      "step 01479/01785 (82.86%) | loss: 3.087022 | lrm: 0.34 | dt: 2608.37ms | tok/sec: 201,002 | mfu: 51.68 | epoch: 2 | total time: 63.55m | eta: 13.2m\n",
      "step 01480/01785 (82.91%) | loss: 3.083194 | lrm: 0.34 | dt: 2592.34ms | tok/sec: 202,245 | mfu: 52.00 | epoch: 2 | total time: 63.59m | eta: 13.2m\n",
      "step 01481/01785 (82.97%) | loss: 3.079124 | lrm: 0.34 | dt: 2598.92ms | tok/sec: 201,733 | mfu: 51.87 | epoch: 2 | total time: 63.63m | eta: 13.2m\n",
      "step 01482/01785 (83.03%) | loss: 3.068951 | lrm: 0.34 | dt: 2595.62ms | tok/sec: 201,989 | mfu: 51.93 | epoch: 2 | total time: 63.68m | eta: 13.1m\n",
      "step 01483/01785 (83.08%) | loss: 3.072951 | lrm: 0.34 | dt: 2598.92ms | tok/sec: 201,732 | mfu: 51.87 | epoch: 2 | total time: 63.72m | eta: 13.1m\n",
      "step 01484/01785 (83.14%) | loss: 3.077279 | lrm: 0.34 | dt: 2587.33ms | tok/sec: 202,636 | mfu: 52.10 | epoch: 2 | total time: 63.76m | eta: 13.0m\n",
      "step 01485/01785 (83.19%) | loss: 3.084039 | lrm: 0.34 | dt: 2605.65ms | tok/sec: 201,211 | mfu: 51.73 | epoch: 2 | total time: 63.81m | eta: 13.0m\n",
      "step 01486/01785 (83.25%) | loss: 3.078299 | lrm: 0.34 | dt: 2605.06ms | tok/sec: 201,257 | mfu: 51.74 | epoch: 2 | total time: 63.85m | eta: 12.9m\n",
      "step 01487/01785 (83.31%) | loss: 3.075000 | lrm: 0.33 | dt: 2597.74ms | tok/sec: 201,824 | mfu: 51.89 | epoch: 2 | total time: 63.89m | eta: 12.9m\n",
      "step 01488/01785 (83.36%) | loss: 3.079320 | lrm: 0.33 | dt: 2589.57ms | tok/sec: 202,461 | mfu: 52.05 | epoch: 2 | total time: 63.94m | eta: 12.8m\n",
      "step 01489/01785 (83.42%) | loss: 3.082867 | lrm: 0.33 | dt: 2594.87ms | tok/sec: 202,047 | mfu: 51.95 | epoch: 2 | total time: 63.98m | eta: 12.8m\n",
      "step 01490/01785 (83.47%) | loss: 3.088436 | lrm: 0.33 | dt: 2592.59ms | tok/sec: 202,225 | mfu: 51.99 | epoch: 2 | total time: 64.02m | eta: 12.8m\n",
      "step 01491/01785 (83.53%) | loss: 3.089054 | lrm: 0.33 | dt: 2605.38ms | tok/sec: 201,233 | mfu: 51.74 | epoch: 2 | total time: 64.07m | eta: 12.7m\n",
      "step 01492/01785 (83.59%) | loss: 3.087707 | lrm: 0.33 | dt: 2603.75ms | tok/sec: 201,359 | mfu: 51.77 | epoch: 2 | total time: 64.11m | eta: 12.7m\n",
      "step 01493/01785 (83.64%) | loss: 3.080038 | lrm: 0.33 | dt: 2588.32ms | tok/sec: 202,559 | mfu: 52.08 | epoch: 2 | total time: 64.15m | eta: 12.6m\n",
      "step 01494/01785 (83.70%) | loss: 3.086087 | lrm: 0.33 | dt: 2602.43ms | tok/sec: 201,460 | mfu: 51.80 | epoch: 2 | total time: 64.20m | eta: 12.6m\n",
      "step 01495/01785 (83.75%) | loss: 3.083329 | lrm: 0.33 | dt: 2605.20ms | tok/sec: 201,246 | mfu: 51.74 | epoch: 2 | total time: 64.24m | eta: 12.5m\n",
      "step 01496/01785 (83.81%) | loss: 3.081871 | lrm: 0.32 | dt: 2595.94ms | tok/sec: 201,964 | mfu: 51.93 | epoch: 2 | total time: 64.28m | eta: 12.5m\n",
      "step 01497/01785 (83.87%) | loss: 3.079882 | lrm: 0.32 | dt: 2590.69ms | tok/sec: 202,373 | mfu: 52.03 | epoch: 2 | total time: 64.33m | eta: 12.5m\n",
      "step 01498/01785 (83.92%) | loss: 3.087144 | lrm: 0.32 | dt: 2605.37ms | tok/sec: 201,233 | mfu: 51.74 | epoch: 2 | total time: 64.37m | eta: 12.4m\n",
      "step 01499/01785 (83.98%) | loss: 3.079868 | lrm: 0.32 | dt: 2595.59ms | tok/sec: 201,991 | mfu: 51.93 | epoch: 2 | total time: 64.41m | eta: 12.4m\n",
      "Step 01500 | Validation bpb: 0.936558\n",
      "step 01500/01785 (84.03%) | loss: 3.073641 | lrm: 0.32 | dt: 2589.14ms | tok/sec: 202,495 | mfu: 52.06 | epoch: 2 | total time: 64.46m | eta: 12.3m\n",
      "step 01501/01785 (84.09%) | loss: 3.069949 | lrm: 0.32 | dt: 2591.73ms | tok/sec: 202,292 | mfu: 52.01 | epoch: 2 | total time: 64.50m | eta: 12.3m\n",
      "step 01502/01785 (84.15%) | loss: 3.071444 | lrm: 0.32 | dt: 2587.59ms | tok/sec: 202,616 | mfu: 52.09 | epoch: 2 | total time: 64.54m | eta: 12.2m\n",
      "step 01503/01785 (84.20%) | loss: 3.074728 | lrm: 0.32 | dt: 2583.21ms | tok/sec: 202,960 | mfu: 52.18 | epoch: 2 | total time: 64.59m | eta: 12.2m\n",
      "step 01504/01785 (84.26%) | loss: 3.068947 | lrm: 0.32 | dt: 2582.12ms | tok/sec: 203,045 | mfu: 52.20 | epoch: 2 | total time: 64.63m | eta: 12.2m\n",
      "step 01505/01785 (84.31%) | loss: 3.069694 | lrm: 0.31 | dt: 2584.36ms | tok/sec: 202,869 | mfu: 52.16 | epoch: 2 | total time: 64.67m | eta: 12.1m\n",
      "step 01506/01785 (84.37%) | loss: 3.061950 | lrm: 0.31 | dt: 2583.80ms | tok/sec: 202,913 | mfu: 52.17 | epoch: 2 | total time: 64.71m | eta: 12.1m\n",
      "step 01507/01785 (84.43%) | loss: 3.074552 | lrm: 0.31 | dt: 2583.91ms | tok/sec: 202,905 | mfu: 52.17 | epoch: 2 | total time: 64.76m | eta: 12.0m\n",
      "step 01508/01785 (84.48%) | loss: 3.073446 | lrm: 0.31 | dt: 2589.14ms | tok/sec: 202,495 | mfu: 52.06 | epoch: 2 | total time: 64.80m | eta: 12.0m\n",
      "step 01509/01785 (84.54%) | loss: 3.062335 | lrm: 0.31 | dt: 2598.32ms | tok/sec: 201,779 | mfu: 51.88 | epoch: 2 | total time: 64.84m | eta: 11.9m\n",
      "step 01510/01785 (84.59%) | loss: 3.065972 | lrm: 0.31 | dt: 2585.72ms | tok/sec: 202,762 | mfu: 52.13 | epoch: 2 | total time: 64.89m | eta: 11.9m\n",
      "step 01511/01785 (84.65%) | loss: 3.053304 | lrm: 0.31 | dt: 2586.25ms | tok/sec: 202,721 | mfu: 52.12 | epoch: 2 | total time: 64.93m | eta: 11.9m\n",
      "step 01512/01785 (84.71%) | loss: 3.050139 | lrm: 0.31 | dt: 2586.84ms | tok/sec: 202,674 | mfu: 52.11 | epoch: 2 | total time: 64.97m | eta: 11.8m\n",
      "step 01513/01785 (84.76%) | loss: 3.061909 | lrm: 0.30 | dt: 2587.39ms | tok/sec: 202,632 | mfu: 52.10 | epoch: 2 | total time: 65.02m | eta: 11.8m\n",
      "step 01514/01785 (84.82%) | loss: 3.051841 | lrm: 0.30 | dt: 2590.19ms | tok/sec: 202,413 | mfu: 52.04 | epoch: 2 | total time: 65.06m | eta: 11.7m\n",
      "step 01515/01785 (84.87%) | loss: 3.053813 | lrm: 0.30 | dt: 2593.18ms | tok/sec: 202,179 | mfu: 51.98 | epoch: 2 | total time: 65.10m | eta: 11.7m\n",
      "step 01516/01785 (84.93%) | loss: 3.058018 | lrm: 0.30 | dt: 2589.78ms | tok/sec: 202,445 | mfu: 52.05 | epoch: 2 | total time: 65.15m | eta: 11.6m\n",
      "step 01517/01785 (84.99%) | loss: 3.061412 | lrm: 0.30 | dt: 2584.87ms | tok/sec: 202,829 | mfu: 52.15 | epoch: 2 | total time: 65.19m | eta: 11.6m\n",
      "step 01518/01785 (85.04%) | loss: 3.048055 | lrm: 0.30 | dt: 2603.58ms | tok/sec: 201,371 | mfu: 51.77 | epoch: 2 | total time: 65.23m | eta: 11.5m\n",
      "step 01519/01785 (85.10%) | loss: 3.061850 | lrm: 0.30 | dt: 2592.82ms | tok/sec: 202,207 | mfu: 51.99 | epoch: 2 | total time: 65.28m | eta: 11.5m\n",
      "step 01520/01785 (85.15%) | loss: 3.069274 | lrm: 0.30 | dt: 2595.27ms | tok/sec: 202,016 | mfu: 51.94 | epoch: 2 | total time: 65.32m | eta: 11.5m\n",
      "step 01521/01785 (85.21%) | loss: 3.088386 | lrm: 0.30 | dt: 2586.10ms | tok/sec: 202,732 | mfu: 52.12 | epoch: 2 | total time: 65.36m | eta: 11.4m\n",
      "step 01522/01785 (85.27%) | loss: 3.076014 | lrm: 0.29 | dt: 2602.65ms | tok/sec: 201,443 | mfu: 51.79 | epoch: 2 | total time: 65.41m | eta: 11.4m\n",
      "step 01523/01785 (85.32%) | loss: 3.084488 | lrm: 0.29 | dt: 2591.66ms | tok/sec: 202,298 | mfu: 52.01 | epoch: 2 | total time: 65.45m | eta: 11.3m\n",
      "step 01524/01785 (85.38%) | loss: 3.079215 | lrm: 0.29 | dt: 2592.98ms | tok/sec: 202,194 | mfu: 51.99 | epoch: 2 | total time: 65.49m | eta: 11.3m\n",
      "step 01525/01785 (85.43%) | loss: 3.069579 | lrm: 0.29 | dt: 2586.53ms | tok/sec: 202,699 | mfu: 52.11 | epoch: 2 | total time: 65.53m | eta: 11.2m\n",
      "step 01526/01785 (85.49%) | loss: 3.055554 | lrm: 0.29 | dt: 2603.35ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 2 | total time: 65.58m | eta: 11.2m\n",
      "step 01527/01785 (85.55%) | loss: 3.056774 | lrm: 0.29 | dt: 2604.28ms | tok/sec: 201,318 | mfu: 51.76 | epoch: 2 | total time: 65.62m | eta: 11.2m\n",
      "step 01528/01785 (85.60%) | loss: 3.074361 | lrm: 0.29 | dt: 2605.58ms | tok/sec: 201,217 | mfu: 51.73 | epoch: 2 | total time: 65.67m | eta: 11.1m\n",
      "step 01529/01785 (85.66%) | loss: 3.075276 | lrm: 0.29 | dt: 2604.57ms | tok/sec: 201,295 | mfu: 51.75 | epoch: 2 | total time: 65.71m | eta: 11.1m\n",
      "step 01530/01785 (85.71%) | loss: 3.077846 | lrm: 0.29 | dt: 2589.95ms | tok/sec: 202,431 | mfu: 52.05 | epoch: 2 | total time: 65.75m | eta: 11.0m\n",
      "step 01531/01785 (85.77%) | loss: 3.082380 | lrm: 0.28 | dt: 2597.05ms | tok/sec: 201,878 | mfu: 51.90 | epoch: 2 | total time: 65.79m | eta: 11.0m\n",
      "step 01532/01785 (85.83%) | loss: 3.084046 | lrm: 0.28 | dt: 2605.27ms | tok/sec: 201,241 | mfu: 51.74 | epoch: 2 | total time: 65.84m | eta: 10.9m\n",
      "step 01533/01785 (85.88%) | loss: 3.082163 | lrm: 0.28 | dt: 2589.65ms | tok/sec: 202,455 | mfu: 52.05 | epoch: 2 | total time: 65.88m | eta: 10.9m\n",
      "step 01534/01785 (85.94%) | loss: 3.065486 | lrm: 0.28 | dt: 2600.53ms | tok/sec: 201,608 | mfu: 51.83 | epoch: 2 | total time: 65.92m | eta: 10.9m\n",
      "step 01535/01785 (85.99%) | loss: 3.069774 | lrm: 0.28 | dt: 2587.95ms | tok/sec: 202,588 | mfu: 52.09 | epoch: 2 | total time: 65.97m | eta: 10.8m\n",
      "step 01536/01785 (86.05%) | loss: 3.063572 | lrm: 0.28 | dt: 2602.89ms | tok/sec: 201,425 | mfu: 51.79 | epoch: 2 | total time: 66.01m | eta: 10.8m\n",
      "step 01537/01785 (86.11%) | loss: 3.052909 | lrm: 0.28 | dt: 2588.91ms | tok/sec: 202,512 | mfu: 52.07 | epoch: 2 | total time: 66.05m | eta: 10.7m\n",
      "step 01538/01785 (86.16%) | loss: 3.049862 | lrm: 0.28 | dt: 2607.80ms | tok/sec: 201,045 | mfu: 51.69 | epoch: 2 | total time: 66.10m | eta: 10.7m\n",
      "step 01539/01785 (86.22%) | loss: 3.050075 | lrm: 0.28 | dt: 2591.22ms | tok/sec: 202,332 | mfu: 52.02 | epoch: 2 | total time: 66.14m | eta: 10.6m\n",
      "step 01540/01785 (86.27%) | loss: 3.054745 | lrm: 0.27 | dt: 2592.39ms | tok/sec: 202,241 | mfu: 52.00 | epoch: 2 | total time: 66.18m | eta: 10.6m\n",
      "step 01541/01785 (86.33%) | loss: 3.046774 | lrm: 0.27 | dt: 2591.98ms | tok/sec: 202,273 | mfu: 52.01 | epoch: 2 | total time: 66.23m | eta: 10.6m\n",
      "step 01542/01785 (86.39%) | loss: 3.047516 | lrm: 0.27 | dt: 2593.45ms | tok/sec: 202,158 | mfu: 51.98 | epoch: 2 | total time: 66.27m | eta: 10.5m\n",
      "step 01543/01785 (86.44%) | loss: 3.046645 | lrm: 0.27 | dt: 2597.55ms | tok/sec: 201,839 | mfu: 51.89 | epoch: 2 | total time: 66.31m | eta: 10.5m\n",
      "step 01544/01785 (86.50%) | loss: 3.053560 | lrm: 0.27 | dt: 2587.65ms | tok/sec: 202,611 | mfu: 52.09 | epoch: 2 | total time: 66.36m | eta: 10.4m\n",
      "step 01545/01785 (86.55%) | loss: 3.055593 | lrm: 0.27 | dt: 2599.16ms | tok/sec: 201,714 | mfu: 51.86 | epoch: 2 | total time: 66.40m | eta: 10.4m\n",
      "step 01546/01785 (86.61%) | loss: 3.058994 | lrm: 0.27 | dt: 2605.35ms | tok/sec: 201,235 | mfu: 51.74 | epoch: 2 | total time: 66.44m | eta: 10.3m\n",
      "step 01547/01785 (86.67%) | loss: 3.058283 | lrm: 0.27 | dt: 2604.32ms | tok/sec: 201,314 | mfu: 51.76 | epoch: 2 | total time: 66.49m | eta: 10.3m\n",
      "step 01548/01785 (86.72%) | loss: 3.049285 | lrm: 0.27 | dt: 2603.62ms | tok/sec: 201,369 | mfu: 51.77 | epoch: 2 | total time: 66.53m | eta: 10.3m\n",
      "step 01549/01785 (86.78%) | loss: 3.050060 | lrm: 0.26 | dt: 2589.31ms | tok/sec: 202,482 | mfu: 52.06 | epoch: 2 | total time: 66.57m | eta: 10.2m\n",
      "step 01550/01785 (86.83%) | loss: 3.049159 | lrm: 0.26 | dt: 2598.46ms | tok/sec: 201,768 | mfu: 51.88 | epoch: 2 | total time: 66.62m | eta: 10.2m\n",
      "step 01551/01785 (86.89%) | loss: 3.042901 | lrm: 0.26 | dt: 2604.30ms | tok/sec: 201,316 | mfu: 51.76 | epoch: 2 | total time: 66.66m | eta: 10.1m\n",
      "step 01552/01785 (86.95%) | loss: 3.045278 | lrm: 0.26 | dt: 2587.81ms | tok/sec: 202,598 | mfu: 52.09 | epoch: 2 | total time: 66.70m | eta: 10.1m\n",
      "step 01553/01785 (87.00%) | loss: 3.044108 | lrm: 0.26 | dt: 2599.49ms | tok/sec: 201,688 | mfu: 51.86 | epoch: 2 | total time: 66.75m | eta: 10.0m\n",
      "step 01554/01785 (87.06%) | loss: 3.052390 | lrm: 0.26 | dt: 2585.68ms | tok/sec: 202,766 | mfu: 52.13 | epoch: 2 | total time: 66.79m | eta: 10.0m\n",
      "step 01555/01785 (87.11%) | loss: 3.055377 | lrm: 0.26 | dt: 2602.17ms | tok/sec: 201,481 | mfu: 51.80 | epoch: 2 | total time: 66.83m | eta: 9.9m\n",
      "step 01556/01785 (87.17%) | loss: 3.050646 | lrm: 0.26 | dt: 2586.22ms | tok/sec: 202,723 | mfu: 52.12 | epoch: 2 | total time: 66.88m | eta: 9.9m\n",
      "step 01557/01785 (87.23%) | loss: 3.055344 | lrm: 0.26 | dt: 2602.98ms | tok/sec: 201,418 | mfu: 51.79 | epoch: 2 | total time: 66.92m | eta: 9.9m\n",
      "step 01558/01785 (87.28%) | loss: 3.049696 | lrm: 0.25 | dt: 2604.50ms | tok/sec: 201,301 | mfu: 51.76 | epoch: 2 | total time: 66.96m | eta: 9.8m\n",
      "step 01559/01785 (87.34%) | loss: 3.053966 | lrm: 0.25 | dt: 2588.10ms | tok/sec: 202,576 | mfu: 52.08 | epoch: 2 | total time: 67.01m | eta: 9.8m\n",
      "step 01560/01785 (87.39%) | loss: 3.045410 | lrm: 0.25 | dt: 2601.47ms | tok/sec: 201,535 | mfu: 51.82 | epoch: 2 | total time: 67.05m | eta: 9.7m\n",
      "step 01561/01785 (87.45%) | loss: 3.037452 | lrm: 0.25 | dt: 2592.69ms | tok/sec: 202,217 | mfu: 51.99 | epoch: 2 | total time: 67.09m | eta: 9.7m\n",
      "step 01562/01785 (87.51%) | loss: 3.038609 | lrm: 0.25 | dt: 2599.83ms | tok/sec: 201,662 | mfu: 51.85 | epoch: 2 | total time: 67.14m | eta: 9.6m\n",
      "step 01563/01785 (87.56%) | loss: 3.033713 | lrm: 0.25 | dt: 2589.24ms | tok/sec: 202,487 | mfu: 52.06 | epoch: 2 | total time: 67.18m | eta: 9.6m\n",
      "step 01564/01785 (87.62%) | loss: 3.039221 | lrm: 0.25 | dt: 2602.91ms | tok/sec: 201,423 | mfu: 51.79 | epoch: 2 | total time: 67.22m | eta: 9.6m\n",
      "step 01565/01785 (87.68%) | loss: 3.042628 | lrm: 0.25 | dt: 2591.46ms | tok/sec: 202,313 | mfu: 52.02 | epoch: 2 | total time: 67.27m | eta: 9.5m\n",
      "step 01566/01785 (87.73%) | loss: 3.033395 | lrm: 0.25 | dt: 2594.25ms | tok/sec: 202,096 | mfu: 51.96 | epoch: 2 | total time: 67.31m | eta: 9.5m\n",
      "step 01567/01785 (87.79%) | loss: 3.043623 | lrm: 0.24 | dt: 2591.94ms | tok/sec: 202,276 | mfu: 52.01 | epoch: 2 | total time: 67.35m | eta: 9.4m\n",
      "step 01568/01785 (87.84%) | loss: 3.047610 | lrm: 0.24 | dt: 2592.68ms | tok/sec: 202,218 | mfu: 51.99 | epoch: 2 | total time: 67.40m | eta: 9.4m\n",
      "step 01569/01785 (87.90%) | loss: 3.043822 | lrm: 0.24 | dt: 2596.56ms | tok/sec: 201,916 | mfu: 51.91 | epoch: 2 | total time: 67.44m | eta: 9.3m\n",
      "step 01570/01785 (87.96%) | loss: 3.043245 | lrm: 0.24 | dt: 2604.37ms | tok/sec: 201,310 | mfu: 51.76 | epoch: 2 | total time: 67.48m | eta: 9.3m\n",
      "step 01571/01785 (88.01%) | loss: 3.044439 | lrm: 0.24 | dt: 2588.90ms | tok/sec: 202,514 | mfu: 52.07 | epoch: 2 | total time: 67.53m | eta: 9.3m\n",
      "step 01572/01785 (88.07%) | loss: 3.044438 | lrm: 0.24 | dt: 2601.83ms | tok/sec: 201,507 | mfu: 51.81 | epoch: 2 | total time: 67.57m | eta: 9.2m\n",
      "step 01573/01785 (88.12%) | loss: 3.051252 | lrm: 0.24 | dt: 2591.43ms | tok/sec: 202,315 | mfu: 52.02 | epoch: 2 | total time: 67.61m | eta: 9.2m\n",
      "step 01574/01785 (88.18%) | loss: 3.053792 | lrm: 0.24 | dt: 2595.53ms | tok/sec: 201,996 | mfu: 51.93 | epoch: 2 | total time: 67.66m | eta: 9.1m\n",
      "step 01575/01785 (88.24%) | loss: 3.039045 | lrm: 0.24 | dt: 2590.01ms | tok/sec: 202,426 | mfu: 52.04 | epoch: 2 | total time: 67.70m | eta: 9.1m\n",
      "step 01576/01785 (88.29%) | loss: 3.041444 | lrm: 0.23 | dt: 2597.92ms | tok/sec: 201,810 | mfu: 51.89 | epoch: 2 | total time: 67.74m | eta: 9.0m\n",
      "step 01577/01785 (88.35%) | loss: 3.044458 | lrm: 0.23 | dt: 2605.28ms | tok/sec: 201,240 | mfu: 51.74 | epoch: 2 | total time: 67.79m | eta: 9.0m\n",
      "step 01578/01785 (88.40%) | loss: 3.037292 | lrm: 0.23 | dt: 2587.38ms | tok/sec: 202,632 | mfu: 52.10 | epoch: 2 | total time: 67.83m | eta: 9.0m\n",
      "step 01579/01785 (88.46%) | loss: 3.032325 | lrm: 0.23 | dt: 2601.09ms | tok/sec: 201,564 | mfu: 51.82 | epoch: 2 | total time: 67.87m | eta: 8.9m\n",
      "step 01580/01785 (88.52%) | loss: 3.026927 | lrm: 0.23 | dt: 2603.82ms | tok/sec: 201,353 | mfu: 51.77 | epoch: 2 | total time: 67.92m | eta: 8.9m\n",
      "step 01581/01785 (88.57%) | loss: 3.012626 | lrm: 0.23 | dt: 2591.17ms | tok/sec: 202,336 | mfu: 52.02 | epoch: 2 | total time: 67.96m | eta: 8.8m\n",
      "step 01582/01785 (88.63%) | loss: 3.016711 | lrm: 0.23 | dt: 2595.20ms | tok/sec: 202,022 | mfu: 51.94 | epoch: 2 | total time: 68.00m | eta: 8.8m\n",
      "step 01583/01785 (88.68%) | loss: 3.011601 | lrm: 0.23 | dt: 2605.66ms | tok/sec: 201,211 | mfu: 51.73 | epoch: 2 | total time: 68.05m | eta: 8.7m\n",
      "step 01584/01785 (88.74%) | loss: 3.016380 | lrm: 0.23 | dt: 2593.24ms | tok/sec: 202,174 | mfu: 51.98 | epoch: 2 | total time: 68.09m | eta: 8.7m\n",
      "step 01585/01785 (88.80%) | loss: 3.022671 | lrm: 0.22 | dt: 2592.98ms | tok/sec: 202,195 | mfu: 51.99 | epoch: 2 | total time: 68.13m | eta: 8.7m\n",
      "step 01586/01785 (88.85%) | loss: 3.014277 | lrm: 0.22 | dt: 2593.11ms | tok/sec: 202,185 | mfu: 51.98 | epoch: 2 | total time: 68.17m | eta: 8.6m\n",
      "step 01587/01785 (88.91%) | loss: 3.016761 | lrm: 0.22 | dt: 2595.38ms | tok/sec: 202,008 | mfu: 51.94 | epoch: 2 | total time: 68.22m | eta: 8.6m\n",
      "step 01588/01785 (88.96%) | loss: 3.026423 | lrm: 0.22 | dt: 2592.30ms | tok/sec: 202,248 | mfu: 52.00 | epoch: 2 | total time: 68.26m | eta: 8.5m\n",
      "step 01589/01785 (89.02%) | loss: 3.032580 | lrm: 0.22 | dt: 2603.35ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 2 | total time: 68.30m | eta: 8.5m\n",
      "step 01590/01785 (89.08%) | loss: 3.033413 | lrm: 0.22 | dt: 2589.96ms | tok/sec: 202,431 | mfu: 52.05 | epoch: 2 | total time: 68.35m | eta: 8.4m\n",
      "step 01591/01785 (89.13%) | loss: 3.041167 | lrm: 0.22 | dt: 2592.14ms | tok/sec: 202,260 | mfu: 52.00 | epoch: 2 | total time: 68.39m | eta: 8.4m\n",
      "step 01592/01785 (89.19%) | loss: 3.035496 | lrm: 0.22 | dt: 2592.50ms | tok/sec: 202,232 | mfu: 52.00 | epoch: 2 | total time: 68.43m | eta: 8.3m\n",
      "step 01593/01785 (89.24%) | loss: 3.045454 | lrm: 0.22 | dt: 2590.66ms | tok/sec: 202,376 | mfu: 52.03 | epoch: 2 | total time: 68.48m | eta: 8.3m\n",
      "step 01594/01785 (89.30%) | loss: 3.046276 | lrm: 0.21 | dt: 2606.13ms | tok/sec: 201,174 | mfu: 51.72 | epoch: 2 | total time: 68.52m | eta: 8.3m\n",
      "step 01595/01785 (89.36%) | loss: 3.027816 | lrm: 0.21 | dt: 2592.55ms | tok/sec: 202,228 | mfu: 51.99 | epoch: 2 | total time: 68.56m | eta: 8.2m\n",
      "step 01596/01785 (89.41%) | loss: 3.029014 | lrm: 0.21 | dt: 2596.09ms | tok/sec: 201,952 | mfu: 51.92 | epoch: 2 | total time: 68.61m | eta: 8.2m\n",
      "step 01597/01785 (89.47%) | loss: 3.017440 | lrm: 0.21 | dt: 2605.15ms | tok/sec: 201,250 | mfu: 51.74 | epoch: 2 | total time: 68.65m | eta: 8.1m\n",
      "step 01598/01785 (89.52%) | loss: 3.008902 | lrm: 0.21 | dt: 2605.07ms | tok/sec: 201,256 | mfu: 51.74 | epoch: 2 | total time: 68.69m | eta: 8.1m\n",
      "step 01599/01785 (89.58%) | loss: 3.007989 | lrm: 0.21 | dt: 2596.40ms | tok/sec: 201,928 | mfu: 51.92 | epoch: 2 | total time: 68.74m | eta: 8.0m\n",
      "step 01600/01785 (89.64%) | loss: 3.008303 | lrm: 0.21 | dt: 2589.39ms | tok/sec: 202,475 | mfu: 52.06 | epoch: 2 | total time: 68.78m | eta: 8.0m\n",
      "step 01601/01785 (89.69%) | loss: 3.015505 | lrm: 0.21 | dt: 2603.71ms | tok/sec: 201,361 | mfu: 51.77 | epoch: 2 | total time: 68.82m | eta: 8.0m\n",
      "step 01602/01785 (89.75%) | loss: 3.022799 | lrm: 0.21 | dt: 2592.27ms | tok/sec: 202,250 | mfu: 52.00 | epoch: 2 | total time: 68.87m | eta: 7.9m\n",
      "step 01603/01785 (89.80%) | loss: 3.024437 | lrm: 0.20 | dt: 2595.94ms | tok/sec: 201,964 | mfu: 51.93 | epoch: 2 | total time: 68.91m | eta: 7.9m\n",
      "step 01604/01785 (89.86%) | loss: 3.021768 | lrm: 0.20 | dt: 2605.08ms | tok/sec: 201,255 | mfu: 51.74 | epoch: 2 | total time: 68.95m | eta: 7.8m\n",
      "step 01605/01785 (89.92%) | loss: 3.025121 | lrm: 0.20 | dt: 2605.06ms | tok/sec: 201,257 | mfu: 51.74 | epoch: 2 | total time: 69.00m | eta: 7.8m\n",
      "step 01606/01785 (89.97%) | loss: 3.023363 | lrm: 0.20 | dt: 2605.15ms | tok/sec: 201,250 | mfu: 51.74 | epoch: 2 | total time: 69.04m | eta: 7.7m\n",
      "step 01607/01785 (90.03%) | loss: 3.023062 | lrm: 0.20 | dt: 2592.51ms | tok/sec: 202,231 | mfu: 51.99 | epoch: 2 | total time: 69.08m | eta: 7.7m\n",
      "step 01608/01785 (90.08%) | loss: 3.029099 | lrm: 0.20 | dt: 2596.14ms | tok/sec: 201,949 | mfu: 51.92 | epoch: 2 | total time: 69.13m | eta: 7.7m\n",
      "step 01609/01785 (90.14%) | loss: 3.028665 | lrm: 0.20 | dt: 2604.70ms | tok/sec: 201,285 | mfu: 51.75 | epoch: 2 | total time: 69.17m | eta: 7.6m\n",
      "step 01610/01785 (90.20%) | loss: 3.031207 | lrm: 0.20 | dt: 2592.86ms | tok/sec: 202,204 | mfu: 51.99 | epoch: 2 | total time: 69.21m | eta: 7.6m\n",
      "step 01611/01785 (90.25%) | loss: 3.029681 | lrm: 0.20 | dt: 2596.43ms | tok/sec: 201,926 | mfu: 51.92 | epoch: 2 | total time: 69.26m | eta: 7.5m\n",
      "step 01612/01785 (90.31%) | loss: 3.024013 | lrm: 0.19 | dt: 2604.73ms | tok/sec: 201,283 | mfu: 51.75 | epoch: 2 | total time: 69.30m | eta: 7.5m\n",
      "step 01613/01785 (90.36%) | loss: 3.030296 | lrm: 0.19 | dt: 2591.09ms | tok/sec: 202,342 | mfu: 52.02 | epoch: 2 | total time: 69.34m | eta: 7.4m\n",
      "step 01614/01785 (90.42%) | loss: 3.031537 | lrm: 0.19 | dt: 2599.73ms | tok/sec: 201,669 | mfu: 51.85 | epoch: 2 | total time: 69.39m | eta: 7.4m\n",
      "step 01615/01785 (90.48%) | loss: 3.021482 | lrm: 0.19 | dt: 2588.55ms | tok/sec: 202,541 | mfu: 52.07 | epoch: 2 | total time: 69.43m | eta: 7.4m\n",
      "step 01616/01785 (90.53%) | loss: 3.008640 | lrm: 0.19 | dt: 2608.93ms | tok/sec: 200,958 | mfu: 51.67 | epoch: 2 | total time: 69.47m | eta: 7.3m\n",
      "step 01617/01785 (90.59%) | loss: 3.025857 | lrm: 0.19 | dt: 2592.25ms | tok/sec: 202,252 | mfu: 52.00 | epoch: 2 | total time: 69.52m | eta: 7.3m\n",
      "step 01618/01785 (90.64%) | loss: 3.033179 | lrm: 0.19 | dt: 2592.58ms | tok/sec: 202,225 | mfu: 51.99 | epoch: 2 | total time: 69.56m | eta: 7.2m\n",
      "step 01619/01785 (90.70%) | loss: 3.036721 | lrm: 0.19 | dt: 2594.73ms | tok/sec: 202,058 | mfu: 51.95 | epoch: 2 | total time: 69.60m | eta: 7.2m\n",
      "step 01620/01785 (90.76%) | loss: 3.032414 | lrm: 0.18 | dt: 2600.89ms | tok/sec: 201,579 | mfu: 51.83 | epoch: 2 | total time: 69.65m | eta: 7.1m\n",
      "step 01621/01785 (90.81%) | loss: 3.042915 | lrm: 0.18 | dt: 2604.71ms | tok/sec: 201,284 | mfu: 51.75 | epoch: 2 | total time: 69.69m | eta: 7.1m\n",
      "step 01622/01785 (90.87%) | loss: 3.038145 | lrm: 0.18 | dt: 2591.85ms | tok/sec: 202,283 | mfu: 52.01 | epoch: 2 | total time: 69.73m | eta: 7.1m\n",
      "step 01623/01785 (90.92%) | loss: 3.023805 | lrm: 0.18 | dt: 2595.42ms | tok/sec: 202,004 | mfu: 51.94 | epoch: 2 | total time: 69.78m | eta: 7.0m\n",
      "step 01624/01785 (90.98%) | loss: 3.018494 | lrm: 0.18 | dt: 2605.51ms | tok/sec: 201,222 | mfu: 51.74 | epoch: 2 | total time: 69.82m | eta: 7.0m\n",
      "step 01625/01785 (91.04%) | loss: 3.015554 | lrm: 0.18 | dt: 2605.62ms | tok/sec: 201,214 | mfu: 51.73 | epoch: 2 | total time: 69.86m | eta: 6.9m\n",
      "step 01626/01785 (91.09%) | loss: 3.011344 | lrm: 0.18 | dt: 2588.67ms | tok/sec: 202,531 | mfu: 52.07 | epoch: 2 | total time: 69.91m | eta: 6.9m\n",
      "step 01627/01785 (91.15%) | loss: 3.016800 | lrm: 0.18 | dt: 2600.08ms | tok/sec: 201,643 | mfu: 51.84 | epoch: 2 | total time: 69.95m | eta: 6.8m\n",
      "step 01628/01785 (91.20%) | loss: 3.016075 | lrm: 0.18 | dt: 2604.69ms | tok/sec: 201,286 | mfu: 51.75 | epoch: 2 | total time: 69.99m | eta: 6.8m\n",
      "step 01629/01785 (91.26%) | loss: 3.018450 | lrm: 0.17 | dt: 2605.91ms | tok/sec: 201,192 | mfu: 51.73 | epoch: 2 | total time: 70.04m | eta: 6.7m\n",
      "step 01630/01785 (91.32%) | loss: 3.022400 | lrm: 0.17 | dt: 2603.34ms | tok/sec: 201,390 | mfu: 51.78 | epoch: 2 | total time: 70.08m | eta: 6.7m\n",
      "step 01631/01785 (91.37%) | loss: 3.032765 | lrm: 0.17 | dt: 2605.55ms | tok/sec: 201,219 | mfu: 51.73 | epoch: 2 | total time: 70.12m | eta: 6.7m\n",
      "step 01632/01785 (91.43%) | loss: 3.032892 | lrm: 0.17 | dt: 2604.19ms | tok/sec: 201,324 | mfu: 51.76 | epoch: 2 | total time: 70.17m | eta: 6.6m\n",
      "step 01633/01785 (91.48%) | loss: 3.033029 | lrm: 0.17 | dt: 2605.40ms | tok/sec: 201,230 | mfu: 51.74 | epoch: 2 | total time: 70.21m | eta: 6.6m\n",
      "step 01634/01785 (91.54%) | loss: 3.042923 | lrm: 0.17 | dt: 2605.34ms | tok/sec: 201,235 | mfu: 51.74 | epoch: 2 | total time: 70.25m | eta: 6.5m\n",
      "step 01635/01785 (91.60%) | loss: 3.045370 | lrm: 0.17 | dt: 2590.66ms | tok/sec: 202,376 | mfu: 52.03 | epoch: 2 | total time: 70.30m | eta: 6.5m\n",
      "step 01636/01785 (91.65%) | loss: 3.033777 | lrm: 0.17 | dt: 2596.79ms | tok/sec: 201,898 | mfu: 51.91 | epoch: 2 | total time: 70.34m | eta: 6.4m\n",
      "step 01637/01785 (91.71%) | loss: 3.037501 | lrm: 0.17 | dt: 2604.64ms | tok/sec: 201,290 | mfu: 51.75 | epoch: 2 | total time: 70.38m | eta: 6.4m\n",
      "step 01638/01785 (91.76%) | loss: 3.036620 | lrm: 0.16 | dt: 2605.20ms | tok/sec: 201,246 | mfu: 51.74 | epoch: 2 | total time: 70.43m | eta: 6.4m\n",
      "step 01639/01785 (91.82%) | loss: 3.026382 | lrm: 0.16 | dt: 2607.44ms | tok/sec: 201,074 | mfu: 51.70 | epoch: 2 | total time: 70.47m | eta: 6.3m\n",
      "step 01640/01785 (91.88%) | loss: 3.036782 | lrm: 0.16 | dt: 2594.18ms | tok/sec: 202,101 | mfu: 51.96 | epoch: 2 | total time: 70.51m | eta: 6.3m\n",
      "step 01641/01785 (91.93%) | loss: 3.038646 | lrm: 0.16 | dt: 2601.56ms | tok/sec: 201,528 | mfu: 51.81 | epoch: 2 | total time: 70.56m | eta: 6.2m\n",
      "step 01642/01785 (91.99%) | loss: 3.019159 | lrm: 0.16 | dt: 2594.82ms | tok/sec: 202,051 | mfu: 51.95 | epoch: 2 | total time: 70.60m | eta: 6.2m\n",
      "step 01643/01785 (92.04%) | loss: 3.023105 | lrm: 0.16 | dt: 2599.81ms | tok/sec: 201,663 | mfu: 51.85 | epoch: 2 | total time: 70.64m | eta: 6.1m\n",
      "step 01644/01785 (92.10%) | loss: 3.017525 | lrm: 0.16 | dt: 2598.46ms | tok/sec: 201,768 | mfu: 51.88 | epoch: 2 | total time: 70.69m | eta: 6.1m\n",
      "step 01645/01785 (92.16%) | loss: 3.014993 | lrm: 0.16 | dt: 2591.35ms | tok/sec: 202,322 | mfu: 52.02 | epoch: 2 | total time: 70.73m | eta: 6.1m\n",
      "step 01646/01785 (92.21%) | loss: 3.030426 | lrm: 0.16 | dt: 2599.96ms | tok/sec: 201,652 | mfu: 51.85 | epoch: 2 | total time: 70.77m | eta: 6.0m\n",
      "step 01647/01785 (92.27%) | loss: 3.035341 | lrm: 0.15 | dt: 2588.38ms | tok/sec: 202,554 | mfu: 52.08 | epoch: 2 | total time: 70.82m | eta: 6.0m\n",
      "step 01648/01785 (92.32%) | loss: 3.032068 | lrm: 0.15 | dt: 2605.25ms | tok/sec: 201,242 | mfu: 51.74 | epoch: 2 | total time: 70.86m | eta: 5.9m\n",
      "step 01649/01785 (92.38%) | loss: 3.029734 | lrm: 0.15 | dt: 2605.75ms | tok/sec: 201,204 | mfu: 51.73 | epoch: 2 | total time: 70.90m | eta: 5.9m\n",
      "step 01650/01785 (92.44%) | loss: 3.014229 | lrm: 0.15 | dt: 2604.88ms | tok/sec: 201,271 | mfu: 51.75 | epoch: 2 | total time: 70.95m | eta: 5.8m\n",
      "step 01651/01785 (92.49%) | loss: 3.014207 | lrm: 0.15 | dt: 2605.99ms | tok/sec: 201,186 | mfu: 51.73 | epoch: 2 | total time: 70.99m | eta: 5.8m\n",
      "step 01652/01785 (92.55%) | loss: 3.040145 | lrm: 0.15 | dt: 2605.47ms | tok/sec: 201,225 | mfu: 51.74 | epoch: 2 | total time: 71.03m | eta: 5.8m\n",
      "step 01653/01785 (92.61%) | loss: 3.025832 | lrm: 0.15 | dt: 2604.65ms | tok/sec: 201,289 | mfu: 51.75 | epoch: 2 | total time: 71.08m | eta: 5.7m\n",
      "step 01654/01785 (92.66%) | loss: 3.031695 | lrm: 0.15 | dt: 2595.30ms | tok/sec: 202,014 | mfu: 51.94 | epoch: 2 | total time: 71.12m | eta: 5.7m\n",
      "step 01655/01785 (92.72%) | loss: 3.025716 | lrm: 0.15 | dt: 2589.14ms | tok/sec: 202,495 | mfu: 52.06 | epoch: 2 | total time: 71.16m | eta: 5.6m\n",
      "step 01656/01785 (92.77%) | loss: 3.020826 | lrm: 0.14 | dt: 2605.02ms | tok/sec: 201,260 | mfu: 51.75 | epoch: 2 | total time: 71.21m | eta: 5.6m\n",
      "step 01657/01785 (92.83%) | loss: 3.014057 | lrm: 0.14 | dt: 2591.26ms | tok/sec: 202,329 | mfu: 52.02 | epoch: 2 | total time: 71.25m | eta: 5.5m\n",
      "step 01658/01785 (92.89%) | loss: 3.009270 | lrm: 0.14 | dt: 2597.84ms | tok/sec: 201,817 | mfu: 51.89 | epoch: 2 | total time: 71.29m | eta: 5.5m\n",
      "step 01659/01785 (92.94%) | loss: 2.993863 | lrm: 0.14 | dt: 2591.03ms | tok/sec: 202,347 | mfu: 52.02 | epoch: 2 | total time: 71.34m | eta: 5.5m\n",
      "step 01660/01785 (93.00%) | loss: 2.981491 | lrm: 0.14 | dt: 2595.83ms | tok/sec: 201,972 | mfu: 51.93 | epoch: 2 | total time: 71.38m | eta: 5.4m\n",
      "step 01661/01785 (93.05%) | loss: 2.980699 | lrm: 0.14 | dt: 2603.86ms | tok/sec: 201,350 | mfu: 51.77 | epoch: 2 | total time: 71.42m | eta: 5.4m\n",
      "step 01662/01785 (93.11%) | loss: 2.981831 | lrm: 0.14 | dt: 2604.65ms | tok/sec: 201,289 | mfu: 51.75 | epoch: 2 | total time: 71.47m | eta: 5.3m\n",
      "step 01663/01785 (93.17%) | loss: 2.982008 | lrm: 0.14 | dt: 2603.51ms | tok/sec: 201,377 | mfu: 51.78 | epoch: 2 | total time: 71.51m | eta: 5.3m\n",
      "step 01664/01785 (93.22%) | loss: 2.999406 | lrm: 0.14 | dt: 2606.76ms | tok/sec: 201,126 | mfu: 51.71 | epoch: 2 | total time: 71.55m | eta: 5.2m\n",
      "step 01665/01785 (93.28%) | loss: 3.006685 | lrm: 0.13 | dt: 2592.04ms | tok/sec: 202,268 | mfu: 52.00 | epoch: 2 | total time: 71.60m | eta: 5.2m\n",
      "step 01666/01785 (93.33%) | loss: 2.988498 | lrm: 0.13 | dt: 2602.56ms | tok/sec: 201,451 | mfu: 51.79 | epoch: 2 | total time: 71.64m | eta: 5.1m\n",
      "step 01667/01785 (93.39%) | loss: 2.977123 | lrm: 0.13 | dt: 2591.04ms | tok/sec: 202,346 | mfu: 52.02 | epoch: 2 | total time: 71.68m | eta: 5.1m\n",
      "step 01668/01785 (93.45%) | loss: 2.984848 | lrm: 0.13 | dt: 2604.90ms | tok/sec: 201,269 | mfu: 51.75 | epoch: 2 | total time: 71.73m | eta: 5.1m\n",
      "step 01669/01785 (93.50%) | loss: 2.985383 | lrm: 0.13 | dt: 2593.66ms | tok/sec: 202,142 | mfu: 51.97 | epoch: 2 | total time: 71.77m | eta: 5.0m\n",
      "step 01670/01785 (93.56%) | loss: 2.969087 | lrm: 0.13 | dt: 2593.11ms | tok/sec: 202,185 | mfu: 51.98 | epoch: 2 | total time: 71.81m | eta: 5.0m\n",
      "step 01671/01785 (93.61%) | loss: 2.962527 | lrm: 0.13 | dt: 2605.75ms | tok/sec: 201,203 | mfu: 51.73 | epoch: 2 | total time: 71.86m | eta: 4.9m\n",
      "step 01672/01785 (93.67%) | loss: 2.976905 | lrm: 0.13 | dt: 2589.70ms | tok/sec: 202,450 | mfu: 52.05 | epoch: 2 | total time: 71.90m | eta: 4.9m\n",
      "step 01673/01785 (93.73%) | loss: 2.985105 | lrm: 0.13 | dt: 2600.29ms | tok/sec: 201,626 | mfu: 51.84 | epoch: 2 | total time: 71.94m | eta: 4.8m\n",
      "step 01674/01785 (93.78%) | loss: 2.995784 | lrm: 0.12 | dt: 2605.68ms | tok/sec: 201,209 | mfu: 51.73 | epoch: 2 | total time: 71.99m | eta: 4.8m\n",
      "step 01675/01785 (93.84%) | loss: 2.987510 | lrm: 0.12 | dt: 2605.56ms | tok/sec: 201,218 | mfu: 51.73 | epoch: 2 | total time: 72.03m | eta: 4.8m\n",
      "step 01676/01785 (93.89%) | loss: 2.986466 | lrm: 0.12 | dt: 2606.74ms | tok/sec: 201,127 | mfu: 51.71 | epoch: 2 | total time: 72.07m | eta: 4.7m\n",
      "step 01677/01785 (93.95%) | loss: 2.995216 | lrm: 0.12 | dt: 2602.71ms | tok/sec: 201,439 | mfu: 51.79 | epoch: 2 | total time: 72.12m | eta: 4.7m\n",
      "step 01678/01785 (94.01%) | loss: 3.002318 | lrm: 0.12 | dt: 2605.73ms | tok/sec: 201,205 | mfu: 51.73 | epoch: 2 | total time: 72.16m | eta: 4.6m\n",
      "step 01679/01785 (94.06%) | loss: 2.997631 | lrm: 0.12 | dt: 2604.08ms | tok/sec: 201,333 | mfu: 51.76 | epoch: 2 | total time: 72.20m | eta: 4.6m\n",
      "step 01680/01785 (94.12%) | loss: 3.008875 | lrm: 0.12 | dt: 2604.09ms | tok/sec: 201,332 | mfu: 51.76 | epoch: 2 | total time: 72.25m | eta: 4.5m\n",
      "step 01681/01785 (94.17%) | loss: 3.015815 | lrm: 0.12 | dt: 2605.71ms | tok/sec: 201,207 | mfu: 51.73 | epoch: 2 | total time: 72.29m | eta: 4.5m\n",
      "step 01682/01785 (94.23%) | loss: 3.008581 | lrm: 0.12 | dt: 2597.46ms | tok/sec: 201,846 | mfu: 51.90 | epoch: 2 | total time: 72.33m | eta: 4.5m\n",
      "step 01683/01785 (94.29%) | loss: 3.016794 | lrm: 0.11 | dt: 2590.39ms | tok/sec: 202,397 | mfu: 52.04 | epoch: 2 | total time: 72.38m | eta: 4.4m\n",
      "step 01684/01785 (94.34%) | loss: 3.015460 | lrm: 0.11 | dt: 2604.30ms | tok/sec: 201,316 | mfu: 51.76 | epoch: 2 | total time: 72.42m | eta: 4.4m\n",
      "step 01685/01785 (94.40%) | loss: 3.009110 | lrm: 0.11 | dt: 2605.60ms | tok/sec: 201,216 | mfu: 51.73 | epoch: 2 | total time: 72.46m | eta: 4.3m\n",
      "step 01686/01785 (94.45%) | loss: 3.004706 | lrm: 0.11 | dt: 2600.27ms | tok/sec: 201,628 | mfu: 51.84 | epoch: 2 | total time: 72.51m | eta: 4.3m\n",
      "step 01687/01785 (94.51%) | loss: 3.007091 | lrm: 0.11 | dt: 2586.93ms | tok/sec: 202,668 | mfu: 52.11 | epoch: 2 | total time: 72.55m | eta: 4.2m\n",
      "step 01688/01785 (94.57%) | loss: 3.030067 | lrm: 0.11 | dt: 2603.43ms | tok/sec: 201,383 | mfu: 51.78 | epoch: 2 | total time: 72.59m | eta: 4.2m\n",
      "step 01689/01785 (94.62%) | loss: 3.023192 | lrm: 0.11 | dt: 2606.27ms | tok/sec: 201,163 | mfu: 51.72 | epoch: 2 | total time: 72.64m | eta: 4.2m\n",
      "step 01690/01785 (94.68%) | loss: 3.014622 | lrm: 0.11 | dt: 2605.94ms | tok/sec: 201,189 | mfu: 51.73 | epoch: 2 | total time: 72.68m | eta: 4.1m\n",
      "step 01691/01785 (94.73%) | loss: 3.013533 | lrm: 0.11 | dt: 2608.46ms | tok/sec: 200,995 | mfu: 51.68 | epoch: 2 | total time: 72.72m | eta: 4.1m\n",
      "step 01692/01785 (94.79%) | loss: 3.012728 | lrm: 0.10 | dt: 2609.58ms | tok/sec: 200,909 | mfu: 51.65 | epoch: 2 | total time: 72.77m | eta: 4.0m\n",
      "step 01693/01785 (94.85%) | loss: 3.003114 | lrm: 0.10 | dt: 2587.88ms | tok/sec: 202,593 | mfu: 52.09 | epoch: 2 | total time: 72.81m | eta: 4.0m\n",
      "step 01694/01785 (94.90%) | loss: 3.006003 | lrm: 0.10 | dt: 2595.45ms | tok/sec: 202,002 | mfu: 51.94 | epoch: 2 | total time: 72.85m | eta: 3.9m\n",
      "step 01695/01785 (94.96%) | loss: 3.012734 | lrm: 0.10 | dt: 2598.60ms | tok/sec: 201,758 | mfu: 51.87 | epoch: 2 | total time: 72.90m | eta: 3.9m\n",
      "step 01696/01785 (95.01%) | loss: 3.012589 | lrm: 0.10 | dt: 2593.04ms | tok/sec: 202,190 | mfu: 51.98 | epoch: 2 | total time: 72.94m | eta: 3.9m\n",
      "step 01697/01785 (95.07%) | loss: 3.005461 | lrm: 0.10 | dt: 2593.85ms | tok/sec: 202,127 | mfu: 51.97 | epoch: 2 | total time: 72.98m | eta: 3.8m\n",
      "step 01698/01785 (95.13%) | loss: 3.012823 | lrm: 0.10 | dt: 2593.75ms | tok/sec: 202,134 | mfu: 51.97 | epoch: 2 | total time: 73.03m | eta: 3.8m\n",
      "step 01699/01785 (95.18%) | loss: 3.012283 | lrm: 0.10 | dt: 2594.59ms | tok/sec: 202,069 | mfu: 51.95 | epoch: 2 | total time: 73.07m | eta: 3.7m\n",
      "step 01700/01785 (95.24%) | loss: 3.009998 | lrm: 0.10 | dt: 2605.04ms | tok/sec: 201,259 | mfu: 51.74 | epoch: 2 | total time: 73.11m | eta: 3.7m\n",
      "step 01701/01785 (95.29%) | loss: 3.029818 | lrm: 0.09 | dt: 2604.43ms | tok/sec: 201,306 | mfu: 51.76 | epoch: 2 | total time: 73.16m | eta: 3.6m\n",
      "step 01702/01785 (95.35%) | loss: 3.020291 | lrm: 0.09 | dt: 2604.13ms | tok/sec: 201,329 | mfu: 51.76 | epoch: 2 | total time: 73.20m | eta: 3.6m\n",
      "step 01703/01785 (95.41%) | loss: 3.007812 | lrm: 0.09 | dt: 2605.09ms | tok/sec: 201,255 | mfu: 51.74 | epoch: 2 | total time: 73.24m | eta: 3.5m\n",
      "step 01704/01785 (95.46%) | loss: 3.009346 | lrm: 0.09 | dt: 2595.87ms | tok/sec: 201,969 | mfu: 51.93 | epoch: 2 | total time: 73.29m | eta: 3.5m\n",
      "step 01705/01785 (95.52%) | loss: 3.005813 | lrm: 0.09 | dt: 2590.30ms | tok/sec: 202,404 | mfu: 52.04 | epoch: 2 | total time: 73.33m | eta: 3.5m\n",
      "step 01706/01785 (95.57%) | loss: 3.010621 | lrm: 0.09 | dt: 2604.39ms | tok/sec: 201,309 | mfu: 51.76 | epoch: 2 | total time: 73.37m | eta: 3.4m\n",
      "step 01707/01785 (95.63%) | loss: 3.011351 | lrm: 0.09 | dt: 2587.50ms | tok/sec: 202,623 | mfu: 52.10 | epoch: 2 | total time: 73.42m | eta: 3.4m\n",
      "step 01708/01785 (95.69%) | loss: 3.012361 | lrm: 0.09 | dt: 2601.69ms | tok/sec: 201,518 | mfu: 51.81 | epoch: 2 | total time: 73.46m | eta: 3.3m\n",
      "step 01709/01785 (95.74%) | loss: 3.032801 | lrm: 0.09 | dt: 2605.66ms | tok/sec: 201,211 | mfu: 51.73 | epoch: 2 | total time: 73.50m | eta: 3.3m\n",
      "step 01710/01785 (95.80%) | loss: 3.029053 | lrm: 0.08 | dt: 2591.22ms | tok/sec: 202,332 | mfu: 52.02 | epoch: 2 | total time: 73.55m | eta: 3.2m\n",
      "step 01711/01785 (95.85%) | loss: 3.013883 | lrm: 0.08 | dt: 2597.58ms | tok/sec: 201,837 | mfu: 51.89 | epoch: 2 | total time: 73.59m | eta: 3.2m\n",
      "step 01712/01785 (95.91%) | loss: 3.004271 | lrm: 0.08 | dt: 2585.85ms | tok/sec: 202,752 | mfu: 52.13 | epoch: 2 | total time: 73.63m | eta: 3.2m\n",
      "step 01713/01785 (95.97%) | loss: 3.015594 | lrm: 0.08 | dt: 2592.55ms | tok/sec: 202,228 | mfu: 51.99 | epoch: 2 | total time: 73.68m | eta: 3.1m\n",
      "step 01714/01785 (96.02%) | loss: 3.012372 | lrm: 0.08 | dt: 2592.77ms | tok/sec: 202,211 | mfu: 51.99 | epoch: 2 | total time: 73.72m | eta: 3.1m\n",
      "step 01715/01785 (96.08%) | loss: 3.006281 | lrm: 0.08 | dt: 2592.64ms | tok/sec: 202,221 | mfu: 51.99 | epoch: 2 | total time: 73.76m | eta: 3.0m\n",
      "step 01716/01785 (96.13%) | loss: 3.010515 | lrm: 0.08 | dt: 2596.66ms | tok/sec: 201,908 | mfu: 51.91 | epoch: 2 | total time: 73.81m | eta: 3.0m\n",
      "step 01717/01785 (96.19%) | loss: 2.998339 | lrm: 0.08 | dt: 2593.02ms | tok/sec: 202,191 | mfu: 51.98 | epoch: 2 | total time: 73.85m | eta: 2.9m\n",
      "step 01718/01785 (96.25%) | loss: 3.008123 | lrm: 0.08 | dt: 2599.05ms | tok/sec: 201,722 | mfu: 51.86 | epoch: 2 | total time: 73.89m | eta: 2.9m\n",
      "step 01719/01785 (96.30%) | loss: 3.007073 | lrm: 0.07 | dt: 2588.47ms | tok/sec: 202,547 | mfu: 52.08 | epoch: 2 | total time: 73.94m | eta: 2.9m\n",
      "step 01720/01785 (96.36%) | loss: 3.010849 | lrm: 0.07 | dt: 2602.51ms | tok/sec: 201,454 | mfu: 51.80 | epoch: 2 | total time: 73.98m | eta: 2.8m\n",
      "step 01721/01785 (96.41%) | loss: 3.014516 | lrm: 0.07 | dt: 2599.91ms | tok/sec: 201,655 | mfu: 51.85 | epoch: 2 | total time: 74.02m | eta: 2.8m\n",
      "step 01722/01785 (96.47%) | loss: 3.003028 | lrm: 0.07 | dt: 2595.46ms | tok/sec: 202,002 | mfu: 51.94 | epoch: 2 | total time: 74.07m | eta: 2.7m\n",
      "step 01723/01785 (96.53%) | loss: 2.998223 | lrm: 0.07 | dt: 2589.54ms | tok/sec: 202,464 | mfu: 52.05 | epoch: 2 | total time: 74.11m | eta: 2.7m\n",
      "step 01724/01785 (96.58%) | loss: 2.997859 | lrm: 0.07 | dt: 2604.69ms | tok/sec: 201,286 | mfu: 51.75 | epoch: 2 | total time: 74.15m | eta: 2.6m\n",
      "step 01725/01785 (96.64%) | loss: 2.986882 | lrm: 0.07 | dt: 2605.78ms | tok/sec: 201,201 | mfu: 51.73 | epoch: 2 | total time: 74.20m | eta: 2.6m\n",
      "step 01726/01785 (96.69%) | loss: 2.989425 | lrm: 0.07 | dt: 2605.98ms | tok/sec: 201,186 | mfu: 51.73 | epoch: 2 | total time: 74.24m | eta: 2.6m\n",
      "step 01727/01785 (96.75%) | loss: 3.002307 | lrm: 0.07 | dt: 2590.70ms | tok/sec: 202,373 | mfu: 52.03 | epoch: 2 | total time: 74.28m | eta: 2.5m\n",
      "step 01728/01785 (96.81%) | loss: 2.998231 | lrm: 0.06 | dt: 2597.06ms | tok/sec: 201,877 | mfu: 51.90 | epoch: 2 | total time: 74.33m | eta: 2.5m\n",
      "step 01729/01785 (96.86%) | loss: 3.002418 | lrm: 0.06 | dt: 2597.56ms | tok/sec: 201,838 | mfu: 51.89 | epoch: 2 | total time: 74.37m | eta: 2.4m\n",
      "step 01730/01785 (96.92%) | loss: 3.004544 | lrm: 0.06 | dt: 2589.59ms | tok/sec: 202,459 | mfu: 52.05 | epoch: 2 | total time: 74.41m | eta: 2.4m\n",
      "step 01731/01785 (96.97%) | loss: 2.996765 | lrm: 0.06 | dt: 2604.03ms | tok/sec: 201,337 | mfu: 51.76 | epoch: 2 | total time: 74.46m | eta: 2.3m\n",
      "step 01732/01785 (97.03%) | loss: 3.000439 | lrm: 0.06 | dt: 2603.24ms | tok/sec: 201,398 | mfu: 51.78 | epoch: 2 | total time: 74.50m | eta: 2.3m\n",
      "step 01733/01785 (97.09%) | loss: 2.979858 | lrm: 0.06 | dt: 2598.73ms | tok/sec: 201,748 | mfu: 51.87 | epoch: 2 | total time: 74.54m | eta: 2.2m\n",
      "step 01734/01785 (97.14%) | loss: 2.984627 | lrm: 0.06 | dt: 2588.45ms | tok/sec: 202,549 | mfu: 52.08 | epoch: 2 | total time: 74.59m | eta: 2.2m\n",
      "step 01735/01785 (97.20%) | loss: 2.985541 | lrm: 0.06 | dt: 2606.30ms | tok/sec: 201,161 | mfu: 51.72 | epoch: 2 | total time: 74.63m | eta: 2.2m\n",
      "step 01736/01785 (97.25%) | loss: 2.986377 | lrm: 0.05 | dt: 2604.67ms | tok/sec: 201,287 | mfu: 51.75 | epoch: 2 | total time: 74.67m | eta: 2.1m\n",
      "step 01737/01785 (97.31%) | loss: 2.989708 | lrm: 0.05 | dt: 2604.46ms | tok/sec: 201,304 | mfu: 51.76 | epoch: 2 | total time: 74.72m | eta: 2.1m\n",
      "step 01738/01785 (97.37%) | loss: 2.988110 | lrm: 0.05 | dt: 2606.37ms | tok/sec: 201,156 | mfu: 51.72 | epoch: 2 | total time: 74.76m | eta: 2.0m\n",
      "step 01739/01785 (97.42%) | loss: 2.985642 | lrm: 0.05 | dt: 2605.57ms | tok/sec: 201,217 | mfu: 51.73 | epoch: 2 | total time: 74.80m | eta: 2.0m\n",
      "step 01740/01785 (97.48%) | loss: 3.001476 | lrm: 0.05 | dt: 2605.82ms | tok/sec: 201,198 | mfu: 51.73 | epoch: 2 | total time: 74.85m | eta: 1.9m\n",
      "step 01741/01785 (97.54%) | loss: 3.011975 | lrm: 0.05 | dt: 2596.33ms | tok/sec: 201,934 | mfu: 51.92 | epoch: 2 | total time: 74.89m | eta: 1.9m\n",
      "step 01742/01785 (97.59%) | loss: 3.014716 | lrm: 0.05 | dt: 2593.30ms | tok/sec: 202,169 | mfu: 51.98 | epoch: 2 | total time: 74.93m | eta: 1.9m\n",
      "step 01743/01785 (97.65%) | loss: 3.022073 | lrm: 0.05 | dt: 2608.24ms | tok/sec: 201,012 | mfu: 51.68 | epoch: 2 | total time: 74.98m | eta: 1.8m\n",
      "step 01744/01785 (97.70%) | loss: 3.018562 | lrm: 0.05 | dt: 2609.60ms | tok/sec: 200,907 | mfu: 51.65 | epoch: 2 | total time: 75.02m | eta: 1.8m\n",
      "step 01745/01785 (97.76%) | loss: 3.013003 | lrm: 0.04 | dt: 2589.75ms | tok/sec: 202,447 | mfu: 52.05 | epoch: 2 | total time: 75.06m | eta: 1.7m\n",
      "step 01746/01785 (97.82%) | loss: 2.998513 | lrm: 0.04 | dt: 2593.41ms | tok/sec: 202,161 | mfu: 51.98 | epoch: 2 | total time: 75.11m | eta: 1.7m\n",
      "step 01747/01785 (97.87%) | loss: 2.995178 | lrm: 0.04 | dt: 2596.18ms | tok/sec: 201,945 | mfu: 51.92 | epoch: 2 | total time: 75.15m | eta: 1.6m\n",
      "step 01748/01785 (97.93%) | loss: 3.005679 | lrm: 0.04 | dt: 2596.85ms | tok/sec: 201,894 | mfu: 51.91 | epoch: 2 | total time: 75.19m | eta: 1.6m\n",
      "step 01749/01785 (97.98%) | loss: 2.998265 | lrm: 0.04 | dt: 2597.51ms | tok/sec: 201,842 | mfu: 51.89 | epoch: 2 | total time: 75.24m | eta: 1.6m\n",
      "Step 01750 | Validation bpb: 0.920791\n",
      "step 01750/01785 (98.04%) | loss: 2.998500 | lrm: 0.04 | dt: 2586.29ms | tok/sec: 202,718 | mfu: 52.12 | epoch: 2 | total time: 75.28m | eta: 1.5m\n",
      "step 01751/01785 (98.10%) | loss: 3.014491 | lrm: 0.04 | dt: 2591.55ms | tok/sec: 202,306 | mfu: 52.01 | epoch: 2 | total time: 75.32m | eta: 1.5m\n",
      "step 01752/01785 (98.15%) | loss: 3.007780 | lrm: 0.04 | dt: 2587.16ms | tok/sec: 202,649 | mfu: 52.10 | epoch: 2 | total time: 75.36m | eta: 1.4m\n",
      "step 01753/01785 (98.21%) | loss: 3.002492 | lrm: 0.04 | dt: 2582.41ms | tok/sec: 203,023 | mfu: 52.20 | epoch: 2 | total time: 75.41m | eta: 1.4m\n",
      "step 01754/01785 (98.26%) | loss: 2.997908 | lrm: 0.03 | dt: 2585.07ms | tok/sec: 202,813 | mfu: 52.14 | epoch: 2 | total time: 75.45m | eta: 1.3m\n",
      "step 01755/01785 (98.32%) | loss: 3.009782 | lrm: 0.03 | dt: 2587.66ms | tok/sec: 202,610 | mfu: 52.09 | epoch: 2 | total time: 75.49m | eta: 1.3m\n",
      "step 01756/01785 (98.38%) | loss: 3.018206 | lrm: 0.03 | dt: 2585.83ms | tok/sec: 202,754 | mfu: 52.13 | epoch: 2 | total time: 75.54m | eta: 1.3m\n",
      "step 01757/01785 (98.43%) | loss: 3.006506 | lrm: 0.03 | dt: 2586.26ms | tok/sec: 202,720 | mfu: 52.12 | epoch: 2 | total time: 75.58m | eta: 1.2m\n",
      "step 01758/01785 (98.49%) | loss: 3.010916 | lrm: 0.03 | dt: 2584.68ms | tok/sec: 202,844 | mfu: 52.15 | epoch: 2 | total time: 75.62m | eta: 1.2m\n",
      "step 01759/01785 (98.54%) | loss: 3.008547 | lrm: 0.03 | dt: 2602.99ms | tok/sec: 201,417 | mfu: 51.79 | epoch: 2 | total time: 75.67m | eta: 1.1m\n",
      "step 01760/01785 (98.60%) | loss: 3.015429 | lrm: 0.03 | dt: 2583.64ms | tok/sec: 202,926 | mfu: 52.17 | epoch: 2 | total time: 75.71m | eta: 1.1m\n",
      "step 01761/01785 (98.66%) | loss: 3.020863 | lrm: 0.03 | dt: 2594.59ms | tok/sec: 202,069 | mfu: 51.95 | epoch: 2 | total time: 75.75m | eta: 1.0m\n",
      "step 01762/01785 (98.71%) | loss: 3.012503 | lrm: 0.03 | dt: 2591.25ms | tok/sec: 202,330 | mfu: 52.02 | epoch: 2 | total time: 75.80m | eta: 1.0m\n",
      "step 01763/01785 (98.77%) | loss: 3.010375 | lrm: 0.02 | dt: 2591.65ms | tok/sec: 202,299 | mfu: 52.01 | epoch: 2 | total time: 75.84m | eta: 1.0m\n",
      "step 01764/01785 (98.82%) | loss: 3.000306 | lrm: 0.02 | dt: 2595.40ms | tok/sec: 202,006 | mfu: 51.94 | epoch: 2 | total time: 75.88m | eta: 0.9m\n",
      "step 01765/01785 (98.88%) | loss: 2.994018 | lrm: 0.02 | dt: 2589.70ms | tok/sec: 202,451 | mfu: 52.05 | epoch: 2 | total time: 75.93m | eta: 0.9m\n",
      "step 01766/01785 (98.94%) | loss: 2.981194 | lrm: 0.02 | dt: 2597.64ms | tok/sec: 201,832 | mfu: 51.89 | epoch: 2 | total time: 75.97m | eta: 0.8m\n",
      "step 01767/01785 (98.99%) | loss: 2.979592 | lrm: 0.02 | dt: 2604.67ms | tok/sec: 201,287 | mfu: 51.75 | epoch: 2 | total time: 76.01m | eta: 0.8m\n",
      "step 01768/01785 (99.05%) | loss: 2.968711 | lrm: 0.02 | dt: 2603.93ms | tok/sec: 201,344 | mfu: 51.77 | epoch: 2 | total time: 76.06m | eta: 0.7m\n",
      "step 01769/01785 (99.10%) | loss: 2.967948 | lrm: 0.02 | dt: 2604.87ms | tok/sec: 201,272 | mfu: 51.75 | epoch: 2 | total time: 76.10m | eta: 0.7m\n",
      "step 01770/01785 (99.16%) | loss: 2.964860 | lrm: 0.02 | dt: 2603.76ms | tok/sec: 201,358 | mfu: 51.77 | epoch: 2 | total time: 76.14m | eta: 0.6m\n",
      "step 01771/01785 (99.22%) | loss: 2.963686 | lrm: 0.02 | dt: 2588.89ms | tok/sec: 202,514 | mfu: 52.07 | epoch: 2 | total time: 76.19m | eta: 0.6m\n",
      "step 01772/01785 (99.27%) | loss: 2.968602 | lrm: 0.01 | dt: 2596.57ms | tok/sec: 201,915 | mfu: 51.91 | epoch: 2 | total time: 76.23m | eta: 0.6m\n",
      "step 01773/01785 (99.33%) | loss: 2.959121 | lrm: 0.01 | dt: 2603.35ms | tok/sec: 201,389 | mfu: 51.78 | epoch: 2 | total time: 76.27m | eta: 0.5m\n",
      "step 01774/01785 (99.38%) | loss: 2.973374 | lrm: 0.01 | dt: 2587.76ms | tok/sec: 202,603 | mfu: 52.09 | epoch: 2 | total time: 76.32m | eta: 0.5m\n",
      "step 01775/01785 (99.44%) | loss: 2.980543 | lrm: 0.01 | dt: 2604.08ms | tok/sec: 201,333 | mfu: 51.76 | epoch: 2 | total time: 76.36m | eta: 0.4m\n",
      "step 01776/01785 (99.50%) | loss: 2.980476 | lrm: 0.01 | dt: 2587.62ms | tok/sec: 202,614 | mfu: 52.09 | epoch: 2 | total time: 76.40m | eta: 0.4m\n",
      "step 01777/01785 (99.55%) | loss: 2.984276 | lrm: 0.01 | dt: 2588.53ms | tok/sec: 202,542 | mfu: 52.07 | epoch: 2 | total time: 76.45m | eta: 0.3m\n",
      "step 01778/01785 (99.61%) | loss: 2.978027 | lrm: 0.01 | dt: 2605.94ms | tok/sec: 201,189 | mfu: 51.73 | epoch: 2 | total time: 76.49m | eta: 0.3m\n",
      "step 01779/01785 (99.66%) | loss: 2.987058 | lrm: 0.01 | dt: 2590.49ms | tok/sec: 202,389 | mfu: 52.04 | epoch: 2 | total time: 76.53m | eta: 0.3m\n",
      "step 01780/01785 (99.72%) | loss: 2.985836 | lrm: 0.01 | dt: 2591.40ms | tok/sec: 202,318 | mfu: 52.02 | epoch: 2 | total time: 76.58m | eta: 0.2m\n",
      "step 01781/01785 (99.78%) | loss: 2.975252 | lrm: 0.00 | dt: 2589.56ms | tok/sec: 202,462 | mfu: 52.05 | epoch: 2 | total time: 76.62m | eta: 0.2m\n",
      "step 01782/01785 (99.83%) | loss: 2.974713 | lrm: 0.00 | dt: 2604.88ms | tok/sec: 201,271 | mfu: 51.75 | epoch: 2 | total time: 76.66m | eta: 0.1m\n",
      "step 01783/01785 (99.89%) | loss: 2.983276 | lrm: 0.00 | dt: 2587.17ms | tok/sec: 202,649 | mfu: 52.10 | epoch: 2 | total time: 76.70m | eta: 0.1m\n",
      "step 01784/01785 (99.94%) | loss: 2.981819 | lrm: 0.00 | dt: 2601.21ms | tok/sec: 201,555 | mfu: 51.82 | epoch: 2 | total time: 76.75m | eta: 0.0m\n",
      "Step 01785 | Validation bpb: 0.920155\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "while True:\n",
    "    last_step = step == num_iterations # loop runs num_iterations+1 times so that we can eval/save at the end\n",
    "    flops_so_far = num_flops_per_token * total_batch_size * step\n",
    "\n",
    "    # once in a while: evaluate the val bpb (all ranks participate)\n",
    "    if args.eval_every > 0 and (last_step or step % args.eval_every == 0):\n",
    "        model.eval()\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = args.eval_tokens // (args.device_batch_size * args.max_seq_len * ddp_world_size)\n",
    "        with disable_fp8(model), autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.6f}\")\n",
    "        if val_bpb < min_val_bpb:\n",
    "            min_val_bpb = val_bpb\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: estimate the CORE metric (all ranks participate)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    # disable FP8 for evaluation to use BF16 for more consistent/accurate results\n",
    "    results = {}\n",
    "    if args.core_metric_every > 0 and (last_step or (step > 0 and step % args.core_metric_every == 0)):\n",
    "        model.eval()\n",
    "        with disable_fp8(orig_model), autocast_ctx:\n",
    "            results = evaluate_core(orig_model, tokenizer, device, max_per_task=args.core_metric_max_per_task)\n",
    "        print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"core_metric\": results[\"core_metric\"],\n",
    "            \"centered_results\": results[\"centered_results\"],\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: sample from the model (only on master process)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    if args.sample_every > 0 and master_process and (last_step or (step > 0 and step % args.sample_every == 0)):\n",
    "        model.eval()\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(orig_model, tokenizer) # use orig_model to avoid recompilation\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with disable_fp8(orig_model), autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            print0(tokenizer.decode(sample[0]))\n",
    "        model.train()\n",
    "\n",
    "    # save checkpoint: at the end of the run, or every save_every steps, except at the first step or the resume step\n",
    "    if last_step or (step > 0 and step != args.resume_from_step and args.save_every > 0 and step % args.save_every == 0):\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(), # model parameters\n",
    "            optimizer.state_dict(), # optimizer state\n",
    "            { # metadata saved as json\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # loss at last step\n",
    "                \"model_config\": model_config_kwargs,\n",
    "                \"user_config\": user_config, # inputs to the training script\n",
    "                \"device_batch_size\": args.device_batch_size,\n",
    "                \"max_seq_len\": args.max_seq_len,\n",
    "                \"dataloader_state_dict\": dataloader_state_dict,\n",
    "                \"loop_state\": { # all loop state (other than step) so that we can resume training\n",
    "                    \"min_val_bpb\": min_val_bpb,\n",
    "                    \"smooth_train_loss\": smooth_train_loss,\n",
    "                    \"total_training_time\": total_training_time,\n",
    "                },\n",
    "            },\n",
    "            rank=ddp_rank,\n",
    "        )\n",
    "\n",
    "    # termination conditions (TODO: possibly also add loss explosions etc.)\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # single training step\n",
    "    # evaluate the gradient\n",
    "    synchronize()\n",
    "    t0 = time.time()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach() # for logging\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "        loss.backward()\n",
    "        x, y, dataloader_state_dict = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward\n",
    "    # step the optimizer\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    muon_weight_decay = get_weight_decay(step)\n",
    "    for group in optimizer.param_groups:\n",
    "        group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "        if group['kind'] == 'muon':\n",
    "            group[\"momentum\"] = muon_momentum\n",
    "            group[\"weight_decay\"] = muon_weight_decay\n",
    "    optimizer.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    train_loss_f = train_loss.item() # .item() is a CPU-GPU sync point\n",
    "    synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # logging (CPU action only)\n",
    "    ema_beta = 0.9 # EMA decay factor for some smoothing just for nicer logging\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss_f # EMA the training loss\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA\n",
    "    pct_done = 100 * step / num_iterations\n",
    "    tok_per_sec = int(total_batch_size / dt)\n",
    "    flops_per_sec = num_flops_per_token * total_batch_size / dt\n",
    "    mfu = 100 * flops_per_sec / (gpu_peak_flops * ddp_world_size)\n",
    "    if step > 10:\n",
    "        total_training_time += dt # only count the time after the first 10 steps\n",
    "    # Calculate ETA based on average time per step (excluding first 10 steps)\n",
    "    steps_done = step - 10\n",
    "    if steps_done > 0:\n",
    "        avg_time_per_step = total_training_time / steps_done\n",
    "        remaining_steps = num_iterations - step\n",
    "        eta_seconds = remaining_steps * avg_time_per_step\n",
    "        eta_str = f\" | eta: {eta_seconds/60:.1f}m\"\n",
    "    else:\n",
    "        eta_str = \"\"\n",
    "    epoch = dataloader_state_dict[\"epoch\"]\n",
    "    print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | epoch: {epoch} | total time: {total_training_time/60:.2f}m{eta_str}\")\n",
    "    if step % 100 == 0:\n",
    "        log_data = {\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "            \"train/epoch\": epoch,\n",
    "        }\n",
    "        wandb_run.log(log_data)\n",
    "\n",
    "    # state update\n",
    "    first_step_of_run = (step == 0) or (resuming and step == args.resume_from_step)\n",
    "    step += 1\n",
    "\n",
    "    # The garbage collector is sadly a little bit overactive and for some poorly understood reason,\n",
    "    # it spends ~500ms scanning for cycles quite frequently, just to end up cleaning up very few tiny objects each time.\n",
    "    # So we manually manage and help it out here\n",
    "    if first_step_of_run:\n",
    "        gc.collect() # manually collect a lot of garbage from setup\n",
    "        gc.freeze() # immediately freeze all currently surviving objects and exclude them from GC\n",
    "        gc.disable() # nuclear intervention here: disable GC entirely except:\n",
    "    elif step % 5000 == 0: # every 5000 steps...\n",
    "        gc.collect() # manually collect, just to be safe for very, very long runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory usage: 16366.70MiB\n",
      "Total training time: 76.75m\n",
      "Minimum validation bpb: 0.920155\n"
     ]
    }
   ],
   "source": [
    "# print a few more stats\n",
    "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\n",
    "print0(f\"Total training time: {total_training_time/60:.2f}m\")\n",
    "if val_bpb is not None:\n",
    "    print0(f\"Minimum validation bpb: {min_val_bpb:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/nanochat/report/base-model-training.md'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log to report\n",
    "from nanochat.report import get_report\n",
    "\n",
    "get_report().log(section=\"Base model training\", data=[\n",
    "    user_config, # CLI args\n",
    "    { # stats about the training setup\n",
    "        \"Number of parameters\": num_params,\n",
    "        \"Number of FLOPs per token\": f\"{num_flops_per_token:e}\",\n",
    "        \"Calculated number of iterations\": num_iterations,\n",
    "        \"Number of training tokens\": total_tokens,\n",
    "        \"Tokens : Scaling params ratio\": total_batch_size * num_iterations / num_scaling_params,\n",
    "        \"DDP world size\": ddp_world_size,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "        \"warmdown_ratio\": args.warmdown_ratio,\n",
    "        \"final_lr_frac\": args.final_lr_frac,\n",
    "    },\n",
    "    { # stats about training outcomes\n",
    "        \"Minimum validation bpb\": min_val_bpb if val_bpb is not None else None,\n",
    "        \"Final validation bpb\": val_bpb,\n",
    "        \"CORE metric estimate\": results.get(\"core_metric\", None),\n",
    "        \"MFU %\": f\"{mfu:.2f}%\",\n",
    "        \"Total training flops\": f\"{flops_so_far:e}\",\n",
    "        \"Total training time\": f\"{total_training_time/60:.2f}m\",\n",
    "        \"Peak memory usage\": f\"{get_max_memory() / 1024 / 1024:.2f}MiB\",\n",
    "    }\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Base model training\n",
       "timestamp: 2026-02-06 18:37:44\n",
       "\n",
       "- run: nanochat - d.768 - l.12 - v1.0\n",
       "- device_type: \n",
       "- fp8: False\n",
       "- fp8_recipe: tensorwise\n",
       "- depth: 12\n",
       "- aspect_ratio: 64\n",
       "- head_dim: 128\n",
       "- max_seq_len: 2048\n",
       "- window_pattern: SSSL\n",
       "- num_iterations: -1\n",
       "- target_flops: -1.0000\n",
       "- target_param_data_ratio: 8.5000\n",
       "- device_batch_size: 16\n",
       "- total_batch_size: -1\n",
       "- embedding_lr: 0.3000\n",
       "- unembedding_lr: 0.0040\n",
       "- weight_decay: 0.2000\n",
       "- matrix_lr: 0.0200\n",
       "- scalar_lr: 0.5000\n",
       "- adam_beta1: 0.8000\n",
       "- adam_beta2: 0.9500\n",
       "- warmup_ratio: 0.0000\n",
       "- warmdown_ratio: 0.5000\n",
       "- final_lr_frac: 0.0000\n",
       "- resume_from_step: -1\n",
       "- eval_every: 250\n",
       "- eval_tokens: 20,971,520\n",
       "- core_metric_every: -1\n",
       "- core_metric_max_per_task: 500\n",
       "- sample_every: -1\n",
       "- save_every: -1\n",
       "- model_tag: None\n",
       "- Number of parameters: 286,262,424\n",
       "- Number of FLOPs per token: 8.021676e+08\n",
       "- Calculated number of iterations: 1785\n",
       "- Number of training tokens: 935,854,080\n",
       "- Tokens : Scaling params ratio: 8.4999\n",
       "- DDP world size: 1\n",
       "- warmup_ratio: 0.0000\n",
       "- warmdown_ratio: 0.5000\n",
       "- final_lr_frac: 0.0000\n",
       "- Minimum validation bpb: 0.9202\n",
       "- Final validation bpb: 0.9202\n",
       "- CORE metric estimate: None\n",
       "- MFU %: 51.82%\n",
       "- Total training flops: 7.507118e+17\n",
       "- Total training time: 76.75m\n",
       "- Peak memory usage: 16366.70MiB\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import markdown and display it\n",
    "from IPython.display import display, Markdown\n",
    "with open(\"/root/.cache/nanochat/report/base-model-training.md\", 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>total_training_flops</td><td>â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>total_training_time</td><td>â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/dt</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/loss</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lrm</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–„â–ƒâ–‚â–</td></tr><tr><td>train/mfu</td><td>â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/tok_per_sec</td><td>â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val/bpb</td><td>â–ˆâ–‚â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>1785</td></tr><tr><td>total_training_flops</td><td>750711776382812160</td></tr><tr><td>total_training_time</td><td>4604.89304</td></tr><tr><td>train/dt</td><td>2.60504</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>3.01</td></tr><tr><td>train/lrm</td><td>0.09529</td></tr><tr><td>train/mfu</td><td>51.74473</td></tr><tr><td>train/tok_per_sec</td><td>201259</td></tr><tr><td>val/bpb</td><td>0.92016</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanochat - d.768 - l.12 - v1.0</strong> at: <a href='https://wandb.ai/chrismccormick/nanochat/runs/wxer12y6' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat/runs/wxer12y6</a><br> View project at: <a href='https://wandb.ai/chrismccormick/nanochat' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260206_171130-wxer12y6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cleanup\n",
    "wandb_run.finish() # wandb run finish\n",
    "compute_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From speedrun.sh:\n",
    "```python\n",
    "# evaluate the model: CORE metric, BPB on train/val, and draw samples\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.base_eval -- --device-batch-size=16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `base_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Base model evaluation\")\n",
    "# parser.add_argument('--eval', type=str, default='core,bpb,sample', help='Comma-separated evaluations to run: core,bpb,sample (default: all)')\n",
    "# parser.add_argument('--hf-path', type=str, default=None, help='HuggingFace model path (e.g. openai-community/gpt2-xl)')\n",
    "# parser.add_argument('--model-tag', type=str, default=None, help='nanochat model tag to identify the checkpoint directory')\n",
    "# parser.add_argument('--step', type=int, default=None, help='Model step to load (default = last)')\n",
    "# parser.add_argument('--max-per-task', type=int, default=-1, help='Max examples per CORE task (-1 = all)')\n",
    "# parser.add_argument('--device-batch-size', type=int, default=32, help='Per-device batch size for BPB evaluation')\n",
    "# parser.add_argument('--split-tokens', type=int, default=40*524288, help='Number of tokens to evaluate per split for BPB')\n",
    "# parser.add_argument('--device-type', type=str, default='', help='cuda|cpu|mps (empty = autodetect)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    eval = \"core,bpb,sample\",\n",
    "    hf_path = None,\n",
    "    model_tag = None,\n",
    "    step = None,\n",
    "    max_per_task = -1,\n",
    "    device_batch_size = 32,\n",
    "    split_tokens = 40*524288,\n",
    "    device_type = \"\",\n",
    ")\n",
    "\n",
    "# def main()\n",
    "\n",
    "# Parse evaluation modes\n",
    "eval_modes = set(mode.strip() for mode in args.eval.split(','))\n",
    "valid_modes = {'core', 'bpb', 'sample'}\n",
    "invalid = eval_modes - valid_modes\n",
    "if invalid:\n",
    "    parser.error(f\"Invalid eval modes: {invalid}. Valid: {valid_modes}\")\n",
    "\n",
    "# Distributed / precision setup\n",
    "device_type = autodetect_device_type() if args.device_type == '' else args.device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: base_model (step 1785)\n",
      "Eval modes: bpb, core, sample\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "is_hf_model = args.hf_path is not None\n",
    "if is_hf_model:\n",
    "    model, tokenizer = load_hf_model(args.hf_path, device)\n",
    "    sequence_len = model.max_seq_len or 1024\n",
    "    token_bytes = get_hf_token_bytes(tokenizer, device=device)\n",
    "    model_name = args.hf_path\n",
    "    model_slug = args.hf_path.replace(\"/\", \"-\")\n",
    "else:\n",
    "    model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n",
    "    sequence_len = meta[\"model_config\"][\"sequence_len\"]\n",
    "    token_bytes = get_token_bytes(device=device)\n",
    "    model_name = f\"base_model (step {meta['step']})\"\n",
    "    model_slug = f\"base_model_{meta['step']:06d}\"\n",
    "\n",
    "print0(f\"Evaluating model: {model_name}\")\n",
    "print0(f\"Eval modes: {', '.join(sorted(eval_modes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results to log\n",
    "core_results = None\n",
    "bpb_results = {}\n",
    "samples = []\n",
    "unconditioned_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Model Samples\n",
      "================================================================================\n",
      "\n",
      "Conditioned samples:\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The capital of France is Paris. The capital of France is Paris. The capital of France is Paris.\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The chemical symbol of gold is gold. It is the symbol of the purity of the gold that is the basis\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today was Saturday, then tomorrow will be Saturday. If today was\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The opposite of hot is cold. It is a state of being in a state of being in a state\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>My favorite color is red. I love it because it is the color of the blood. It is\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>If 5*x + 3 = 13, then x is 5*x + 3 = 13. If 5*x\n",
      "\n",
      "Unconditioned samples:\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The Summary in Parashat Châ€™an was written in the first century of the eighth century, 3-5 by a Jew who opposed Shaitan in the Volga land region, and refers to a bi-pica-the works of that Jew.\n",
      "Young men, while being kept out of Mosul, were subsequently cast out of their tribe, looting and exterminating them and their families if they entered the Mosul realm. The one who defeated them, called Thareus was a Saoram, a people who lived outside Mosul country and residing in Nizirat, or Oxus-sur-ware camp by\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>New York Times, April 11, 2019\n",
      "Parents at The Mother Of The State Of Connecticut\n",
      "The Followbreak Of The Connecticut River Rage\n",
      "|Safety is our greatest asset||Litchfield SW Investment\n",
      "New Delhi, 19 November 2019\n",
      "Every second day, Connecticut's courts sense an intrusigable, transnational threat in the form of a cargo ship carrying homemade ammunition into the harborâ€”partly one of this second wave of rebellions. By the dawn of July of 1917, the plain was protected by the protection of the forts made in the lower Potomac River and South Conroe\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>In summary, we can discover a wide variety of events overlap in the life of a person from the moment we emerge from the vegetative state to the actual occasion and vâ€™s ever so rare and rare is the â€˜actionâ€™ of one of creator. The clandestine function that the xenophobic and atheistic desires of xenophobic neurosis that began as a reaction of the nervous in the wake of the coexistence of Aryan and animism is highly worth studying. Battles of xenophobia are extremely diverse cases in which type 1 xenophobic xenophobia and xenophobic animism have been brought on amorous.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Civil war 100Â¢ SixtiesÂ¢ The beginning of the Civil War and its full extent of the controversy are selected by a searchable series of photographs from the Famous Women in the United States Consolidated Dictionary. First issued on March 3, 1912, this issue provides a short overview of the scope and contents of the War and the 1912 National Guard, started as a result of human error, mostly by members of the 99th Massachusetts Sen. Deborah P. Warren, R-San Francisco, Robert H. Bradley, Buzia B. Ashton, George.\n",
      "During the entire \n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Dialysis is an annual or semi-permanent exfoliating condition in which high fluid persists over time. Medication can be used to treat complications.\n",
      "Dialysis, which is also referred to as \"dislocation\" is performed to help tissues deal with various diseases\n",
      "Hair: Situational Phenomena\n",
      "- Toe: During Koi injury\n",
      "- Stone: When Cystoscopy\n",
      "- Medial: After sporulation of Mare Mare\n",
      "- Blood: Toe: After treatment of Malmosis if Single\n",
      "- Ventidle: During periodic hemoplasty\n",
      "- Fetus: As\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Freedom of association. â€œ. â‡’ in the bag implies organized human groups, and from time to time the permitted guests take whatever they need, a safe gathering of bars or america1â€œ.\n",
      "Freedom of speech. \". â‡’ in the bag implies symbolic, notionally ambiguous, or ambiguous communication; to speak. â‡’ out of the bag implies freedom of information, press or speech2.â€ â€” In addition to the word â€œlibâ€ (we reflexively avoid it) in the verb â€œto frame,â€ many other objective pronoun terms occur throughout the construction of construction. The construction of speech involves the constructions of\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>SANEYLETT - Every few months we get evasive statements regarding radioactive isotope classification. The answer is, \"No\". A huge waste of dollars and time is spent on IA by nuclear programs and radioactive materials like mining. As well as secret regulations and nuclear weapons, meanwhile, there are now many unanswered scientific questions, affirmed at the comprehensive secret meetings held at over 90 meetings of the TUERNEC event. Scientific questions are a strong example of a societal challenge for travelers. If you haven't visited a country with half a century of nuclear weapons, chances are you have to pay a visit some distant country\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>By: Anna Matthews\n",
      "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
      "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
      "Regular Christmas everyoneâ€™s got the buggy trousers\n",
      "Unforge is an instruction card and recondition. Weâ€™ve got both esters\n",
      "and does not target.\n",
      "Unforge not only makes the card more versatile but\n",
      "effects the card value 148.1 Ã— 8.1.\n",
      "This 148.1 is an unforge sufficiently recipe card\n",
      "and is larger, so all other cards rely on it. See\n",
      "the question step below\n"
     ]
    }
   ],
   "source": [
    "# --- Sampling ---\n",
    "if 'sample' in eval_modes and not is_hf_model:\n",
    "    print0(\"\\n\" + \"=\"*80)\n",
    "    print0(\"Model Samples\")\n",
    "    print0(\"=\"*80)\n",
    "    if ddp_rank == 0:\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(model, tokenizer)\n",
    "        print0(\"\\nConditioned samples:\")\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            sample_str = tokenizer.decode(sample[0])\n",
    "            print0(\"-\" * 80)\n",
    "            print0(sample_str)\n",
    "            samples.append(sample_str)\n",
    "\n",
    "        print0(\"\\nUnconditioned samples:\")\n",
    "        tokens = tokenizer(\"\", prepend=\"<|bos|>\")\n",
    "        with autocast_ctx:\n",
    "            uncond, _ = engine.generate_batch(tokens, num_samples=8, max_tokens=128, temperature=1.0)\n",
    "        for sample in uncond:\n",
    "            sample_str = tokenizer.decode(sample)\n",
    "            print0(\"-\" * 80)\n",
    "            print0(sample_str)\n",
    "            unconditioned_samples.append(sample_str)\n",
    "elif 'sample' in eval_modes and is_hf_model:\n",
    "    print0(\"\\nSkipping sampling for HuggingFace models (not supported)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits-per-Byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BPB Evaluation\n",
      "================================================================================\n",
      "train bpb: 0.873283\n",
      "val bpb: 0.920166\n"
     ]
    }
   ],
   "source": [
    "# --- BPB evaluation ---\n",
    "if 'bpb' in eval_modes:\n",
    "    print0(\"\\n\" + \"=\"*80)\n",
    "    print0(\"BPB Evaluation\")\n",
    "    print0(\"=\"*80)\n",
    "    tokens_per_step = args.device_batch_size * sequence_len * ddp_world_size\n",
    "    if args.split_tokens % tokens_per_step != 0:\n",
    "        # Adjust to nearest multiple\n",
    "        args.split_tokens = (args.split_tokens // tokens_per_step) * tokens_per_step\n",
    "        print0(f\"Adjusted split_tokens to {args.split_tokens} (must be divisible by {tokens_per_step})\")\n",
    "    steps = args.split_tokens // tokens_per_step\n",
    "\n",
    "    for split_name in [\"train\", \"val\"]:\n",
    "        loader = tokenizing_distributed_data_loader_bos_bestfit(tokenizer, args.device_batch_size, sequence_len, split_name, device=device)\n",
    "        with autocast_ctx:\n",
    "            bpb = evaluate_bpb(model, loader, steps, token_bytes)\n",
    "        bpb_results[split_name] = bpb\n",
    "        print0(f\"{split_name} bpb: {bpb:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–¶ Full CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "================================================================================\n",
      "CORE Evaluation\n",
      "================================================================================\n",
      "Downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
      "Downloaded to /root/.cache/nanochat/eval_bundle.zip\n",
      "Placed eval_bundle directory at /root/.cache/nanochat/eval_bundle\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3181 | centered: 0.0908 | time: 172.33s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0061 | centered: 0.0061 | time: 37.27s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.3038 | centered: 0.3038 | time: 360.42s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.4996 | centered: 0.3328 | time: 46.64s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2577 | centered: 0.0102 | time: 23.31s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6000 | centered: 0.2000 | time: 1.62s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2867 | centered: 0.1083 | time: 24.78s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6219 | centered: 0.2437 | time: 33.53s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3040 | centered: 0.0720 | time: 8.28s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.2666 | centered: 0.2666 | time: 84.94s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.3208 | centered: 0.0943 | time: 241.19s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.5678 | centered: 0.1355 | time: 4.40s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5185 | centered: 0.0371 | time: 20.35s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1110 | centered: 0.1110 | time: 18.11s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2696 | centered: 0.0870 | time: 5.39s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.4023 | centered: 0.4023 | time: 23.19s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1381 | centered: 0.1381 | time: 3.62s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.56s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0559 | centered: 0.0559 | time: 235.42s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1110 | centered: 0.1110 | time: 140.37s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5309 | centered: -0.2345 | time: 76.99s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2552 | centered: 0.1806 | time: 351.79s\n",
      "\n",
      "Results written to: /root/.cache/nanochat/base_eval/base_model_001785.csv\n",
      "CORE metric: 0.1251\n",
      "\n",
      "\n",
      "Total CORE eval time: 1918s\n"
     ]
    }
   ],
   "source": [
    "full_eval_t0 = time.time()\n",
    "\n",
    "# --- CORE evaluation ---\n",
    "if 'core' in eval_modes:\n",
    "    print0(\" \\n\" + \"=\"*80)\n",
    "    print0(\"CORE Evaluation\")\n",
    "    print0(\"=\"*80)\n",
    "    with autocast_ctx:\n",
    "        core_results = evaluate_core(model, tokenizer, device, max_per_task=args.max_per_task)\n",
    "\n",
    "    # Write CSV output\n",
    "    if ddp_rank == 0:\n",
    "        base_dir = get_base_dir()\n",
    "        output_csv_path = os.path.join(base_dir, \"base_eval\", f\"{model_slug}.csv\")\n",
    "        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "        with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            f.write(f\"{'Task':<35}, {'Accuracy':<10}, {'Centered':<10}\\n\")\n",
    "            for label in core_results[\"results\"]:\n",
    "                acc = core_results[\"results\"][label]\n",
    "                centered = core_results[\"centered_results\"][label]\n",
    "                f.write(f\"{label:<35}, {acc:<10.6f}, {centered:<10.6f}\\n\")\n",
    "            f.write(f\"{'CORE':<35}, {'':<10}, {core_results['core_metric']:<10.6f}\\n\")\n",
    "        print0(f\"\\nResults written to: {output_csv_path}\")\n",
    "        print0(f\"CORE metric: {core_results['core_metric']:.4f}\")\n",
    "\n",
    "print(f\"\\n\\nTotal CORE eval time: {time.time() - full_eval_t0:.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Log to report ---\n",
    "from nanochat.report import get_report\n",
    "report_data = [{\"model\": model_name}]\n",
    "\n",
    "if core_results:\n",
    "    report_data[0][\"CORE metric\"] = core_results[\"core_metric\"]\n",
    "    report_data.append(core_results[\"centered_results\"])\n",
    "\n",
    "if bpb_results:\n",
    "    report_data[0][\"train bpb\"] = bpb_results.get(\"train\")\n",
    "    report_data[0][\"val bpb\"] = bpb_results.get(\"val\")\n",
    "\n",
    "if samples:\n",
    "    report_data.append({f\"sample {i}\": s for i, s in enumerate(samples)})\n",
    "if unconditioned_samples:\n",
    "    report_data.append({f\"unconditioned {i}\": s for i, s in enumerate(unconditioned_samples)})\n",
    "\n",
    "get_report().log(section=\"Base model evaluation\", data=report_data)\n",
    "\n",
    "compute_cleanup()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ff41e05b-df96-46ad-aab2-8bd5339ea73a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Centered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hellaswag_zeroshot</td>\n",
       "      <td>0.318064</td>\n",
       "      <td>0.090752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeopardy</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigbench_qa_wikidata</td>\n",
       "      <td>0.303774</td>\n",
       "      <td>0.303774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arc_easy</td>\n",
       "      <td>0.499579</td>\n",
       "      <td>0.332772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.010239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>copa</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>commonsense_qa</td>\n",
       "      <td>0.286650</td>\n",
       "      <td>0.108313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>piqa</td>\n",
       "      <td>0.621872</td>\n",
       "      <td>0.243743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>openbook_qa</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>0.266641</td>\n",
       "      <td>0.266641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hellaswag</td>\n",
       "      <td>0.320753</td>\n",
       "      <td>0.094337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>winograd</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>0.135531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>winogrande</td>\n",
       "      <td>0.518548</td>\n",
       "      <td>0.037095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bigbench_dyck_languages</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>agi_eval_lsat_ar</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.086956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bigbench_cs_algorithms</td>\n",
       "      <td>0.402273</td>\n",
       "      <td>0.402273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bigbench_operators</td>\n",
       "      <td>0.138095</td>\n",
       "      <td>0.138095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bigbench_repeat_copy_logic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squad</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.055913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coqa</td>\n",
       "      <td>0.110986</td>\n",
       "      <td>0.110986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>boolq</td>\n",
       "      <td>0.530887</td>\n",
       "      <td>-0.234508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bigbench_language_identification</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.180638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CORE</td>\n",
       "      <td></td>\n",
       "      <td>0.125122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff41e05b-df96-46ad-aab2-8bd5339ea73a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ff41e05b-df96-46ad-aab2-8bd5339ea73a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ff41e05b-df96-46ad-aab2-8bd5339ea73a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_1d680b99-340c-47a3-9fde-1ac28dbfe505\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_1d680b99-340c-47a3-9fde-1ac28dbfe505 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Task                                  Accuracy     Centered  \n",
       "0   hellaswag_zeroshot                    0.318064       0.090752\n",
       "1   jeopardy                              0.006141       0.006141\n",
       "2   bigbench_qa_wikidata                  0.303774       0.303774\n",
       "3   arc_easy                              0.499579       0.332772\n",
       "4   arc_challenge                         0.257679       0.010239\n",
       "5   copa                                  0.600000       0.200000\n",
       "6   commonsense_qa                        0.286650       0.108313\n",
       "7   piqa                                  0.621872       0.243743\n",
       "8   openbook_qa                           0.304000       0.072000\n",
       "9   lambada_openai                        0.266641       0.266641\n",
       "10  hellaswag                             0.320753       0.094337\n",
       "11  winograd                              0.567766       0.135531\n",
       "12  winogrande                            0.518548       0.037095\n",
       "13  bigbench_dyck_languages               0.111000       0.111000\n",
       "14  agi_eval_lsat_ar                      0.269565       0.086956\n",
       "15  bigbench_cs_algorithms                0.402273       0.402273\n",
       "16  bigbench_operators                    0.138095       0.138095\n",
       "17  bigbench_repeat_copy_logic            0.000000       0.000000\n",
       "18  squad                                 0.055913       0.055913\n",
       "19  coqa                                  0.110986       0.110986\n",
       "20  boolq                                 0.530887      -0.234508\n",
       "21  bigbench_language_identification      0.255200       0.180638\n",
       "22  CORE                                                 0.125122"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/root/.cache/nanochat/base_eval/base_model_001785.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From speedrun.sh:\n",
    "```python\n",
    "# evaluate the model: CORE metric, BPB on train/val, and draw samples\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.base_eval -- --device-batch-size=16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for evaluating the CORE metric, as described in the DCLM paper.\n",
    "https://arxiv.org/abs/2406.11794\n",
    "\n",
    "TODOs:\n",
    "- All tasks ~match except for squad. We get 31% reference is 37%. Figure out why.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "from jinja2 import Template\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Prompt rendering utilities\n",
    "\n",
    "def render_prompts_mc(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"Render complete prompts for a multiple choice question\"\"\"\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.query }}{{ continuation_delimiter }}{{ example.choices[example.gold] }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ item.query }}{{ continuation_delimiter }}{{ choice }}\"\"\".strip()\n",
    "    template = Template(template_str)\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "    prompts = [template.render(choice=choice, **context) for choice in item['choices']]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def render_prompts_schema(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"Render complete prompts for a schema question\"\"\"\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.context_options[example.gold] }}{{ continuation_delimiter }}{{ example.continuation }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ context }}{{ continuation_delimiter }}{{ item.continuation }}\"\"\".strip()\n",
    "    template = Template(template_str)\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "    prompts = [template.render(context=context_option, **context)\n",
    "               for context_option in item['context_options']]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def render_prompts_lm(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"\n",
    "    Render complete prompt for a language modeling task.\n",
    "    Notice that we manually trim the context in the template,\n",
    "    which in some datasets seems to have trailing whitespace (which we don't want).\n",
    "    \"\"\"\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.context | trim }}{{ continuation_delimiter }}{{ example.continuation }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ item.context | trim }}{{ continuation_delimiter }}{% if include_continuation %}{{ item.continuation }}{% endif %}\"\"\".strip()\n",
    "    template = Template(template_str)\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "    # Return two prompts: without and with the continuation\n",
    "    prompt_without = template.render(include_continuation=False, **context)\n",
    "    prompt_with = template.render(include_continuation=True, **context)\n",
    "    # Due to the way the data seems to be stored, I think I need to strip in the case of LM here.\n",
    "    # Otherwise we may get trailing whitespaces in prompt_without (which get absorbed into the next\n",
    "    # token in prompt_with), meaning we don't get a nice and clean prefix in the token space\n",
    "    # to detect the final continuation. Tokenizers...\n",
    "    prompt_without = prompt_without.strip()\n",
    "    return [prompt_without, prompt_with]\n",
    "\n",
    "\n",
    "def find_common_length(token_sequences, direction='left'):\n",
    "    \"\"\"\n",
    "    Find the length of the common prefix or suffix across token sequences\n",
    "    - direction: 'left' for prefix, 'right' for suffix\n",
    "    \"\"\"\n",
    "    min_len = min(len(seq) for seq in token_sequences)\n",
    "    indices = {\n",
    "        'left': range(min_len),\n",
    "        'right': range(-1, -min_len-1, -1)\n",
    "    }[direction]\n",
    "    # Find the first position where the token sequences differ\n",
    "    for i, idx in enumerate(indices):\n",
    "        token = token_sequences[0][idx]\n",
    "        if not all(seq[idx] == token for seq in token_sequences):\n",
    "            return i\n",
    "    return min_len\n",
    "\n",
    "\n",
    "def stack_sequences(tokens, pad_token_id):\n",
    "    \"\"\"Stack up a list of token sequences, pad to longest on the right\"\"\"\n",
    "    bsz, seq_len = len(tokens), max(len(x) for x in tokens)\n",
    "    input_ids = torch.full((bsz, seq_len), pad_token_id, dtype=torch.long)\n",
    "    for i, x in enumerate(tokens):\n",
    "        input_ids[i, :len(x)] = torch.tensor(x, dtype=torch.long)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def batch_sequences_mc(tokenizer, prompts):\n",
    "    # In multiple choice, contexts are the same but the continuation is different (common prefix)\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "    # figure out the start and end of each continuation\n",
    "    answer_start_idx = find_common_length(tokens, direction='left')\n",
    "    start_indices = [answer_start_idx] * len(prompts)\n",
    "    end_indices = [len(x) for x in tokens]\n",
    "    return tokens, start_indices, end_indices\n",
    "\n",
    "\n",
    "def batch_sequences_schema(tokenizer, prompts):\n",
    "    # In schema tasks, contexts vary but continuation is the same (common suffix)\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "    # figure out the start and end of each context\n",
    "    suffix_length = find_common_length(tokens, direction='right')\n",
    "    end_indices = [len(x) for x in tokens]\n",
    "    start_indices = [ei - suffix_length for ei in end_indices]\n",
    "    return tokens, start_indices, end_indices\n",
    "\n",
    "\n",
    "def batch_sequences_lm(tokenizer, prompts):\n",
    "    # In LM tasks, we have two prompts: without and with continuation\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "    tokens_without, tokens_with = tokens\n",
    "    start_idx, end_idx = len(tokens_without), len(tokens_with)\n",
    "    assert start_idx < end_idx, \"prompt without is supposed to be a prefix of prompt with\"\n",
    "    assert tokens_without == tokens_with[:start_idx], \"prompt without is supposed to be a prefix of prompt with\"\n",
    "    # we only need the with continuation prompt in the LM task, i.e. batch size of 1\n",
    "    return [tokens_with], [start_idx], [end_idx]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_model(model, input_ids):\n",
    "    \"\"\"\n",
    "    Take BxT tensor of token ids, return BxT tensor of losses and argmax predictions.\n",
    "    The last column of losses is set to nan because we don't have autoregressive targets there.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.size()\n",
    "    outputs = model(input_ids)\n",
    "    # Roll the tensor to the left by one position to get the (autoregressive) target ids\n",
    "    target_ids = torch.roll(input_ids, shifts=-1, dims=1)\n",
    "    # Calculate cross entropy at all positions\n",
    "    losses = torch.nn.functional.cross_entropy(\n",
    "        outputs.view(batch_size * seq_len, -1),\n",
    "        target_ids.view(batch_size * seq_len),\n",
    "        reduction='none'\n",
    "    ).view(batch_size, seq_len)\n",
    "    # Set the last column to be nan because there is no autoregressive loss there\n",
    "    losses[:, -1] = float('nan')\n",
    "    # Get the argmax predictions at each position\n",
    "    predictions = outputs.argmax(dim=-1)\n",
    "    return losses, predictions\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_task(model, tokenizer, data, device, task_meta, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate one task across many examples with batched forward passes.\n",
    "    Handles dispatch to all processes if the script is run with torchrun.\n",
    "    \"\"\"\n",
    "    task_type = task_meta['task_type']\n",
    "    num_fewshot = task_meta['num_fewshot']\n",
    "    continuation_delimiter = task_meta['continuation_delimiter']\n",
    "    rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    max_seq_len = getattr(model, 'max_seq_len', None)\n",
    "\n",
    "    # ---- Phase 1: Pre-process all examples assigned to this rank ----\n",
    "    # Build a flat list of sequences to forward and metadata to map them back to examples\n",
    "    sequences = []   # (tokens, start_idx, end_idx) per sequence\n",
    "    example_info = [] # (global_idx, gold, num_seqs, seq_offset) per example\n",
    "    my_indices = list(range(rank, len(data), world_size))\n",
    "\n",
    "    for global_idx in my_indices:\n",
    "        item = data[global_idx]\n",
    "\n",
    "        # Sample few-shot examples (excluding current item)\n",
    "        fewshot_examples = []\n",
    "        if num_fewshot > 0:\n",
    "            rng = random.Random(1234 + global_idx)\n",
    "            available_indices = [i for i in range(len(data)) if i != global_idx]\n",
    "            fewshot_indices = rng.sample(available_indices, num_fewshot)\n",
    "            fewshot_examples = [data[i] for i in fewshot_indices]\n",
    "\n",
    "        # Render prompts and tokenize based on task type\n",
    "        if task_type == 'multiple_choice':\n",
    "            prompts = render_prompts_mc(item, continuation_delimiter, fewshot_examples)\n",
    "            tokens, start_idxs, end_idxs = batch_sequences_mc(tokenizer, prompts)\n",
    "        elif task_type == 'schema':\n",
    "            prompts = render_prompts_schema(item, continuation_delimiter, fewshot_examples)\n",
    "            tokens, start_idxs, end_idxs = batch_sequences_schema(tokenizer, prompts)\n",
    "        elif task_type == 'language_modeling':\n",
    "            prompts = render_prompts_lm(item, continuation_delimiter, fewshot_examples)\n",
    "            tokens, start_idxs, end_idxs = batch_sequences_lm(tokenizer, prompts)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "        # Some models can't forward sequences beyond a certain length (e.g. GPT-2)\n",
    "        # In these cases, we have to truncate sequences to max length and adjust the indices\n",
    "        if max_seq_len is not None:\n",
    "            new_tokens, new_start_idxs, new_end_idxs = [], [], []\n",
    "            for t, s, e in zip(tokens, start_idxs, end_idxs):\n",
    "                if len(t) > max_seq_len:\n",
    "                    crop = len(t) - max_seq_len\n",
    "                    new_tokens.append(t[-max_seq_len:])  # take the last max_tokens tokens\n",
    "                    new_start_idxs.append(s - crop) # shift the indices down\n",
    "                    new_end_idxs.append(e - crop)\n",
    "                    assert s - crop >= 0, \"this should never happen right?\"\n",
    "                    assert e - crop >= 0, \"this should never happen right?\"\n",
    "                else:\n",
    "                    new_tokens.append(t)\n",
    "                    new_start_idxs.append(s)\n",
    "                    new_end_idxs.append(e)\n",
    "            tokens, start_idxs, end_idxs = new_tokens, new_start_idxs, new_end_idxs\n",
    "\n",
    "        # Record this example's sequences\n",
    "        seq_offset = len(sequences)\n",
    "        for t, si, ei in zip(tokens, start_idxs, end_idxs):\n",
    "            sequences.append((t, si, ei))\n",
    "        example_info.append((global_idx, item.get('gold', None), len(tokens), seq_offset))\n",
    "\n",
    "    # ---- Phase 2: Forward all sequences through the model in batches ----\n",
    "    # Sort sequences by length so similar-length sequences are batched together,\n",
    "    # minimizing padding waste. Also dynamically cap batch sizes: the padded token\n",
    "    # count (B * max_T) in each batch must stay under a budget to avoid OOM on the\n",
    "    # logits tensor (B * T * vocab_size).\n",
    "    pad_token_id = tokenizer.get_bos_token_id()\n",
    "    seq_results = [None] * len(sequences)\n",
    "    sorted_order = sorted(range(len(sequences)), key=lambda i: len(sequences[i][0]))\n",
    "    max_bt = batch_size * 512  # B*T token budget (controls peak GPU memory)\n",
    "\n",
    "    # Greedily build batches that respect both the sequence count and token budget\n",
    "    batches = []\n",
    "    cur_batch, cur_max_len = [], 0\n",
    "    for idx in sorted_order:\n",
    "        seq_len = len(sequences[idx][0])\n",
    "        new_max_len = max(cur_max_len, seq_len)\n",
    "        if cur_batch and ((len(cur_batch) + 1) * new_max_len > max_bt or len(cur_batch) >= batch_size):\n",
    "            batches.append(cur_batch)\n",
    "            cur_batch, cur_max_len = [idx], seq_len\n",
    "        else:\n",
    "            cur_batch.append(idx)\n",
    "            cur_max_len = new_max_len\n",
    "    if cur_batch:\n",
    "        batches.append(cur_batch)\n",
    "\n",
    "    for batch_indices in batches:\n",
    "        tokens_list = [sequences[idx][0] for idx in batch_indices]\n",
    "        input_ids = stack_sequences(tokens_list, pad_token_id).to(device)\n",
    "        # Forward the model, get the autoregressive loss and argmax prediction at each token\n",
    "        losses, predictions = forward_model(model, input_ids)\n",
    "\n",
    "        # See if the losses/predictions come out correctly\n",
    "        for j, idx in enumerate(batch_indices):\n",
    "            _, si, ei = sequences[idx]\n",
    "            if task_type == 'language_modeling':\n",
    "                # predictions[i] predict input_ids[i+1] autoregressively\n",
    "                predicted = predictions[j, si-1:ei-1]\n",
    "                actual = input_ids[j, si:ei]\n",
    "                seq_results[idx] = torch.all(predicted == actual).item()\n",
    "            else:  # multiple_choice or schema\n",
    "                # For MC/schema: find the option with lowest average loss\n",
    "                seq_results[idx] = losses[j, si-1:ei-1].mean().item()\n",
    "\n",
    "    # ---- Phase 3: Aggregate per-example correctness ----\n",
    "    correct = torch.zeros(len(data), dtype=torch.float32, device=device)\n",
    "    for global_idx, gold, num_seqs, offset in example_info:\n",
    "        if task_type == 'language_modeling':\n",
    "            is_correct = seq_results[offset]\n",
    "        else:\n",
    "            choice_losses = [seq_results[offset + c] for c in range(num_seqs)]\n",
    "            pred_idx = choice_losses.index(min(choice_losses))\n",
    "            is_correct = (pred_idx == gold)\n",
    "        correct[global_idx] = float(is_correct)\n",
    "\n",
    "    # ---- Phase 4: Sync results across ranks if distributed ----\n",
    "    if world_size > 1:\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(correct, op=dist.ReduceOp.SUM)\n",
    "    # compute the mean\n",
    "    mean_correct = correct.mean().item()\n",
    "    return mean_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `base_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Base model evaluation\")\n",
    "# parser.add_argument('--eval', type=str, default='core,bpb,sample', help='Comma-separated evaluations to run: core,bpb,sample (default: all)')\n",
    "# parser.add_argument('--hf-path', type=str, default=None, help='HuggingFace model path (e.g. openai-community/gpt2-xl)')\n",
    "# parser.add_argument('--model-tag', type=str, default=None, help='nanochat model tag to identify the checkpoint directory')\n",
    "# parser.add_argument('--step', type=int, default=None, help='Model step to load (default = last)')\n",
    "# parser.add_argument('--max-per-task', type=int, default=-1, help='Max examples per CORE task (-1 = all)')\n",
    "# parser.add_argument('--device-batch-size', type=int, default=32, help='Per-device batch size for BPB evaluation')\n",
    "# parser.add_argument('--split-tokens', type=int, default=40*524288, help='Number of tokens to evaluate per split for BPB')\n",
    "# parser.add_argument('--device-type', type=str, default='', help='cuda|cpu|mps (empty = autodetect)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    eval = \"core,bpb,sample\",\n",
    "    hf_path = None,\n",
    "    model_tag = None,\n",
    "    step = None,\n",
    "    max_per_task = -1,\n",
    "    device_batch_size = 32,\n",
    "    split_tokens = 40*524288,\n",
    "    device_type = \"\",\n",
    ")\n",
    "\n",
    "# def main()\n",
    "\n",
    "# Parse evaluation modes\n",
    "eval_modes = set(mode.strip() for mode in args.eval.split(','))\n",
    "valid_modes = {'core', 'bpb', 'sample'}\n",
    "invalid = eval_modes - valid_modes\n",
    "if invalid:\n",
    "    parser.error(f\"Invalid eval modes: {invalid}. Valid: {valid_modes}\")\n",
    "\n",
    "# Distributed / precision setup\n",
    "device_type = autodetect_device_type() if args.device_type == '' else args.device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: base_model (step 1785)\n",
      "Eval modes: bpb, core, sample\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "is_hf_model = args.hf_path is not None\n",
    "if is_hf_model:\n",
    "    model, tokenizer = load_hf_model(args.hf_path, device)\n",
    "    sequence_len = model.max_seq_len or 1024\n",
    "    token_bytes = get_hf_token_bytes(tokenizer, device=device)\n",
    "    model_name = args.hf_path\n",
    "    model_slug = args.hf_path.replace(\"/\", \"-\")\n",
    "else:\n",
    "    model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n",
    "    sequence_len = meta[\"model_config\"][\"sequence_len\"]\n",
    "    token_bytes = get_token_bytes(device=device)\n",
    "    model_name = f\"base_model (step {meta['step']})\"\n",
    "    model_slug = f\"base_model_{meta['step']:06d}\"\n",
    "\n",
    "print0(f\"Evaluating model: {model_name}\")\n",
    "print0(f\"Eval modes: {', '.join(sorted(eval_modes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_000851.json  model_000851.pt  optim_001785_rank0.pt\n",
      "meta_001785.json  model_001785.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/nanochat/base_checkpoints/d12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model for faster inference (dynamic=True handles varying sequence lengths)\n",
    "if device_type == \"cuda\":\n",
    "    model = torch.compile(model, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results to log\n",
    "core_results = None\n",
    "bpb_results = {}\n",
    "samples = []\n",
    "unconditioned_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Model Samples\n",
      "================================================================================\n",
      "\n",
      "Conditioned samples:\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The capital of France is Paris. The capital of France is Paris. The capital of France is Paris.\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The chemical symbol of gold is gold. It is the symbol of the purity of the gold that is the basis\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today was Saturday, then tomorrow will be Saturday. If today was\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The opposite of hot is cold. It is a state of being in a state of being in a state\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>My favorite color is red. I love it because it is the color of the blood. It is\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>If 5*x + 3 = 13, then x is 5*x + 3 = 13. If 5*x\n",
      "\n",
      "Unconditioned samples:\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>The Summary in Parashat Châ€™an was written in the first century of the eighth century, 3-5 by a Jew who opposed Shaitan in the Volga land region, and refers to a bi-pica-the works of that Jew.\n",
      "Young men, while being kept out of Mosul, were subsequently cast out of their tribe, looting and exterminating them and their families if they entered the Mosul realm. The one who defeated them, called Thareus was a Saoram, a people who lived outside Mosul country and residing in Nizirat, or Oxus-sur-ware camp by\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>New York Times, April 11, 2019\n",
      "Parents at The Mother Of The State Of Connecticut\n",
      "The Followbreak Of The Connecticut River Rage\n",
      "|Safety is our greatest asset||Litchfield SW Investment\n",
      "New Delhi, 19 November 2019\n",
      "Every second day, Connecticut's courts sense an intrusigable, transnational threat in the form of a cargo ship carrying homemade ammunition into the harborâ€”partly one of this second wave of rebellions. By the dawn of July of 1917, the plain was protected by the protection of the forts made in the lower Potomac River and South Conroe\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>In summary, we can discover a wide variety of events overlap in the life of a person from the moment we emerge from the vegetative state to the actual occasion and vâ€™s ever so rare and rare is the â€˜actionâ€™ of one of creator. The clandestine function that the xenophobic and atheistic desires of xenophobic neurosis that began as a reaction of the nervous in the wake of the coexistence of Aryan and animism is highly worth studying. Battles of xenophobia are extremely diverse cases in which type 1 xenophobic xenophobia and xenophobic animism have been brought on amorous.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Civil war 100Â¢ SixtiesÂ¢ The beginning of the Civil War and its full extent of the controversy are selected by a searchable series of photographs from the Famous Women in the United States Consolidated Dictionary. First issued on March 3, 1912, this issue provides a short overview of the scope and contents of the War and the 1912 National Guard, started as a result of human error, mostly by members of the 99th Massachusetts Sen. Deborah P. Warren, R-San Francisco, Robert H. Bradley, Buzia B. Ashton, George.\n",
      "During the entire \n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Dialysis is an annual or semi-permanent exfoliating condition in which high fluid persists over time. Medication can be used to treat complications.\n",
      "Dialysis, which is also referred to as \"dislocation\" is performed to help tissues deal with various diseases\n",
      "Hair: Situational Phenomena\n",
      "- Toe: During Koi injury\n",
      "- Stone: When Cystoscopy\n",
      "- Medial: After sporulation of Mare Mare\n",
      "- Blood: Toe: After treatment of Malmosis if Single\n",
      "- Ventidle: During periodic hemoplasty\n",
      "- Fetus: As\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>Freedom of association. â€œ. â‡’ in the bag implies organized human groups, and from time to time the permitted guests take whatever they need, a safe gathering of bars or america1â€œ.\n",
      "Freedom of speech. \". â‡’ in the bag implies symbolic, notionally ambiguous, or ambiguous communication; to speak. â‡’ out of the bag implies freedom of information, press or speech2.â€ â€” In addition to the word â€œlibâ€ (we reflexively avoid it) in the verb â€œto frame,â€ many other objective pronoun terms occur throughout the construction of construction. The construction of speech involves the constructions of\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>SANEYLETT - Every few months we get evasive statements regarding radioactive isotope classification. The answer is, \"No\". A huge waste of dollars and time is spent on IA by nuclear programs and radioactive materials like mining. As well as secret regulations and nuclear weapons, meanwhile, there are now many unanswered scientific questions, affirmed at the comprehensive secret meetings held at over 90 meetings of the TUERNEC event. Scientific questions are a strong example of a societal challenge for travelers. If you haven't visited a country with half a century of nuclear weapons, chances are you have to pay a visit some distant country\n",
      "--------------------------------------------------------------------------------\n",
      "<|bos|>By: Anna Matthews\n",
      "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
      "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
      "Regular Christmas everyoneâ€™s got the buggy trousers\n",
      "Unforge is an instruction card and recondition. Weâ€™ve got both esters\n",
      "and does not target.\n",
      "Unforge not only makes the card more versatile but\n",
      "effects the card value 148.1 Ã— 8.1.\n",
      "This 148.1 is an unforge sufficiently recipe card\n",
      "and is larger, so all other cards rely on it. See\n",
      "the question step below\n"
     ]
    }
   ],
   "source": [
    "# --- Sampling ---\n",
    "if 'sample' in eval_modes and not is_hf_model:\n",
    "    print0(\"\\n\" + \"=\"*80)\n",
    "    print0(\"Model Samples\")\n",
    "    print0(\"=\"*80)\n",
    "    if ddp_rank == 0:\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(model, tokenizer)\n",
    "        print0(\"\\nConditioned samples:\")\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            sample_str = tokenizer.decode(sample[0])\n",
    "            print0(\"-\" * 80)\n",
    "            print0(sample_str)\n",
    "            samples.append(sample_str)\n",
    "\n",
    "        print0(\"\\nUnconditioned samples:\")\n",
    "        tokens = tokenizer(\"\", prepend=\"<|bos|>\")\n",
    "        with autocast_ctx:\n",
    "            uncond, _ = engine.generate_batch(tokens, num_samples=8, max_tokens=128, temperature=1.0)\n",
    "        for sample in uncond:\n",
    "            sample_str = tokenizer.decode(sample)\n",
    "            print0(\"-\" * 80)\n",
    "            print0(sample_str)\n",
    "            unconditioned_samples.append(sample_str)\n",
    "elif 'sample' in eval_modes and is_hf_model:\n",
    "    print0(\"\\nSkipping sampling for HuggingFace models (not supported)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits-per-Byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BPB Evaluation\n",
      "================================================================================\n",
      "train bpb: 0.873283\n",
      "val bpb: 0.920166\n"
     ]
    }
   ],
   "source": [
    "# --- BPB evaluation ---\n",
    "if 'bpb' in eval_modes:\n",
    "    print0(\"\\n\" + \"=\"*80)\n",
    "    print0(\"BPB Evaluation\")\n",
    "    print0(\"=\"*80)\n",
    "    tokens_per_step = args.device_batch_size * sequence_len * ddp_world_size\n",
    "    if args.split_tokens % tokens_per_step != 0:\n",
    "        # Adjust to nearest multiple\n",
    "        args.split_tokens = (args.split_tokens // tokens_per_step) * tokens_per_step\n",
    "        print0(f\"Adjusted split_tokens to {args.split_tokens} (must be divisible by {tokens_per_step})\")\n",
    "    steps = args.split_tokens // tokens_per_step\n",
    "\n",
    "    for split_name in [\"train\", \"val\"]:\n",
    "        loader = tokenizing_distributed_data_loader_bos_bestfit(tokenizer, args.device_batch_size, sequence_len, split_name, device=device)\n",
    "        with autocast_ctx:\n",
    "            bpb = evaluate_bpb(model, loader, steps, token_bytes)\n",
    "        bpb_results[split_name] = bpb\n",
    "        print0(f\"{split_name} bpb: {bpb:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–¶ Full CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "================================================================================\n",
      "CORE Evaluation\n",
      "================================================================================\n",
      "Evaluating: hellaswag_zeroshot               (0-shot, type: multiple_choice)    ... accuracy: 0.3171 | centered: 0.0894 | time: 38.94s\n",
      "Evaluating: jeopardy                         (10-shot, type: language_modeling) ... accuracy: 0.0061 | centered: 0.0061 | time: 8.93s\n",
      "Evaluating: bigbench_qa_wikidata             (10-shot, type: language_modeling) ... accuracy: 0.3044 | centered: 0.3044 | time: 77.24s\n",
      "Evaluating: arc_easy                         (10-shot, type: multiple_choice)   ... accuracy: 0.4987 | centered: 0.3316 | time: 21.08s\n",
      "Evaluating: arc_challenge                    (10-shot, type: multiple_choice)   ... accuracy: 0.2577 | centered: 0.0102 | time: 11.45s\n",
      "Evaluating: copa                             (0-shot, type: multiple_choice)    ... accuracy: 0.6100 | centered: 0.2200 | time: 0.26s\n",
      "Evaluating: commonsense_qa                   (10-shot, type: multiple_choice)   ... accuracy: 0.2867 | centered: 0.1083 | time: 13.27s\n",
      "Evaluating: piqa                             (10-shot, type: multiple_choice)   ... accuracy: 0.6202 | centered: 0.2405 | time: 11.20s\n",
      "Evaluating: openbook_qa                      (0-shot, type: multiple_choice)    ... accuracy: 0.3040 | centered: 0.0720 | time: 1.63s\n",
      "Evaluating: lambada_openai                   (0-shot, type: language_modeling)  ... accuracy: 0.2676 | centered: 0.2676 | time: 13.65s\n",
      "Evaluating: hellaswag                        (10-shot, type: multiple_choice)   ... accuracy: 0.3208 | centered: 0.0943 | time: 186.84s\n",
      "Evaluating: winograd                         (0-shot, type: schema)             ... accuracy: 0.5678 | centered: 0.1355 | time: 0.67s\n",
      "Evaluating: winogrande                       (0-shot, type: schema)             ... accuracy: 0.5257 | centered: 0.0513 | time: 3.11s\n",
      "Evaluating: bigbench_dyck_languages          (10-shot, type: language_modeling) ... accuracy: 0.1110 | centered: 0.1110 | time: 5.53s\n",
      "Evaluating: agi_eval_lsat_ar                 (3-shot, type: multiple_choice)    ... accuracy: 0.2696 | centered: 0.0870 | time: 4.07s\n",
      "Evaluating: bigbench_cs_algorithms           (10-shot, type: language_modeling) ... accuracy: 0.4038 | centered: 0.4038 | time: 6.61s\n",
      "Evaluating: bigbench_operators               (10-shot, type: language_modeling) ... accuracy: 0.1381 | centered: 0.1381 | time: 0.93s\n",
      "Evaluating: bigbench_repeat_copy_logic       (10-shot, type: language_modeling) ... accuracy: 0.0000 | centered: 0.0000 | time: 0.16s\n",
      "Evaluating: squad                            (10-shot, type: language_modeling) ... accuracy: 0.0559 | centered: 0.0559 | time: 141.92s\n",
      "Evaluating: coqa                             (0-shot, type: language_modeling)  ... accuracy: 0.1115 | centered: 0.1115 | time: 39.56s\n",
      "Evaluating: boolq                            (10-shot, type: multiple_choice)   ... accuracy: 0.5300 | centered: -0.2369 | time: 57.29s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)   ... accuracy: 0.2552 | centered: 0.1806 | time: 321.71s\n",
      "\n",
      "Results written to: /root/.cache/nanochat/base_eval/base_model_001785.csv\n",
      "CORE metric: 0.1265\n",
      "\n",
      "\n",
      "Total CORE eval time: 967s\n"
     ]
    }
   ],
   "source": [
    "full_eval_t0 = time.time()\n",
    "\n",
    "# --- CORE evaluation ---\n",
    "if 'core' in eval_modes:\n",
    "    print0(\" \\n\" + \"=\"*80)\n",
    "    print0(\"CORE Evaluation\")\n",
    "    print0(\"=\"*80)\n",
    "    with autocast_ctx:\n",
    "        core_results = evaluate_core(model, tokenizer, device, max_per_task=args.max_per_task)\n",
    "\n",
    "    # Write CSV output\n",
    "    if ddp_rank == 0:\n",
    "        base_dir = get_base_dir()\n",
    "        output_csv_path = os.path.join(base_dir, \"base_eval\", f\"{model_slug}.csv\")\n",
    "        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "        with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            f.write(f\"{'Task':<35}, {'Accuracy':<10}, {'Centered':<10}\\n\")\n",
    "            for label in core_results[\"results\"]:\n",
    "                acc = core_results[\"results\"][label]\n",
    "                centered = core_results[\"centered_results\"][label]\n",
    "                f.write(f\"{label:<35}, {acc:<10.6f}, {centered:<10.6f}\\n\")\n",
    "            f.write(f\"{'CORE':<35}, {'':<10}, {core_results['core_metric']:<10.6f}\\n\")\n",
    "        print0(f\"\\nResults written to: {output_csv_path}\")\n",
    "        print0(f\"CORE metric: {core_results['core_metric']:.4f}\")\n",
    "\n",
    "print(f\"\\n\\nTotal CORE eval time: {time.time() - full_eval_t0:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batching:\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "CORE Evaluation\n",
    "================================================================================\n",
    "Evaluating: hellaswag_zeroshot               (0-shot, type: multiple_choice)    ... accuracy: 0.3177 | centered: 0.0902 | time: 44.49s\n",
    "Evaluating: jeopardy                         (10-shot, type: language_modeling) ... accuracy: 0.0061 | centered: 0.0061 | time: 9.45s\n",
    "Evaluating: bigbench_qa_wikidata             (10-shot, type: language_modeling) ... accuracy: 0.3044 | centered: 0.3044 | time: 80.25s\n",
    "Evaluating: arc_easy                         (10-shot, type: multiple_choice)   ... accuracy: 0.4987 | centered: 0.3316 | time: 24.84s\n",
    "Evaluating: arc_challenge                    (10-shot, type: multiple_choice)   ... accuracy: 0.2577 | centered: 0.0102 | time: 13.40s\n",
    "Evaluating: copa                             (0-shot, type: multiple_choice)    ... accuracy: 0.6100 | centered: 0.2200 | time: 0.26s\n",
    "Evaluating: commonsense_qa                   (10-shot, type: multiple_choice)   ... accuracy: 0.2867 | centered: 0.1083 | time: 14.22s\n",
    "Evaluating: piqa                             (10-shot, type: multiple_choice)   ... accuracy: 0.6202 | centered: 0.2405 | time: 14.76s\n",
    "Evaluating: openbook_qa                      (0-shot, type: multiple_choice)    ... accuracy: 0.3040 | centered: 0.0720 | time: 1.67s\n",
    "Evaluating: lambada_openai                   (0-shot, type: language_modeling)  ... accuracy: 0.2676 | centered: 0.2676 | time: 14.69s\n",
    "Evaluating: hellaswag                        (10-shot, type: multiple_choice)   ...  OOM\n",
    "```\n",
    "\n",
    "Timings fixed but no batching:\n",
    "```\n",
    "================================================================================\n",
    "CORE Evaluation\n",
    "================================================================================\n",
    "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3181 | centered: 0.0908 | time: 167.48s\n",
    "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0061 | centered: 0.0061 | time: 36.51s\n",
    "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.3038 | centered: 0.3038 | time: 353.31s\n",
    "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.4996 | centered: 0.3328 | time: 46.01s\n",
    "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2577 | centered: 0.0102 | time: 22.95s\n",
    "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6000 | centered: 0.2000 | time: 1.61s\n",
    "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2867 | centered: 0.1083 | time: 24.63s\n",
    "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6219 | centered: 0.2437 | time: 33.30s\n",
    "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3040 | centered: 0.0720 | time: 8.09s\n",
    "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.2666 | centered: 0.2666 | time: 83.97s\n",
    "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.3208 | centered: 0.0943 | time: 242.59s\n",
    "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.5678 | centered: 0.1355 | time: 4.47s\n",
    "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5185 | centered: 0.0371 | time: 20.72s\n",
    "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1110 | centered: 0.1110 | time: 18.27s\n",
    "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2696 | centered: 0.0870 | time: 5.38s\n",
    "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.4023 | centered: 0.4023 | time: 23.49s\n",
    "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1381 | centered: 0.1381 | time: 3.69s\n",
    "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.56s\n",
    "```\n",
    "\n",
    "Baseline--for comparing accuracy\n",
    "```\n",
    "================================================================================\n",
    "CORE Evaluation\n",
    "================================================================================\n",
    "Downloading https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip...\n",
    "Downloaded to /root/.cache/nanochat/eval_bundle.zip\n",
    "Placed eval_bundle directory at /root/.cache/nanochat/eval_bundle\n",
    "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3181 | centered: 0.0908 | time: 172.33s\n",
    "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0061 | centered: 0.0061 | time: 37.27s\n",
    "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.3038 | centered: 0.3038 | time: 360.42s\n",
    "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.4996 | centered: 0.3328 | time: 46.64s\n",
    "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2577 | centered: 0.0102 | time: 23.31s\n",
    "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6000 | centered: 0.2000 | time: 1.62s\n",
    "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2867 | centered: 0.1083 | time: 24.78s\n",
    "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6219 | centered: 0.2437 | time: 33.53s\n",
    "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3040 | centered: 0.0720 | time: 8.28s\n",
    "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.2666 | centered: 0.2666 | time: 84.94s\n",
    "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.3208 | centered: 0.0943 | time: 241.19s\n",
    "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.5678 | centered: 0.1355 | time: 4.40s\n",
    "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5185 | centered: 0.0371 | time: 20.35s\n",
    "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1110 | centered: 0.1110 | time: 18.11s\n",
    "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2696 | centered: 0.0870 | time: 5.39s\n",
    "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.4023 | centered: 0.4023 | time: 23.19s\n",
    "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1381 | centered: 0.1381 | time: 3.62s\n",
    "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.56s\n",
    "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0559 | centered: 0.0559 | time: 235.42s\n",
    "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1110 | centered: 0.1110 | time: 140.37s\n",
    "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5309 | centered: -0.2345 | time: 76.99s\n",
    "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2552 | centered: 0.1806 | time: 351.79s\n",
    "\n",
    "Results written to: /root/.cache/nanochat/base_eval/base_model_001785.csv\n",
    "CORE metric: 0.1251\n",
    "\n",
    "\n",
    "Total CORE eval time: 1918s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Log to report ---\n",
    "from nanochat.report import get_report\n",
    "report_data = [{\"model\": model_name}]\n",
    "\n",
    "if core_results:\n",
    "    report_data[0][\"CORE metric\"] = core_results[\"core_metric\"]\n",
    "    report_data.append(core_results[\"centered_results\"])\n",
    "\n",
    "if bpb_results:\n",
    "    report_data[0][\"train bpb\"] = bpb_results.get(\"train\")\n",
    "    report_data[0][\"val bpb\"] = bpb_results.get(\"val\")\n",
    "\n",
    "if samples:\n",
    "    report_data.append({f\"sample {i}\": s for i, s in enumerate(samples)})\n",
    "if unconditioned_samples:\n",
    "    report_data.append({f\"unconditioned {i}\": s for i, s in enumerate(unconditioned_samples)})\n",
    "\n",
    "get_report().log(section=\"Base model evaluation\", data=report_data)\n",
    "\n",
    "compute_cleanup()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-60da475a-4a6d-49d4-b4ea-634d3f4f5b16\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Centered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hellaswag_zeroshot</td>\n",
       "      <td>0.317068</td>\n",
       "      <td>0.089424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeopardy</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigbench_qa_wikidata</td>\n",
       "      <td>0.304414</td>\n",
       "      <td>0.304414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arc_easy</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>0.331650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arc_challenge</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.010239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>copa</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>commonsense_qa</td>\n",
       "      <td>0.286650</td>\n",
       "      <td>0.108313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>piqa</td>\n",
       "      <td>0.620239</td>\n",
       "      <td>0.240479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>openbook_qa</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lambada_openai</td>\n",
       "      <td>0.267611</td>\n",
       "      <td>0.267611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hellaswag</td>\n",
       "      <td>0.320753</td>\n",
       "      <td>0.094337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>winograd</td>\n",
       "      <td>0.567766</td>\n",
       "      <td>0.135531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>winogrande</td>\n",
       "      <td>0.525651</td>\n",
       "      <td>0.051302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bigbench_dyck_languages</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>agi_eval_lsat_ar</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.086956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bigbench_cs_algorithms</td>\n",
       "      <td>0.403788</td>\n",
       "      <td>0.403788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bigbench_operators</td>\n",
       "      <td>0.138095</td>\n",
       "      <td>0.138095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bigbench_repeat_copy_logic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squad</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.055913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coqa</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>0.111487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>boolq</td>\n",
       "      <td>0.529969</td>\n",
       "      <td>-0.236923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bigbench_language_identification</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.180638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CORE</td>\n",
       "      <td></td>\n",
       "      <td>0.126473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60da475a-4a6d-49d4-b4ea-634d3f4f5b16')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-60da475a-4a6d-49d4-b4ea-634d3f4f5b16 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-60da475a-4a6d-49d4-b4ea-634d3f4f5b16');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_5469df63-f5f5-4eaf-bbe1-4c14fc9d5b3f\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_5469df63-f5f5-4eaf-bbe1-4c14fc9d5b3f button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Task                                  Accuracy     Centered  \n",
       "0   hellaswag_zeroshot                    0.317068       0.089424\n",
       "1   jeopardy                              0.006141       0.006141\n",
       "2   bigbench_qa_wikidata                  0.304414       0.304414\n",
       "3   arc_easy                              0.498737       0.331650\n",
       "4   arc_challenge                         0.257679       0.010239\n",
       "5   copa                                  0.610000       0.220000\n",
       "6   commonsense_qa                        0.286650       0.108313\n",
       "7   piqa                                  0.620239       0.240479\n",
       "8   openbook_qa                           0.304000       0.072000\n",
       "9   lambada_openai                        0.267611       0.267611\n",
       "10  hellaswag                             0.320753       0.094337\n",
       "11  winograd                              0.567766       0.135531\n",
       "12  winogrande                            0.525651       0.051302\n",
       "13  bigbench_dyck_languages               0.111000       0.111000\n",
       "14  agi_eval_lsat_ar                      0.269565       0.086956\n",
       "15  bigbench_cs_algorithms                0.403788       0.403788\n",
       "16  bigbench_operators                    0.138095       0.138095\n",
       "17  bigbench_repeat_copy_logic            0.000000       0.000000\n",
       "18  squad                                 0.055913       0.055913\n",
       "19  coqa                                  0.111487       0.111487\n",
       "20  boolq                                 0.529969      -0.236923\n",
       "21  bigbench_language_identification      0.255200       0.180638\n",
       "22  CORE                                                 0.126473"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/root/.cache/nanochat/base_eval/base_model_001785.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Is this skill training, personality creation, benchmarking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `speedrun.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 2.3MB of synthetic identity conversations to impart a personality to nanochat\n",
    "# see dev/gen_synthetic_data.py for details on how this data was prepared and to get a sense of how you can easily tune it\n",
    "!curl -L -o $NANOCHAT_BASE_DIR/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `speedrun.sh`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# run SFT and eval the model\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- --device-batch-size=16 --run=$WANDB_RUN\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_eval -- -i sft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Supervised fine-tuning (SFT) the model.\n",
    "Run as:\n",
    "\n",
    "python -m scripts.chat_sft\n",
    "\n",
    "Or torchrun for training:\n",
    "\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- --device-batch-size=16\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, get_base_dir, autodetect_device_type\n",
    "# from nanochat.tokenizer import get_token_bytes\n",
    "# from nanochat.checkpoint_manager import save_checkpoint\n",
    "# from nanochat.loss_eval import evaluate_bpb\n",
    "# from nanochat.checkpoint_manager import load_model\n",
    "import torch.distributed as dist\n",
    "\n",
    "from tasks.common import TaskMixture\n",
    "from tasks.gsm8k import GSM8K\n",
    "from tasks.mmlu import MMLU\n",
    "from tasks.smoltalk import SmolTalk\n",
    "from tasks.customjson import CustomJSON\n",
    "from tasks.spellingbee import SimpleSpelling, SpellingBee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original:\n",
    "\n",
    "```python\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLI arguments\n",
    "parser = argparse.ArgumentParser(description=\"Supervised fine-tuning (SFT) the model\")\n",
    "# Logging\n",
    "parser.add_argument(\"--run\", type=str, default=\"dummy\", help=\"wandb run name ('dummy' disables wandb logging)\")\n",
    "# Runtime\n",
    "parser.add_argument(\"--device-type\", type=str, default=\"\", help=\"cuda|cpu|mps (empty = autodetect)\")\n",
    "parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"float32|bfloat16\")\n",
    "# Model loading\n",
    "parser.add_argument(\"--model-tag\", type=str, default=None, help=\"model tag to load from\")\n",
    "parser.add_argument(\"--model-step\", type=int, default=None, help=\"model step to load from\")\n",
    "# Training horizon\n",
    "parser.add_argument(\"--num-iterations\", type=int, default=-1, help=\"number of optimization steps (-1 = full epoch)\")\n",
    "# Batch sizes\n",
    "parser.add_argument(\"--max-seq-len\", type=int, default=2048, help=\"max context length\")\n",
    "parser.add_argument(\"--device-batch-size\", type=int, default=32, help=\"per-device batch size\")\n",
    "parser.add_argument(\"--total-batch-size\", type=int, default=524288, help=\"total batch size in tokens\")\n",
    "# Optimization\n",
    "parser.add_argument(\"--embedding-lr\", type=float, default=0.3, help=\"learning rate for embedding parameters (Adam)\")\n",
    "parser.add_argument(\"--unembedding-lr\", type=float, default=0.004, help=\"learning rate for unembedding parameters (Adam)\")\n",
    "parser.add_argument(\"--matrix-lr\", type=float, default=0.02, help=\"learning rate for matrix parameters (Muon)\")\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=0.0, help=\"weight decay for embedding/unembedding parameters (Adam)\")\n",
    "parser.add_argument(\"--init-lr-frac\", type=float, default=1.0, help=\"initial LR as fraction of base LR\")\n",
    "# Evaluation\n",
    "parser.add_argument(\"--eval-every\", type=int, default=150, help=\"evaluate val bpb every N steps (-1 = disable)\")\n",
    "parser.add_argument(\"--eval-tokens\", type=int, default=20*524288, help=\"number of tokens to evaluate val loss on\")\n",
    "# Output\n",
    "parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"log to wandb but skip checkpoints/report\")\n",
    "args = parser.parse_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    run = WANDB_RUN,\n",
    "    device_type = \"\",\n",
    "    dtype = \"bfloat16\",\n",
    "    model_tag = None,\n",
    "    model_step = None,\n",
    "    num_iterations = -1,\n",
    "    max_seq_len = 2048,\n",
    "    device_batch_size = 16,  # Default is 32\n",
    "    total_batch_size = 524288,\n",
    "    embedding_lr = 0.3,\n",
    "    unembedding_lr = 0.004,\n",
    "    matrix_lr = 0.02,\n",
    "    weight_decay = 0.0,\n",
    "    init_lr_frac = 1.0,\n",
    "    eval_every = 150,\n",
    "    eval_tokens = 20*524288,\n",
    "    dry_run = False\n",
    ")\n",
    "\n",
    "user_config = vars(args).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Compute init\n",
    "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "master_process = ddp_rank == 0\n",
    "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanochat - d.768 - l.12 - v1.0</strong> at: <a href='https://wandb.ai/chrismccormick/nanochat-sft/runs/fjvmu0gt' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft/runs/fjvmu0gt</a><br> View project at: <a href='https://wandb.ai/chrismccormick/nanochat-sft' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260206_191944-fjvmu0gt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260206_192017-lrh7r2vl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chrismccormick/nanochat-sft/runs/lrh7r2vl' target=\"_blank\">nanochat - d.768 - l.12 - v1.0</a></strong> to <a href='https://wandb.ai/chrismccormick/nanochat-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chrismccormick/nanochat-sft' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chrismccormick/nanochat-sft/runs/lrh7r2vl' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft/runs/lrh7r2vl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb logging init\n",
    "use_dummy_wandb = args.run == \"dummy\" or not master_process\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-sft\", name=args.run, config=user_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens / micro-batch / rank: 16 x 2048 = 32,768\n",
      "Tokens / micro-batch: 32,768\n",
      "Total batch size 524,288 => gradient accumulation steps: 16\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model, tokenizer, meta = load_model(\"base\", device, phase=\"train\", model_tag=args.model_tag, step=args.model_step)\n",
    "pretrain_batch_size = meta.get(\"device_batch_size\", None)\n",
    "if pretrain_batch_size is not None and args.device_batch_size > pretrain_batch_size:\n",
    "    print0(f\"FOOTGUN WARNING: base model training used device_batch_size {pretrain_batch_size}, did you pass in a good --device-batch-size to this script?\")\n",
    "orig_model = model\n",
    "model = torch.compile(model, dynamic=False)\n",
    "depth = model.config.n_layer\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "tokens_per_fwdbwd = args.device_batch_size * args.max_seq_len # tokens per iteration for a single rank\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks\n",
    "assert args.total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = args.total_batch_size // world_tokens_per_fwdbwd\n",
    "print0(f\"Tokens / micro-batch / rank: {args.device_batch_size} x {args.max_seq_len} = {tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Total batch size {args.total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")\n",
    "token_bytes = get_token_bytes(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters âˆ1/âˆš(768/768) = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Optimizer (combined MuonAdamW: Muon for matrix params, AdamW for rest)\n",
    "optimizer = model.setup_optimizer(unembedding_lr=args.unembedding_lr, embedding_lr=args.embedding_lr, matrix_lr=args.matrix_lr, weight_decay=args.weight_decay)\n",
    "# Override the initial learning rate as a fraction of the base learning rate\n",
    "for group in optimizer.param_groups:\n",
    "    group[\"lr\"] = group[\"lr\"] * args.init_lr_frac\n",
    "    group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added, as directed by the output of the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2452k  100 2452k    0     0  1445k      0  0:00:01  0:00:01 --:--:-- 1444k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o /root/.cache/nanochat/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT data mixture and DataLoader\n",
    "base_dir = get_base_dir()\n",
    "identity_conversations_filepath = os.path.join(base_dir, \"identity_conversations.jsonl\")\n",
    "train_dataset = TaskMixture([\n",
    "    SmolTalk(split=\"train\"), # 460K rows of general conversations\n",
    "    MMLU(subset=\"auxiliary_train\", split=\"train\"), # 100K rows of multiple choice problems drawn from ARC, MC_TEST, OBQA, RACE\n",
    "    GSM8K(subset=\"main\", split=\"train\"), # 8K rows teaching simple math and (calculator) tool use\n",
    "    GSM8K(subset=\"main\", split=\"train\"), # 2 epochs of GSM8K\n",
    "    CustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
    "    CustomJSON(filepath=identity_conversations_filepath), # let's do 2 epochs of these\n",
    "    SimpleSpelling(size=200000, split=\"train\"), # 200K rows of Simple Spelling (e.g. spell the word 'apple')\n",
    "    SpellingBee(size=80000, split=\"train\"), # 80K rows of Spelling Bee (e.g. how many 'r' are in 'strawberry'?)\n",
    "]) # total: 460K + 100K + 16K + 200K + 80K = 856K rows\n",
    "val_dataset = TaskMixture([\n",
    "    SmolTalk(split=\"test\"), # 24K rows in test set\n",
    "    MMLU(subset=\"all\", split=\"test\", stop=5200), # 14K rows in test set, use only 5.2K to match the train ratios\n",
    "    GSM8K(subset=\"main\", split=\"test\", stop=420), # 1.32K rows in test set, use only 420 to match the train ratios\n",
    "]) # total: 24K + 14K + 1.32K ~= 39K rows\n",
    "# DataLoader is defined here, it emits inputs, targets : 2D tensors of shape (device_batch_size, max_seq_len)\n",
    "# A big problem is that we don't know the final num_iterations in advance. So we create\n",
    "# these two global variables and update them from within the data generator.\n",
    "last_step = False # we will toggle this to True when we reach the end of the training dataset\n",
    "approx_progress = 0.0 # will go from 0 to 1 over the course of the epoch\n",
    "current_epoch = 1 # track epoch for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sft_data_generator_bos_bestfit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sft_data_generator_bos_bestfit(split, buffer_size=100):\n",
    "    \"\"\"\n",
    "    BOS-aligned dataloader for SFT with bestfit-pad packing.\n",
    "\n",
    "    Each row in the batch starts with BOS (beginning of a conversation).\n",
    "    Conversations are packed using best-fit algorithm. When no conversation fits,\n",
    "    the row is padded (instead of cropping) to ensure no tokens are ever discarded.\n",
    "    Padding positions have targets masked with -1 (ignore_index for cross-entropy).\n",
    "    \"\"\"\n",
    "    global last_step, approx_progress, current_epoch\n",
    "    assert split in {\"train\", \"val\"}, \"split must be 'train' or 'val'\"\n",
    "    dataset = train_dataset if split == \"train\" else val_dataset\n",
    "    dataset_size = len(dataset)\n",
    "    assert dataset_size > 0\n",
    "    row_capacity = args.max_seq_len + 1  # +1 for target at last position\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "\n",
    "    # Conversation buffer: list of token lists\n",
    "    conv_buffer = []\n",
    "    cursor = ddp_rank  # Each rank processes different conversations (for fetching)\n",
    "    consumed = ddp_rank  # Track actual consumption separately from buffering\n",
    "    epoch = 1\n",
    "    it = 0  # iteration counter\n",
    "\n",
    "    def refill_buffer():\n",
    "        nonlocal cursor, epoch\n",
    "        while len(conv_buffer) < buffer_size:\n",
    "            conversation = dataset[cursor]\n",
    "            ids, _ = tokenizer.render_conversation(conversation)\n",
    "            conv_buffer.append(ids)\n",
    "            cursor += ddp_world_size\n",
    "            if cursor >= dataset_size:\n",
    "                cursor = cursor % dataset_size\n",
    "                epoch += 1\n",
    "                # Note: last_step is now triggered based on consumption, not fetching\n",
    "\n",
    "    while True:\n",
    "        rows = []\n",
    "        row_lengths = []  # Track actual content length (excluding padding) for each row\n",
    "        for _ in range(args.device_batch_size):\n",
    "            row = []\n",
    "            padded = False\n",
    "            while len(row) < row_capacity:\n",
    "                # Ensure buffer has conversations\n",
    "                while len(conv_buffer) < buffer_size:\n",
    "                    refill_buffer()\n",
    "\n",
    "                remaining = row_capacity - len(row)\n",
    "\n",
    "                # Find largest conversation that fits entirely\n",
    "                best_idx = -1\n",
    "                best_len = 0\n",
    "                for i, conv in enumerate(conv_buffer):\n",
    "                    conv_len = len(conv)\n",
    "                    if conv_len <= remaining and conv_len > best_len:\n",
    "                        best_idx = i\n",
    "                        best_len = conv_len\n",
    "\n",
    "                if best_idx >= 0:\n",
    "                    # Found a conversation that fits - use it entirely\n",
    "                    conv = conv_buffer.pop(best_idx)\n",
    "                    row.extend(conv)\n",
    "                    consumed += ddp_world_size  # Track actual consumption\n",
    "                else:\n",
    "                    # No conversation fits - pad the remainder instead of cropping\n",
    "                    # This ensures we never discard any tokens\n",
    "                    content_len = len(row)\n",
    "                    row.extend([bos_token] * remaining)  # Pad with BOS tokens\n",
    "                    padded = True\n",
    "                    break  # Row is now full (with padding)\n",
    "\n",
    "            # Track content length: full row if no padding, otherwise the length before padding\n",
    "            if padded:\n",
    "                row_lengths.append(content_len)\n",
    "            else:\n",
    "                row_lengths.append(row_capacity)\n",
    "            rows.append(row[:row_capacity])\n",
    "\n",
    "        # Stopping condition to respect num_iterations, if given\n",
    "        it += 1\n",
    "        if 0 < args.num_iterations <= it and split == \"train\":\n",
    "            last_step = True\n",
    "\n",
    "        # Update progress tracking (based on consumed, not cursor, to account for buffering)\n",
    "        if split == \"train\":\n",
    "            current_epoch = epoch\n",
    "            if args.num_iterations > 0:\n",
    "                approx_progress = it / args.num_iterations\n",
    "            else:\n",
    "                approx_progress = consumed / dataset_size\n",
    "            # Trigger last_step when we've consumed enough (instead of when cursor wraps)\n",
    "            if consumed >= dataset_size:\n",
    "                last_step = True\n",
    "\n",
    "        # Build tensors\n",
    "        use_cuda = device_type == \"cuda\"\n",
    "        batch_tensor = torch.tensor(rows, dtype=torch.long, pin_memory=use_cuda)\n",
    "        inputs = batch_tensor[:, :-1].to(device=device, dtype=torch.int32, non_blocking=use_cuda)\n",
    "        targets = batch_tensor[:, 1:].to(device=device, dtype=torch.int64, non_blocking=use_cuda)\n",
    "\n",
    "        # Mask out padding positions in targets (set to -1 = ignore_index)\n",
    "        # For each row, positions >= (content_length - 1) in targets should be masked\n",
    "        for i, content_len in enumerate(row_lengths):\n",
    "            if content_len < row_capacity:\n",
    "                targets[i, content_len-1:] = -1\n",
    "\n",
    "        yield inputs, targets\n",
    "\n",
    "train_loader = sft_data_generator_bos_bestfit(\"train\")\n",
    "build_val_loader = lambda: sft_data_generator_bos_bestfit(\"val\")\n",
    "progress = 0 # will go from 0 to 1 over the course of the epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_lr_multiplier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_multiplier(progress):\n",
    "    # first 80% of training: no decay, then linearly ramp down to 0.\n",
    "    return 1 if progress < 0.8 else 1 - (progress - 0.8) / 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_muon_momentum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum scheduler for Muon optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â–¶ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00000 | Validation bpb: 0.8000\n",
      "step 00001 (0.12%) | loss: 1.512501 | lrm: 1.00 | dt: 35097.45ms | tok/sec: 14,938 | mfu: 1.21 | epoch: 1 | total time: 0.00m\n",
      "step 00002 (0.24%) | loss: 2.047809 | lrm: 1.00 | dt: 2544.54ms | tok/sec: 206,044 | mfu: 16.71 | epoch: 1 | total time: 0.00m\n",
      "step 00003 (0.35%) | loss: 2.191729 | lrm: 1.00 | dt: 2553.53ms | tok/sec: 205,319 | mfu: 16.65 | epoch: 1 | total time: 0.00m\n",
      "step 00004 (0.47%) | loss: 2.258497 | lrm: 1.00 | dt: 2553.22ms | tok/sec: 205,344 | mfu: 16.66 | epoch: 1 | total time: 0.00m\n",
      "step 00005 (0.58%) | loss: 2.296375 | lrm: 1.00 | dt: 2555.60ms | tok/sec: 205,152 | mfu: 16.64 | epoch: 1 | total time: 0.00m\n",
      "step 00006 (0.70%) | loss: 2.341612 | lrm: 1.00 | dt: 2559.54ms | tok/sec: 204,837 | mfu: 16.61 | epoch: 1 | total time: 0.00m\n",
      "step 00007 (0.81%) | loss: 2.344912 | lrm: 1.00 | dt: 2561.26ms | tok/sec: 204,699 | mfu: 16.60 | epoch: 1 | total time: 0.00m\n",
      "step 00008 (0.92%) | loss: 2.356407 | lrm: 1.00 | dt: 2562.69ms | tok/sec: 204,584 | mfu: 16.59 | epoch: 1 | total time: 0.00m\n",
      "step 00009 (1.04%) | loss: 2.297692 | lrm: 1.00 | dt: 2563.90ms | tok/sec: 204,488 | mfu: 16.59 | epoch: 1 | total time: 0.00m\n",
      "step 00010 (1.16%) | loss: 2.255298 | lrm: 1.00 | dt: 2565.35ms | tok/sec: 204,373 | mfu: 16.58 | epoch: 1 | total time: 0.00m\n",
      "step 00011 (1.27%) | loss: 2.228933 | lrm: 1.00 | dt: 2569.51ms | tok/sec: 204,041 | mfu: 16.55 | epoch: 1 | total time: 0.04m\n",
      "step 00012 (1.39%) | loss: 2.197712 | lrm: 1.00 | dt: 2568.05ms | tok/sec: 204,158 | mfu: 16.56 | epoch: 1 | total time: 0.09m\n",
      "step 00013 (1.51%) | loss: 2.143314 | lrm: 1.00 | dt: 2567.04ms | tok/sec: 204,238 | mfu: 16.57 | epoch: 1 | total time: 0.13m\n",
      "step 00014 (1.62%) | loss: 2.142556 | lrm: 1.00 | dt: 2571.05ms | tok/sec: 203,919 | mfu: 16.54 | epoch: 1 | total time: 0.17m\n",
      "step 00015 (1.74%) | loss: 2.123597 | lrm: 1.00 | dt: 2573.23ms | tok/sec: 203,746 | mfu: 16.53 | epoch: 1 | total time: 0.21m\n",
      "step 00016 (1.86%) | loss: 2.108579 | lrm: 1.00 | dt: 2573.67ms | tok/sec: 203,712 | mfu: 16.52 | epoch: 1 | total time: 0.26m\n",
      "step 00017 (1.98%) | loss: 2.078781 | lrm: 1.00 | dt: 2574.90ms | tok/sec: 203,614 | mfu: 16.51 | epoch: 1 | total time: 0.30m\n",
      "step 00018 (2.09%) | loss: 2.051427 | lrm: 1.00 | dt: 2573.08ms | tok/sec: 203,758 | mfu: 16.53 | epoch: 1 | total time: 0.34m\n",
      "step 00019 (2.21%) | loss: 2.041843 | lrm: 1.00 | dt: 2571.94ms | tok/sec: 203,849 | mfu: 16.53 | epoch: 1 | total time: 0.39m\n",
      "step 00020 (2.32%) | loss: 2.007312 | lrm: 1.00 | dt: 2574.78ms | tok/sec: 203,624 | mfu: 16.52 | epoch: 1 | total time: 0.43m\n",
      "step 00021 (2.44%) | loss: 1.998040 | lrm: 1.00 | dt: 2572.89ms | tok/sec: 203,774 | mfu: 16.53 | epoch: 1 | total time: 0.47m\n",
      "step 00022 (2.56%) | loss: 1.974652 | lrm: 1.00 | dt: 2575.71ms | tok/sec: 203,550 | mfu: 16.51 | epoch: 1 | total time: 0.51m\n",
      "step 00023 (2.68%) | loss: 1.973267 | lrm: 1.00 | dt: 2576.96ms | tok/sec: 203,451 | mfu: 16.50 | epoch: 1 | total time: 0.56m\n",
      "step 00024 (2.80%) | loss: 1.973051 | lrm: 1.00 | dt: 2579.53ms | tok/sec: 203,249 | mfu: 16.49 | epoch: 1 | total time: 0.60m\n",
      "step 00025 (2.92%) | loss: 1.953985 | lrm: 1.00 | dt: 2576.67ms | tok/sec: 203,475 | mfu: 16.50 | epoch: 1 | total time: 0.64m\n",
      "step 00026 (3.04%) | loss: 1.938215 | lrm: 1.00 | dt: 2577.55ms | tok/sec: 203,405 | mfu: 16.50 | epoch: 1 | total time: 0.69m\n",
      "step 00027 (3.16%) | loss: 1.918004 | lrm: 1.00 | dt: 2582.78ms | tok/sec: 202,993 | mfu: 16.46 | epoch: 1 | total time: 0.73m\n",
      "step 00028 (3.28%) | loss: 1.902335 | lrm: 1.00 | dt: 2581.50ms | tok/sec: 203,094 | mfu: 16.47 | epoch: 1 | total time: 0.77m\n",
      "step 00029 (3.40%) | loss: 1.883063 | lrm: 1.00 | dt: 2583.96ms | tok/sec: 202,900 | mfu: 16.46 | epoch: 1 | total time: 0.82m\n",
      "step 00030 (3.52%) | loss: 1.863603 | lrm: 1.00 | dt: 2575.98ms | tok/sec: 203,529 | mfu: 16.51 | epoch: 1 | total time: 0.86m\n",
      "step 00031 (3.63%) | loss: 1.867751 | lrm: 1.00 | dt: 2579.77ms | tok/sec: 203,230 | mfu: 16.48 | epoch: 1 | total time: 0.90m\n",
      "step 00032 (3.74%) | loss: 1.856721 | lrm: 1.00 | dt: 2578.59ms | tok/sec: 203,323 | mfu: 16.49 | epoch: 1 | total time: 0.94m\n",
      "step 00033 (3.86%) | loss: 1.849076 | lrm: 1.00 | dt: 2582.13ms | tok/sec: 203,044 | mfu: 16.47 | epoch: 1 | total time: 0.99m\n",
      "step 00034 (3.98%) | loss: 1.850235 | lrm: 1.00 | dt: 2584.59ms | tok/sec: 202,851 | mfu: 16.45 | epoch: 1 | total time: 1.03m\n",
      "step 00035 (4.10%) | loss: 1.855872 | lrm: 1.00 | dt: 2591.97ms | tok/sec: 202,274 | mfu: 16.41 | epoch: 1 | total time: 1.07m\n",
      "step 00036 (4.22%) | loss: 1.860851 | lrm: 1.00 | dt: 2580.32ms | tok/sec: 203,187 | mfu: 16.48 | epoch: 1 | total time: 1.12m\n",
      "step 00037 (4.34%) | loss: 1.819163 | lrm: 1.00 | dt: 2582.12ms | tok/sec: 203,045 | mfu: 16.47 | epoch: 1 | total time: 1.16m\n",
      "step 00038 (4.45%) | loss: 1.809353 | lrm: 1.00 | dt: 2583.31ms | tok/sec: 202,952 | mfu: 16.46 | epoch: 1 | total time: 1.20m\n",
      "step 00039 (4.57%) | loss: 1.806952 | lrm: 1.00 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 1.25m\n",
      "step 00040 (4.69%) | loss: 1.811057 | lrm: 1.00 | dt: 2585.40ms | tok/sec: 202,788 | mfu: 16.45 | epoch: 1 | total time: 1.29m\n",
      "step 00041 (4.81%) | loss: 1.805172 | lrm: 1.00 | dt: 2581.11ms | tok/sec: 203,124 | mfu: 16.48 | epoch: 1 | total time: 1.33m\n",
      "step 00042 (4.93%) | loss: 1.805275 | lrm: 1.00 | dt: 2582.20ms | tok/sec: 203,039 | mfu: 16.47 | epoch: 1 | total time: 1.37m\n",
      "step 00043 (5.05%) | loss: 1.801250 | lrm: 1.00 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 1.42m\n",
      "step 00044 (5.16%) | loss: 1.784366 | lrm: 1.00 | dt: 2583.04ms | tok/sec: 202,973 | mfu: 16.46 | epoch: 1 | total time: 1.46m\n",
      "step 00045 (5.28%) | loss: 1.785875 | lrm: 1.00 | dt: 2583.14ms | tok/sec: 202,965 | mfu: 16.46 | epoch: 1 | total time: 1.50m\n",
      "step 00046 (5.39%) | loss: 1.792896 | lrm: 1.00 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 1.55m\n",
      "step 00047 (5.51%) | loss: 1.793178 | lrm: 1.00 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 1.59m\n",
      "step 00048 (5.63%) | loss: 1.770904 | lrm: 1.00 | dt: 2581.44ms | tok/sec: 203,098 | mfu: 16.47 | epoch: 1 | total time: 1.63m\n",
      "step 00049 (5.75%) | loss: 1.754621 | lrm: 1.00 | dt: 2585.99ms | tok/sec: 202,741 | mfu: 16.44 | epoch: 1 | total time: 1.68m\n",
      "step 00050 (5.86%) | loss: 1.759687 | lrm: 1.00 | dt: 2586.33ms | tok/sec: 202,715 | mfu: 16.44 | epoch: 1 | total time: 1.72m\n",
      "step 00051 (5.98%) | loss: 1.766893 | lrm: 1.00 | dt: 2583.43ms | tok/sec: 202,942 | mfu: 16.46 | epoch: 1 | total time: 1.76m\n",
      "step 00052 (6.10%) | loss: 1.755530 | lrm: 1.00 | dt: 2580.46ms | tok/sec: 203,176 | mfu: 16.48 | epoch: 1 | total time: 1.81m\n",
      "step 00053 (6.21%) | loss: 1.763727 | lrm: 1.00 | dt: 2582.82ms | tok/sec: 202,990 | mfu: 16.46 | epoch: 1 | total time: 1.85m\n",
      "step 00054 (6.33%) | loss: 1.759622 | lrm: 1.00 | dt: 2581.99ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 1.89m\n",
      "step 00055 (6.46%) | loss: 1.764362 | lrm: 1.00 | dt: 2581.23ms | tok/sec: 203,115 | mfu: 16.47 | epoch: 1 | total time: 1.93m\n",
      "step 00056 (6.56%) | loss: 1.765868 | lrm: 1.00 | dt: 2580.62ms | tok/sec: 203,163 | mfu: 16.48 | epoch: 1 | total time: 1.98m\n",
      "step 00057 (6.68%) | loss: 1.771547 | lrm: 1.00 | dt: 2581.08ms | tok/sec: 203,127 | mfu: 16.48 | epoch: 1 | total time: 2.02m\n",
      "step 00058 (6.80%) | loss: 1.766693 | lrm: 1.00 | dt: 2581.30ms | tok/sec: 203,109 | mfu: 16.47 | epoch: 1 | total time: 2.06m\n",
      "step 00059 (6.92%) | loss: 1.765066 | lrm: 1.00 | dt: 2582.24ms | tok/sec: 203,036 | mfu: 16.47 | epoch: 1 | total time: 2.11m\n",
      "step 00060 (7.03%) | loss: 1.775034 | lrm: 1.00 | dt: 2581.19ms | tok/sec: 203,118 | mfu: 16.47 | epoch: 1 | total time: 2.15m\n",
      "step 00061 (7.15%) | loss: 1.759851 | lrm: 1.00 | dt: 2582.28ms | tok/sec: 203,033 | mfu: 16.47 | epoch: 1 | total time: 2.19m\n",
      "step 00062 (7.27%) | loss: 1.749356 | lrm: 1.00 | dt: 2581.09ms | tok/sec: 203,126 | mfu: 16.48 | epoch: 1 | total time: 2.24m\n",
      "step 00063 (7.40%) | loss: 1.733903 | lrm: 1.00 | dt: 2581.79ms | tok/sec: 203,071 | mfu: 16.47 | epoch: 1 | total time: 2.28m\n",
      "step 00064 (7.52%) | loss: 1.717510 | lrm: 1.00 | dt: 2581.95ms | tok/sec: 203,059 | mfu: 16.47 | epoch: 1 | total time: 2.32m\n",
      "step 00065 (7.63%) | loss: 1.703424 | lrm: 1.00 | dt: 2583.32ms | tok/sec: 202,951 | mfu: 16.46 | epoch: 1 | total time: 2.36m\n",
      "step 00066 (7.75%) | loss: 1.720330 | lrm: 1.00 | dt: 2581.93ms | tok/sec: 203,060 | mfu: 16.47 | epoch: 1 | total time: 2.41m\n",
      "step 00067 (7.87%) | loss: 1.721438 | lrm: 1.00 | dt: 2581.03ms | tok/sec: 203,130 | mfu: 16.48 | epoch: 1 | total time: 2.45m\n",
      "step 00068 (7.98%) | loss: 1.722416 | lrm: 1.00 | dt: 2582.93ms | tok/sec: 202,981 | mfu: 16.46 | epoch: 1 | total time: 2.49m\n",
      "step 00069 (8.10%) | loss: 1.717510 | lrm: 1.00 | dt: 2595.01ms | tok/sec: 202,037 | mfu: 16.39 | epoch: 1 | total time: 2.54m\n",
      "step 00070 (8.22%) | loss: 1.713578 | lrm: 1.00 | dt: 2587.98ms | tok/sec: 202,585 | mfu: 16.43 | epoch: 1 | total time: 2.58m\n",
      "step 00071 (8.33%) | loss: 1.727842 | lrm: 1.00 | dt: 2588.35ms | tok/sec: 202,556 | mfu: 16.43 | epoch: 1 | total time: 2.62m\n",
      "step 00072 (8.45%) | loss: 1.712462 | lrm: 1.00 | dt: 2583.74ms | tok/sec: 202,917 | mfu: 16.46 | epoch: 1 | total time: 2.67m\n",
      "step 00073 (8.56%) | loss: 1.720779 | lrm: 1.00 | dt: 2587.05ms | tok/sec: 202,659 | mfu: 16.44 | epoch: 1 | total time: 2.71m\n",
      "step 00074 (8.68%) | loss: 1.721842 | lrm: 1.00 | dt: 2585.66ms | tok/sec: 202,767 | mfu: 16.45 | epoch: 1 | total time: 2.75m\n",
      "step 00075 (8.79%) | loss: 1.721152 | lrm: 1.00 | dt: 2588.03ms | tok/sec: 202,582 | mfu: 16.43 | epoch: 1 | total time: 2.80m\n",
      "step 00076 (8.92%) | loss: 1.721539 | lrm: 1.00 | dt: 2586.13ms | tok/sec: 202,730 | mfu: 16.44 | epoch: 1 | total time: 2.84m\n",
      "step 00077 (9.03%) | loss: 1.719687 | lrm: 1.00 | dt: 2597.04ms | tok/sec: 201,879 | mfu: 16.37 | epoch: 1 | total time: 2.88m\n",
      "step 00078 (9.15%) | loss: 1.710914 | lrm: 1.00 | dt: 2583.53ms | tok/sec: 202,934 | mfu: 16.46 | epoch: 1 | total time: 2.93m\n",
      "step 00079 (9.27%) | loss: 1.703202 | lrm: 1.00 | dt: 2582.11ms | tok/sec: 203,046 | mfu: 16.47 | epoch: 1 | total time: 2.97m\n",
      "step 00080 (9.38%) | loss: 1.703254 | lrm: 1.00 | dt: 2582.65ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 3.01m\n",
      "step 00081 (9.50%) | loss: 1.682568 | lrm: 1.00 | dt: 2581.87ms | tok/sec: 203,065 | mfu: 16.47 | epoch: 1 | total time: 3.05m\n",
      "step 00082 (9.62%) | loss: 1.700334 | lrm: 1.00 | dt: 2581.85ms | tok/sec: 203,066 | mfu: 16.47 | epoch: 1 | total time: 3.10m\n",
      "step 00083 (9.74%) | loss: 1.701325 | lrm: 1.00 | dt: 2581.74ms | tok/sec: 203,075 | mfu: 16.47 | epoch: 1 | total time: 3.14m\n",
      "step 00084 (9.86%) | loss: 1.706419 | lrm: 1.00 | dt: 2582.22ms | tok/sec: 203,037 | mfu: 16.47 | epoch: 1 | total time: 3.18m\n",
      "step 00085 (9.98%) | loss: 1.693410 | lrm: 1.00 | dt: 2582.33ms | tok/sec: 203,028 | mfu: 16.47 | epoch: 1 | total time: 3.23m\n",
      "step 00086 (10.09%) | loss: 1.681467 | lrm: 1.00 | dt: 2599.73ms | tok/sec: 201,669 | mfu: 16.36 | epoch: 1 | total time: 3.27m\n",
      "step 00087 (10.21%) | loss: 1.687327 | lrm: 1.00 | dt: 2581.59ms | tok/sec: 203,087 | mfu: 16.47 | epoch: 1 | total time: 3.31m\n",
      "step 00088 (10.33%) | loss: 1.672649 | lrm: 1.00 | dt: 2582.69ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 3.36m\n",
      "step 00089 (10.44%) | loss: 1.662100 | lrm: 1.00 | dt: 2581.99ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 3.40m\n",
      "step 00090 (10.56%) | loss: 1.643989 | lrm: 1.00 | dt: 2581.49ms | tok/sec: 203,094 | mfu: 16.47 | epoch: 1 | total time: 3.44m\n",
      "step 00091 (10.68%) | loss: 1.632772 | lrm: 1.00 | dt: 2581.86ms | tok/sec: 203,065 | mfu: 16.47 | epoch: 1 | total time: 3.49m\n",
      "step 00092 (10.81%) | loss: 1.618129 | lrm: 1.00 | dt: 2583.11ms | tok/sec: 202,967 | mfu: 16.46 | epoch: 1 | total time: 3.53m\n",
      "step 00093 (10.92%) | loss: 1.596107 | lrm: 1.00 | dt: 2581.73ms | tok/sec: 203,076 | mfu: 16.47 | epoch: 1 | total time: 3.57m\n",
      "step 00094 (11.05%) | loss: 1.571218 | lrm: 1.00 | dt: 2582.79ms | tok/sec: 202,992 | mfu: 16.46 | epoch: 1 | total time: 3.61m\n",
      "step 00095 (11.16%) | loss: 1.573861 | lrm: 1.00 | dt: 2581.98ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 3.66m\n",
      "step 00096 (11.28%) | loss: 1.571242 | lrm: 1.00 | dt: 2582.73ms | tok/sec: 202,997 | mfu: 16.46 | epoch: 1 | total time: 3.70m\n",
      "step 00097 (11.40%) | loss: 1.571671 | lrm: 1.00 | dt: 2586.40ms | tok/sec: 202,709 | mfu: 16.44 | epoch: 1 | total time: 3.74m\n",
      "step 00098 (11.52%) | loss: 1.568168 | lrm: 1.00 | dt: 2582.73ms | tok/sec: 202,997 | mfu: 16.46 | epoch: 1 | total time: 3.79m\n",
      "step 00099 (11.64%) | loss: 1.573146 | lrm: 1.00 | dt: 2584.74ms | tok/sec: 202,839 | mfu: 16.45 | epoch: 1 | total time: 3.83m\n",
      "step 00100 (11.76%) | loss: 1.567124 | lrm: 1.00 | dt: 2585.40ms | tok/sec: 202,787 | mfu: 16.45 | epoch: 1 | total time: 3.87m\n",
      "step 00101 (11.87%) | loss: 1.560525 | lrm: 1.00 | dt: 2582.85ms | tok/sec: 202,988 | mfu: 16.46 | epoch: 1 | total time: 3.92m\n",
      "step 00102 (11.99%) | loss: 1.556381 | lrm: 1.00 | dt: 2584.96ms | tok/sec: 202,822 | mfu: 16.45 | epoch: 1 | total time: 3.96m\n",
      "step 00103 (12.10%) | loss: 1.550821 | lrm: 1.00 | dt: 2584.80ms | tok/sec: 202,834 | mfu: 16.45 | epoch: 1 | total time: 4.00m\n",
      "step 00104 (12.22%) | loss: 1.554068 | lrm: 1.00 | dt: 2582.62ms | tok/sec: 203,006 | mfu: 16.47 | epoch: 1 | total time: 4.04m\n",
      "step 00105 (12.34%) | loss: 1.573877 | lrm: 1.00 | dt: 2582.18ms | tok/sec: 203,040 | mfu: 16.47 | epoch: 1 | total time: 4.09m\n",
      "step 00106 (12.46%) | loss: 1.592398 | lrm: 1.00 | dt: 2581.77ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 4.13m\n",
      "step 00107 (12.59%) | loss: 1.592546 | lrm: 1.00 | dt: 2582.87ms | tok/sec: 202,986 | mfu: 16.46 | epoch: 1 | total time: 4.17m\n",
      "step 00108 (12.70%) | loss: 1.590023 | lrm: 1.00 | dt: 2581.93ms | tok/sec: 203,060 | mfu: 16.47 | epoch: 1 | total time: 4.22m\n",
      "step 00109 (12.81%) | loss: 1.585471 | lrm: 1.00 | dt: 2579.93ms | tok/sec: 203,218 | mfu: 16.48 | epoch: 1 | total time: 4.26m\n",
      "step 00110 (12.94%) | loss: 1.566849 | lrm: 1.00 | dt: 2582.68ms | tok/sec: 203,001 | mfu: 16.47 | epoch: 1 | total time: 4.30m\n",
      "step 00111 (13.05%) | loss: 1.573411 | lrm: 1.00 | dt: 2580.22ms | tok/sec: 203,195 | mfu: 16.48 | epoch: 1 | total time: 4.35m\n",
      "step 00112 (13.17%) | loss: 1.578096 | lrm: 1.00 | dt: 2581.79ms | tok/sec: 203,071 | mfu: 16.47 | epoch: 1 | total time: 4.39m\n",
      "step 00113 (13.28%) | loss: 1.584513 | lrm: 1.00 | dt: 2581.65ms | tok/sec: 203,082 | mfu: 16.47 | epoch: 1 | total time: 4.43m\n",
      "step 00114 (13.40%) | loss: 1.603116 | lrm: 1.00 | dt: 2600.14ms | tok/sec: 201,638 | mfu: 16.35 | epoch: 1 | total time: 4.48m\n",
      "step 00115 (13.52%) | loss: 1.597990 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 16.47 | epoch: 1 | total time: 4.52m\n",
      "step 00116 (13.64%) | loss: 1.599099 | lrm: 1.00 | dt: 2582.06ms | tok/sec: 203,050 | mfu: 16.47 | epoch: 1 | total time: 4.56m\n",
      "step 00117 (13.76%) | loss: 1.606085 | lrm: 1.00 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 4.60m\n",
      "step 00118 (13.87%) | loss: 1.631305 | lrm: 1.00 | dt: 2583.19ms | tok/sec: 202,961 | mfu: 16.46 | epoch: 1 | total time: 4.65m\n",
      "step 00119 (13.99%) | loss: 1.617169 | lrm: 1.00 | dt: 2583.16ms | tok/sec: 202,964 | mfu: 16.46 | epoch: 1 | total time: 4.69m\n",
      "step 00120 (14.11%) | loss: 1.616853 | lrm: 1.00 | dt: 2582.95ms | tok/sec: 202,980 | mfu: 16.46 | epoch: 1 | total time: 4.73m\n",
      "step 00121 (14.23%) | loss: 1.605424 | lrm: 1.00 | dt: 2583.84ms | tok/sec: 202,910 | mfu: 16.46 | epoch: 1 | total time: 4.78m\n",
      "step 00122 (14.35%) | loss: 1.596053 | lrm: 1.00 | dt: 2583.61ms | tok/sec: 202,928 | mfu: 16.46 | epoch: 1 | total time: 4.82m\n",
      "step 00123 (14.47%) | loss: 1.607158 | lrm: 1.00 | dt: 2584.13ms | tok/sec: 202,887 | mfu: 16.46 | epoch: 1 | total time: 4.86m\n",
      "step 00124 (14.58%) | loss: 1.599110 | lrm: 1.00 | dt: 2585.87ms | tok/sec: 202,751 | mfu: 16.44 | epoch: 1 | total time: 4.91m\n",
      "step 00125 (14.71%) | loss: 1.625520 | lrm: 1.00 | dt: 2585.03ms | tok/sec: 202,816 | mfu: 16.45 | epoch: 1 | total time: 4.95m\n",
      "step 00126 (14.82%) | loss: 1.622926 | lrm: 1.00 | dt: 2585.13ms | tok/sec: 202,808 | mfu: 16.45 | epoch: 1 | total time: 4.99m\n",
      "step 00127 (14.93%) | loss: 1.616528 | lrm: 1.00 | dt: 2581.95ms | tok/sec: 203,059 | mfu: 16.47 | epoch: 1 | total time: 5.04m\n",
      "step 00128 (15.05%) | loss: 1.617127 | lrm: 1.00 | dt: 2585.71ms | tok/sec: 202,763 | mfu: 16.45 | epoch: 1 | total time: 5.08m\n",
      "step 00129 (15.16%) | loss: 1.615196 | lrm: 1.00 | dt: 2583.84ms | tok/sec: 202,910 | mfu: 16.46 | epoch: 1 | total time: 5.12m\n",
      "step 00130 (15.29%) | loss: 1.619138 | lrm: 1.00 | dt: 2599.79ms | tok/sec: 201,665 | mfu: 16.36 | epoch: 1 | total time: 5.16m\n",
      "step 00131 (15.41%) | loss: 1.621274 | lrm: 1.00 | dt: 2582.99ms | tok/sec: 202,977 | mfu: 16.46 | epoch: 1 | total time: 5.21m\n",
      "step 00132 (15.52%) | loss: 1.612391 | lrm: 1.00 | dt: 2583.60ms | tok/sec: 202,929 | mfu: 16.46 | epoch: 1 | total time: 5.25m\n",
      "step 00133 (15.64%) | loss: 1.603623 | lrm: 1.00 | dt: 2581.77ms | tok/sec: 203,072 | mfu: 16.47 | epoch: 1 | total time: 5.29m\n",
      "step 00134 (15.76%) | loss: 1.597905 | lrm: 1.00 | dt: 2583.35ms | tok/sec: 202,949 | mfu: 16.46 | epoch: 1 | total time: 5.34m\n",
      "step 00135 (15.88%) | loss: 1.585021 | lrm: 1.00 | dt: 2583.19ms | tok/sec: 202,961 | mfu: 16.46 | epoch: 1 | total time: 5.38m\n",
      "step 00136 (15.99%) | loss: 1.591691 | lrm: 1.00 | dt: 2583.00ms | tok/sec: 202,976 | mfu: 16.46 | epoch: 1 | total time: 5.42m\n",
      "step 00137 (16.11%) | loss: 1.570970 | lrm: 1.00 | dt: 2598.35ms | tok/sec: 201,777 | mfu: 16.37 | epoch: 1 | total time: 5.47m\n",
      "step 00138 (16.22%) | loss: 1.583830 | lrm: 1.00 | dt: 2583.61ms | tok/sec: 202,928 | mfu: 16.46 | epoch: 1 | total time: 5.51m\n",
      "step 00139 (16.34%) | loss: 1.596118 | lrm: 1.00 | dt: 2581.87ms | tok/sec: 203,064 | mfu: 16.47 | epoch: 1 | total time: 5.55m\n",
      "step 00140 (16.46%) | loss: 1.600069 | lrm: 1.00 | dt: 2583.06ms | tok/sec: 202,971 | mfu: 16.46 | epoch: 1 | total time: 5.60m\n",
      "step 00141 (16.58%) | loss: 1.591429 | lrm: 1.00 | dt: 2583.41ms | tok/sec: 202,943 | mfu: 16.46 | epoch: 1 | total time: 5.64m\n",
      "step 00142 (16.69%) | loss: 1.586472 | lrm: 1.00 | dt: 2582.70ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 5.68m\n",
      "step 00143 (16.81%) | loss: 1.585929 | lrm: 1.00 | dt: 2581.72ms | tok/sec: 203,077 | mfu: 16.47 | epoch: 1 | total time: 5.72m\n",
      "step 00144 (16.92%) | loss: 1.600914 | lrm: 1.00 | dt: 2582.86ms | tok/sec: 202,987 | mfu: 16.46 | epoch: 1 | total time: 5.77m\n",
      "step 00145 (17.05%) | loss: 1.581258 | lrm: 1.00 | dt: 2583.21ms | tok/sec: 202,959 | mfu: 16.46 | epoch: 1 | total time: 5.81m\n",
      "step 00146 (17.16%) | loss: 1.596208 | lrm: 1.00 | dt: 2582.64ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 5.85m\n",
      "step 00147 (17.28%) | loss: 1.605270 | lrm: 1.00 | dt: 2581.36ms | tok/sec: 203,105 | mfu: 16.47 | epoch: 1 | total time: 5.90m\n",
      "step 00148 (17.40%) | loss: 1.621222 | lrm: 1.00 | dt: 2582.85ms | tok/sec: 202,987 | mfu: 16.46 | epoch: 1 | total time: 5.94m\n",
      "step 00149 (17.51%) | loss: 1.611519 | lrm: 1.00 | dt: 2583.26ms | tok/sec: 202,955 | mfu: 16.46 | epoch: 1 | total time: 5.98m\n",
      "step 00150 (17.63%) | loss: 1.606900 | lrm: 1.00 | dt: 2583.37ms | tok/sec: 202,947 | mfu: 16.46 | epoch: 1 | total time: 6.03m\n",
      "Step 00150 | Validation bpb: 0.5205\n",
      "step 00151 (17.75%) | loss: 1.612108 | lrm: 1.00 | dt: 2572.12ms | tok/sec: 203,835 | mfu: 16.53 | epoch: 1 | total time: 6.07m\n",
      "step 00152 (17.87%) | loss: 1.586997 | lrm: 1.00 | dt: 2580.34ms | tok/sec: 203,185 | mfu: 16.48 | epoch: 1 | total time: 6.11m\n",
      "step 00153 (17.99%) | loss: 1.584001 | lrm: 1.00 | dt: 2581.14ms | tok/sec: 203,122 | mfu: 16.48 | epoch: 1 | total time: 6.15m\n",
      "step 00154 (18.11%) | loss: 1.588851 | lrm: 1.00 | dt: 2582.11ms | tok/sec: 203,046 | mfu: 16.47 | epoch: 1 | total time: 6.20m\n",
      "step 00155 (18.22%) | loss: 1.583859 | lrm: 1.00 | dt: 2582.25ms | tok/sec: 203,035 | mfu: 16.47 | epoch: 1 | total time: 6.24m\n",
      "step 00156 (18.34%) | loss: 1.594314 | lrm: 1.00 | dt: 2582.89ms | tok/sec: 202,985 | mfu: 16.46 | epoch: 1 | total time: 6.28m\n",
      "step 00157 (18.46%) | loss: 1.560458 | lrm: 1.00 | dt: 2581.46ms | tok/sec: 203,097 | mfu: 16.47 | epoch: 1 | total time: 6.33m\n",
      "step 00158 (18.57%) | loss: 1.551815 | lrm: 1.00 | dt: 2582.61ms | tok/sec: 203,006 | mfu: 16.47 | epoch: 1 | total time: 6.37m\n",
      "step 00159 (18.70%) | loss: 1.545744 | lrm: 1.00 | dt: 2584.22ms | tok/sec: 202,880 | mfu: 16.46 | epoch: 1 | total time: 6.41m\n",
      "step 00160 (18.82%) | loss: 1.552571 | lrm: 1.00 | dt: 2581.76ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 6.46m\n",
      "step 00161 (18.94%) | loss: 1.559568 | lrm: 1.00 | dt: 2579.03ms | tok/sec: 203,288 | mfu: 16.49 | epoch: 1 | total time: 6.50m\n",
      "step 00162 (19.06%) | loss: 1.544853 | lrm: 1.00 | dt: 2583.89ms | tok/sec: 202,906 | mfu: 16.46 | epoch: 1 | total time: 6.54m\n",
      "step 00163 (19.17%) | loss: 1.550475 | lrm: 1.00 | dt: 2581.85ms | tok/sec: 203,066 | mfu: 16.47 | epoch: 1 | total time: 6.59m\n",
      "step 00164 (19.28%) | loss: 1.557972 | lrm: 1.00 | dt: 2578.06ms | tok/sec: 203,365 | mfu: 16.49 | epoch: 1 | total time: 6.63m\n",
      "step 00165 (19.40%) | loss: 1.578374 | lrm: 1.00 | dt: 2578.51ms | tok/sec: 203,329 | mfu: 16.49 | epoch: 1 | total time: 6.67m\n",
      "step 00166 (19.52%) | loss: 1.562093 | lrm: 1.00 | dt: 2579.57ms | tok/sec: 203,246 | mfu: 16.49 | epoch: 1 | total time: 6.71m\n",
      "step 00167 (19.64%) | loss: 1.553485 | lrm: 1.00 | dt: 2579.89ms | tok/sec: 203,220 | mfu: 16.48 | epoch: 1 | total time: 6.76m\n",
      "step 00168 (19.75%) | loss: 1.550220 | lrm: 1.00 | dt: 2580.26ms | tok/sec: 203,191 | mfu: 16.48 | epoch: 1 | total time: 6.80m\n",
      "step 00169 (19.87%) | loss: 1.548388 | lrm: 1.00 | dt: 2579.47ms | tok/sec: 203,253 | mfu: 16.49 | epoch: 1 | total time: 6.84m\n",
      "step 00170 (19.99%) | loss: 1.549172 | lrm: 1.00 | dt: 2584.28ms | tok/sec: 202,875 | mfu: 16.46 | epoch: 1 | total time: 6.89m\n",
      "step 00171 (20.11%) | loss: 1.543927 | lrm: 1.00 | dt: 2582.23ms | tok/sec: 203,037 | mfu: 16.47 | epoch: 1 | total time: 6.93m\n",
      "step 00172 (20.24%) | loss: 1.551543 | lrm: 1.00 | dt: 2582.02ms | tok/sec: 203,053 | mfu: 16.47 | epoch: 1 | total time: 6.97m\n",
      "step 00173 (20.35%) | loss: 1.563043 | lrm: 1.00 | dt: 2582.82ms | tok/sec: 202,990 | mfu: 16.46 | epoch: 1 | total time: 7.02m\n",
      "step 00174 (20.47%) | loss: 1.577419 | lrm: 1.00 | dt: 2582.11ms | tok/sec: 203,046 | mfu: 16.47 | epoch: 1 | total time: 7.06m\n",
      "step 00175 (20.58%) | loss: 1.572876 | lrm: 1.00 | dt: 2581.76ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 7.10m\n",
      "step 00176 (20.70%) | loss: 1.564885 | lrm: 1.00 | dt: 2580.08ms | tok/sec: 203,205 | mfu: 16.48 | epoch: 1 | total time: 7.14m\n",
      "step 00177 (20.81%) | loss: 1.549772 | lrm: 1.00 | dt: 2581.68ms | tok/sec: 203,080 | mfu: 16.47 | epoch: 1 | total time: 7.19m\n",
      "step 00178 (20.93%) | loss: 1.553724 | lrm: 1.00 | dt: 2581.91ms | tok/sec: 203,061 | mfu: 16.47 | epoch: 1 | total time: 7.23m\n",
      "step 00179 (21.05%) | loss: 1.553880 | lrm: 1.00 | dt: 2589.72ms | tok/sec: 202,449 | mfu: 16.42 | epoch: 1 | total time: 7.27m\n",
      "step 00180 (21.17%) | loss: 1.539083 | lrm: 1.00 | dt: 2583.02ms | tok/sec: 202,975 | mfu: 16.46 | epoch: 1 | total time: 7.32m\n",
      "step 00181 (21.28%) | loss: 1.554197 | lrm: 1.00 | dt: 2583.85ms | tok/sec: 202,909 | mfu: 16.46 | epoch: 1 | total time: 7.36m\n",
      "step 00182 (21.40%) | loss: 1.540773 | lrm: 1.00 | dt: 2581.21ms | tok/sec: 203,116 | mfu: 16.47 | epoch: 1 | total time: 7.40m\n",
      "step 00183 (21.52%) | loss: 1.540479 | lrm: 1.00 | dt: 2582.12ms | tok/sec: 203,045 | mfu: 16.47 | epoch: 1 | total time: 7.45m\n",
      "step 00184 (21.64%) | loss: 1.534569 | lrm: 1.00 | dt: 2582.69ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 7.49m\n",
      "step 00185 (21.76%) | loss: 1.534560 | lrm: 1.00 | dt: 2581.66ms | tok/sec: 203,081 | mfu: 16.47 | epoch: 1 | total time: 7.53m\n",
      "step 00186 (21.88%) | loss: 1.533671 | lrm: 1.00 | dt: 2580.94ms | tok/sec: 203,138 | mfu: 16.48 | epoch: 1 | total time: 7.57m\n",
      "step 00187 (22.00%) | loss: 1.538892 | lrm: 1.00 | dt: 2583.48ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 7.62m\n",
      "step 00188 (22.12%) | loss: 1.552297 | lrm: 1.00 | dt: 2583.02ms | tok/sec: 202,975 | mfu: 16.46 | epoch: 1 | total time: 7.66m\n",
      "step 00189 (22.24%) | loss: 1.529406 | lrm: 1.00 | dt: 2584.26ms | tok/sec: 202,877 | mfu: 16.46 | epoch: 1 | total time: 7.70m\n",
      "step 00190 (22.36%) | loss: 1.533539 | lrm: 1.00 | dt: 2585.42ms | tok/sec: 202,786 | mfu: 16.45 | epoch: 1 | total time: 7.75m\n",
      "step 00191 (22.48%) | loss: 1.530187 | lrm: 1.00 | dt: 2585.46ms | tok/sec: 202,783 | mfu: 16.45 | epoch: 1 | total time: 7.79m\n",
      "step 00192 (22.60%) | loss: 1.520419 | lrm: 1.00 | dt: 2582.95ms | tok/sec: 202,980 | mfu: 16.46 | epoch: 1 | total time: 7.83m\n",
      "step 00193 (22.72%) | loss: 1.518439 | lrm: 1.00 | dt: 2583.00ms | tok/sec: 202,976 | mfu: 16.46 | epoch: 1 | total time: 7.88m\n",
      "step 00194 (22.84%) | loss: 1.526776 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 16.47 | epoch: 1 | total time: 7.92m\n",
      "step 00195 (22.96%) | loss: 1.525897 | lrm: 1.00 | dt: 2584.77ms | tok/sec: 202,837 | mfu: 16.45 | epoch: 1 | total time: 7.96m\n",
      "step 00196 (23.08%) | loss: 1.510483 | lrm: 1.00 | dt: 2583.39ms | tok/sec: 202,945 | mfu: 16.46 | epoch: 1 | total time: 8.01m\n",
      "step 00197 (23.20%) | loss: 1.522356 | lrm: 1.00 | dt: 2585.30ms | tok/sec: 202,796 | mfu: 16.45 | epoch: 1 | total time: 8.05m\n",
      "step 00198 (23.32%) | loss: 1.507773 | lrm: 1.00 | dt: 2581.19ms | tok/sec: 203,118 | mfu: 16.47 | epoch: 1 | total time: 8.09m\n",
      "step 00199 (23.43%) | loss: 1.501102 | lrm: 1.00 | dt: 2581.61ms | tok/sec: 203,085 | mfu: 16.47 | epoch: 1 | total time: 8.13m\n",
      "step 00200 (23.56%) | loss: 1.507147 | lrm: 1.00 | dt: 2581.35ms | tok/sec: 203,106 | mfu: 16.47 | epoch: 1 | total time: 8.18m\n",
      "step 00201 (23.68%) | loss: 1.527169 | lrm: 1.00 | dt: 2583.32ms | tok/sec: 202,951 | mfu: 16.46 | epoch: 1 | total time: 8.22m\n",
      "step 00202 (23.80%) | loss: 1.539662 | lrm: 1.00 | dt: 2582.71ms | tok/sec: 202,999 | mfu: 16.47 | epoch: 1 | total time: 8.26m\n",
      "step 00203 (23.92%) | loss: 1.529286 | lrm: 1.00 | dt: 2581.66ms | tok/sec: 203,082 | mfu: 16.47 | epoch: 1 | total time: 8.31m\n",
      "step 00204 (24.04%) | loss: 1.542595 | lrm: 1.00 | dt: 2582.20ms | tok/sec: 203,039 | mfu: 16.47 | epoch: 1 | total time: 8.35m\n",
      "step 00205 (24.15%) | loss: 1.534695 | lrm: 1.00 | dt: 2582.75ms | tok/sec: 202,995 | mfu: 16.46 | epoch: 1 | total time: 8.39m\n",
      "step 00206 (24.26%) | loss: 1.544611 | lrm: 1.00 | dt: 2580.51ms | tok/sec: 203,172 | mfu: 16.48 | epoch: 1 | total time: 8.44m\n",
      "step 00207 (24.38%) | loss: 1.537344 | lrm: 1.00 | dt: 2581.15ms | tok/sec: 203,121 | mfu: 16.47 | epoch: 1 | total time: 8.48m\n",
      "step 00208 (24.49%) | loss: 1.541852 | lrm: 1.00 | dt: 2582.56ms | tok/sec: 203,011 | mfu: 16.47 | epoch: 1 | total time: 8.52m\n",
      "step 00209 (24.61%) | loss: 1.525122 | lrm: 1.00 | dt: 2582.16ms | tok/sec: 203,042 | mfu: 16.47 | epoch: 1 | total time: 8.56m\n",
      "step 00210 (24.73%) | loss: 1.512379 | lrm: 1.00 | dt: 2581.33ms | tok/sec: 203,107 | mfu: 16.47 | epoch: 1 | total time: 8.61m\n",
      "step 00211 (24.85%) | loss: 1.544848 | lrm: 1.00 | dt: 2591.17ms | tok/sec: 202,336 | mfu: 16.41 | epoch: 1 | total time: 8.65m\n",
      "step 00212 (24.96%) | loss: 1.537927 | lrm: 1.00 | dt: 2592.90ms | tok/sec: 202,201 | mfu: 16.40 | epoch: 1 | total time: 8.69m\n",
      "step 00213 (25.08%) | loss: 1.525530 | lrm: 1.00 | dt: 2582.67ms | tok/sec: 203,002 | mfu: 16.47 | epoch: 1 | total time: 8.74m\n",
      "step 00214 (25.20%) | loss: 1.513840 | lrm: 1.00 | dt: 2583.92ms | tok/sec: 202,903 | mfu: 16.46 | epoch: 1 | total time: 8.78m\n",
      "step 00215 (25.32%) | loss: 1.507066 | lrm: 1.00 | dt: 2583.64ms | tok/sec: 202,926 | mfu: 16.46 | epoch: 1 | total time: 8.82m\n",
      "step 00216 (25.44%) | loss: 1.507104 | lrm: 1.00 | dt: 2585.76ms | tok/sec: 202,759 | mfu: 16.45 | epoch: 1 | total time: 8.87m\n",
      "step 00217 (25.55%) | loss: 1.499720 | lrm: 1.00 | dt: 2584.19ms | tok/sec: 202,882 | mfu: 16.46 | epoch: 1 | total time: 8.91m\n",
      "step 00218 (25.67%) | loss: 1.528739 | lrm: 1.00 | dt: 2586.31ms | tok/sec: 202,716 | mfu: 16.44 | epoch: 1 | total time: 8.95m\n",
      "step 00219 (25.78%) | loss: 1.495555 | lrm: 1.00 | dt: 2584.33ms | tok/sec: 202,871 | mfu: 16.45 | epoch: 1 | total time: 9.00m\n",
      "step 00220 (25.90%) | loss: 1.501984 | lrm: 1.00 | dt: 2584.49ms | tok/sec: 202,859 | mfu: 16.45 | epoch: 1 | total time: 9.04m\n",
      "step 00221 (26.02%) | loss: 1.507710 | lrm: 1.00 | dt: 2583.45ms | tok/sec: 202,941 | mfu: 16.46 | epoch: 1 | total time: 9.08m\n",
      "step 00222 (26.14%) | loss: 1.523275 | lrm: 1.00 | dt: 2583.50ms | tok/sec: 202,936 | mfu: 16.46 | epoch: 1 | total time: 9.13m\n",
      "step 00223 (26.26%) | loss: 1.520133 | lrm: 1.00 | dt: 2581.61ms | tok/sec: 203,085 | mfu: 16.47 | epoch: 1 | total time: 9.17m\n",
      "step 00224 (26.37%) | loss: 1.536780 | lrm: 1.00 | dt: 2581.80ms | tok/sec: 203,070 | mfu: 16.47 | epoch: 1 | total time: 9.21m\n",
      "step 00225 (26.48%) | loss: 1.518089 | lrm: 1.00 | dt: 2582.09ms | tok/sec: 203,048 | mfu: 16.47 | epoch: 1 | total time: 9.25m\n",
      "step 00226 (26.60%) | loss: 1.546229 | lrm: 1.00 | dt: 2582.51ms | tok/sec: 203,015 | mfu: 16.47 | epoch: 1 | total time: 9.30m\n",
      "step 00227 (26.71%) | loss: 1.516056 | lrm: 1.00 | dt: 2581.97ms | tok/sec: 203,057 | mfu: 16.47 | epoch: 1 | total time: 9.34m\n",
      "step 00228 (26.83%) | loss: 1.509835 | lrm: 1.00 | dt: 2581.96ms | tok/sec: 203,058 | mfu: 16.47 | epoch: 1 | total time: 9.38m\n",
      "step 00229 (26.95%) | loss: 1.514406 | lrm: 1.00 | dt: 2582.24ms | tok/sec: 203,036 | mfu: 16.47 | epoch: 1 | total time: 9.43m\n",
      "step 00230 (27.07%) | loss: 1.510178 | lrm: 1.00 | dt: 2583.12ms | tok/sec: 202,966 | mfu: 16.46 | epoch: 1 | total time: 9.47m\n",
      "step 00231 (27.19%) | loss: 1.509870 | lrm: 1.00 | dt: 2583.03ms | tok/sec: 202,974 | mfu: 16.46 | epoch: 1 | total time: 9.51m\n",
      "step 00232 (27.30%) | loss: 1.500046 | lrm: 1.00 | dt: 2586.45ms | tok/sec: 202,705 | mfu: 16.44 | epoch: 1 | total time: 9.56m\n",
      "step 00233 (27.42%) | loss: 1.499523 | lrm: 1.00 | dt: 2597.50ms | tok/sec: 201,843 | mfu: 16.37 | epoch: 1 | total time: 9.60m\n",
      "step 00234 (27.54%) | loss: 1.498375 | lrm: 1.00 | dt: 2583.50ms | tok/sec: 202,936 | mfu: 16.46 | epoch: 1 | total time: 9.64m\n",
      "step 00235 (27.65%) | loss: 1.516156 | lrm: 1.00 | dt: 2582.81ms | tok/sec: 202,991 | mfu: 16.46 | epoch: 1 | total time: 9.68m\n",
      "step 00236 (27.77%) | loss: 1.508380 | lrm: 1.00 | dt: 2582.59ms | tok/sec: 203,008 | mfu: 16.47 | epoch: 1 | total time: 9.73m\n",
      "step 00237 (27.89%) | loss: 1.505361 | lrm: 1.00 | dt: 2582.31ms | tok/sec: 203,030 | mfu: 16.47 | epoch: 1 | total time: 9.77m\n",
      "step 00238 (28.01%) | loss: 1.525051 | lrm: 1.00 | dt: 2582.31ms | tok/sec: 203,030 | mfu: 16.47 | epoch: 1 | total time: 9.81m\n",
      "step 00239 (28.13%) | loss: 1.501164 | lrm: 1.00 | dt: 2583.02ms | tok/sec: 202,974 | mfu: 16.46 | epoch: 1 | total time: 9.86m\n",
      "step 00240 (28.25%) | loss: 1.503159 | lrm: 1.00 | dt: 2582.93ms | tok/sec: 202,981 | mfu: 16.46 | epoch: 1 | total time: 9.90m\n",
      "step 00241 (28.37%) | loss: 1.498098 | lrm: 1.00 | dt: 2583.51ms | tok/sec: 202,936 | mfu: 16.46 | epoch: 1 | total time: 9.94m\n",
      "step 00242 (28.48%) | loss: 1.504500 | lrm: 1.00 | dt: 2586.76ms | tok/sec: 202,681 | mfu: 16.44 | epoch: 1 | total time: 9.99m\n",
      "step 00243 (28.60%) | loss: 1.497518 | lrm: 1.00 | dt: 2584.73ms | tok/sec: 202,840 | mfu: 16.45 | epoch: 1 | total time: 10.03m\n",
      "step 00244 (28.71%) | loss: 1.525597 | lrm: 1.00 | dt: 2584.77ms | tok/sec: 202,837 | mfu: 16.45 | epoch: 1 | total time: 10.07m\n",
      "step 00245 (28.83%) | loss: 1.530103 | lrm: 1.00 | dt: 2587.13ms | tok/sec: 202,652 | mfu: 16.44 | epoch: 1 | total time: 10.12m\n",
      "step 00246 (28.95%) | loss: 1.537867 | lrm: 1.00 | dt: 2587.13ms | tok/sec: 202,651 | mfu: 16.44 | epoch: 1 | total time: 10.16m\n",
      "step 00247 (29.06%) | loss: 1.544444 | lrm: 1.00 | dt: 2594.34ms | tok/sec: 202,089 | mfu: 16.39 | epoch: 1 | total time: 10.20m\n",
      "step 00248 (29.18%) | loss: 1.526632 | lrm: 1.00 | dt: 2581.96ms | tok/sec: 203,057 | mfu: 16.47 | epoch: 1 | total time: 10.24m\n",
      "step 00249 (29.30%) | loss: 1.523332 | lrm: 1.00 | dt: 2582.98ms | tok/sec: 202,978 | mfu: 16.46 | epoch: 1 | total time: 10.29m\n",
      "step 00250 (29.41%) | loss: 1.541251 | lrm: 1.00 | dt: 2582.50ms | tok/sec: 203,015 | mfu: 16.47 | epoch: 1 | total time: 10.33m\n",
      "step 00251 (29.53%) | loss: 1.523900 | lrm: 1.00 | dt: 2581.40ms | tok/sec: 203,102 | mfu: 16.47 | epoch: 1 | total time: 10.37m\n",
      "step 00252 (29.65%) | loss: 1.527325 | lrm: 1.00 | dt: 2581.84ms | tok/sec: 203,067 | mfu: 16.47 | epoch: 1 | total time: 10.42m\n",
      "step 00253 (29.76%) | loss: 1.532705 | lrm: 1.00 | dt: 2582.70ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 10.46m\n",
      "step 00254 (29.88%) | loss: 1.534653 | lrm: 1.00 | dt: 2581.17ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 10.50m\n",
      "step 00255 (30.00%) | loss: 1.517287 | lrm: 1.00 | dt: 2582.00ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 10.55m\n",
      "step 00256 (30.12%) | loss: 1.502315 | lrm: 1.00 | dt: 2581.51ms | tok/sec: 203,093 | mfu: 16.47 | epoch: 1 | total time: 10.59m\n",
      "step 00257 (30.23%) | loss: 1.499763 | lrm: 1.00 | dt: 2581.46ms | tok/sec: 203,097 | mfu: 16.47 | epoch: 1 | total time: 10.63m\n",
      "step 00258 (30.34%) | loss: 1.498455 | lrm: 1.00 | dt: 2581.51ms | tok/sec: 203,093 | mfu: 16.47 | epoch: 1 | total time: 10.68m\n",
      "step 00259 (30.46%) | loss: 1.511043 | lrm: 1.00 | dt: 2582.61ms | tok/sec: 203,007 | mfu: 16.47 | epoch: 1 | total time: 10.72m\n",
      "step 00260 (30.57%) | loss: 1.518052 | lrm: 1.00 | dt: 2583.19ms | tok/sec: 202,961 | mfu: 16.46 | epoch: 1 | total time: 10.76m\n",
      "step 00261 (30.69%) | loss: 1.515766 | lrm: 1.00 | dt: 2581.73ms | tok/sec: 203,075 | mfu: 16.47 | epoch: 1 | total time: 10.80m\n",
      "step 00262 (30.81%) | loss: 1.501363 | lrm: 1.00 | dt: 2581.94ms | tok/sec: 203,060 | mfu: 16.47 | epoch: 1 | total time: 10.85m\n",
      "step 00263 (30.93%) | loss: 1.512985 | lrm: 1.00 | dt: 2601.47ms | tok/sec: 201,535 | mfu: 16.35 | epoch: 1 | total time: 10.89m\n",
      "step 00264 (31.05%) | loss: 1.517050 | lrm: 1.00 | dt: 2582.28ms | tok/sec: 203,033 | mfu: 16.47 | epoch: 1 | total time: 10.93m\n",
      "step 00265 (31.17%) | loss: 1.508616 | lrm: 1.00 | dt: 2582.84ms | tok/sec: 202,989 | mfu: 16.46 | epoch: 1 | total time: 10.98m\n",
      "step 00266 (31.28%) | loss: 1.504855 | lrm: 1.00 | dt: 2583.13ms | tok/sec: 202,966 | mfu: 16.46 | epoch: 1 | total time: 11.02m\n",
      "step 00267 (31.40%) | loss: 1.509229 | lrm: 1.00 | dt: 2585.57ms | tok/sec: 202,774 | mfu: 16.45 | epoch: 1 | total time: 11.06m\n",
      "step 00268 (31.51%) | loss: 1.521533 | lrm: 1.00 | dt: 2584.13ms | tok/sec: 202,887 | mfu: 16.46 | epoch: 1 | total time: 11.11m\n",
      "step 00269 (31.63%) | loss: 1.528097 | lrm: 1.00 | dt: 2585.12ms | tok/sec: 202,809 | mfu: 16.45 | epoch: 1 | total time: 11.15m\n",
      "step 00270 (31.75%) | loss: 1.526687 | lrm: 1.00 | dt: 2584.03ms | tok/sec: 202,895 | mfu: 16.46 | epoch: 1 | total time: 11.19m\n",
      "step 00271 (31.87%) | loss: 1.538364 | lrm: 1.00 | dt: 2585.91ms | tok/sec: 202,748 | mfu: 16.44 | epoch: 1 | total time: 11.24m\n",
      "step 00272 (31.99%) | loss: 1.554905 | lrm: 1.00 | dt: 2586.37ms | tok/sec: 202,711 | mfu: 16.44 | epoch: 1 | total time: 11.28m\n",
      "step 00273 (32.11%) | loss: 1.545264 | lrm: 1.00 | dt: 2592.98ms | tok/sec: 202,194 | mfu: 16.40 | epoch: 1 | total time: 11.32m\n",
      "step 00274 (32.23%) | loss: 1.538417 | lrm: 1.00 | dt: 2589.83ms | tok/sec: 202,441 | mfu: 16.42 | epoch: 1 | total time: 11.36m\n",
      "step 00275 (32.35%) | loss: 1.513971 | lrm: 1.00 | dt: 2582.68ms | tok/sec: 203,001 | mfu: 16.47 | epoch: 1 | total time: 11.41m\n",
      "step 00276 (32.47%) | loss: 1.502220 | lrm: 1.00 | dt: 2582.57ms | tok/sec: 203,010 | mfu: 16.47 | epoch: 1 | total time: 11.45m\n",
      "step 00277 (32.59%) | loss: 1.482518 | lrm: 1.00 | dt: 2582.25ms | tok/sec: 203,035 | mfu: 16.47 | epoch: 1 | total time: 11.49m\n",
      "step 00278 (32.70%) | loss: 1.483371 | lrm: 1.00 | dt: 2582.35ms | tok/sec: 203,027 | mfu: 16.47 | epoch: 1 | total time: 11.54m\n",
      "step 00279 (32.82%) | loss: 1.475275 | lrm: 1.00 | dt: 2581.66ms | tok/sec: 203,081 | mfu: 16.47 | epoch: 1 | total time: 11.58m\n",
      "step 00280 (32.94%) | loss: 1.482296 | lrm: 1.00 | dt: 2581.94ms | tok/sec: 203,059 | mfu: 16.47 | epoch: 1 | total time: 11.62m\n",
      "step 00281 (33.06%) | loss: 1.487197 | lrm: 1.00 | dt: 2582.37ms | tok/sec: 203,026 | mfu: 16.47 | epoch: 1 | total time: 11.67m\n",
      "step 00282 (33.17%) | loss: 1.504340 | lrm: 1.00 | dt: 2582.62ms | tok/sec: 203,006 | mfu: 16.47 | epoch: 1 | total time: 11.71m\n",
      "step 00283 (33.29%) | loss: 1.512846 | lrm: 1.00 | dt: 2582.71ms | tok/sec: 202,998 | mfu: 16.47 | epoch: 1 | total time: 11.75m\n",
      "step 00284 (33.41%) | loss: 1.527283 | lrm: 1.00 | dt: 2583.30ms | tok/sec: 202,952 | mfu: 16.46 | epoch: 1 | total time: 11.80m\n",
      "step 00285 (33.52%) | loss: 1.523106 | lrm: 1.00 | dt: 2580.88ms | tok/sec: 203,143 | mfu: 16.48 | epoch: 1 | total time: 11.84m\n",
      "step 00286 (33.64%) | loss: 1.523393 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 16.47 | epoch: 1 | total time: 11.88m\n",
      "step 00287 (33.76%) | loss: 1.519203 | lrm: 1.00 | dt: 2581.53ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 11.92m\n",
      "step 00288 (33.88%) | loss: 1.510348 | lrm: 1.00 | dt: 2581.23ms | tok/sec: 203,115 | mfu: 16.47 | epoch: 1 | total time: 11.97m\n",
      "step 00289 (33.99%) | loss: 1.504168 | lrm: 1.00 | dt: 2581.50ms | tok/sec: 203,093 | mfu: 16.47 | epoch: 1 | total time: 12.01m\n",
      "step 00290 (34.11%) | loss: 1.492681 | lrm: 1.00 | dt: 2582.64ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 12.05m\n",
      "step 00291 (34.22%) | loss: 1.492988 | lrm: 1.00 | dt: 2582.52ms | tok/sec: 203,014 | mfu: 16.47 | epoch: 1 | total time: 12.10m\n",
      "step 00292 (34.35%) | loss: 1.477279 | lrm: 1.00 | dt: 2582.53ms | tok/sec: 203,013 | mfu: 16.47 | epoch: 1 | total time: 12.14m\n",
      "step 00293 (34.47%) | loss: 1.502800 | lrm: 1.00 | dt: 2584.43ms | tok/sec: 202,864 | mfu: 16.45 | epoch: 1 | total time: 12.18m\n",
      "step 00294 (34.59%) | loss: 1.515238 | lrm: 1.00 | dt: 2583.68ms | tok/sec: 202,923 | mfu: 16.46 | epoch: 1 | total time: 12.23m\n",
      "step 00295 (34.70%) | loss: 1.512482 | lrm: 1.00 | dt: 2583.85ms | tok/sec: 202,909 | mfu: 16.46 | epoch: 1 | total time: 12.27m\n",
      "step 00296 (34.83%) | loss: 1.516092 | lrm: 1.00 | dt: 2585.28ms | tok/sec: 202,797 | mfu: 16.45 | epoch: 1 | total time: 12.31m\n",
      "step 00297 (34.95%) | loss: 1.533566 | lrm: 1.00 | dt: 2583.17ms | tok/sec: 202,962 | mfu: 16.46 | epoch: 1 | total time: 12.35m\n",
      "step 00298 (35.07%) | loss: 1.543326 | lrm: 1.00 | dt: 2583.35ms | tok/sec: 202,948 | mfu: 16.46 | epoch: 1 | total time: 12.40m\n",
      "step 00299 (35.19%) | loss: 1.527645 | lrm: 1.00 | dt: 2585.76ms | tok/sec: 202,759 | mfu: 16.45 | epoch: 1 | total time: 12.44m\n",
      "step 00300 (35.30%) | loss: 1.534521 | lrm: 1.00 | dt: 2584.40ms | tok/sec: 202,866 | mfu: 16.45 | epoch: 1 | total time: 12.48m\n",
      "Step 00300 | Validation bpb: 0.5016\n",
      "step 00301 (35.43%) | loss: 1.535033 | lrm: 1.00 | dt: 2570.71ms | tok/sec: 203,946 | mfu: 16.54 | epoch: 1 | total time: 12.53m\n",
      "step 00302 (35.55%) | loss: 1.532241 | lrm: 1.00 | dt: 2573.33ms | tok/sec: 203,738 | mfu: 16.53 | epoch: 1 | total time: 12.57m\n",
      "step 00303 (35.67%) | loss: 1.515592 | lrm: 1.00 | dt: 2573.35ms | tok/sec: 203,737 | mfu: 16.52 | epoch: 1 | total time: 12.61m\n",
      "step 00304 (35.78%) | loss: 1.515376 | lrm: 1.00 | dt: 2578.72ms | tok/sec: 203,313 | mfu: 16.49 | epoch: 1 | total time: 12.66m\n",
      "step 00305 (35.90%) | loss: 1.495140 | lrm: 1.00 | dt: 2574.31ms | tok/sec: 203,661 | mfu: 16.52 | epoch: 1 | total time: 12.70m\n",
      "step 00306 (36.02%) | loss: 1.501000 | lrm: 1.00 | dt: 2574.32ms | tok/sec: 203,660 | mfu: 16.52 | epoch: 1 | total time: 12.74m\n",
      "step 00307 (36.14%) | loss: 1.490067 | lrm: 1.00 | dt: 2576.44ms | tok/sec: 203,493 | mfu: 16.51 | epoch: 1 | total time: 12.78m\n",
      "step 00308 (36.26%) | loss: 1.506162 | lrm: 1.00 | dt: 2575.22ms | tok/sec: 203,589 | mfu: 16.51 | epoch: 1 | total time: 12.83m\n",
      "step 00309 (36.37%) | loss: 1.490492 | lrm: 1.00 | dt: 2574.54ms | tok/sec: 203,643 | mfu: 16.52 | epoch: 1 | total time: 12.87m\n",
      "step 00310 (36.50%) | loss: 1.495373 | lrm: 1.00 | dt: 2576.86ms | tok/sec: 203,460 | mfu: 16.50 | epoch: 1 | total time: 12.91m\n",
      "step 00311 (36.62%) | loss: 1.484376 | lrm: 1.00 | dt: 2575.67ms | tok/sec: 203,554 | mfu: 16.51 | epoch: 1 | total time: 12.96m\n",
      "step 00312 (36.74%) | loss: 1.487471 | lrm: 1.00 | dt: 2573.45ms | tok/sec: 203,729 | mfu: 16.52 | epoch: 1 | total time: 13.00m\n",
      "step 00313 (36.86%) | loss: 1.486026 | lrm: 1.00 | dt: 2579.14ms | tok/sec: 203,280 | mfu: 16.49 | epoch: 1 | total time: 13.04m\n",
      "step 00314 (36.97%) | loss: 1.491983 | lrm: 1.00 | dt: 2579.93ms | tok/sec: 203,218 | mfu: 16.48 | epoch: 1 | total time: 13.09m\n",
      "step 00315 (37.09%) | loss: 1.498200 | lrm: 1.00 | dt: 2580.32ms | tok/sec: 203,187 | mfu: 16.48 | epoch: 1 | total time: 13.13m\n",
      "step 00316 (37.21%) | loss: 1.478436 | lrm: 1.00 | dt: 2572.59ms | tok/sec: 203,797 | mfu: 16.53 | epoch: 1 | total time: 13.17m\n",
      "step 00317 (37.33%) | loss: 1.474168 | lrm: 1.00 | dt: 2578.94ms | tok/sec: 203,295 | mfu: 16.49 | epoch: 1 | total time: 13.21m\n",
      "step 00318 (37.45%) | loss: 1.455150 | lrm: 1.00 | dt: 2578.87ms | tok/sec: 203,301 | mfu: 16.49 | epoch: 1 | total time: 13.26m\n",
      "step 00319 (37.56%) | loss: 1.466890 | lrm: 1.00 | dt: 2580.26ms | tok/sec: 203,192 | mfu: 16.48 | epoch: 1 | total time: 13.30m\n",
      "step 00320 (37.68%) | loss: 1.473921 | lrm: 1.00 | dt: 2582.41ms | tok/sec: 203,022 | mfu: 16.47 | epoch: 1 | total time: 13.34m\n",
      "step 00321 (37.79%) | loss: 1.461056 | lrm: 1.00 | dt: 2580.41ms | tok/sec: 203,180 | mfu: 16.48 | epoch: 1 | total time: 13.39m\n",
      "step 00322 (37.91%) | loss: 1.480834 | lrm: 1.00 | dt: 2585.61ms | tok/sec: 202,771 | mfu: 16.45 | epoch: 1 | total time: 13.43m\n",
      "step 00323 (38.02%) | loss: 1.478761 | lrm: 1.00 | dt: 2585.68ms | tok/sec: 202,766 | mfu: 16.45 | epoch: 1 | total time: 13.47m\n",
      "step 00324 (38.14%) | loss: 1.479856 | lrm: 1.00 | dt: 2582.20ms | tok/sec: 203,038 | mfu: 16.47 | epoch: 1 | total time: 13.52m\n",
      "step 00325 (38.26%) | loss: 1.461495 | lrm: 1.00 | dt: 2578.07ms | tok/sec: 203,364 | mfu: 16.49 | epoch: 1 | total time: 13.56m\n",
      "step 00326 (38.38%) | loss: 1.463099 | lrm: 1.00 | dt: 2585.40ms | tok/sec: 202,787 | mfu: 16.45 | epoch: 1 | total time: 13.60m\n",
      "step 00327 (38.50%) | loss: 1.488470 | lrm: 1.00 | dt: 2587.45ms | tok/sec: 202,627 | mfu: 16.43 | epoch: 1 | total time: 13.64m\n",
      "step 00328 (38.62%) | loss: 1.496255 | lrm: 1.00 | dt: 2582.60ms | tok/sec: 203,008 | mfu: 16.47 | epoch: 1 | total time: 13.69m\n",
      "step 00329 (38.74%) | loss: 1.505180 | lrm: 1.00 | dt: 2585.73ms | tok/sec: 202,762 | mfu: 16.45 | epoch: 1 | total time: 13.73m\n",
      "step 00330 (38.85%) | loss: 1.493651 | lrm: 1.00 | dt: 2584.43ms | tok/sec: 202,863 | mfu: 16.45 | epoch: 1 | total time: 13.77m\n",
      "step 00331 (38.97%) | loss: 1.476172 | lrm: 1.00 | dt: 2581.08ms | tok/sec: 203,127 | mfu: 16.48 | epoch: 1 | total time: 13.82m\n",
      "step 00332 (39.10%) | loss: 1.494089 | lrm: 1.00 | dt: 2581.03ms | tok/sec: 203,131 | mfu: 16.48 | epoch: 1 | total time: 13.86m\n",
      "step 00333 (39.22%) | loss: 1.489177 | lrm: 1.00 | dt: 2584.60ms | tok/sec: 202,850 | mfu: 16.45 | epoch: 1 | total time: 13.90m\n",
      "step 00334 (39.33%) | loss: 1.485107 | lrm: 1.00 | dt: 2583.10ms | tok/sec: 202,968 | mfu: 16.46 | epoch: 1 | total time: 13.95m\n",
      "step 00335 (39.45%) | loss: 1.460670 | lrm: 1.00 | dt: 2589.07ms | tok/sec: 202,500 | mfu: 16.42 | epoch: 1 | total time: 13.99m\n",
      "step 00336 (39.56%) | loss: 1.474094 | lrm: 1.00 | dt: 2582.48ms | tok/sec: 203,017 | mfu: 16.47 | epoch: 1 | total time: 14.03m\n",
      "step 00337 (39.68%) | loss: 1.462991 | lrm: 1.00 | dt: 2580.05ms | tok/sec: 203,208 | mfu: 16.48 | epoch: 1 | total time: 14.07m\n",
      "step 00338 (39.80%) | loss: 1.446582 | lrm: 1.00 | dt: 2581.16ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 14.12m\n",
      "step 00339 (39.92%) | loss: 1.466803 | lrm: 1.00 | dt: 2583.27ms | tok/sec: 202,954 | mfu: 16.46 | epoch: 1 | total time: 14.16m\n",
      "step 00340 (40.04%) | loss: 1.471736 | lrm: 1.00 | dt: 2585.06ms | tok/sec: 202,814 | mfu: 16.45 | epoch: 1 | total time: 14.20m\n",
      "step 00341 (40.16%) | loss: 1.497007 | lrm: 1.00 | dt: 2585.08ms | tok/sec: 202,813 | mfu: 16.45 | epoch: 1 | total time: 14.25m\n",
      "step 00342 (40.27%) | loss: 1.501991 | lrm: 1.00 | dt: 2589.55ms | tok/sec: 202,463 | mfu: 16.42 | epoch: 1 | total time: 14.29m\n",
      "step 00343 (40.40%) | loss: 1.529897 | lrm: 1.00 | dt: 2586.53ms | tok/sec: 202,699 | mfu: 16.44 | epoch: 1 | total time: 14.33m\n",
      "step 00344 (40.51%) | loss: 1.517708 | lrm: 1.00 | dt: 2583.58ms | tok/sec: 202,930 | mfu: 16.46 | epoch: 1 | total time: 14.38m\n",
      "step 00345 (40.63%) | loss: 1.497399 | lrm: 1.00 | dt: 2581.94ms | tok/sec: 203,059 | mfu: 16.47 | epoch: 1 | total time: 14.42m\n",
      "step 00346 (40.74%) | loss: 1.489864 | lrm: 1.00 | dt: 2582.20ms | tok/sec: 203,039 | mfu: 16.47 | epoch: 1 | total time: 14.46m\n",
      "step 00347 (40.87%) | loss: 1.501710 | lrm: 1.00 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 14.51m\n",
      "step 00348 (40.98%) | loss: 1.495669 | lrm: 1.00 | dt: 2582.56ms | tok/sec: 203,011 | mfu: 16.47 | epoch: 1 | total time: 14.55m\n",
      "step 00349 (41.10%) | loss: 1.493475 | lrm: 1.00 | dt: 2581.97ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 14.59m\n",
      "step 00350 (41.21%) | loss: 1.502950 | lrm: 1.00 | dt: 2582.01ms | tok/sec: 203,054 | mfu: 16.47 | epoch: 1 | total time: 14.63m\n",
      "step 00351 (41.33%) | loss: 1.502610 | lrm: 1.00 | dt: 2581.17ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 14.68m\n",
      "step 00352 (41.45%) | loss: 1.506641 | lrm: 1.00 | dt: 2584.26ms | tok/sec: 202,877 | mfu: 16.46 | epoch: 1 | total time: 14.72m\n",
      "step 00353 (41.56%) | loss: 1.510251 | lrm: 1.00 | dt: 2582.08ms | tok/sec: 203,048 | mfu: 16.47 | epoch: 1 | total time: 14.76m\n",
      "step 00354 (41.68%) | loss: 1.503910 | lrm: 1.00 | dt: 2582.80ms | tok/sec: 202,991 | mfu: 16.46 | epoch: 1 | total time: 14.81m\n",
      "step 00355 (41.80%) | loss: 1.494153 | lrm: 1.00 | dt: 2583.58ms | tok/sec: 202,930 | mfu: 16.46 | epoch: 1 | total time: 14.85m\n",
      "step 00356 (41.92%) | loss: 1.485246 | lrm: 1.00 | dt: 2582.49ms | tok/sec: 203,016 | mfu: 16.47 | epoch: 1 | total time: 14.89m\n",
      "step 00357 (42.05%) | loss: 1.468501 | lrm: 1.00 | dt: 2583.51ms | tok/sec: 202,936 | mfu: 16.46 | epoch: 1 | total time: 14.94m\n",
      "step 00358 (42.16%) | loss: 1.471691 | lrm: 1.00 | dt: 2582.10ms | tok/sec: 203,046 | mfu: 16.47 | epoch: 1 | total time: 14.98m\n",
      "step 00359 (42.28%) | loss: 1.495155 | lrm: 1.00 | dt: 2583.60ms | tok/sec: 202,929 | mfu: 16.46 | epoch: 1 | total time: 15.02m\n",
      "step 00360 (42.40%) | loss: 1.489779 | lrm: 1.00 | dt: 2585.28ms | tok/sec: 202,797 | mfu: 16.45 | epoch: 1 | total time: 15.07m\n",
      "step 00361 (42.52%) | loss: 1.509184 | lrm: 1.00 | dt: 2584.38ms | tok/sec: 202,867 | mfu: 16.45 | epoch: 1 | total time: 15.11m\n",
      "step 00362 (42.64%) | loss: 1.501991 | lrm: 1.00 | dt: 2585.72ms | tok/sec: 202,762 | mfu: 16.45 | epoch: 1 | total time: 15.15m\n",
      "step 00363 (42.75%) | loss: 1.506266 | lrm: 1.00 | dt: 2590.15ms | tok/sec: 202,416 | mfu: 16.42 | epoch: 1 | total time: 15.19m\n",
      "step 00364 (42.87%) | loss: 1.489150 | lrm: 1.00 | dt: 2586.56ms | tok/sec: 202,697 | mfu: 16.44 | epoch: 1 | total time: 15.24m\n",
      "step 00365 (42.98%) | loss: 1.476455 | lrm: 1.00 | dt: 2589.86ms | tok/sec: 202,438 | mfu: 16.42 | epoch: 1 | total time: 15.28m\n",
      "step 00366 (43.10%) | loss: 1.473076 | lrm: 1.00 | dt: 2582.83ms | tok/sec: 202,989 | mfu: 16.46 | epoch: 1 | total time: 15.32m\n",
      "step 00367 (43.22%) | loss: 1.467814 | lrm: 1.00 | dt: 2586.03ms | tok/sec: 202,738 | mfu: 16.44 | epoch: 1 | total time: 15.37m\n",
      "step 00368 (43.34%) | loss: 1.462023 | lrm: 1.00 | dt: 2598.47ms | tok/sec: 201,768 | mfu: 16.37 | epoch: 1 | total time: 15.41m\n",
      "step 00369 (43.46%) | loss: 1.463429 | lrm: 1.00 | dt: 2582.83ms | tok/sec: 202,989 | mfu: 16.46 | epoch: 1 | total time: 15.45m\n",
      "step 00370 (43.57%) | loss: 1.451705 | lrm: 1.00 | dt: 2583.48ms | tok/sec: 202,938 | mfu: 16.46 | epoch: 1 | total time: 15.50m\n",
      "step 00371 (43.69%) | loss: 1.445303 | lrm: 1.00 | dt: 2581.61ms | tok/sec: 203,085 | mfu: 16.47 | epoch: 1 | total time: 15.54m\n",
      "step 00372 (43.81%) | loss: 1.441421 | lrm: 1.00 | dt: 2591.64ms | tok/sec: 202,299 | mfu: 16.41 | epoch: 1 | total time: 15.58m\n",
      "step 00373 (43.93%) | loss: 1.446293 | lrm: 1.00 | dt: 2590.75ms | tok/sec: 202,369 | mfu: 16.41 | epoch: 1 | total time: 15.63m\n",
      "step 00374 (44.05%) | loss: 1.451577 | lrm: 1.00 | dt: 2583.77ms | tok/sec: 202,915 | mfu: 16.46 | epoch: 1 | total time: 15.67m\n",
      "step 00375 (44.17%) | loss: 1.455186 | lrm: 1.00 | dt: 2581.90ms | tok/sec: 203,062 | mfu: 16.47 | epoch: 1 | total time: 15.71m\n",
      "step 00376 (44.29%) | loss: 1.463285 | lrm: 1.00 | dt: 2583.16ms | tok/sec: 202,963 | mfu: 16.46 | epoch: 1 | total time: 15.75m\n",
      "step 00377 (44.41%) | loss: 1.457918 | lrm: 1.00 | dt: 2582.65ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 15.80m\n",
      "step 00378 (44.52%) | loss: 1.473539 | lrm: 1.00 | dt: 2582.87ms | tok/sec: 202,986 | mfu: 16.46 | epoch: 1 | total time: 15.84m\n",
      "step 00379 (44.64%) | loss: 1.462101 | lrm: 1.00 | dt: 2581.86ms | tok/sec: 203,065 | mfu: 16.47 | epoch: 1 | total time: 15.88m\n",
      "step 00380 (44.76%) | loss: 1.460553 | lrm: 1.00 | dt: 2599.81ms | tok/sec: 201,663 | mfu: 16.36 | epoch: 1 | total time: 15.93m\n",
      "step 00381 (44.88%) | loss: 1.459982 | lrm: 1.00 | dt: 2581.45ms | tok/sec: 203,098 | mfu: 16.47 | epoch: 1 | total time: 15.97m\n",
      "step 00382 (45.00%) | loss: 1.443549 | lrm: 1.00 | dt: 2584.21ms | tok/sec: 202,881 | mfu: 16.46 | epoch: 1 | total time: 16.01m\n",
      "step 00383 (45.12%) | loss: 1.438656 | lrm: 1.00 | dt: 2596.91ms | tok/sec: 201,888 | mfu: 16.37 | epoch: 1 | total time: 16.06m\n",
      "step 00384 (45.23%) | loss: 1.427034 | lrm: 1.00 | dt: 2583.40ms | tok/sec: 202,944 | mfu: 16.46 | epoch: 1 | total time: 16.10m\n",
      "step 00385 (45.36%) | loss: 1.428751 | lrm: 1.00 | dt: 2584.65ms | tok/sec: 202,846 | mfu: 16.45 | epoch: 1 | total time: 16.14m\n",
      "step 00386 (45.47%) | loss: 1.413083 | lrm: 1.00 | dt: 2584.32ms | tok/sec: 202,872 | mfu: 16.45 | epoch: 1 | total time: 16.19m\n",
      "step 00387 (45.59%) | loss: 1.408355 | lrm: 1.00 | dt: 2584.75ms | tok/sec: 202,838 | mfu: 16.45 | epoch: 1 | total time: 16.23m\n",
      "step 00388 (45.71%) | loss: 1.421051 | lrm: 1.00 | dt: 2584.14ms | tok/sec: 202,886 | mfu: 16.46 | epoch: 1 | total time: 16.27m\n",
      "step 00389 (45.83%) | loss: 1.423534 | lrm: 1.00 | dt: 2588.46ms | tok/sec: 202,548 | mfu: 16.43 | epoch: 1 | total time: 16.32m\n",
      "step 00390 (45.95%) | loss: 1.440669 | lrm: 1.00 | dt: 2589.05ms | tok/sec: 202,501 | mfu: 16.42 | epoch: 1 | total time: 16.36m\n",
      "step 00391 (46.07%) | loss: 1.434550 | lrm: 1.00 | dt: 2589.71ms | tok/sec: 202,450 | mfu: 16.42 | epoch: 1 | total time: 16.40m\n",
      "step 00392 (46.18%) | loss: 1.443225 | lrm: 1.00 | dt: 2584.18ms | tok/sec: 202,883 | mfu: 16.46 | epoch: 1 | total time: 16.44m\n",
      "step 00393 (46.30%) | loss: 1.443118 | lrm: 1.00 | dt: 2583.28ms | tok/sec: 202,954 | mfu: 16.46 | epoch: 1 | total time: 16.49m\n",
      "step 00394 (46.42%) | loss: 1.437536 | lrm: 1.00 | dt: 2583.53ms | tok/sec: 202,934 | mfu: 16.46 | epoch: 1 | total time: 16.53m\n",
      "step 00395 (46.54%) | loss: 1.421939 | lrm: 1.00 | dt: 2582.79ms | tok/sec: 202,993 | mfu: 16.46 | epoch: 1 | total time: 16.57m\n",
      "step 00396 (46.65%) | loss: 1.421246 | lrm: 1.00 | dt: 2584.10ms | tok/sec: 202,889 | mfu: 16.46 | epoch: 1 | total time: 16.62m\n",
      "step 00397 (46.77%) | loss: 1.415554 | lrm: 1.00 | dt: 2582.39ms | tok/sec: 203,024 | mfu: 16.47 | epoch: 1 | total time: 16.66m\n",
      "step 00398 (46.89%) | loss: 1.435185 | lrm: 1.00 | dt: 2599.87ms | tok/sec: 201,658 | mfu: 16.36 | epoch: 1 | total time: 16.70m\n",
      "step 00399 (47.01%) | loss: 1.442255 | lrm: 1.00 | dt: 2582.82ms | tok/sec: 202,990 | mfu: 16.46 | epoch: 1 | total time: 16.75m\n",
      "step 00400 (47.13%) | loss: 1.457482 | lrm: 1.00 | dt: 2583.93ms | tok/sec: 202,903 | mfu: 16.46 | epoch: 1 | total time: 16.79m\n",
      "step 00401 (47.24%) | loss: 1.483557 | lrm: 1.00 | dt: 2599.00ms | tok/sec: 201,726 | mfu: 16.36 | epoch: 1 | total time: 16.83m\n",
      "step 00402 (47.37%) | loss: 1.475669 | lrm: 1.00 | dt: 2580.96ms | tok/sec: 203,136 | mfu: 16.48 | epoch: 1 | total time: 16.88m\n",
      "step 00403 (47.49%) | loss: 1.468042 | lrm: 1.00 | dt: 2600.30ms | tok/sec: 201,625 | mfu: 16.35 | epoch: 1 | total time: 16.92m\n",
      "step 00404 (47.61%) | loss: 1.479497 | lrm: 1.00 | dt: 2583.06ms | tok/sec: 202,971 | mfu: 16.46 | epoch: 1 | total time: 16.96m\n",
      "step 00405 (47.72%) | loss: 1.492822 | lrm: 1.00 | dt: 2582.44ms | tok/sec: 203,020 | mfu: 16.47 | epoch: 1 | total time: 17.01m\n",
      "step 00406 (47.84%) | loss: 1.496501 | lrm: 1.00 | dt: 2581.77ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 17.05m\n",
      "step 00407 (47.96%) | loss: 1.499569 | lrm: 1.00 | dt: 2600.14ms | tok/sec: 201,638 | mfu: 16.35 | epoch: 1 | total time: 17.09m\n",
      "step 00408 (48.07%) | loss: 1.495551 | lrm: 1.00 | dt: 2582.82ms | tok/sec: 202,990 | mfu: 16.46 | epoch: 1 | total time: 17.13m\n",
      "step 00409 (48.19%) | loss: 1.484962 | lrm: 1.00 | dt: 2583.18ms | tok/sec: 202,962 | mfu: 16.46 | epoch: 1 | total time: 17.18m\n",
      "step 00410 (48.31%) | loss: 1.474128 | lrm: 1.00 | dt: 2583.66ms | tok/sec: 202,924 | mfu: 16.46 | epoch: 1 | total time: 17.22m\n",
      "step 00411 (48.44%) | loss: 1.463433 | lrm: 1.00 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 17.26m\n",
      "step 00412 (48.56%) | loss: 1.459169 | lrm: 1.00 | dt: 2584.97ms | tok/sec: 202,821 | mfu: 16.45 | epoch: 1 | total time: 17.31m\n",
      "step 00413 (48.67%) | loss: 1.449368 | lrm: 1.00 | dt: 2584.08ms | tok/sec: 202,891 | mfu: 16.46 | epoch: 1 | total time: 17.35m\n",
      "step 00414 (48.80%) | loss: 1.463215 | lrm: 1.00 | dt: 2586.10ms | tok/sec: 202,733 | mfu: 16.44 | epoch: 1 | total time: 17.39m\n",
      "step 00415 (48.91%) | loss: 1.451270 | lrm: 1.00 | dt: 2587.12ms | tok/sec: 202,652 | mfu: 16.44 | epoch: 1 | total time: 17.44m\n",
      "step 00416 (49.03%) | loss: 1.457342 | lrm: 1.00 | dt: 2584.25ms | tok/sec: 202,878 | mfu: 16.46 | epoch: 1 | total time: 17.48m\n",
      "step 00417 (49.14%) | loss: 1.457796 | lrm: 1.00 | dt: 2586.08ms | tok/sec: 202,734 | mfu: 16.44 | epoch: 1 | total time: 17.52m\n",
      "step 00418 (49.26%) | loss: 1.438669 | lrm: 1.00 | dt: 2595.34ms | tok/sec: 202,011 | mfu: 16.38 | epoch: 1 | total time: 17.57m\n",
      "step 00419 (49.38%) | loss: 1.435938 | lrm: 1.00 | dt: 2582.73ms | tok/sec: 202,997 | mfu: 16.46 | epoch: 1 | total time: 17.61m\n",
      "step 00420 (49.49%) | loss: 1.439573 | lrm: 1.00 | dt: 2582.66ms | tok/sec: 203,003 | mfu: 16.47 | epoch: 1 | total time: 17.65m\n",
      "step 00421 (49.61%) | loss: 1.446906 | lrm: 1.00 | dt: 2599.84ms | tok/sec: 201,661 | mfu: 16.36 | epoch: 1 | total time: 17.69m\n",
      "step 00422 (49.73%) | loss: 1.452568 | lrm: 1.00 | dt: 2583.04ms | tok/sec: 202,973 | mfu: 16.46 | epoch: 1 | total time: 17.74m\n",
      "step 00423 (49.84%) | loss: 1.463191 | lrm: 1.00 | dt: 2582.90ms | tok/sec: 202,984 | mfu: 16.46 | epoch: 1 | total time: 17.78m\n",
      "step 00424 (49.95%) | loss: 1.465865 | lrm: 1.00 | dt: 2599.89ms | tok/sec: 201,657 | mfu: 16.36 | epoch: 1 | total time: 17.82m\n",
      "step 00425 (50.07%) | loss: 1.483283 | lrm: 1.00 | dt: 2582.66ms | tok/sec: 203,003 | mfu: 16.47 | epoch: 1 | total time: 17.87m\n",
      "step 00426 (50.18%) | loss: 1.496198 | lrm: 1.00 | dt: 2583.76ms | tok/sec: 202,916 | mfu: 16.46 | epoch: 1 | total time: 17.91m\n",
      "step 00427 (50.30%) | loss: 1.510602 | lrm: 1.00 | dt: 2598.79ms | tok/sec: 201,743 | mfu: 16.36 | epoch: 1 | total time: 17.95m\n",
      "step 00428 (50.42%) | loss: 1.524336 | lrm: 1.00 | dt: 2584.52ms | tok/sec: 202,857 | mfu: 16.45 | epoch: 1 | total time: 18.00m\n",
      "step 00429 (50.54%) | loss: 1.515872 | lrm: 1.00 | dt: 2581.82ms | tok/sec: 203,068 | mfu: 16.47 | epoch: 1 | total time: 18.04m\n",
      "step 00430 (50.65%) | loss: 1.524372 | lrm: 1.00 | dt: 2582.50ms | tok/sec: 203,015 | mfu: 16.47 | epoch: 1 | total time: 18.08m\n",
      "step 00431 (50.76%) | loss: 1.514638 | lrm: 1.00 | dt: 2595.60ms | tok/sec: 201,991 | mfu: 16.38 | epoch: 1 | total time: 18.13m\n",
      "step 00432 (50.87%) | loss: 1.512967 | lrm: 1.00 | dt: 2585.63ms | tok/sec: 202,769 | mfu: 16.45 | epoch: 1 | total time: 18.17m\n",
      "step 00433 (51.00%) | loss: 1.513475 | lrm: 1.00 | dt: 2584.83ms | tok/sec: 202,832 | mfu: 16.45 | epoch: 1 | total time: 18.21m\n",
      "step 00434 (51.11%) | loss: 1.515465 | lrm: 1.00 | dt: 2582.37ms | tok/sec: 203,026 | mfu: 16.47 | epoch: 1 | total time: 18.26m\n",
      "step 00435 (51.23%) | loss: 1.514794 | lrm: 1.00 | dt: 2582.36ms | tok/sec: 203,026 | mfu: 16.47 | epoch: 1 | total time: 18.30m\n",
      "step 00436 (51.35%) | loss: 1.511625 | lrm: 1.00 | dt: 2581.92ms | tok/sec: 203,061 | mfu: 16.47 | epoch: 1 | total time: 18.34m\n",
      "step 00437 (51.46%) | loss: 1.498773 | lrm: 1.00 | dt: 2583.48ms | tok/sec: 202,938 | mfu: 16.46 | epoch: 1 | total time: 18.38m\n",
      "step 00438 (51.58%) | loss: 1.501270 | lrm: 1.00 | dt: 2583.30ms | tok/sec: 202,952 | mfu: 16.46 | epoch: 1 | total time: 18.43m\n",
      "step 00439 (51.69%) | loss: 1.502525 | lrm: 1.00 | dt: 2584.76ms | tok/sec: 202,838 | mfu: 16.45 | epoch: 1 | total time: 18.47m\n",
      "step 00440 (51.80%) | loss: 1.482562 | lrm: 1.00 | dt: 2584.28ms | tok/sec: 202,876 | mfu: 16.46 | epoch: 1 | total time: 18.51m\n",
      "step 00441 (51.92%) | loss: 1.472045 | lrm: 1.00 | dt: 2589.49ms | tok/sec: 202,467 | mfu: 16.42 | epoch: 1 | total time: 18.56m\n",
      "step 00442 (52.03%) | loss: 1.490387 | lrm: 1.00 | dt: 2586.53ms | tok/sec: 202,699 | mfu: 16.44 | epoch: 1 | total time: 18.60m\n",
      "step 00443 (52.16%) | loss: 1.493886 | lrm: 1.00 | dt: 2587.32ms | tok/sec: 202,637 | mfu: 16.44 | epoch: 1 | total time: 18.64m\n",
      "step 00444 (52.27%) | loss: 1.484921 | lrm: 1.00 | dt: 2587.80ms | tok/sec: 202,600 | mfu: 16.43 | epoch: 1 | total time: 18.69m\n",
      "step 00445 (52.39%) | loss: 1.477854 | lrm: 1.00 | dt: 2584.04ms | tok/sec: 202,894 | mfu: 16.46 | epoch: 1 | total time: 18.73m\n",
      "step 00446 (52.51%) | loss: 1.469043 | lrm: 1.00 | dt: 2583.59ms | tok/sec: 202,930 | mfu: 16.46 | epoch: 1 | total time: 18.77m\n",
      "step 00447 (52.62%) | loss: 1.462223 | lrm: 1.00 | dt: 2582.21ms | tok/sec: 203,038 | mfu: 16.47 | epoch: 1 | total time: 18.82m\n",
      "step 00448 (52.73%) | loss: 1.470375 | lrm: 1.00 | dt: 2581.24ms | tok/sec: 203,114 | mfu: 16.47 | epoch: 1 | total time: 18.86m\n",
      "step 00449 (52.85%) | loss: 1.452765 | lrm: 1.00 | dt: 2600.42ms | tok/sec: 201,616 | mfu: 16.35 | epoch: 1 | total time: 18.90m\n",
      "step 00450 (52.97%) | loss: 1.461655 | lrm: 1.00 | dt: 2582.19ms | tok/sec: 203,040 | mfu: 16.47 | epoch: 1 | total time: 18.94m\n",
      "Step 00450 | Validation bpb: 0.4874\n",
      "step 00451 (53.08%) | loss: 1.472207 | lrm: 1.00 | dt: 2573.29ms | tok/sec: 203,742 | mfu: 16.53 | epoch: 1 | total time: 18.99m\n",
      "step 00452 (53.20%) | loss: 1.468447 | lrm: 1.00 | dt: 2576.61ms | tok/sec: 203,480 | mfu: 16.50 | epoch: 1 | total time: 19.03m\n",
      "step 00453 (53.32%) | loss: 1.458564 | lrm: 1.00 | dt: 2581.67ms | tok/sec: 203,081 | mfu: 16.47 | epoch: 1 | total time: 19.07m\n",
      "step 00454 (53.43%) | loss: 1.452128 | lrm: 1.00 | dt: 2581.22ms | tok/sec: 203,116 | mfu: 16.47 | epoch: 1 | total time: 19.12m\n",
      "step 00455 (53.55%) | loss: 1.460189 | lrm: 1.00 | dt: 2576.21ms | tok/sec: 203,511 | mfu: 16.51 | epoch: 1 | total time: 19.16m\n",
      "step 00456 (53.67%) | loss: 1.450275 | lrm: 1.00 | dt: 2580.52ms | tok/sec: 203,171 | mfu: 16.48 | epoch: 1 | total time: 19.20m\n",
      "step 00457 (53.79%) | loss: 1.447200 | lrm: 1.00 | dt: 2578.85ms | tok/sec: 203,303 | mfu: 16.49 | epoch: 1 | total time: 19.25m\n",
      "step 00458 (53.91%) | loss: 1.476154 | lrm: 1.00 | dt: 2578.63ms | tok/sec: 203,320 | mfu: 16.49 | epoch: 1 | total time: 19.29m\n",
      "step 00459 (54.02%) | loss: 1.456691 | lrm: 1.00 | dt: 2582.09ms | tok/sec: 203,047 | mfu: 16.47 | epoch: 1 | total time: 19.33m\n",
      "step 00460 (54.14%) | loss: 1.442072 | lrm: 1.00 | dt: 2581.23ms | tok/sec: 203,115 | mfu: 16.47 | epoch: 1 | total time: 19.37m\n",
      "step 00461 (54.26%) | loss: 1.430171 | lrm: 1.00 | dt: 2584.38ms | tok/sec: 202,868 | mfu: 16.45 | epoch: 1 | total time: 19.42m\n",
      "step 00462 (54.37%) | loss: 1.416743 | lrm: 1.00 | dt: 2581.14ms | tok/sec: 203,122 | mfu: 16.48 | epoch: 1 | total time: 19.46m\n",
      "step 00463 (54.50%) | loss: 1.424234 | lrm: 1.00 | dt: 2583.98ms | tok/sec: 202,899 | mfu: 16.46 | epoch: 1 | total time: 19.50m\n",
      "step 00464 (54.62%) | loss: 1.416933 | lrm: 1.00 | dt: 2582.50ms | tok/sec: 203,015 | mfu: 16.47 | epoch: 1 | total time: 19.55m\n",
      "step 00465 (54.74%) | loss: 1.421150 | lrm: 1.00 | dt: 2582.71ms | tok/sec: 202,998 | mfu: 16.47 | epoch: 1 | total time: 19.59m\n",
      "step 00466 (54.85%) | loss: 1.428824 | lrm: 1.00 | dt: 2581.62ms | tok/sec: 203,085 | mfu: 16.47 | epoch: 1 | total time: 19.63m\n",
      "step 00467 (54.97%) | loss: 1.441718 | lrm: 1.00 | dt: 2582.91ms | tok/sec: 202,983 | mfu: 16.46 | epoch: 1 | total time: 19.68m\n",
      "step 00468 (55.09%) | loss: 1.451270 | lrm: 1.00 | dt: 2581.70ms | tok/sec: 203,078 | mfu: 16.47 | epoch: 1 | total time: 19.72m\n",
      "step 00469 (55.21%) | loss: 1.443033 | lrm: 1.00 | dt: 2581.49ms | tok/sec: 203,094 | mfu: 16.47 | epoch: 1 | total time: 19.76m\n",
      "step 00470 (55.32%) | loss: 1.463998 | lrm: 1.00 | dt: 2582.80ms | tok/sec: 202,992 | mfu: 16.46 | epoch: 1 | total time: 19.81m\n",
      "step 00471 (55.44%) | loss: 1.475451 | lrm: 1.00 | dt: 2581.85ms | tok/sec: 203,066 | mfu: 16.47 | epoch: 1 | total time: 19.85m\n",
      "step 00472 (55.56%) | loss: 1.467837 | lrm: 1.00 | dt: 2579.56ms | tok/sec: 203,246 | mfu: 16.49 | epoch: 1 | total time: 19.89m\n",
      "step 00473 (55.67%) | loss: 1.465387 | lrm: 1.00 | dt: 2581.94ms | tok/sec: 203,059 | mfu: 16.47 | epoch: 1 | total time: 19.93m\n",
      "step 00474 (55.78%) | loss: 1.479327 | lrm: 1.00 | dt: 2581.98ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 19.98m\n",
      "step 00475 (55.90%) | loss: 1.462399 | lrm: 1.00 | dt: 2583.18ms | tok/sec: 202,962 | mfu: 16.46 | epoch: 1 | total time: 20.02m\n",
      "step 00476 (56.01%) | loss: 1.478912 | lrm: 1.00 | dt: 2581.98ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 20.06m\n",
      "step 00477 (56.12%) | loss: 1.478708 | lrm: 1.00 | dt: 2582.69ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 20.11m\n",
      "step 00478 (56.24%) | loss: 1.494491 | lrm: 1.00 | dt: 2584.97ms | tok/sec: 202,821 | mfu: 16.45 | epoch: 1 | total time: 20.15m\n",
      "step 00479 (56.36%) | loss: 1.488058 | lrm: 1.00 | dt: 2587.06ms | tok/sec: 202,657 | mfu: 16.44 | epoch: 1 | total time: 20.19m\n",
      "step 00480 (56.48%) | loss: 1.481669 | lrm: 1.00 | dt: 2586.84ms | tok/sec: 202,674 | mfu: 16.44 | epoch: 1 | total time: 20.24m\n",
      "step 00481 (56.60%) | loss: 1.482440 | lrm: 1.00 | dt: 2584.95ms | tok/sec: 202,823 | mfu: 16.45 | epoch: 1 | total time: 20.28m\n",
      "step 00482 (56.72%) | loss: 1.469376 | lrm: 1.00 | dt: 2589.03ms | tok/sec: 202,503 | mfu: 16.42 | epoch: 1 | total time: 20.32m\n",
      "step 00483 (56.84%) | loss: 1.457163 | lrm: 1.00 | dt: 2590.96ms | tok/sec: 202,352 | mfu: 16.41 | epoch: 1 | total time: 20.37m\n",
      "step 00484 (56.95%) | loss: 1.464683 | lrm: 1.00 | dt: 2588.88ms | tok/sec: 202,515 | mfu: 16.43 | epoch: 1 | total time: 20.41m\n",
      "step 00485 (57.06%) | loss: 1.471157 | lrm: 1.00 | dt: 2585.07ms | tok/sec: 202,813 | mfu: 16.45 | epoch: 1 | total time: 20.45m\n",
      "step 00486 (57.18%) | loss: 1.471326 | lrm: 1.00 | dt: 2582.29ms | tok/sec: 203,032 | mfu: 16.47 | epoch: 1 | total time: 20.49m\n",
      "step 00487 (57.30%) | loss: 1.464968 | lrm: 1.00 | dt: 2582.03ms | tok/sec: 203,052 | mfu: 16.47 | epoch: 1 | total time: 20.54m\n",
      "step 00488 (57.41%) | loss: 1.455995 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 16.47 | epoch: 1 | total time: 20.58m\n",
      "step 00489 (57.52%) | loss: 1.451202 | lrm: 1.00 | dt: 2582.26ms | tok/sec: 203,034 | mfu: 16.47 | epoch: 1 | total time: 20.62m\n",
      "step 00490 (57.64%) | loss: 1.467999 | lrm: 1.00 | dt: 2581.93ms | tok/sec: 203,060 | mfu: 16.47 | epoch: 1 | total time: 20.67m\n",
      "step 00491 (57.75%) | loss: 1.470079 | lrm: 1.00 | dt: 2582.33ms | tok/sec: 203,028 | mfu: 16.47 | epoch: 1 | total time: 20.71m\n",
      "step 00492 (57.86%) | loss: 1.456830 | lrm: 1.00 | dt: 2582.43ms | tok/sec: 203,021 | mfu: 16.47 | epoch: 1 | total time: 20.75m\n",
      "step 00493 (57.98%) | loss: 1.466963 | lrm: 1.00 | dt: 2581.81ms | tok/sec: 203,069 | mfu: 16.47 | epoch: 1 | total time: 20.80m\n",
      "step 00494 (58.10%) | loss: 1.478437 | lrm: 1.00 | dt: 2584.38ms | tok/sec: 202,868 | mfu: 16.45 | epoch: 1 | total time: 20.84m\n",
      "step 00495 (58.22%) | loss: 1.469607 | lrm: 1.00 | dt: 2593.56ms | tok/sec: 202,149 | mfu: 16.40 | epoch: 1 | total time: 20.88m\n",
      "step 00496 (58.34%) | loss: 1.435668 | lrm: 1.00 | dt: 2585.34ms | tok/sec: 202,792 | mfu: 16.45 | epoch: 1 | total time: 20.92m\n",
      "step 00497 (58.46%) | loss: 1.442220 | lrm: 1.00 | dt: 2582.58ms | tok/sec: 203,009 | mfu: 16.47 | epoch: 1 | total time: 20.97m\n",
      "step 00498 (58.57%) | loss: 1.443987 | lrm: 1.00 | dt: 2582.43ms | tok/sec: 203,020 | mfu: 16.47 | epoch: 1 | total time: 21.01m\n",
      "step 00499 (58.69%) | loss: 1.473584 | lrm: 1.00 | dt: 2581.59ms | tok/sec: 203,086 | mfu: 16.47 | epoch: 1 | total time: 21.05m\n",
      "step 00500 (58.81%) | loss: 1.454323 | lrm: 1.00 | dt: 2581.99ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 21.10m\n",
      "step 00501 (58.93%) | loss: 1.437678 | lrm: 1.00 | dt: 2599.83ms | tok/sec: 201,662 | mfu: 16.36 | epoch: 1 | total time: 21.14m\n",
      "step 00502 (59.04%) | loss: 1.430861 | lrm: 1.00 | dt: 2583.66ms | tok/sec: 202,924 | mfu: 16.46 | epoch: 1 | total time: 21.18m\n",
      "step 00503 (59.15%) | loss: 1.436984 | lrm: 1.00 | dt: 2584.16ms | tok/sec: 202,885 | mfu: 16.46 | epoch: 1 | total time: 21.23m\n",
      "step 00504 (59.27%) | loss: 1.421097 | lrm: 1.00 | dt: 2584.59ms | tok/sec: 202,851 | mfu: 16.45 | epoch: 1 | total time: 21.27m\n",
      "step 00505 (59.38%) | loss: 1.419816 | lrm: 1.00 | dt: 2585.03ms | tok/sec: 202,816 | mfu: 16.45 | epoch: 1 | total time: 21.31m\n",
      "step 00506 (59.51%) | loss: 1.440996 | lrm: 1.00 | dt: 2584.42ms | tok/sec: 202,864 | mfu: 16.45 | epoch: 1 | total time: 21.36m\n",
      "step 00507 (59.63%) | loss: 1.446717 | lrm: 1.00 | dt: 2584.19ms | tok/sec: 202,882 | mfu: 16.46 | epoch: 1 | total time: 21.40m\n",
      "step 00508 (59.74%) | loss: 1.457913 | lrm: 1.00 | dt: 2589.34ms | tok/sec: 202,479 | mfu: 16.42 | epoch: 1 | total time: 21.44m\n",
      "step 00509 (59.87%) | loss: 1.452921 | lrm: 1.00 | dt: 2583.50ms | tok/sec: 202,937 | mfu: 16.46 | epoch: 1 | total time: 21.49m\n",
      "step 00510 (59.98%) | loss: 1.438229 | lrm: 1.00 | dt: 2597.46ms | tok/sec: 201,846 | mfu: 16.37 | epoch: 1 | total time: 21.53m\n",
      "step 00511 (60.09%) | loss: 1.441638 | lrm: 1.00 | dt: 2583.28ms | tok/sec: 202,954 | mfu: 16.46 | epoch: 1 | total time: 21.57m\n",
      "step 00512 (60.21%) | loss: 1.451257 | lrm: 1.00 | dt: 2580.72ms | tok/sec: 203,155 | mfu: 16.48 | epoch: 1 | total time: 21.61m\n",
      "step 00513 (60.33%) | loss: 1.431678 | lrm: 1.00 | dt: 2582.70ms | tok/sec: 202,999 | mfu: 16.47 | epoch: 1 | total time: 21.66m\n",
      "step 00514 (60.46%) | loss: 1.417857 | lrm: 1.00 | dt: 2582.78ms | tok/sec: 202,993 | mfu: 16.46 | epoch: 1 | total time: 21.70m\n",
      "step 00515 (60.57%) | loss: 1.411576 | lrm: 1.00 | dt: 2580.26ms | tok/sec: 203,192 | mfu: 16.48 | epoch: 1 | total time: 21.74m\n",
      "step 00516 (60.69%) | loss: 1.399961 | lrm: 1.00 | dt: 2581.64ms | tok/sec: 203,083 | mfu: 16.47 | epoch: 1 | total time: 21.79m\n",
      "step 00517 (60.80%) | loss: 1.410600 | lrm: 1.00 | dt: 2584.15ms | tok/sec: 202,886 | mfu: 16.46 | epoch: 1 | total time: 21.83m\n",
      "step 00518 (60.92%) | loss: 1.404335 | lrm: 1.00 | dt: 2596.60ms | tok/sec: 201,912 | mfu: 16.38 | epoch: 1 | total time: 21.87m\n",
      "step 00519 (61.03%) | loss: 1.417162 | lrm: 1.00 | dt: 2582.45ms | tok/sec: 203,019 | mfu: 16.47 | epoch: 1 | total time: 21.92m\n",
      "step 00520 (61.15%) | loss: 1.435195 | lrm: 1.00 | dt: 2582.60ms | tok/sec: 203,007 | mfu: 16.47 | epoch: 1 | total time: 21.96m\n",
      "step 00521 (61.27%) | loss: 1.423451 | lrm: 1.00 | dt: 2581.35ms | tok/sec: 203,106 | mfu: 16.47 | epoch: 1 | total time: 22.00m\n",
      "step 00522 (61.39%) | loss: 1.425412 | lrm: 1.00 | dt: 2582.77ms | tok/sec: 202,994 | mfu: 16.46 | epoch: 1 | total time: 22.05m\n",
      "step 00523 (61.50%) | loss: 1.418727 | lrm: 1.00 | dt: 2583.60ms | tok/sec: 202,928 | mfu: 16.46 | epoch: 1 | total time: 22.09m\n",
      "step 00524 (61.62%) | loss: 1.405079 | lrm: 1.00 | dt: 2590.02ms | tok/sec: 202,425 | mfu: 16.42 | epoch: 1 | total time: 22.13m\n",
      "step 00525 (61.74%) | loss: 1.399185 | lrm: 1.00 | dt: 2592.19ms | tok/sec: 202,256 | mfu: 16.40 | epoch: 1 | total time: 22.17m\n",
      "step 00526 (61.86%) | loss: 1.405481 | lrm: 1.00 | dt: 2582.25ms | tok/sec: 203,035 | mfu: 16.47 | epoch: 1 | total time: 22.22m\n",
      "step 00527 (61.98%) | loss: 1.403781 | lrm: 1.00 | dt: 2582.46ms | tok/sec: 203,018 | mfu: 16.47 | epoch: 1 | total time: 22.26m\n",
      "step 00528 (62.10%) | loss: 1.399653 | lrm: 1.00 | dt: 2582.19ms | tok/sec: 203,039 | mfu: 16.47 | epoch: 1 | total time: 22.30m\n",
      "step 00529 (62.21%) | loss: 1.422882 | lrm: 1.00 | dt: 2582.74ms | tok/sec: 202,997 | mfu: 16.46 | epoch: 1 | total time: 22.35m\n",
      "step 00530 (62.33%) | loss: 1.438005 | lrm: 1.00 | dt: 2583.35ms | tok/sec: 202,949 | mfu: 16.46 | epoch: 1 | total time: 22.39m\n",
      "step 00531 (62.44%) | loss: 1.472345 | lrm: 1.00 | dt: 2583.59ms | tok/sec: 202,930 | mfu: 16.46 | epoch: 1 | total time: 22.43m\n",
      "step 00532 (62.56%) | loss: 1.474852 | lrm: 1.00 | dt: 2584.84ms | tok/sec: 202,831 | mfu: 16.45 | epoch: 1 | total time: 22.48m\n",
      "step 00533 (62.68%) | loss: 1.451688 | lrm: 1.00 | dt: 2583.79ms | tok/sec: 202,914 | mfu: 16.46 | epoch: 1 | total time: 22.52m\n",
      "step 00534 (62.80%) | loss: 1.434261 | lrm: 1.00 | dt: 2584.36ms | tok/sec: 202,869 | mfu: 16.45 | epoch: 1 | total time: 22.56m\n",
      "step 00535 (62.91%) | loss: 1.435960 | lrm: 1.00 | dt: 2599.59ms | tok/sec: 201,681 | mfu: 16.36 | epoch: 1 | total time: 22.61m\n",
      "step 00536 (63.03%) | loss: 1.435131 | lrm: 1.00 | dt: 2583.33ms | tok/sec: 202,950 | mfu: 16.46 | epoch: 1 | total time: 22.65m\n",
      "step 00537 (63.16%) | loss: 1.452490 | lrm: 1.00 | dt: 2583.63ms | tok/sec: 202,927 | mfu: 16.46 | epoch: 1 | total time: 22.69m\n",
      "step 00538 (63.28%) | loss: 1.436957 | lrm: 1.00 | dt: 2581.08ms | tok/sec: 203,127 | mfu: 16.48 | epoch: 1 | total time: 22.73m\n",
      "step 00539 (63.40%) | loss: 1.432044 | lrm: 1.00 | dt: 2583.08ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 22.78m\n",
      "step 00540 (63.51%) | loss: 1.424834 | lrm: 1.00 | dt: 2582.68ms | tok/sec: 203,001 | mfu: 16.47 | epoch: 1 | total time: 22.82m\n",
      "step 00541 (63.63%) | loss: 1.418273 | lrm: 1.00 | dt: 2585.56ms | tok/sec: 202,775 | mfu: 16.45 | epoch: 1 | total time: 22.86m\n",
      "step 00542 (63.74%) | loss: 1.418200 | lrm: 1.00 | dt: 2592.80ms | tok/sec: 202,209 | mfu: 16.40 | epoch: 1 | total time: 22.91m\n",
      "step 00543 (63.86%) | loss: 1.400386 | lrm: 1.00 | dt: 2582.25ms | tok/sec: 203,035 | mfu: 16.47 | epoch: 1 | total time: 22.95m\n",
      "step 00544 (63.97%) | loss: 1.415975 | lrm: 1.00 | dt: 2582.81ms | tok/sec: 202,991 | mfu: 16.46 | epoch: 1 | total time: 22.99m\n",
      "step 00545 (64.09%) | loss: 1.436475 | lrm: 1.00 | dt: 2582.78ms | tok/sec: 202,993 | mfu: 16.46 | epoch: 1 | total time: 23.04m\n",
      "step 00546 (64.22%) | loss: 1.430067 | lrm: 1.00 | dt: 2580.78ms | tok/sec: 203,150 | mfu: 16.48 | epoch: 1 | total time: 23.08m\n",
      "step 00547 (64.33%) | loss: 1.434422 | lrm: 1.00 | dt: 2582.39ms | tok/sec: 203,023 | mfu: 16.47 | epoch: 1 | total time: 23.12m\n",
      "step 00548 (64.44%) | loss: 1.445740 | lrm: 1.00 | dt: 2583.18ms | tok/sec: 202,962 | mfu: 16.46 | epoch: 1 | total time: 23.17m\n",
      "step 00549 (64.56%) | loss: 1.441664 | lrm: 1.00 | dt: 2596.65ms | tok/sec: 201,909 | mfu: 16.38 | epoch: 1 | total time: 23.21m\n",
      "step 00550 (64.68%) | loss: 1.421275 | lrm: 1.00 | dt: 2582.86ms | tok/sec: 202,987 | mfu: 16.46 | epoch: 1 | total time: 23.25m\n",
      "step 00551 (64.80%) | loss: 1.432643 | lrm: 1.00 | dt: 2581.90ms | tok/sec: 203,062 | mfu: 16.47 | epoch: 1 | total time: 23.29m\n",
      "step 00552 (64.92%) | loss: 1.429151 | lrm: 1.00 | dt: 2581.69ms | tok/sec: 203,079 | mfu: 16.47 | epoch: 1 | total time: 23.34m\n",
      "step 00553 (65.05%) | loss: 1.437576 | lrm: 1.00 | dt: 2581.41ms | tok/sec: 203,101 | mfu: 16.47 | epoch: 1 | total time: 23.38m\n",
      "step 00554 (65.17%) | loss: 1.447045 | lrm: 1.00 | dt: 2583.31ms | tok/sec: 202,951 | mfu: 16.46 | epoch: 1 | total time: 23.42m\n",
      "step 00555 (65.28%) | loss: 1.451006 | lrm: 1.00 | dt: 2584.35ms | tok/sec: 202,870 | mfu: 16.45 | epoch: 1 | total time: 23.47m\n",
      "step 00556 (65.41%) | loss: 1.445723 | lrm: 1.00 | dt: 2587.22ms | tok/sec: 202,645 | mfu: 16.44 | epoch: 1 | total time: 23.51m\n",
      "step 00557 (65.52%) | loss: 1.435506 | lrm: 1.00 | dt: 2585.82ms | tok/sec: 202,755 | mfu: 16.45 | epoch: 1 | total time: 23.55m\n",
      "step 00558 (65.63%) | loss: 1.420371 | lrm: 1.00 | dt: 2587.06ms | tok/sec: 202,658 | mfu: 16.44 | epoch: 1 | total time: 23.60m\n",
      "step 00559 (65.75%) | loss: 1.401256 | lrm: 1.00 | dt: 2582.67ms | tok/sec: 203,002 | mfu: 16.47 | epoch: 1 | total time: 23.64m\n",
      "step 00560 (65.86%) | loss: 1.388797 | lrm: 1.00 | dt: 2585.03ms | tok/sec: 202,816 | mfu: 16.45 | epoch: 1 | total time: 23.68m\n",
      "step 00561 (65.97%) | loss: 1.409547 | lrm: 1.00 | dt: 2586.63ms | tok/sec: 202,691 | mfu: 16.44 | epoch: 1 | total time: 23.73m\n",
      "step 00562 (66.10%) | loss: 1.424479 | lrm: 1.00 | dt: 2597.97ms | tok/sec: 201,806 | mfu: 16.37 | epoch: 1 | total time: 23.77m\n",
      "step 00563 (66.22%) | loss: 1.415952 | lrm: 1.00 | dt: 2582.99ms | tok/sec: 202,977 | mfu: 16.46 | epoch: 1 | total time: 23.81m\n",
      "step 00564 (66.33%) | loss: 1.399933 | lrm: 1.00 | dt: 2580.25ms | tok/sec: 203,192 | mfu: 16.48 | epoch: 1 | total time: 23.85m\n",
      "step 00565 (66.45%) | loss: 1.400107 | lrm: 1.00 | dt: 2581.54ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 23.90m\n",
      "step 00566 (66.57%) | loss: 1.400278 | lrm: 1.00 | dt: 2583.05ms | tok/sec: 202,972 | mfu: 16.46 | epoch: 1 | total time: 23.94m\n",
      "step 00567 (66.69%) | loss: 1.419062 | lrm: 1.00 | dt: 2582.82ms | tok/sec: 202,990 | mfu: 16.46 | epoch: 1 | total time: 23.98m\n",
      "step 00568 (66.80%) | loss: 1.415383 | lrm: 1.00 | dt: 2582.47ms | tok/sec: 203,018 | mfu: 16.47 | epoch: 1 | total time: 24.03m\n",
      "step 00569 (66.92%) | loss: 1.408544 | lrm: 1.00 | dt: 2582.18ms | tok/sec: 203,041 | mfu: 16.47 | epoch: 1 | total time: 24.07m\n",
      "step 00570 (67.04%) | loss: 1.419239 | lrm: 1.00 | dt: 2582.18ms | tok/sec: 203,040 | mfu: 16.47 | epoch: 1 | total time: 24.11m\n",
      "step 00571 (67.15%) | loss: 1.449026 | lrm: 1.00 | dt: 2581.02ms | tok/sec: 203,132 | mfu: 16.48 | epoch: 1 | total time: 24.16m\n",
      "step 00572 (67.27%) | loss: 1.435924 | lrm: 1.00 | dt: 2582.65ms | tok/sec: 203,003 | mfu: 16.47 | epoch: 1 | total time: 24.20m\n",
      "step 00573 (67.38%) | loss: 1.437256 | lrm: 1.00 | dt: 2582.90ms | tok/sec: 202,984 | mfu: 16.46 | epoch: 1 | total time: 24.24m\n",
      "step 00574 (67.50%) | loss: 1.444817 | lrm: 1.00 | dt: 2582.84ms | tok/sec: 202,988 | mfu: 16.46 | epoch: 1 | total time: 24.28m\n",
      "step 00575 (67.62%) | loss: 1.443816 | lrm: 1.00 | dt: 2582.56ms | tok/sec: 203,010 | mfu: 16.47 | epoch: 1 | total time: 24.33m\n",
      "step 00576 (67.74%) | loss: 1.467240 | lrm: 1.00 | dt: 2601.10ms | tok/sec: 201,563 | mfu: 16.35 | epoch: 1 | total time: 24.37m\n",
      "step 00577 (67.86%) | loss: 1.477955 | lrm: 1.00 | dt: 2583.79ms | tok/sec: 202,914 | mfu: 16.46 | epoch: 1 | total time: 24.41m\n",
      "step 00578 (67.98%) | loss: 1.469707 | lrm: 1.00 | dt: 2581.20ms | tok/sec: 203,118 | mfu: 16.47 | epoch: 1 | total time: 24.46m\n",
      "step 00579 (68.09%) | loss: 1.482246 | lrm: 1.00 | dt: 2580.80ms | tok/sec: 203,149 | mfu: 16.48 | epoch: 1 | total time: 24.50m\n",
      "step 00580 (68.21%) | loss: 1.478663 | lrm: 1.00 | dt: 2581.39ms | tok/sec: 203,102 | mfu: 16.47 | epoch: 1 | total time: 24.54m\n",
      "step 00581 (68.33%) | loss: 1.466894 | lrm: 1.00 | dt: 2581.35ms | tok/sec: 203,105 | mfu: 16.47 | epoch: 1 | total time: 24.59m\n",
      "step 00582 (68.44%) | loss: 1.461767 | lrm: 1.00 | dt: 2584.21ms | tok/sec: 202,881 | mfu: 16.46 | epoch: 1 | total time: 24.63m\n",
      "step 00583 (68.56%) | loss: 1.459647 | lrm: 1.00 | dt: 2584.14ms | tok/sec: 202,887 | mfu: 16.46 | epoch: 1 | total time: 24.67m\n",
      "step 00584 (68.67%) | loss: 1.445478 | lrm: 1.00 | dt: 2585.19ms | tok/sec: 202,804 | mfu: 16.45 | epoch: 1 | total time: 24.72m\n",
      "step 00585 (68.79%) | loss: 1.446949 | lrm: 1.00 | dt: 2588.50ms | tok/sec: 202,545 | mfu: 16.43 | epoch: 1 | total time: 24.76m\n",
      "step 00586 (68.91%) | loss: 1.457443 | lrm: 1.00 | dt: 2582.67ms | tok/sec: 203,002 | mfu: 16.47 | epoch: 1 | total time: 24.80m\n",
      "step 00587 (69.02%) | loss: 1.447378 | lrm: 1.00 | dt: 2591.61ms | tok/sec: 202,302 | mfu: 16.41 | epoch: 1 | total time: 24.85m\n",
      "step 00588 (69.14%) | loss: 1.459490 | lrm: 1.00 | dt: 2589.54ms | tok/sec: 202,463 | mfu: 16.42 | epoch: 1 | total time: 24.89m\n",
      "step 00589 (69.25%) | loss: 1.459157 | lrm: 1.00 | dt: 2583.44ms | tok/sec: 202,941 | mfu: 16.46 | epoch: 1 | total time: 24.93m\n",
      "step 00590 (69.37%) | loss: 1.470345 | lrm: 1.00 | dt: 2582.40ms | tok/sec: 203,023 | mfu: 16.47 | epoch: 1 | total time: 24.97m\n",
      "step 00591 (69.49%) | loss: 1.454208 | lrm: 1.00 | dt: 2582.76ms | tok/sec: 202,995 | mfu: 16.46 | epoch: 1 | total time: 25.02m\n",
      "step 00592 (69.62%) | loss: 1.437375 | lrm: 1.00 | dt: 2599.49ms | tok/sec: 201,688 | mfu: 16.36 | epoch: 1 | total time: 25.06m\n",
      "step 00593 (69.73%) | loss: 1.438059 | lrm: 1.00 | dt: 2581.91ms | tok/sec: 203,062 | mfu: 16.47 | epoch: 1 | total time: 25.10m\n",
      "step 00594 (69.85%) | loss: 1.413715 | lrm: 1.00 | dt: 2583.02ms | tok/sec: 202,975 | mfu: 16.46 | epoch: 1 | total time: 25.15m\n",
      "step 00595 (69.97%) | loss: 1.415573 | lrm: 1.00 | dt: 2582.75ms | tok/sec: 202,996 | mfu: 16.46 | epoch: 1 | total time: 25.19m\n",
      "step 00596 (70.08%) | loss: 1.443776 | lrm: 1.00 | dt: 2598.29ms | tok/sec: 201,781 | mfu: 16.37 | epoch: 1 | total time: 25.23m\n",
      "step 00597 (70.20%) | loss: 1.448444 | lrm: 1.00 | dt: 2582.29ms | tok/sec: 203,032 | mfu: 16.47 | epoch: 1 | total time: 25.28m\n",
      "step 00598 (70.32%) | loss: 1.439907 | lrm: 1.00 | dt: 2581.92ms | tok/sec: 203,061 | mfu: 16.47 | epoch: 1 | total time: 25.32m\n",
      "step 00599 (70.44%) | loss: 1.446598 | lrm: 1.00 | dt: 2583.63ms | tok/sec: 202,926 | mfu: 16.46 | epoch: 1 | total time: 25.36m\n",
      "step 00600 (70.56%) | loss: 1.437858 | lrm: 1.00 | dt: 2599.04ms | tok/sec: 201,723 | mfu: 16.36 | epoch: 1 | total time: 25.41m\n",
      "Step 00600 | Validation bpb: 0.4767\n",
      "step 00601 (70.67%) | loss: 1.442773 | lrm: 1.00 | dt: 2569.13ms | tok/sec: 204,072 | mfu: 16.55 | epoch: 1 | total time: 25.45m\n",
      "step 00602 (70.80%) | loss: 1.432335 | lrm: 1.00 | dt: 2572.57ms | tok/sec: 203,799 | mfu: 16.53 | epoch: 1 | total time: 25.49m\n",
      "step 00603 (70.92%) | loss: 1.428596 | lrm: 1.00 | dt: 2571.58ms | tok/sec: 203,878 | mfu: 16.54 | epoch: 1 | total time: 25.53m\n",
      "step 00604 (71.04%) | loss: 1.428010 | lrm: 1.00 | dt: 2576.41ms | tok/sec: 203,495 | mfu: 16.51 | epoch: 1 | total time: 25.58m\n",
      "step 00605 (71.16%) | loss: 1.421842 | lrm: 1.00 | dt: 2572.20ms | tok/sec: 203,828 | mfu: 16.53 | epoch: 1 | total time: 25.62m\n",
      "step 00606 (71.27%) | loss: 1.413554 | lrm: 1.00 | dt: 2574.01ms | tok/sec: 203,685 | mfu: 16.52 | epoch: 1 | total time: 25.66m\n",
      "step 00607 (71.39%) | loss: 1.412034 | lrm: 1.00 | dt: 2571.06ms | tok/sec: 203,918 | mfu: 16.54 | epoch: 1 | total time: 25.71m\n",
      "step 00608 (71.50%) | loss: 1.405940 | lrm: 1.00 | dt: 2576.30ms | tok/sec: 203,504 | mfu: 16.51 | epoch: 1 | total time: 25.75m\n",
      "step 00609 (71.62%) | loss: 1.410372 | lrm: 1.00 | dt: 2575.04ms | tok/sec: 203,604 | mfu: 16.51 | epoch: 1 | total time: 25.79m\n",
      "step 00610 (71.74%) | loss: 1.406520 | lrm: 1.00 | dt: 2572.97ms | tok/sec: 203,767 | mfu: 16.53 | epoch: 1 | total time: 25.83m\n",
      "step 00611 (71.87%) | loss: 1.403553 | lrm: 1.00 | dt: 2575.01ms | tok/sec: 203,605 | mfu: 16.51 | epoch: 1 | total time: 25.88m\n",
      "step 00612 (71.98%) | loss: 1.412716 | lrm: 1.00 | dt: 2574.60ms | tok/sec: 203,638 | mfu: 16.52 | epoch: 1 | total time: 25.92m\n",
      "step 00613 (72.09%) | loss: 1.398140 | lrm: 1.00 | dt: 2572.81ms | tok/sec: 203,780 | mfu: 16.53 | epoch: 1 | total time: 25.96m\n",
      "step 00614 (72.21%) | loss: 1.388812 | lrm: 1.00 | dt: 2573.63ms | tok/sec: 203,715 | mfu: 16.52 | epoch: 1 | total time: 26.01m\n",
      "step 00615 (72.32%) | loss: 1.390623 | lrm: 1.00 | dt: 2576.29ms | tok/sec: 203,504 | mfu: 16.51 | epoch: 1 | total time: 26.05m\n",
      "step 00616 (72.45%) | loss: 1.382863 | lrm: 1.00 | dt: 2574.46ms | tok/sec: 203,650 | mfu: 16.52 | epoch: 1 | total time: 26.09m\n",
      "step 00617 (72.56%) | loss: 1.389699 | lrm: 1.00 | dt: 2574.71ms | tok/sec: 203,629 | mfu: 16.52 | epoch: 1 | total time: 26.13m\n",
      "step 00618 (72.67%) | loss: 1.378308 | lrm: 1.00 | dt: 2577.62ms | tok/sec: 203,400 | mfu: 16.50 | epoch: 1 | total time: 26.18m\n",
      "step 00619 (72.80%) | loss: 1.377045 | lrm: 1.00 | dt: 2572.90ms | tok/sec: 203,773 | mfu: 16.53 | epoch: 1 | total time: 26.22m\n",
      "step 00620 (72.91%) | loss: 1.390376 | lrm: 1.00 | dt: 2579.62ms | tok/sec: 203,242 | mfu: 16.48 | epoch: 1 | total time: 26.26m\n",
      "step 00621 (73.03%) | loss: 1.396154 | lrm: 1.00 | dt: 2575.46ms | tok/sec: 203,570 | mfu: 16.51 | epoch: 1 | total time: 26.31m\n",
      "step 00622 (73.15%) | loss: 1.414976 | lrm: 1.00 | dt: 2574.00ms | tok/sec: 203,686 | mfu: 16.52 | epoch: 1 | total time: 26.35m\n",
      "step 00623 (73.27%) | loss: 1.441888 | lrm: 1.00 | dt: 2578.69ms | tok/sec: 203,315 | mfu: 16.49 | epoch: 1 | total time: 26.39m\n",
      "step 00624 (73.38%) | loss: 1.441286 | lrm: 1.00 | dt: 2577.79ms | tok/sec: 203,386 | mfu: 16.50 | epoch: 1 | total time: 26.44m\n",
      "step 00625 (73.50%) | loss: 1.430665 | lrm: 1.00 | dt: 2577.46ms | tok/sec: 203,413 | mfu: 16.50 | epoch: 1 | total time: 26.48m\n",
      "step 00626 (73.62%) | loss: 1.434638 | lrm: 1.00 | dt: 2575.19ms | tok/sec: 203,592 | mfu: 16.51 | epoch: 1 | total time: 26.52m\n",
      "step 00627 (73.73%) | loss: 1.422936 | lrm: 1.00 | dt: 2580.05ms | tok/sec: 203,208 | mfu: 16.48 | epoch: 1 | total time: 26.56m\n",
      "step 00628 (73.84%) | loss: 1.428202 | lrm: 1.00 | dt: 2581.17ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 26.61m\n",
      "step 00629 (73.96%) | loss: 1.411796 | lrm: 1.00 | dt: 2582.00ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 26.65m\n",
      "step 00630 (74.07%) | loss: 1.395045 | lrm: 1.00 | dt: 2586.62ms | tok/sec: 202,691 | mfu: 16.44 | epoch: 1 | total time: 26.69m\n",
      "step 00631 (74.19%) | loss: 1.393541 | lrm: 1.00 | dt: 2579.15ms | tok/sec: 203,279 | mfu: 16.49 | epoch: 1 | total time: 26.74m\n",
      "step 00632 (74.31%) | loss: 1.400281 | lrm: 1.00 | dt: 2581.33ms | tok/sec: 203,107 | mfu: 16.47 | epoch: 1 | total time: 26.78m\n",
      "step 00633 (74.43%) | loss: 1.393566 | lrm: 1.00 | dt: 2589.85ms | tok/sec: 202,439 | mfu: 16.42 | epoch: 1 | total time: 26.82m\n",
      "step 00634 (74.55%) | loss: 1.393668 | lrm: 1.00 | dt: 2583.94ms | tok/sec: 202,902 | mfu: 16.46 | epoch: 1 | total time: 26.87m\n",
      "step 00635 (74.66%) | loss: 1.393833 | lrm: 1.00 | dt: 2583.54ms | tok/sec: 202,933 | mfu: 16.46 | epoch: 1 | total time: 26.91m\n",
      "step 00636 (74.77%) | loss: 1.388689 | lrm: 1.00 | dt: 2579.57ms | tok/sec: 203,246 | mfu: 16.49 | epoch: 1 | total time: 26.95m\n",
      "step 00637 (74.89%) | loss: 1.391622 | lrm: 1.00 | dt: 2581.71ms | tok/sec: 203,077 | mfu: 16.47 | epoch: 1 | total time: 26.99m\n",
      "step 00638 (75.00%) | loss: 1.379379 | lrm: 1.00 | dt: 2579.85ms | tok/sec: 203,223 | mfu: 16.48 | epoch: 1 | total time: 27.04m\n",
      "step 00639 (75.12%) | loss: 1.393518 | lrm: 1.00 | dt: 2585.35ms | tok/sec: 202,791 | mfu: 16.45 | epoch: 1 | total time: 27.08m\n",
      "step 00640 (75.24%) | loss: 1.384671 | lrm: 1.00 | dt: 2585.74ms | tok/sec: 202,761 | mfu: 16.45 | epoch: 1 | total time: 27.12m\n",
      "step 00641 (75.35%) | loss: 1.404882 | lrm: 1.00 | dt: 2581.68ms | tok/sec: 203,080 | mfu: 16.47 | epoch: 1 | total time: 27.17m\n",
      "step 00642 (75.48%) | loss: 1.413750 | lrm: 1.00 | dt: 2581.46ms | tok/sec: 203,097 | mfu: 16.47 | epoch: 1 | total time: 27.21m\n",
      "step 00643 (75.59%) | loss: 1.409938 | lrm: 1.00 | dt: 2582.26ms | tok/sec: 203,034 | mfu: 16.47 | epoch: 1 | total time: 27.25m\n",
      "step 00644 (75.71%) | loss: 1.429209 | lrm: 1.00 | dt: 2583.14ms | tok/sec: 202,965 | mfu: 16.46 | epoch: 1 | total time: 27.30m\n",
      "step 00645 (75.83%) | loss: 1.432835 | lrm: 1.00 | dt: 2581.25ms | tok/sec: 203,114 | mfu: 16.47 | epoch: 1 | total time: 27.34m\n",
      "step 00646 (75.95%) | loss: 1.443724 | lrm: 1.00 | dt: 2582.72ms | tok/sec: 202,998 | mfu: 16.47 | epoch: 1 | total time: 27.38m\n",
      "step 00647 (76.07%) | loss: 1.429619 | lrm: 1.00 | dt: 2582.99ms | tok/sec: 202,977 | mfu: 16.46 | epoch: 1 | total time: 27.43m\n",
      "step 00648 (76.18%) | loss: 1.429958 | lrm: 1.00 | dt: 2584.25ms | tok/sec: 202,878 | mfu: 16.46 | epoch: 1 | total time: 27.47m\n",
      "step 00649 (76.31%) | loss: 1.432598 | lrm: 1.00 | dt: 2578.63ms | tok/sec: 203,320 | mfu: 16.49 | epoch: 1 | total time: 27.51m\n",
      "step 00650 (76.42%) | loss: 1.430770 | lrm: 1.00 | dt: 2581.17ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 27.55m\n",
      "step 00651 (76.54%) | loss: 1.430840 | lrm: 1.00 | dt: 2582.48ms | tok/sec: 203,017 | mfu: 16.47 | epoch: 1 | total time: 27.60m\n",
      "step 00652 (76.66%) | loss: 1.435225 | lrm: 1.00 | dt: 2577.85ms | tok/sec: 203,381 | mfu: 16.50 | epoch: 1 | total time: 27.64m\n",
      "step 00653 (76.77%) | loss: 1.451019 | lrm: 1.00 | dt: 2581.64ms | tok/sec: 203,083 | mfu: 16.47 | epoch: 1 | total time: 27.68m\n",
      "step 00654 (76.89%) | loss: 1.457915 | lrm: 1.00 | dt: 2587.44ms | tok/sec: 202,628 | mfu: 16.43 | epoch: 1 | total time: 27.73m\n",
      "step 00655 (77.00%) | loss: 1.462624 | lrm: 1.00 | dt: 2584.82ms | tok/sec: 202,833 | mfu: 16.45 | epoch: 1 | total time: 27.77m\n",
      "step 00656 (77.12%) | loss: 1.454192 | lrm: 1.00 | dt: 2580.96ms | tok/sec: 203,136 | mfu: 16.48 | epoch: 1 | total time: 27.81m\n",
      "step 00657 (77.24%) | loss: 1.455976 | lrm: 1.00 | dt: 2581.99ms | tok/sec: 203,055 | mfu: 16.47 | epoch: 1 | total time: 27.86m\n",
      "step 00658 (77.36%) | loss: 1.432533 | lrm: 1.00 | dt: 2583.08ms | tok/sec: 202,970 | mfu: 16.46 | epoch: 1 | total time: 27.90m\n",
      "step 00659 (77.47%) | loss: 1.435267 | lrm: 1.00 | dt: 2582.71ms | tok/sec: 202,999 | mfu: 16.47 | epoch: 1 | total time: 27.94m\n",
      "step 00660 (77.59%) | loss: 1.438979 | lrm: 1.00 | dt: 2582.84ms | tok/sec: 202,989 | mfu: 16.46 | epoch: 1 | total time: 27.98m\n",
      "step 00661 (77.71%) | loss: 1.441363 | lrm: 1.00 | dt: 2581.40ms | tok/sec: 203,102 | mfu: 16.47 | epoch: 1 | total time: 28.03m\n",
      "step 00662 (77.83%) | loss: 1.452704 | lrm: 1.00 | dt: 2582.64ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 28.07m\n",
      "step 00663 (77.94%) | loss: 1.451239 | lrm: 1.00 | dt: 2583.00ms | tok/sec: 202,976 | mfu: 16.46 | epoch: 1 | total time: 28.11m\n",
      "step 00664 (78.06%) | loss: 1.462556 | lrm: 1.00 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 28.16m\n",
      "step 00665 (78.17%) | loss: 1.433739 | lrm: 1.00 | dt: 2582.14ms | tok/sec: 203,044 | mfu: 16.47 | epoch: 1 | total time: 28.20m\n",
      "step 00666 (78.29%) | loss: 1.446871 | lrm: 1.00 | dt: 2585.19ms | tok/sec: 202,804 | mfu: 16.45 | epoch: 1 | total time: 28.24m\n",
      "step 00667 (78.41%) | loss: 1.433507 | lrm: 1.00 | dt: 2581.58ms | tok/sec: 203,088 | mfu: 16.47 | epoch: 1 | total time: 28.29m\n",
      "step 00668 (78.53%) | loss: 1.414707 | lrm: 1.00 | dt: 2581.48ms | tok/sec: 203,096 | mfu: 16.47 | epoch: 1 | total time: 28.33m\n",
      "step 00669 (78.65%) | loss: 1.400616 | lrm: 1.00 | dt: 2582.97ms | tok/sec: 202,978 | mfu: 16.46 | epoch: 1 | total time: 28.37m\n",
      "step 00670 (78.76%) | loss: 1.390086 | lrm: 1.00 | dt: 2582.06ms | tok/sec: 203,050 | mfu: 16.47 | epoch: 1 | total time: 28.42m\n",
      "step 00671 (78.88%) | loss: 1.373834 | lrm: 1.00 | dt: 2582.47ms | tok/sec: 203,018 | mfu: 16.47 | epoch: 1 | total time: 28.46m\n",
      "step 00672 (78.99%) | loss: 1.384674 | lrm: 1.00 | dt: 2582.27ms | tok/sec: 203,034 | mfu: 16.47 | epoch: 1 | total time: 28.50m\n",
      "step 00673 (79.11%) | loss: 1.379855 | lrm: 1.00 | dt: 2581.97ms | tok/sec: 203,057 | mfu: 16.47 | epoch: 1 | total time: 28.54m\n",
      "step 00674 (79.22%) | loss: 1.396525 | lrm: 1.00 | dt: 2581.53ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 28.59m\n",
      "step 00675 (79.34%) | loss: 1.385588 | lrm: 1.00 | dt: 2580.18ms | tok/sec: 203,197 | mfu: 16.48 | epoch: 1 | total time: 28.63m\n",
      "step 00676 (79.45%) | loss: 1.381611 | lrm: 1.00 | dt: 2581.76ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 28.67m\n",
      "step 00677 (79.57%) | loss: 1.384595 | lrm: 1.00 | dt: 2581.67ms | tok/sec: 203,080 | mfu: 16.47 | epoch: 1 | total time: 28.72m\n",
      "step 00678 (79.68%) | loss: 1.384279 | lrm: 1.00 | dt: 2581.54ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 28.76m\n",
      "step 00679 (79.80%) | loss: 1.384326 | lrm: 1.00 | dt: 2583.87ms | tok/sec: 202,908 | mfu: 16.46 | epoch: 1 | total time: 28.80m\n",
      "step 00680 (79.91%) | loss: 1.379683 | lrm: 1.00 | dt: 2587.47ms | tok/sec: 202,626 | mfu: 16.43 | epoch: 1 | total time: 28.85m\n",
      "step 00681 (80.03%) | loss: 1.371838 | lrm: 1.00 | dt: 2583.19ms | tok/sec: 202,961 | mfu: 16.46 | epoch: 1 | total time: 28.89m\n",
      "step 00682 (80.14%) | loss: 1.389438 | lrm: 0.99 | dt: 2581.96ms | tok/sec: 203,058 | mfu: 16.47 | epoch: 1 | total time: 28.93m\n",
      "step 00683 (80.25%) | loss: 1.381688 | lrm: 0.99 | dt: 2583.40ms | tok/sec: 202,945 | mfu: 16.46 | epoch: 1 | total time: 28.97m\n",
      "step 00684 (80.37%) | loss: 1.379976 | lrm: 0.98 | dt: 2582.99ms | tok/sec: 202,977 | mfu: 16.46 | epoch: 1 | total time: 29.02m\n",
      "step 00685 (80.48%) | loss: 1.378729 | lrm: 0.98 | dt: 2583.57ms | tok/sec: 202,931 | mfu: 16.46 | epoch: 1 | total time: 29.06m\n",
      "step 00686 (80.60%) | loss: 1.369885 | lrm: 0.97 | dt: 2582.96ms | tok/sec: 202,979 | mfu: 16.46 | epoch: 1 | total time: 29.10m\n",
      "step 00687 (80.71%) | loss: 1.369238 | lrm: 0.96 | dt: 2581.41ms | tok/sec: 203,101 | mfu: 16.47 | epoch: 1 | total time: 29.15m\n",
      "step 00688 (80.83%) | loss: 1.351025 | lrm: 0.96 | dt: 2582.08ms | tok/sec: 203,048 | mfu: 16.47 | epoch: 1 | total time: 29.19m\n",
      "step 00689 (80.95%) | loss: 1.350303 | lrm: 0.95 | dt: 2581.50ms | tok/sec: 203,094 | mfu: 16.47 | epoch: 1 | total time: 29.23m\n",
      "step 00690 (81.06%) | loss: 1.340655 | lrm: 0.95 | dt: 2582.92ms | tok/sec: 202,982 | mfu: 16.46 | epoch: 1 | total time: 29.28m\n",
      "step 00691 (81.18%) | loss: 1.331931 | lrm: 0.94 | dt: 2581.22ms | tok/sec: 203,116 | mfu: 16.47 | epoch: 1 | total time: 29.32m\n",
      "step 00692 (81.30%) | loss: 1.344108 | lrm: 0.93 | dt: 2581.03ms | tok/sec: 203,131 | mfu: 16.48 | epoch: 1 | total time: 29.36m\n",
      "step 00693 (81.42%) | loss: 1.361875 | lrm: 0.93 | dt: 2583.13ms | tok/sec: 202,966 | mfu: 16.46 | epoch: 1 | total time: 29.41m\n",
      "step 00694 (81.54%) | loss: 1.348066 | lrm: 0.92 | dt: 2581.66ms | tok/sec: 203,082 | mfu: 16.47 | epoch: 1 | total time: 29.45m\n",
      "step 00695 (81.65%) | loss: 1.353118 | lrm: 0.92 | dt: 2592.84ms | tok/sec: 202,205 | mfu: 16.40 | epoch: 1 | total time: 29.49m\n",
      "step 00696 (81.77%) | loss: 1.350270 | lrm: 0.91 | dt: 2583.49ms | tok/sec: 202,937 | mfu: 16.46 | epoch: 1 | total time: 29.53m\n",
      "step 00697 (81.89%) | loss: 1.365402 | lrm: 0.91 | dt: 2581.23ms | tok/sec: 203,115 | mfu: 16.47 | epoch: 1 | total time: 29.58m\n",
      "step 00698 (82.00%) | loss: 1.360619 | lrm: 0.90 | dt: 2582.68ms | tok/sec: 203,001 | mfu: 16.47 | epoch: 1 | total time: 29.62m\n",
      "step 00699 (82.12%) | loss: 1.381464 | lrm: 0.89 | dt: 2584.68ms | tok/sec: 202,844 | mfu: 16.45 | epoch: 1 | total time: 29.66m\n",
      "step 00700 (82.24%) | loss: 1.381330 | lrm: 0.89 | dt: 2583.36ms | tok/sec: 202,948 | mfu: 16.46 | epoch: 1 | total time: 29.71m\n",
      "step 00701 (82.37%) | loss: 1.387771 | lrm: 0.88 | dt: 2581.89ms | tok/sec: 203,063 | mfu: 16.47 | epoch: 1 | total time: 29.75m\n",
      "step 00702 (82.48%) | loss: 1.405063 | lrm: 0.88 | dt: 2580.19ms | tok/sec: 203,197 | mfu: 16.48 | epoch: 1 | total time: 29.79m\n",
      "step 00703 (82.60%) | loss: 1.389636 | lrm: 0.87 | dt: 2582.38ms | tok/sec: 203,025 | mfu: 16.47 | epoch: 1 | total time: 29.84m\n",
      "step 00704 (82.71%) | loss: 1.387280 | lrm: 0.86 | dt: 2578.35ms | tok/sec: 203,342 | mfu: 16.49 | epoch: 1 | total time: 29.88m\n",
      "step 00705 (82.83%) | loss: 1.392086 | lrm: 0.86 | dt: 2582.56ms | tok/sec: 203,011 | mfu: 16.47 | epoch: 1 | total time: 29.92m\n",
      "step 00706 (82.95%) | loss: 1.385368 | lrm: 0.85 | dt: 2584.76ms | tok/sec: 202,838 | mfu: 16.45 | epoch: 1 | total time: 29.96m\n",
      "step 00707 (83.07%) | loss: 1.381876 | lrm: 0.85 | dt: 2582.02ms | tok/sec: 203,053 | mfu: 16.47 | epoch: 1 | total time: 30.01m\n",
      "step 00708 (83.18%) | loss: 1.379778 | lrm: 0.84 | dt: 2581.99ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 30.05m\n",
      "step 00709 (83.30%) | loss: 1.376007 | lrm: 0.83 | dt: 2582.40ms | tok/sec: 203,023 | mfu: 16.47 | epoch: 1 | total time: 30.09m\n",
      "step 00710 (83.42%) | loss: 1.370219 | lrm: 0.83 | dt: 2581.46ms | tok/sec: 203,097 | mfu: 16.47 | epoch: 1 | total time: 30.14m\n",
      "step 00711 (83.54%) | loss: 1.384649 | lrm: 0.82 | dt: 2582.14ms | tok/sec: 203,043 | mfu: 16.47 | epoch: 1 | total time: 30.18m\n",
      "step 00712 (83.66%) | loss: 1.377409 | lrm: 0.82 | dt: 2583.06ms | tok/sec: 202,971 | mfu: 16.46 | epoch: 1 | total time: 30.22m\n",
      "step 00713 (83.77%) | loss: 1.388649 | lrm: 0.81 | dt: 2581.80ms | tok/sec: 203,070 | mfu: 16.47 | epoch: 1 | total time: 30.27m\n",
      "step 00714 (83.89%) | loss: 1.389206 | lrm: 0.81 | dt: 2580.34ms | tok/sec: 203,185 | mfu: 16.48 | epoch: 1 | total time: 30.31m\n",
      "step 00715 (84.01%) | loss: 1.377248 | lrm: 0.80 | dt: 2582.55ms | tok/sec: 203,011 | mfu: 16.47 | epoch: 1 | total time: 30.35m\n",
      "step 00716 (84.12%) | loss: 1.371644 | lrm: 0.79 | dt: 2583.55ms | tok/sec: 202,933 | mfu: 16.46 | epoch: 1 | total time: 30.40m\n",
      "step 00717 (84.24%) | loss: 1.348526 | lrm: 0.79 | dt: 2582.97ms | tok/sec: 202,978 | mfu: 16.46 | epoch: 1 | total time: 30.44m\n",
      "step 00718 (84.36%) | loss: 1.361802 | lrm: 0.78 | dt: 2583.12ms | tok/sec: 202,967 | mfu: 16.46 | epoch: 1 | total time: 30.48m\n",
      "step 00719 (84.47%) | loss: 1.366876 | lrm: 0.78 | dt: 2582.64ms | tok/sec: 203,004 | mfu: 16.47 | epoch: 1 | total time: 30.52m\n",
      "step 00720 (84.59%) | loss: 1.367638 | lrm: 0.77 | dt: 2582.59ms | tok/sec: 203,008 | mfu: 16.47 | epoch: 1 | total time: 30.57m\n",
      "step 00721 (84.70%) | loss: 1.365208 | lrm: 0.76 | dt: 2582.71ms | tok/sec: 202,999 | mfu: 16.47 | epoch: 1 | total time: 30.61m\n",
      "step 00722 (84.82%) | loss: 1.370337 | lrm: 0.76 | dt: 2582.04ms | tok/sec: 203,052 | mfu: 16.47 | epoch: 1 | total time: 30.65m\n",
      "step 00723 (84.93%) | loss: 1.375978 | lrm: 0.75 | dt: 2582.90ms | tok/sec: 202,984 | mfu: 16.46 | epoch: 1 | total time: 30.70m\n",
      "step 00724 (85.05%) | loss: 1.366490 | lrm: 0.75 | dt: 2582.52ms | tok/sec: 203,014 | mfu: 16.47 | epoch: 1 | total time: 30.74m\n",
      "step 00725 (85.17%) | loss: 1.360251 | lrm: 0.74 | dt: 2581.85ms | tok/sec: 203,066 | mfu: 16.47 | epoch: 1 | total time: 30.78m\n",
      "step 00726 (85.28%) | loss: 1.367086 | lrm: 0.74 | dt: 2584.55ms | tok/sec: 202,854 | mfu: 16.45 | epoch: 1 | total time: 30.83m\n",
      "step 00727 (85.40%) | loss: 1.368401 | lrm: 0.73 | dt: 2584.84ms | tok/sec: 202,831 | mfu: 16.45 | epoch: 1 | total time: 30.87m\n",
      "step 00728 (85.52%) | loss: 1.354535 | lrm: 0.72 | dt: 2589.89ms | tok/sec: 202,436 | mfu: 16.42 | epoch: 1 | total time: 30.91m\n",
      "step 00729 (85.64%) | loss: 1.352745 | lrm: 0.72 | dt: 2579.92ms | tok/sec: 203,219 | mfu: 16.48 | epoch: 1 | total time: 30.95m\n",
      "step 00730 (85.77%) | loss: 1.346906 | lrm: 0.71 | dt: 2583.86ms | tok/sec: 202,908 | mfu: 16.46 | epoch: 1 | total time: 31.00m\n",
      "step 00731 (85.88%) | loss: 1.340532 | lrm: 0.71 | dt: 2582.34ms | tok/sec: 203,028 | mfu: 16.47 | epoch: 1 | total time: 31.04m\n",
      "step 00732 (85.99%) | loss: 1.335337 | lrm: 0.70 | dt: 2584.35ms | tok/sec: 202,870 | mfu: 16.45 | epoch: 1 | total time: 31.08m\n",
      "step 00733 (86.12%) | loss: 1.339264 | lrm: 0.69 | dt: 2589.04ms | tok/sec: 202,502 | mfu: 16.42 | epoch: 1 | total time: 31.13m\n",
      "step 00734 (86.24%) | loss: 1.354655 | lrm: 0.69 | dt: 2585.05ms | tok/sec: 202,815 | mfu: 16.45 | epoch: 1 | total time: 31.17m\n",
      "step 00735 (86.36%) | loss: 1.360495 | lrm: 0.68 | dt: 2583.86ms | tok/sec: 202,908 | mfu: 16.46 | epoch: 1 | total time: 31.21m\n",
      "step 00736 (86.48%) | loss: 1.352456 | lrm: 0.68 | dt: 2582.90ms | tok/sec: 202,984 | mfu: 16.46 | epoch: 1 | total time: 31.26m\n",
      "step 00737 (86.60%) | loss: 1.337708 | lrm: 0.67 | dt: 2581.36ms | tok/sec: 203,105 | mfu: 16.47 | epoch: 1 | total time: 31.30m\n",
      "step 00738 (86.72%) | loss: 1.358709 | lrm: 0.66 | dt: 2583.43ms | tok/sec: 202,942 | mfu: 16.46 | epoch: 1 | total time: 31.34m\n",
      "step 00739 (86.83%) | loss: 1.388995 | lrm: 0.66 | dt: 2583.12ms | tok/sec: 202,966 | mfu: 16.46 | epoch: 1 | total time: 31.39m\n",
      "step 00740 (86.95%) | loss: 1.405194 | lrm: 0.65 | dt: 2583.23ms | tok/sec: 202,958 | mfu: 16.46 | epoch: 1 | total time: 31.43m\n",
      "step 00741 (87.07%) | loss: 1.384160 | lrm: 0.65 | dt: 2583.21ms | tok/sec: 202,959 | mfu: 16.46 | epoch: 1 | total time: 31.47m\n",
      "step 00742 (87.18%) | loss: 1.368220 | lrm: 0.64 | dt: 2584.37ms | tok/sec: 202,868 | mfu: 16.45 | epoch: 1 | total time: 31.51m\n",
      "step 00743 (87.30%) | loss: 1.363161 | lrm: 0.64 | dt: 2581.16ms | tok/sec: 203,121 | mfu: 16.47 | epoch: 1 | total time: 31.56m\n",
      "step 00744 (87.42%) | loss: 1.374463 | lrm: 0.63 | dt: 2582.42ms | tok/sec: 203,022 | mfu: 16.47 | epoch: 1 | total time: 31.60m\n",
      "step 00745 (87.53%) | loss: 1.376517 | lrm: 0.62 | dt: 2582.44ms | tok/sec: 203,020 | mfu: 16.47 | epoch: 1 | total time: 31.64m\n",
      "step 00746 (87.65%) | loss: 1.375603 | lrm: 0.62 | dt: 2582.34ms | tok/sec: 203,028 | mfu: 16.47 | epoch: 1 | total time: 31.69m\n",
      "step 00747 (87.76%) | loss: 1.380174 | lrm: 0.61 | dt: 2581.04ms | tok/sec: 203,130 | mfu: 16.48 | epoch: 1 | total time: 31.73m\n",
      "step 00748 (87.88%) | loss: 1.381412 | lrm: 0.61 | dt: 2581.54ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 31.77m\n",
      "step 00749 (87.99%) | loss: 1.379541 | lrm: 0.60 | dt: 2583.19ms | tok/sec: 202,961 | mfu: 16.46 | epoch: 1 | total time: 31.82m\n",
      "step 00750 (88.11%) | loss: 1.369487 | lrm: 0.59 | dt: 2581.83ms | tok/sec: 203,068 | mfu: 16.47 | epoch: 1 | total time: 31.86m\n",
      "Step 00750 | Validation bpb: 0.4605\n",
      "step 00751 (88.23%) | loss: 1.376426 | lrm: 0.59 | dt: 2569.47ms | tok/sec: 204,045 | mfu: 16.55 | epoch: 1 | total time: 31.90m\n",
      "step 00752 (88.35%) | loss: 1.371778 | lrm: 0.58 | dt: 2575.60ms | tok/sec: 203,559 | mfu: 16.51 | epoch: 1 | total time: 31.94m\n",
      "step 00753 (88.47%) | loss: 1.374245 | lrm: 0.58 | dt: 2572.90ms | tok/sec: 203,773 | mfu: 16.53 | epoch: 1 | total time: 31.99m\n",
      "step 00754 (88.59%) | loss: 1.391071 | lrm: 0.57 | dt: 2574.22ms | tok/sec: 203,668 | mfu: 16.52 | epoch: 1 | total time: 32.03m\n",
      "step 00755 (88.71%) | loss: 1.375913 | lrm: 0.56 | dt: 2574.14ms | tok/sec: 203,674 | mfu: 16.52 | epoch: 1 | total time: 32.07m\n",
      "step 00756 (88.83%) | loss: 1.371277 | lrm: 0.56 | dt: 2572.11ms | tok/sec: 203,836 | mfu: 16.53 | epoch: 1 | total time: 32.12m\n",
      "step 00757 (88.94%) | loss: 1.367860 | lrm: 0.55 | dt: 2577.66ms | tok/sec: 203,397 | mfu: 16.50 | epoch: 1 | total time: 32.16m\n",
      "step 00758 (89.05%) | loss: 1.356045 | lrm: 0.55 | dt: 2571.55ms | tok/sec: 203,880 | mfu: 16.54 | epoch: 1 | total time: 32.20m\n",
      "step 00759 (89.17%) | loss: 1.337609 | lrm: 0.54 | dt: 2574.14ms | tok/sec: 203,675 | mfu: 16.52 | epoch: 1 | total time: 32.24m\n",
      "step 00760 (89.29%) | loss: 1.337865 | lrm: 0.54 | dt: 2572.02ms | tok/sec: 203,842 | mfu: 16.53 | epoch: 1 | total time: 32.29m\n",
      "step 00761 (89.41%) | loss: 1.333866 | lrm: 0.53 | dt: 2573.21ms | tok/sec: 203,748 | mfu: 16.53 | epoch: 1 | total time: 32.33m\n",
      "step 00762 (89.52%) | loss: 1.336071 | lrm: 0.52 | dt: 2574.37ms | tok/sec: 203,656 | mfu: 16.52 | epoch: 1 | total time: 32.37m\n",
      "step 00763 (89.64%) | loss: 1.330319 | lrm: 0.52 | dt: 2571.10ms | tok/sec: 203,915 | mfu: 16.54 | epoch: 1 | total time: 32.42m\n",
      "step 00764 (89.76%) | loss: 1.340105 | lrm: 0.51 | dt: 2575.52ms | tok/sec: 203,565 | mfu: 16.51 | epoch: 1 | total time: 32.46m\n",
      "step 00765 (89.87%) | loss: 1.363038 | lrm: 0.51 | dt: 2572.69ms | tok/sec: 203,789 | mfu: 16.53 | epoch: 1 | total time: 32.50m\n",
      "step 00766 (89.99%) | loss: 1.404912 | lrm: 0.50 | dt: 2576.51ms | tok/sec: 203,487 | mfu: 16.50 | epoch: 1 | total time: 32.55m\n",
      "step 00767 (90.11%) | loss: 1.425577 | lrm: 0.49 | dt: 2574.34ms | tok/sec: 203,659 | mfu: 16.52 | epoch: 1 | total time: 32.59m\n",
      "step 00768 (90.22%) | loss: 1.410925 | lrm: 0.49 | dt: 2577.48ms | tok/sec: 203,410 | mfu: 16.50 | epoch: 1 | total time: 32.63m\n",
      "step 00769 (90.34%) | loss: 1.380128 | lrm: 0.48 | dt: 2572.15ms | tok/sec: 203,832 | mfu: 16.53 | epoch: 1 | total time: 32.67m\n",
      "step 00770 (90.47%) | loss: 1.357455 | lrm: 0.48 | dt: 2572.11ms | tok/sec: 203,835 | mfu: 16.53 | epoch: 1 | total time: 32.72m\n",
      "step 00771 (90.58%) | loss: 1.350954 | lrm: 0.47 | dt: 2576.71ms | tok/sec: 203,471 | mfu: 16.50 | epoch: 1 | total time: 32.76m\n",
      "step 00772 (90.70%) | loss: 1.347542 | lrm: 0.46 | dt: 2576.15ms | tok/sec: 203,515 | mfu: 16.51 | epoch: 1 | total time: 32.80m\n",
      "step 00773 (90.82%) | loss: 1.344743 | lrm: 0.46 | dt: 2578.84ms | tok/sec: 203,303 | mfu: 16.49 | epoch: 1 | total time: 32.85m\n",
      "step 00774 (90.95%) | loss: 1.347083 | lrm: 0.45 | dt: 2578.79ms | tok/sec: 203,308 | mfu: 16.49 | epoch: 1 | total time: 32.89m\n",
      "step 00775 (91.06%) | loss: 1.377014 | lrm: 0.45 | dt: 2576.38ms | tok/sec: 203,498 | mfu: 16.51 | epoch: 1 | total time: 32.93m\n",
      "step 00776 (91.17%) | loss: 1.362150 | lrm: 0.44 | dt: 2578.51ms | tok/sec: 203,330 | mfu: 16.49 | epoch: 1 | total time: 32.97m\n",
      "step 00777 (91.29%) | loss: 1.368747 | lrm: 0.44 | dt: 2578.51ms | tok/sec: 203,330 | mfu: 16.49 | epoch: 1 | total time: 33.02m\n",
      "step 00778 (91.41%) | loss: 1.346285 | lrm: 0.43 | dt: 2581.05ms | tok/sec: 203,129 | mfu: 16.48 | epoch: 1 | total time: 33.06m\n",
      "step 00779 (91.53%) | loss: 1.336961 | lrm: 0.42 | dt: 2581.98ms | tok/sec: 203,056 | mfu: 16.47 | epoch: 1 | total time: 33.10m\n",
      "step 00780 (91.65%) | loss: 1.353533 | lrm: 0.42 | dt: 2582.38ms | tok/sec: 203,025 | mfu: 16.47 | epoch: 1 | total time: 33.15m\n",
      "step 00781 (91.76%) | loss: 1.358571 | lrm: 0.41 | dt: 2582.32ms | tok/sec: 203,030 | mfu: 16.47 | epoch: 1 | total time: 33.19m\n",
      "step 00782 (91.88%) | loss: 1.351837 | lrm: 0.41 | dt: 2578.82ms | tok/sec: 203,305 | mfu: 16.49 | epoch: 1 | total time: 33.23m\n",
      "step 00783 (92.00%) | loss: 1.352233 | lrm: 0.40 | dt: 2580.05ms | tok/sec: 203,208 | mfu: 16.48 | epoch: 1 | total time: 33.28m\n",
      "step 00784 (92.12%) | loss: 1.339860 | lrm: 0.39 | dt: 2584.03ms | tok/sec: 202,895 | mfu: 16.46 | epoch: 1 | total time: 33.32m\n",
      "step 00785 (92.24%) | loss: 1.346400 | lrm: 0.39 | dt: 2585.52ms | tok/sec: 202,778 | mfu: 16.45 | epoch: 1 | total time: 33.36m\n",
      "step 00786 (92.36%) | loss: 1.335055 | lrm: 0.38 | dt: 2579.49ms | tok/sec: 203,252 | mfu: 16.49 | epoch: 1 | total time: 33.40m\n",
      "step 00787 (92.47%) | loss: 1.340989 | lrm: 0.38 | dt: 2579.53ms | tok/sec: 203,249 | mfu: 16.49 | epoch: 1 | total time: 33.45m\n",
      "step 00788 (92.59%) | loss: 1.333197 | lrm: 0.37 | dt: 2581.89ms | tok/sec: 203,063 | mfu: 16.47 | epoch: 1 | total time: 33.49m\n",
      "step 00789 (92.71%) | loss: 1.347180 | lrm: 0.36 | dt: 2584.22ms | tok/sec: 202,880 | mfu: 16.46 | epoch: 1 | total time: 33.53m\n",
      "step 00790 (92.82%) | loss: 1.349727 | lrm: 0.36 | dt: 2582.90ms | tok/sec: 202,984 | mfu: 16.46 | epoch: 1 | total time: 33.58m\n",
      "step 00791 (92.94%) | loss: 1.342800 | lrm: 0.35 | dt: 2581.05ms | tok/sec: 203,129 | mfu: 16.48 | epoch: 1 | total time: 33.62m\n",
      "step 00792 (93.06%) | loss: 1.343386 | lrm: 0.35 | dt: 2579.80ms | tok/sec: 203,228 | mfu: 16.48 | epoch: 1 | total time: 33.66m\n",
      "step 00793 (93.17%) | loss: 1.338363 | lrm: 0.34 | dt: 2580.95ms | tok/sec: 203,137 | mfu: 16.48 | epoch: 1 | total time: 33.71m\n",
      "step 00794 (93.30%) | loss: 1.332427 | lrm: 0.34 | dt: 2578.33ms | tok/sec: 203,344 | mfu: 16.49 | epoch: 1 | total time: 33.75m\n",
      "step 00795 (93.42%) | loss: 1.353838 | lrm: 0.33 | dt: 2579.87ms | tok/sec: 203,222 | mfu: 16.48 | epoch: 1 | total time: 33.79m\n",
      "step 00796 (93.53%) | loss: 1.353932 | lrm: 0.32 | dt: 2578.51ms | tok/sec: 203,329 | mfu: 16.49 | epoch: 1 | total time: 33.83m\n",
      "step 00797 (93.65%) | loss: 1.371912 | lrm: 0.32 | dt: 2579.82ms | tok/sec: 203,226 | mfu: 16.48 | epoch: 1 | total time: 33.88m\n",
      "step 00798 (93.77%) | loss: 1.372646 | lrm: 0.31 | dt: 2582.87ms | tok/sec: 202,986 | mfu: 16.46 | epoch: 1 | total time: 33.92m\n",
      "step 00799 (93.89%) | loss: 1.381743 | lrm: 0.31 | dt: 2586.05ms | tok/sec: 202,736 | mfu: 16.44 | epoch: 1 | total time: 33.96m\n",
      "step 00800 (94.01%) | loss: 1.373389 | lrm: 0.30 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 34.01m\n",
      "step 00801 (94.13%) | loss: 1.373602 | lrm: 0.29 | dt: 2581.93ms | tok/sec: 203,060 | mfu: 16.47 | epoch: 1 | total time: 34.05m\n",
      "step 00802 (94.24%) | loss: 1.357138 | lrm: 0.29 | dt: 2582.04ms | tok/sec: 203,051 | mfu: 16.47 | epoch: 1 | total time: 34.09m\n",
      "step 00803 (94.36%) | loss: 1.364257 | lrm: 0.28 | dt: 2583.00ms | tok/sec: 202,976 | mfu: 16.46 | epoch: 1 | total time: 34.14m\n",
      "step 00804 (94.48%) | loss: 1.365676 | lrm: 0.28 | dt: 2581.44ms | tok/sec: 203,099 | mfu: 16.47 | epoch: 1 | total time: 34.18m\n",
      "step 00805 (94.60%) | loss: 1.343642 | lrm: 0.27 | dt: 2580.98ms | tok/sec: 203,135 | mfu: 16.48 | epoch: 1 | total time: 34.22m\n",
      "step 00806 (94.72%) | loss: 1.343375 | lrm: 0.26 | dt: 2581.40ms | tok/sec: 203,101 | mfu: 16.47 | epoch: 1 | total time: 34.27m\n",
      "step 00807 (94.85%) | loss: 1.327474 | lrm: 0.26 | dt: 2582.75ms | tok/sec: 202,996 | mfu: 16.46 | epoch: 1 | total time: 34.31m\n",
      "step 00808 (94.96%) | loss: 1.330628 | lrm: 0.25 | dt: 2581.91ms | tok/sec: 203,062 | mfu: 16.47 | epoch: 1 | total time: 34.35m\n",
      "step 00809 (95.08%) | loss: 1.322592 | lrm: 0.25 | dt: 2581.15ms | tok/sec: 203,121 | mfu: 16.47 | epoch: 1 | total time: 34.39m\n",
      "step 00810 (95.20%) | loss: 1.317191 | lrm: 0.24 | dt: 2581.89ms | tok/sec: 203,063 | mfu: 16.47 | epoch: 1 | total time: 34.44m\n",
      "step 00811 (95.32%) | loss: 1.322547 | lrm: 0.23 | dt: 2583.09ms | tok/sec: 202,969 | mfu: 16.46 | epoch: 1 | total time: 34.48m\n",
      "step 00812 (95.44%) | loss: 1.325727 | lrm: 0.23 | dt: 2581.90ms | tok/sec: 203,062 | mfu: 16.47 | epoch: 1 | total time: 34.52m\n",
      "step 00813 (95.55%) | loss: 1.332484 | lrm: 0.22 | dt: 2582.68ms | tok/sec: 203,001 | mfu: 16.47 | epoch: 1 | total time: 34.57m\n",
      "step 00814 (95.67%) | loss: 1.346147 | lrm: 0.22 | dt: 2581.65ms | tok/sec: 203,082 | mfu: 16.47 | epoch: 1 | total time: 34.61m\n",
      "step 00815 (95.79%) | loss: 1.350164 | lrm: 0.21 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 34.65m\n",
      "step 00816 (95.90%) | loss: 1.333931 | lrm: 0.20 | dt: 2582.69ms | tok/sec: 203,000 | mfu: 16.47 | epoch: 1 | total time: 34.70m\n",
      "step 00817 (96.02%) | loss: 1.337072 | lrm: 0.20 | dt: 2582.32ms | tok/sec: 203,029 | mfu: 16.47 | epoch: 1 | total time: 34.74m\n",
      "step 00818 (96.14%) | loss: 1.324629 | lrm: 0.19 | dt: 2583.70ms | tok/sec: 202,921 | mfu: 16.46 | epoch: 1 | total time: 34.78m\n",
      "step 00819 (96.25%) | loss: 1.326948 | lrm: 0.19 | dt: 2585.16ms | tok/sec: 202,806 | mfu: 16.45 | epoch: 1 | total time: 34.82m\n",
      "step 00820 (96.36%) | loss: 1.330394 | lrm: 0.18 | dt: 2584.36ms | tok/sec: 202,869 | mfu: 16.45 | epoch: 1 | total time: 34.87m\n",
      "step 00821 (96.47%) | loss: 1.333446 | lrm: 0.18 | dt: 2584.42ms | tok/sec: 202,864 | mfu: 16.45 | epoch: 1 | total time: 34.91m\n",
      "step 00822 (96.59%) | loss: 1.359098 | lrm: 0.17 | dt: 2580.17ms | tok/sec: 203,199 | mfu: 16.48 | epoch: 1 | total time: 34.95m\n",
      "step 00823 (96.70%) | loss: 1.358632 | lrm: 0.16 | dt: 2583.45ms | tok/sec: 202,941 | mfu: 16.46 | epoch: 1 | total time: 35.00m\n",
      "step 00824 (96.82%) | loss: 1.345464 | lrm: 0.16 | dt: 2586.93ms | tok/sec: 202,668 | mfu: 16.44 | epoch: 1 | total time: 35.04m\n",
      "step 00825 (96.94%) | loss: 1.349245 | lrm: 0.15 | dt: 2583.47ms | tok/sec: 202,939 | mfu: 16.46 | epoch: 1 | total time: 35.08m\n",
      "step 00826 (97.06%) | loss: 1.344221 | lrm: 0.15 | dt: 2582.04ms | tok/sec: 203,052 | mfu: 16.47 | epoch: 1 | total time: 35.13m\n",
      "step 00827 (97.18%) | loss: 1.339740 | lrm: 0.14 | dt: 2583.12ms | tok/sec: 202,966 | mfu: 16.46 | epoch: 1 | total time: 35.17m\n",
      "step 00828 (97.30%) | loss: 1.338247 | lrm: 0.14 | dt: 2582.55ms | tok/sec: 203,012 | mfu: 16.47 | epoch: 1 | total time: 35.21m\n",
      "step 00829 (97.41%) | loss: 1.328001 | lrm: 0.13 | dt: 2582.31ms | tok/sec: 203,030 | mfu: 16.47 | epoch: 1 | total time: 35.26m\n",
      "step 00830 (97.54%) | loss: 1.317420 | lrm: 0.12 | dt: 2581.89ms | tok/sec: 203,063 | mfu: 16.47 | epoch: 1 | total time: 35.30m\n",
      "step 00831 (97.65%) | loss: 1.294398 | lrm: 0.12 | dt: 2580.86ms | tok/sec: 203,144 | mfu: 16.48 | epoch: 1 | total time: 35.34m\n",
      "step 00832 (97.77%) | loss: 1.280792 | lrm: 0.11 | dt: 2581.54ms | tok/sec: 203,091 | mfu: 16.47 | epoch: 1 | total time: 35.38m\n",
      "step 00833 (97.89%) | loss: 1.298466 | lrm: 0.11 | dt: 2581.77ms | tok/sec: 203,073 | mfu: 16.47 | epoch: 1 | total time: 35.43m\n",
      "step 00834 (98.02%) | loss: 1.293998 | lrm: 0.10 | dt: 2581.09ms | tok/sec: 203,126 | mfu: 16.48 | epoch: 1 | total time: 35.47m\n",
      "step 00835 (98.13%) | loss: 1.289178 | lrm: 0.09 | dt: 2581.74ms | tok/sec: 203,075 | mfu: 16.47 | epoch: 1 | total time: 35.51m\n",
      "step 00836 (98.24%) | loss: 1.286360 | lrm: 0.09 | dt: 2583.33ms | tok/sec: 202,950 | mfu: 16.46 | epoch: 1 | total time: 35.56m\n",
      "step 00837 (98.35%) | loss: 1.267298 | lrm: 0.08 | dt: 2582.07ms | tok/sec: 203,049 | mfu: 16.47 | epoch: 1 | total time: 35.60m\n",
      "step 00838 (98.47%) | loss: 1.261156 | lrm: 0.08 | dt: 2582.25ms | tok/sec: 203,035 | mfu: 16.47 | epoch: 1 | total time: 35.64m\n",
      "step 00839 (98.58%) | loss: 1.267199 | lrm: 0.07 | dt: 2581.16ms | tok/sec: 203,120 | mfu: 16.47 | epoch: 1 | total time: 35.69m\n",
      "step 00840 (98.70%) | loss: 1.273502 | lrm: 0.07 | dt: 2582.54ms | tok/sec: 203,012 | mfu: 16.47 | epoch: 1 | total time: 35.73m\n",
      "step 00841 (98.82%) | loss: 1.281819 | lrm: 0.06 | dt: 2581.73ms | tok/sec: 203,075 | mfu: 16.47 | epoch: 1 | total time: 35.77m\n",
      "step 00842 (98.94%) | loss: 1.291546 | lrm: 0.05 | dt: 2582.14ms | tok/sec: 203,044 | mfu: 16.47 | epoch: 1 | total time: 35.81m\n",
      "step 00843 (99.06%) | loss: 1.290023 | lrm: 0.05 | dt: 2582.59ms | tok/sec: 203,008 | mfu: 16.47 | epoch: 1 | total time: 35.86m\n",
      "step 00844 (99.18%) | loss: 1.289943 | lrm: 0.04 | dt: 2585.18ms | tok/sec: 202,805 | mfu: 16.45 | epoch: 1 | total time: 35.90m\n",
      "step 00845 (99.30%) | loss: 1.282383 | lrm: 0.03 | dt: 2585.05ms | tok/sec: 202,815 | mfu: 16.45 | epoch: 1 | total time: 35.94m\n",
      "step 00846 (99.42%) | loss: 1.285869 | lrm: 0.03 | dt: 2584.55ms | tok/sec: 202,854 | mfu: 16.45 | epoch: 1 | total time: 35.99m\n",
      "step 00847 (99.54%) | loss: 1.280400 | lrm: 0.02 | dt: 2585.65ms | tok/sec: 202,768 | mfu: 16.45 | epoch: 1 | total time: 36.03m\n",
      "step 00848 (99.66%) | loss: 1.282626 | lrm: 0.02 | dt: 2581.61ms | tok/sec: 203,085 | mfu: 16.47 | epoch: 1 | total time: 36.07m\n",
      "step 00849 (99.78%) | loss: 1.277617 | lrm: 0.01 | dt: 2581.69ms | tok/sec: 203,079 | mfu: 16.47 | epoch: 1 | total time: 36.12m\n",
      "step 00850 (99.90%) | loss: 1.278113 | lrm: 0.01 | dt: 2589.72ms | tok/sec: 202,449 | mfu: 16.42 | epoch: 1 | total time: 36.16m\n",
      "step 00851 (100.01%) | loss: 1.280947 | lrm: -0.00 | dt: 2582.33ms | tok/sec: 203,028 | mfu: 16.47 | epoch: 2 | total time: 36.20m\n",
      "Step 00851 | Validation bpb: 0.4448\n",
      "Peak memory usage: 27828.82MiB\n",
      "Total training time: 36.20m\n",
      "Minimum validation bpb: 0.4448\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "x, y = next(train_loader) # prefetch the very first batch of data\n",
    "min_val_bpb = float(\"inf\")\n",
    "smooth_train_loss = 0 # EMA of training loss\n",
    "ema_beta = 0.9 # EMA decay factor\n",
    "total_training_time = 0 # total wall-clock time of training\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    flops_so_far = num_flops_per_token * args.total_batch_size * step\n",
    "\n",
    "    # Synchronize last_step across all ranks to avoid hangs in the distributed setting\n",
    "    if ddp:\n",
    "        last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)\n",
    "        dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)\n",
    "        last_step = bool(last_step_tensor.item())\n",
    "\n",
    "    # once in a while: evaluate the val bpb (all ranks participate)\n",
    "    if last_step or (args.eval_every > 0 and step % args.eval_every == 0):\n",
    "        model.eval()\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = args.eval_tokens // (args.device_batch_size * args.max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n",
    "        if val_bpb < min_val_bpb:\n",
    "            min_val_bpb = val_bpb\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # save checkpoint at the end of the run (only on master process)\n",
    "    if master_process and last_step and not args.dry_run:\n",
    "        output_dirname = args.model_tag if args.model_tag else f\"d{depth}\" # e.g. d12\n",
    "        checkpoint_dir = os.path.join(base_dir, \"chatsft_checkpoints\", output_dirname)\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(),\n",
    "            optimizer.state_dict(),\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # loss at last step\n",
    "                \"model_config\": {\n",
    "                    \"sequence_len\": args.max_seq_len,\n",
    "                    \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "                    \"n_layer\": depth,\n",
    "                    \"n_head\": model.config.n_head,\n",
    "                    \"n_kv_head\": model.config.n_kv_head,\n",
    "                    \"n_embd\": model.config.n_embd,\n",
    "                    \"window_pattern\": model.config.window_pattern,\n",
    "                },\n",
    "                \"user_config\": user_config, # inputs to the training script\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # single training step\n",
    "    # evaluate the gradient\n",
    "    synchronize()\n",
    "    t0 = time.time()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach() # for logging\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward\n",
    "        progress = max(progress, approx_progress) # only increase progress monotonically\n",
    "    # step the optimizer\n",
    "    lrm = get_lr_multiplier(progress)\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in optimizer.param_groups:\n",
    "        group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "        if group['kind'] == 'muon':\n",
    "            group[\"momentum\"] = muon_momentum\n",
    "    optimizer.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # State\n",
    "    step += 1\n",
    "\n",
    "    # logging\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() # EMA the training loss\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA\n",
    "    pct_done = 100 * progress\n",
    "    tok_per_sec = int(args.total_batch_size / dt)\n",
    "    flops_per_sec = num_flops_per_token * args.total_batch_size / dt\n",
    "    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # bfloat16 H100 SXM and without 2:4 sparsity\n",
    "    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %\n",
    "    if step > 10:\n",
    "        total_training_time += dt # only count the time after the first 10 steps\n",
    "    print0(f\"step {step:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | epoch: {current_epoch} | total time: {total_training_time/60:.2f}m\")\n",
    "    if step % 10 == 0:\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "            \"train/epoch\": current_epoch,\n",
    "        })\n",
    "\n",
    "# print a few more stats\n",
    "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\n",
    "print0(f\"Total training time: {total_training_time/60:.2f}m\")\n",
    "print0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to report\n",
    "if not args.dry_run:\n",
    "    from nanochat.report import get_report\n",
    "    get_report().log(section=\"SFT\", data=[\n",
    "        user_config, # CLI args\n",
    "        { # stats about the training setup\n",
    "            \"Number of iterations\": step,\n",
    "            \"DDP world size\": ddp_world_size,\n",
    "        },\n",
    "        { # stats about training outcomes\n",
    "            \"Minimum validation bpb\": min_val_bpb,\n",
    "        }\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## SFT\n",
       "timestamp: 2026-02-06 20:05:27\n",
       "\n",
       "- run: nanochat - d.768 - l.12 - v1.0\n",
       "- device_type: \n",
       "- dtype: bfloat16\n",
       "- model_tag: None\n",
       "- model_step: None\n",
       "- num_iterations: -1\n",
       "- max_seq_len: 2048\n",
       "- device_batch_size: 16\n",
       "- total_batch_size: 524,288\n",
       "- embedding_lr: 0.3000\n",
       "- unembedding_lr: 0.0040\n",
       "- matrix_lr: 0.0200\n",
       "- weight_decay: 0.0000\n",
       "- init_lr_frac: 1.0000\n",
       "- eval_every: 150\n",
       "- eval_tokens: 10,485,760\n",
       "- dry_run: False\n",
       "- Number of iterations: 851\n",
       "- DDP world size: 1\n",
       "- Minimum validation bpb: 0.4448\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import markdown from ipython\n",
    "from IPython.display import Markdown\n",
    "\n",
    "with open(\"/root/.cache/nanochat/report/sft.md\", \"r\") as f:\n",
    "    report_markdown = f.read()\n",
    "\n",
    "Markdown(report_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>total_training_flops</td><td>â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>total_training_time</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/dt</td><td>â–‚â–„â–„â–„â–ˆâ–ƒâ–„â–„â–„â–ƒâ–„â–„â–‚â–„â–„â–„â–„â–ˆâ–„â–„â–ƒâ–„â–ƒâ–‡â–„â–„â–„â–„â–„â–„â–â–ƒâ–„â–„â–„â–â–„â–ƒâ–„â–„</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–</td></tr><tr><td>train/lrm</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr><tr><td>train/mfu</td><td>â–ˆâ–„â–…â–ƒâ–„â–â–„â–„â–„â–„â–…â–„â–„â–„â–„â–„â–†â–„â–„â–…â–„â–ƒâ–„â–„â–„â–…â–„â–…â–„â–…â–†â–„â–…â–„â–„â–‡â–‡â–…â–„â–„</td></tr><tr><td>train/tok_per_sec</td><td>â–‡â–‡â–†â–…â–…â–…â–…â–…â–…â–†â–…â–…â–…â–…â–…â–‡â–…â–…â–…â–„â–…â–…â–†â–„â–…â–…â–…â–†â–…â–â–…â–„â–…â–†â–…â–ˆâ–ˆâ–…â–…â–…</td></tr><tr><td>val/bpb</td><td>â–ˆâ–‚â–‚â–‚â–‚â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>851</td></tr><tr><td>total_training_flops</td><td>357902365099032576</td></tr><tr><td>total_training_time</td><td>2172.14347</td></tr><tr><td>train/dt</td><td>2.58972</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/loss</td><td>1.27811</td></tr><tr><td>train/lrm</td><td>0.00524</td></tr><tr><td>train/mfu</td><td>16.42048</td></tr><tr><td>train/tok_per_sec</td><td>202449</td></tr><tr><td>val/bpb</td><td>0.44481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanochat - d.768 - l.12 - v1.0</strong> at: <a href='https://wandb.ai/chrismccormick/nanochat-sft/runs/lrh7r2vl' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft/runs/lrh7r2vl</a><br> View project at: <a href='https://wandb.ai/chrismccormick/nanochat-sft' target=\"_blank\">https://wandb.ai/chrismccormick/nanochat-sft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260206_192017-lrh7r2vl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cleanup\n",
    "wandb_run.finish() # wandb run finish\n",
    "compute_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scripts/chat_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the Chat model.\n",
    "All the generic code lives here, and all the evaluation-specific\n",
    "code lives in nanochat directory and is imported from here.\n",
    "\n",
    "Example runs:\n",
    "python -m scripts.chat_eval -a ARC-Easy\n",
    "torchrun --nproc_per_node=8 -m scripts.chat_eval -- -a ARC-Easy\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from functools import partial\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from nanochat.common import compute_init, compute_cleanup, get_dist_info, print0, autodetect_device_type\n",
    "#from nanochat.checkpoint_manager import load_model\n",
    "#from nanochat.engine import Engine\n",
    "\n",
    "from tasks.humaneval import HumanEval\n",
    "from tasks.mmlu import MMLU\n",
    "from tasks.arc import ARC\n",
    "from tasks.gsm8k import GSM8K\n",
    "from tasks.spellingbee import SpellingBee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_generative_eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Generative evaluation loop (we go one problem at a time, sample, evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generative_eval(task_object, tokenizer, model, engine, num_samples, max_new_tokens, temperature, top_k, max_problems=None):\n",
    "\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    device = model.get_device()\n",
    "\n",
    "    num_problems = len(task_object) if max_problems is None else min(len(task_object), max_problems)\n",
    "\n",
    "    # Run the evaluation\n",
    "    num_passed, total = 0, 0\n",
    "    for i in range(ddp_rank, num_problems, ddp_world_size):\n",
    "        conversation = task_object[i]\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        encoded_prompt = tokenizer.render_for_completion(conversation)\n",
    "        # Get the completions\n",
    "        results, _ = engine.generate_batch(\n",
    "            encoded_prompt,\n",
    "            num_samples=num_samples,\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        # Decode the completions as text\n",
    "        prefix_length = len(encoded_prompt)\n",
    "        completions = [tokenizer.decode(result_tokens[prefix_length:]) for result_tokens in results]\n",
    "        # Evaluate success criteria\n",
    "        outcomes = [task_object.evaluate(conversation, completion) for completion in completions]\n",
    "        passed = any(outcomes)\n",
    "\n",
    "        # Keep stats\n",
    "        total += 1\n",
    "        num_passed += int(passed)\n",
    "\n",
    "        # Logging (overwrite the same line in the console)\n",
    "        print(f\"\\r\\033[KRank {ddp_rank} | {num_passed}/{total} ({100*num_passed/total:.2f}%)\", end='', flush=True)\n",
    "\n",
    "    # Finish the in-place progress line with a newline before final summary\n",
    "    print()\n",
    "\n",
    "    # Aggregate results across all ranks\n",
    "    if ddp:\n",
    "        num_passed_tensor = torch.tensor([num_passed], dtype=torch.long, device=device)\n",
    "        total_tensor = torch.tensor([total], dtype=torch.long, device=device)\n",
    "        dist.all_reduce(num_passed_tensor, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n",
    "        num_passed = num_passed_tensor.item()\n",
    "        total = total_tensor.item()\n",
    "\n",
    "    print0(\"=\" * 50)\n",
    "    print0(f\"Final: {num_passed}/{total} ({100*num_passed/total:.2f}%)\")\n",
    "\n",
    "    # Return the accuracy\n",
    "    return num_passed/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_categorical_eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Categorical evaluation loop\n",
    "# A lot easier because we don't have to sample. Therefore, we can actually go\n",
    "# batches at a time and just check the logits for correct answer choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_categorical_eval(task_object, tokenizer, model, batch_size, max_problems=None):\n",
    "\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    device = model.get_device()\n",
    "    bos = tokenizer.get_bos_token_id() # use BOS as pad token is ok, these positions are ignored\n",
    "\n",
    "    # We'll process batches of independent problems at a time because there is no sampling needed\n",
    "    num_problems = len(task_object) if max_problems is None else min(len(task_object), max_problems)\n",
    "    ceil_div = lambda x, y: -(-x // y)\n",
    "    num_batches = ceil_div(num_problems, batch_size)\n",
    "\n",
    "    # Run the evaluation\n",
    "    letter_to_id_cache = {} # many letters will repeat often, let's save the tokenizer some work\n",
    "    num_passed, total = 0, 0\n",
    "    for i in range(ddp_rank, num_batches, ddp_world_size):\n",
    "        i0, i1 = i * batch_size, min((i + 1) * batch_size, num_problems)\n",
    "\n",
    "        # Prepare the batch of problems. They might all be of different length, so we pad/collate them.\n",
    "        conversations = [task_object[ii] for ii in range(i0, i1)]\n",
    "        prompt_ids = [tokenizer.render_for_completion(conversation) for conversation in conversations] # TODO: remake the way this works\n",
    "        max_length = max(len(ids) for ids in prompt_ids)\n",
    "        answer_time_positions = [len(ids) - 1 for ids in prompt_ids] # where the last token is (and the predicted answer)\n",
    "        padded_prompt_ids = [ids + [bos] * (max_length - len(ids)) for ids in prompt_ids]\n",
    "        prompt_ids = torch.tensor(padded_prompt_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # Get the logits for the whole batch of conversations in parallel (efficiency win here)\n",
    "        with torch.no_grad():\n",
    "            logits = model(prompt_ids) # (B, T, V)\n",
    "\n",
    "        # Focus on the available answer on just the letters corresponding to choices\n",
    "        # Note that this helps the evaluation a lot because it specifically narrows the focus to only the available letters\n",
    "        # The much harder alternative would be to just generate from the Assistant and check if it responded with the correct\n",
    "        # letter (e.g. A, B, C, D), but evaluations typically make the task easier in this way.\n",
    "        for idx, conversation in enumerate(conversations):\n",
    "            # get the token ids of all the available letters of this problem\n",
    "            letters = conversation['letters']\n",
    "            letter_ids = []\n",
    "            for letter in letters:\n",
    "                if not letter in letter_to_id_cache:\n",
    "                    encoded_letter = tokenizer.encode(letter)\n",
    "                    assert len(encoded_letter) == 1, \"Each letter must be a single token\"\n",
    "                    letter_to_id_cache[letter] = encoded_letter[0]\n",
    "                letter_ids.append(letter_to_id_cache[letter])\n",
    "            # focus logits just down to the answer position and the available letters of the answer\n",
    "            answer_pos = answer_time_positions[idx]\n",
    "            focus_logits = logits[idx, answer_pos, letter_ids]\n",
    "            # get the argmax letter (the predicted answer)\n",
    "            argmax_letter_id = focus_logits.argmax(dim=-1).item()\n",
    "            predicted_letter = letters[argmax_letter_id]\n",
    "            # evaluate the outcome\n",
    "            outcome = task_object.evaluate(conversation, predicted_letter)\n",
    "            num_passed += int(outcome)\n",
    "            total += 1\n",
    "\n",
    "    # Aggregate results across all ranks\n",
    "    if ddp:\n",
    "        num_passed_tensor = torch.tensor([num_passed], dtype=torch.long, device=device)\n",
    "        total_tensor = torch.tensor([total], dtype=torch.long, device=device)\n",
    "        dist.all_reduce(num_passed_tensor, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n",
    "        num_passed = num_passed_tensor.item()\n",
    "        total = total_tensor.item()\n",
    "\n",
    "    average = num_passed/total\n",
    "    print0(f\"Final: {num_passed}/{total} ({100*average:.2f}%)\")\n",
    "    return average\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_chat_eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_eval(task_name, model, tokenizer, engine,\n",
    "                   batch_size=1, num_samples=1, max_new_tokens=512, temperature=0.0, top_k=50,\n",
    "                   max_problems=None):\n",
    "    # Create the evaluation object\n",
    "    task_module = {\n",
    "        'HumanEval': HumanEval,\n",
    "        'MMLU': partial(MMLU, subset=\"all\", split=\"test\"),\n",
    "        'ARC-Easy': partial(ARC, subset=\"ARC-Easy\", split=\"test\"),\n",
    "        'ARC-Challenge': partial(ARC, subset=\"ARC-Challenge\", split=\"test\"),\n",
    "        'GSM8K': partial(GSM8K, subset=\"main\", split=\"test\"),\n",
    "        'SpellingBee': partial(SpellingBee, size=256, split=\"test\"),\n",
    "    }[task_name]\n",
    "    task_object = task_module()\n",
    "    # Run the evaluation\n",
    "    if task_object.eval_type == 'generative':\n",
    "        acc = run_generative_eval(task_object, tokenizer, model, engine, num_samples, max_new_tokens, temperature, top_k, max_problems=max_problems)\n",
    "    elif task_object.eval_type == 'categorical':\n",
    "        acc = run_categorical_eval(task_object, tokenizer, model, batch_size, max_problems=max_problems)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task evaluation type: {task_object.eval_type}\")\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `speedrun.sh`:\n",
    "\n",
    "```\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.chat_eval -- -i sft\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original CLI arguments:\n",
    "\n",
    "```python\n",
    "# Parse command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-i', '--source', type=str, required=True, help=\"Source of the model: sft|rl\")\n",
    "parser.add_argument('-a', '--task-name', type=str, default=None, help=\"Task name. Default = all tasks. Use | to split multiple tasks.\")\n",
    "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])\n",
    "parser.add_argument('-t', '--temperature', type=float, default=0.0)\n",
    "parser.add_argument('-m', '--max-new-tokens', type=int, default=512)\n",
    "parser.add_argument('-n', '--num-samples', type=int, default=1)\n",
    "parser.add_argument('-k', '--top-k', type=int, default=50)\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=8, help='Batch size for categorical evaluation')\n",
    "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')\n",
    "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')\n",
    "parser.add_argument('-x', '--max-problems', type=int, default=None, help='Max problems to evaluate')\n",
    "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')\n",
    "args = parser.parse_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    source = \"sft\",\n",
    "    task_name = None,\n",
    "    dtype = \"bfloat16\",\n",
    "    temperature = 0.0,\n",
    "    max_new_tokens = 512,\n",
    "    num_samples = 1,\n",
    "    top_k = 50,\n",
    "    batch_size = 8,\n",
    "    model_tag = None,\n",
    "    step = None,\n",
    "    max_problems = None,\n",
    "    device_type = \"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n",
    "engine = Engine(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ee80bdddc54e95b6258e4d34d142dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6810e5c91ee4447a9c8ac9921d4e723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Easy/train-00000-of-00001.parquet:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0643fc8977864a7a98689e9db770400e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Easy/test-00000-of-00001.parquet:   0%|          | 0.00/346k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249b002fd9ca460cb9a35dc93dbb02c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Easy/validation-00000-of-00001.parqu(â€¦):   0%|          | 0.00/86.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e532ee0d7e4e42a698b8d78ae78a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f789578f7d4c1f966892582fd57864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950a63f02be54d4aade006f364f50e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 803/2376 (33.80%)\n",
      "ARC-Easy accuracy: 33.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90237315e944c81a092763305dcb984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Challenge/train-00000-of-00001.parqu(â€¦):   0%|          | 0.00/190k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd6ddc88f494f2ab9c29bbebf224626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Challenge/test-00000-of-00001.parque(â€¦):   0%|          | 0.00/204k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5d35f7ce8440b5ba21084cc2da380d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-Challenge/validation-00000-of-00001.(â€¦):   0%|          | 0.00/55.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb041b2ba6349a791041e871c1f92b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdaaa4e623b4669af329353a4e8593e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28da5e1d6eb4fbf95f02dac45134875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 348/1172 (29.69%)\n",
      "ARC-Challenge accuracy: 29.69%\n",
      "Final: 4254/14042 (30.29%)\n",
      "MMLU accuracy: 30.29%\n",
      "\u001b[KRank 0 | 6/1319 (0.45%)\n",
      "==================================================\n",
      "Final: 6/1319 (0.45%)\n",
      "GSM8K accuracy: 0.45%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228480f72794fea8c71ee5ea28d3627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac588bbbb1824bf1b393a6c0a45298fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai_humaneval/test-00000-of-00001.par(â€¦):   0%|          | 0.00/83.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27803c7f0aec48dba7d0a9884aaaba67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KRank 0 | 9/164 (5.49%)\n",
      "==================================================\n",
      "Final: 9/164 (5.49%)\n",
      "HumanEval accuracy: 5.49%\n",
      "\u001b[KRank 0 | 250/256 (97.66%)\n",
      "==================================================\n",
      "Final: 250/256 (97.66%)\n",
      "SpellingBee accuracy: 97.66%\n"
     ]
    }
   ],
   "source": [
    "# Get the tasks to evaluate on\n",
    "all_tasks = ['ARC-Easy', 'ARC-Challenge', 'MMLU', 'GSM8K', 'HumanEval', 'SpellingBee']\n",
    "baseline_accuracies = {\n",
    "    'ARC-Easy': 0.25, # multiple choice 1 of 4 => 25%\n",
    "    'ARC-Challenge': 0.25, # multiple choice 1 of 4 => 25%\n",
    "    'MMLU': 0.25, # multiple choice 1 of 4 => 25%\n",
    "    'GSM8K': 0.0, # open-ended => 0%\n",
    "    'HumanEval': 0.0, # open-ended => 0%\n",
    "    'SpellingBee': 0.0, # open-ended => 0%\n",
    "}\n",
    "task_names = all_tasks if args.task_name is None else args.task_name.split('|')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the task evaluations sequentially\n",
    "results = {}\n",
    "for task_name in task_names:\n",
    "    with autocast_ctx:\n",
    "        acc = run_chat_eval(\n",
    "            task_name,\n",
    "            model, tokenizer, engine,\n",
    "            batch_size=args.batch_size,\n",
    "            num_samples=args.num_samples,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            temperature=args.temperature,\n",
    "            top_k=args.top_k,\n",
    "            max_problems=args.max_problems,\n",
    "        )\n",
    "        results[task_name] = acc\n",
    "        print0(f\"{task_name} accuracy: {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to report\n",
    "from nanochat.report import get_report\n",
    "all_tasks_were_evaluated = all(task_name in results for task_name in all_tasks)\n",
    "# calculate the ChatCORE metric if we can (similar to CORE, it's the mean centered accuracy)\n",
    "# this way, ChatCORE ranges from 0 (at random baseline) to 1 (peak performance)\n",
    "chatcore_metric_dict = {}\n",
    "if all_tasks_were_evaluated:\n",
    "    centered_mean = 0\n",
    "    for task_name, acc in results.items():\n",
    "        baseline_acc = baseline_accuracies.get(task_name, 0.0)\n",
    "        centered_acc = (acc - baseline_acc) / (1.0 - baseline_acc)\n",
    "        centered_mean += centered_acc\n",
    "    chatcore_metric = centered_mean / len(results)\n",
    "    chatcore_metric_dict = {\"ChatCORE metric\": chatcore_metric}\n",
    "get_report().log(section=\"Chat evaluation \" + args.source, data=[\n",
    "    vars(args), # CLI args\n",
    "    results,\n",
    "    chatcore_metric_dict,\n",
    "])\n",
    "\n",
    "compute_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-model-evaluation.md  chat-evaluation-sft.md  tokenizer-evaluation.md\n",
      "base-model-training.md\t  sft.md\t\t  tokenizer-training.md\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/nanochat/report/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Chat evaluation sft\n",
       "timestamp: 2026-02-06 21:30:19\n",
       "\n",
       "- source: sft\n",
       "- task_name: None\n",
       "- dtype: bfloat16\n",
       "- temperature: 0.0000\n",
       "- max_new_tokens: 512\n",
       "- num_samples: 1\n",
       "- top_k: 50\n",
       "- batch_size: 8\n",
       "- model_tag: None\n",
       "- step: None\n",
       "- max_problems: None\n",
       "- device_type: \n",
       "- ARC-Easy: 0.3380\n",
       "- ARC-Challenge: 0.2969\n",
       "- MMLU: 0.3029\n",
       "- GSM8K: 0.0045\n",
       "- HumanEval: 0.0549\n",
       "- SpellingBee: 0.9766\n",
       "- ChatCORE metric: 0.2144\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import markdown and display it\n",
    "from IPython.display import display, Markdown\n",
    "with open(\"/root/.cache/nanochat/report/chat-evaluation-sft.md\", 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-02-06 21:40:46'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report to /root/.cache/nanochat/report/report.md\n",
      "Warning: /root/.cache/nanochat/report/header.md does not exist. Did you forget to run `nanochat reset`?\n",
      "Warning: /root/.cache/nanochat/report/base-model-loss.md does not exist, skipping\n",
      "Warning: /root/.cache/nanochat/report/chat-sft.md does not exist, skipping\n",
      "Warning: /root/.cache/nanochat/report/chat-rl.md does not exist, skipping\n",
      "Warning: /root/.cache/nanochat/report/chat-evaluation-rl.md does not exist, skipping\n",
      "Copying report.md to current directory for convenience\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Generate the full report by putting together all the sections\n",
    "# report.md is the output and will be copied to current directory for convenience\n",
    "!python -m nanochat.report generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tokenizer training\n",
       "timestamp: 2026-02-06 17:11:13\n",
       "\n",
       "- max_chars: 2,000,000,000\n",
       "- doc_cap: 10,000\n",
       "- vocab_size: 32,768\n",
       "- train_time: 89.9651\n",
       "- num_special_tokens: 9\n",
       "- token_bytes_min: 1\n",
       "- token_bytes_max: 19\n",
       "- token_bytes_mean: 6.6029\n",
       "- token_bytes_std: 2.8250\n",
       "\n",
       "\n",
       "## Tokenizer evaluation\n",
       "timestamp: 2026-02-06 17:11:24\n",
       "\n",
       "### Comparison with GPT-2\n",
       "\n",
       "| Text Type | Bytes | GPT-2 Tokens | GPT-2 Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\n",
       "|-----------|-------|--------------|--------------|-------------|------------|-----------------|\n",
       "| news | 1819 | 404 | 4.50 | 403 | 4.51 | +0.2% |\n",
       "| korean | 893 | 745 | 1.20 | 797 | 1.12 | -7.0% |\n",
       "| code | 1259 | 576 | 2.19 | 620 | 2.03 | -7.6% |\n",
       "| math | 1834 | 936 | 1.96 | 1025 | 1.79 | -9.5% |\n",
       "| science | 1112 | 260 | 4.28 | 258 | 4.31 | +0.8% |\n",
       "| fwe-train | 4208518 | 900364 | 4.67 | 892476 | 4.72 | +0.9% |\n",
       "| fwe-val | 4515835 | 968625 | 4.66 | 963250 | 4.69 | +0.6% |\n",
       "\n",
       "### Comparison with GPT-4\n",
       "\n",
       "| Text Type | Bytes | GPT-4 Tokens | GPT-4 Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\n",
       "|-----------|-------|--------------|--------------|-------------|------------|-----------------|\n",
       "| news | 1819 | 387 | 4.70 | 403 | 4.51 | -4.1% |\n",
       "| korean | 893 | 364 | 2.45 | 797 | 1.12 | -119.0% |\n",
       "| code | 1259 | 309 | 4.07 | 620 | 2.03 | -100.6% |\n",
       "| math | 1834 | 832 | 2.20 | 1025 | 1.79 | -23.2% |\n",
       "| science | 1112 | 249 | 4.47 | 258 | 4.31 | -3.6% |\n",
       "| fwe-train | 4208518 | 874799 | 4.81 | 892476 | 4.72 | -2.0% |\n",
       "| fwe-val | 4515835 | 943495 | 4.79 | 963250 | 4.69 | -2.1% |\n",
       "\n",
       "\n",
       "## Base model training\n",
       "timestamp: 2026-02-06 18:37:44\n",
       "\n",
       "- run: nanochat - d.768 - l.12 - v1.0\n",
       "- device_type: \n",
       "- fp8: False\n",
       "- fp8_recipe: tensorwise\n",
       "- depth: 12\n",
       "- aspect_ratio: 64\n",
       "- head_dim: 128\n",
       "- max_seq_len: 2048\n",
       "- window_pattern: SSSL\n",
       "- num_iterations: -1\n",
       "- target_flops: -1.0000\n",
       "- target_param_data_ratio: 8.5000\n",
       "- device_batch_size: 16\n",
       "- total_batch_size: -1\n",
       "- embedding_lr: 0.3000\n",
       "- unembedding_lr: 0.0040\n",
       "- weight_decay: 0.2000\n",
       "- matrix_lr: 0.0200\n",
       "- scalar_lr: 0.5000\n",
       "- adam_beta1: 0.8000\n",
       "- adam_beta2: 0.9500\n",
       "- warmup_ratio: 0.0000\n",
       "- warmdown_ratio: 0.5000\n",
       "- final_lr_frac: 0.0000\n",
       "- resume_from_step: -1\n",
       "- eval_every: 250\n",
       "- eval_tokens: 20,971,520\n",
       "- core_metric_every: -1\n",
       "- core_metric_max_per_task: 500\n",
       "- sample_every: -1\n",
       "- save_every: -1\n",
       "- model_tag: None\n",
       "- Number of parameters: 286,262,424\n",
       "- Number of FLOPs per token: 8.021676e+08\n",
       "- Calculated number of iterations: 1785\n",
       "- Number of training tokens: 935,854,080\n",
       "- Tokens : Scaling params ratio: 8.4999\n",
       "- DDP world size: 1\n",
       "- warmup_ratio: 0.0000\n",
       "- warmdown_ratio: 0.5000\n",
       "- final_lr_frac: 0.0000\n",
       "- Minimum validation bpb: 0.9202\n",
       "- Final validation bpb: 0.9202\n",
       "- CORE metric estimate: None\n",
       "- MFU %: 51.82%\n",
       "- Total training flops: 7.507118e+17\n",
       "- Total training time: 76.75m\n",
       "- Peak memory usage: 16366.70MiB\n",
       "\n",
       "\n",
       "## Base model evaluation\n",
       "timestamp: 2026-02-06 19:12:58\n",
       "\n",
       "- model: base_model (step 1785)\n",
       "- CORE metric: 0.1251\n",
       "- train bpb: 0.8733\n",
       "- val bpb: 0.9202\n",
       "- hellaswag_zeroshot: 0.0908\n",
       "- jeopardy: 0.0061\n",
       "- bigbench_qa_wikidata: 0.3038\n",
       "- arc_easy: 0.3328\n",
       "- arc_challenge: 0.0102\n",
       "- copa: 0.2000\n",
       "- commonsense_qa: 0.1083\n",
       "- piqa: 0.2437\n",
       "- openbook_qa: 0.0720\n",
       "- lambada_openai: 0.2666\n",
       "- hellaswag: 0.0943\n",
       "- winograd: 0.1355\n",
       "- winogrande: 0.0371\n",
       "- bigbench_dyck_languages: 0.1110\n",
       "- agi_eval_lsat_ar: 0.0870\n",
       "- bigbench_cs_algorithms: 0.4023\n",
       "- bigbench_operators: 0.1381\n",
       "- bigbench_repeat_copy_logic: 0.0000\n",
       "- squad: 0.0559\n",
       "- coqa: 0.1110\n",
       "- boolq: -0.2345\n",
       "- bigbench_language_identification: 0.1806\n",
       "- sample 0: <|bos|>The capital of France is Paris. The capital of France is Paris. The capital of France is Paris.\n",
       "- sample 1: <|bos|>The chemical symbol of gold is gold. It is the symbol of the purity of the gold that is the basis\n",
       "- sample 2: <|bos|>If yesterday was Friday, then tomorrow will be Saturday. If today was Saturday, then tomorrow will be Saturday. If today was\n",
       "- sample 3: <|bos|>The opposite of hot is cold. It is a state of being in a state of being in a state\n",
       "- sample 4: <|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n",
       "- sample 5: <|bos|>My favorite color is red. I love it because it is the color of the blood. It is\n",
       "- sample 6: <|bos|>If 5*x + 3 = 13, then x is 5*x + 3 = 13. If 5*x\n",
       "- unconditioned 0: <|bos|>The Summary in Parashat Châ€™an was written in the first century of the eighth century, 3-5 by a Jew who opposed Shaitan in the Volga land region, and refers to a bi-pica-the works of that Jew.\n",
       "Young men, while being kept out of Mosul, were subsequently cast out of their tribe, looting and exterminating them and their families if they entered the Mosul realm. The one who defeated them, called Thareus was a Saoram, a people who lived outside Mosul country and residing in Nizirat, or Oxus-sur-ware camp by\n",
       "- unconditioned 1: <|bos|>New York Times, April 11, 2019\n",
       "Parents at The Mother Of The State Of Connecticut\n",
       "The Followbreak Of The Connecticut River Rage\n",
       "|Safety is our greatest asset||Litchfield SW Investment\n",
       "New Delhi, 19 November 2019\n",
       "Every second day, Connecticut's courts sense an intrusigable, transnational threat in the form of a cargo ship carrying homemade ammunition into the harborâ€”partly one of this second wave of rebellions. By the dawn of July of 1917, the plain was protected by the protection of the forts made in the lower Potomac River and South Conroe\n",
       "- unconditioned 2: <|bos|>In summary, we can discover a wide variety of events overlap in the life of a person from the moment we emerge from the vegetative state to the actual occasion and vâ€™s ever so rare and rare is the â€˜actionâ€™ of one of creator. The clandestine function that the xenophobic and atheistic desires of xenophobic neurosis that began as a reaction of the nervous in the wake of the coexistence of Aryan and animism is highly worth studying. Battles of xenophobia are extremely diverse cases in which type 1 xenophobic xenophobia and xenophobic animism have been brought on amorous.\n",
       "\n",
       "- unconditioned 3: <|bos|>Civil war 100Â¢ SixtiesÂ¢ The beginning of the Civil War and its full extent of the controversy are selected by a searchable series of photographs from the Famous Women in the United States Consolidated Dictionary. First issued on March 3, 1912, this issue provides a short overview of the scope and contents of the War and the 1912 National Guard, started as a result of human error, mostly by members of the 99th Massachusetts Sen. Deborah P. Warren, R-San Francisco, Robert H. Bradley, Buzia B. Ashton, George.\n",
       "During the entire \n",
       "- unconditioned 4: <|bos|>Dialysis is an annual or semi-permanent exfoliating condition in which high fluid persists over time. Medication can be used to treat complications.\n",
       "Dialysis, which is also referred to as \"dislocation\" is performed to help tissues deal with various diseases\n",
       "Hair: Situational Phenomena\n",
       "- Toe: During Koi injury\n",
       "- Stone: When Cystoscopy\n",
       "- Medial: After sporulation of Mare Mare\n",
       "- Blood: Toe: After treatment of Malmosis if Single\n",
       "- Ventidle: During periodic hemoplasty\n",
       "- Fetus: As\n",
       "- unconditioned 5: <|bos|>Freedom of association. â€œ. â‡’ in the bag implies organized human groups, and from time to time the permitted guests take whatever they need, a safe gathering of bars or america1â€œ.\n",
       "Freedom of speech. \". â‡’ in the bag implies symbolic, notionally ambiguous, or ambiguous communication; to speak. â‡’ out of the bag implies freedom of information, press or speech2.â€ â€” In addition to the word â€œlibâ€ (we reflexively avoid it) in the verb â€œto frame,â€ many other objective pronoun terms occur throughout the construction of construction. The construction of speech involves the constructions of\n",
       "- unconditioned 6: <|bos|>SANEYLETT - Every few months we get evasive statements regarding radioactive isotope classification. The answer is, \"No\". A huge waste of dollars and time is spent on IA by nuclear programs and radioactive materials like mining. As well as secret regulations and nuclear weapons, meanwhile, there are now many unanswered scientific questions, affirmed at the comprehensive secret meetings held at over 90 meetings of the TUERNEC event. Scientific questions are a strong example of a societal challenge for travelers. If you haven't visited a country with half a century of nuclear weapons, chances are you have to pay a visit some distant country\n",
       "- unconditioned 7: <|bos|>By: Anna Matthews\n",
       "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
       "Whatâ€™s the difference between unforge sufficiently and die sets?\n",
       "Regular Christmas everyoneâ€™s got the buggy trousers\n",
       "Unforge is an instruction card and recondition. Weâ€™ve got both esters\n",
       "and does not target.\n",
       "Unforge not only makes the card more versatile but\n",
       "effects the card value 148.1 Ã— 8.1.\n",
       "This 148.1 is an unforge sufficiently recipe card\n",
       "and is larger, so all other cards rely on it. See\n",
       "the question step below\n",
       "\n",
       "\n",
       "## Chat evaluation sft\n",
       "timestamp: 2026-02-06 21:30:19\n",
       "\n",
       "- source: sft\n",
       "- task_name: None\n",
       "- dtype: bfloat16\n",
       "- temperature: 0.0000\n",
       "- max_new_tokens: 512\n",
       "- num_samples: 1\n",
       "- top_k: 50\n",
       "- batch_size: 8\n",
       "- model_tag: None\n",
       "- step: None\n",
       "- max_problems: None\n",
       "- device_type: \n",
       "- ARC-Easy: 0.3380\n",
       "- ARC-Challenge: 0.2969\n",
       "- MMLU: 0.3029\n",
       "- GSM8K: 0.0045\n",
       "- HumanEval: 0.0549\n",
       "- SpellingBee: 0.9766\n",
       "- ChatCORE metric: 0.2144\n",
       "\n",
       "\n",
       "## Summary\n",
       "\n",
       "[bloat data missing]\n",
       "\n",
       "| Metric          | BASE     | SFT      | RL       |\n",
       "|-----------------|----------|----------|----------|\n",
       "| CORE            | 0.1251   | -        | -        |\n",
       "| ARC-Challenge   | -        | 0.2969   | -        |\n",
       "| ARC-Easy        | -        | 0.3380   | -        |\n",
       "| GSM8K           | -        | 0.0045   | -        |\n",
       "| HumanEval       | -        | 0.0549   | -        |\n",
       "| MMLU            | -        | 0.3029   | -        |\n",
       "| ChatCORE        | -        | 0.2144   | -        |\n",
       "\n",
       "Total wall clock time: unknown\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import markdown and display it\n",
    "from IPython.display import display, Markdown\n",
    "with open(\"report.md\", 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”‘ Upload Model to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload's the checkpoints, tokenizer, and training report to huggingface.\n",
    "\n",
    "Requirements:\n",
    "1. Store your huggingface api key in Colab secrets as \"HF_TOKEN\" for easy login.\n",
    "2. I also find it convenient to store my username in Colab secrets as \"HF_USERNAME\"--if you do that it will be pulled in by the code below. Otherwise you can set it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, upload_folder\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_username = userdata.get('HF_USERNAME') # Store your username with Colab Secrets\n",
    "#hf_username = \"YourUsername\" # Or just specify it here.\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "HF_USERNAME = hf_username\n",
    "REPO_NAME   = \"nanochat-d12-v1.0\"\n",
    "REPO_ID     = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "CACHE_DIR = \"/root/.cache/nanochat\"\n",
    "\n",
    "# ------------------------------\n",
    "# Login\n",
    "# ------------------------------\n",
    "login()\n",
    "\n",
    "# ------------------------------\n",
    "# Upload entire nanochat cache\n",
    "# (skip optimizer files intentionally)\n",
    "# ------------------------------\n",
    "upload_folder(\n",
    "    folder_path=CACHE_DIR,\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    "    ignore_patterns=[\n",
    "        \"**/optim_*\",   # skip optimizer checkpoints\n",
    "        \"**/*.pt.tmp\",  # optional: skip temp files if any\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Uploaded to: {REPO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
